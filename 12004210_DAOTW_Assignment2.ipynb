{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew110395/12004210_DataAnalytics/blob/main/12004210_DAOTW_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "This assignment builds on the New York taxi problem identified in assignment 1, where the City of New York is looking for a way to accurately predict the number of collisions on a particular day of the week. Within this document two different types of machine learning models will be utilised to predict the number of collisions. The linear relationships identified in assignment 1 will be used to create a linear regression model. A Deep Learning Neural Network (DNN) will be used to predict the number of collisions where the relationship is not linear. "
      ],
      "metadata": {
        "id": "-MIIDzFYc6Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n",
        "To prepare data for machine learning the pandas package has been used. The numpy package has been used to aid with mathematical functions.\n",
        "\n",
        "As within part 1 of this assignment the data file containing location data exceeds the size limit for hosting within github. To overcome this the file was zipped. In order to extract the data from the zip file the zipfile package has been used.\n",
        "\n",
        "Within this document, Tensorflow has been used for machine learning, with both a linear regression model and a Deep Neural Network model. Tensorflow version 1 is unsupported within Google Colab, therefore must be installed using the pip package manager.\n",
        "\n",
        "Shutil has also been imported to allow for easy file management, in particular the removal of saved models."
      ],
      "metadata": {
        "id": "PP_OBRRWE3tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "\n",
        "!pip install tensorflow==1.15.2\n",
        "import tensorflow as tf\n",
        "# needed for high-level file management\n",
        "import shutil  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Vp_IJAE49d",
        "outputId": "06e45cb3-fdac-4c5c-c5ed-6b7e0c58986b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 36 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.38.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=25149e31bc61c329234451f92b31bb7aff15e42d5593faf567256e7bd9c747d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.17.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regressor\n",
        "Throughout assignment 1 a number of linear relationships were uncovered within the dataset. These relationsips form the basis of the linear regression models below.\n",
        "\n",
        "A linear regressor is used to predict an output variable based on one or more input variables. REF https://www.ibm.com/uk-en/topics/linear-regression 17/11\n",
        "\n"
      ],
      "metadata": {
        "id": "SxU2LFGDjN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the accuracy of the model the target values are scaled or standarised. This reduces the range of collisions from 188-1161 to 0.1619... - 1, this allows for quicker training of the model. https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0 REF 17.11"
      ],
      "metadata": {
        "id": "vKNbHovb6hXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_COLLISIONS=1161"
      ],
      "metadata": {
        "id": "k-aEMQX9EuJJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Precipition\n",
        "As uncovered in assignment 1, as the volume of precipition increases, the number of collisions increase. \n",
        "\n",
        "The cleaned datafile produced in assignment is imported."
      ],
      "metadata": {
        "id": "VtJ7HqhA3bIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean.csv', index_col=0, )\n",
        "print(df_prcp[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0ljmfBkDwE",
        "outputId": "7e0784e3-18bf-4582-dd9a-adbdca0880dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create the linear regression model, extra columns are removed to simplify the model with the aim of reducing error values within the model.\n",
        "\n",
        "The incomplete years (2012 and 2022) are removed, along with the eronious data for 2020 and 2021.\n",
        "\n",
        "To aid with the production of the model the target, in this case the number of collisions is moved to the end of the data table.\n",
        "\n",
        "The summary below shows there is 2539 rows within the dataset after the data has been cleansed further, as this count exists for each column each column or predcictor contains data."
      ],
      "metadata": {
        "id": "NnrlNKFM0R-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = df_prcp.drop(columns=['collision_date', 'temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] != 2012]\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] < 2020]\n",
        "cols = df_prcp['NUM_COLLISIONS']\n",
        "df_prcp = df_prcp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp.insert(loc=9, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp[:6])\n",
        "df_prcp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "nRcEly727YQb",
        "outputId": "8328214f-caee-47ff-cdca-6912e9fd83c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "49    4  2016   1  28  0.09    0             0                 0     0   \n",
            "51    5  2014   1  17  0.00    1             0                 0     0   \n",
            "54    1  2016   1  25  0.02    0             0                 0     0   \n",
            "55    5  2016   1  29  0.00    0             0                 0     0   \n",
            "58    5  2017   1  20  0.00    0             0                 0     0   \n",
            "59    7  2013   1  13  0.01    1             0                 0     0   \n",
            "\n",
            "    NUM_COLLISIONS  \n",
            "49             681  \n",
            "51             589  \n",
            "54             658  \n",
            "55             645  \n",
            "58             605  \n",
            "59             373  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         prcp  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean      3.998425  2015.989366     6.518708    15.745569     0.122588   \n",
              "std       2.003542     1.996126     3.455211     8.803199     0.329143   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     0.000000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000     0.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000     0.000000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000     0.060000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000     3.760000   \n",
              "\n",
              "               fog  rain_drizzle  snow_ice_pellets         hail  \\\n",
              "count  2539.000000   2539.000000       2539.000000  2539.000000   \n",
              "mean      0.253249      0.375345          0.085467     0.000394   \n",
              "std       0.434958      0.484307          0.279630     0.019846   \n",
              "min       0.000000      0.000000          0.000000     0.000000   \n",
              "25%       0.000000      0.000000          0.000000     0.000000   \n",
              "50%       0.000000      0.000000          0.000000     0.000000   \n",
              "75%       1.000000      1.000000          0.000000     0.000000   \n",
              "max       1.000000      1.000000          1.000000     1.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2539.000000  \n",
              "mean       599.135093  \n",
              "std        100.299164  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3aabf566-9805-49b5-ab2a-623ea8897c7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998425</td>\n",
              "      <td>2015.989366</td>\n",
              "      <td>6.518708</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.003542</td>\n",
              "      <td>1.996126</td>\n",
              "      <td>3.455211</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aabf566-9805-49b5-ab2a-623ea8897c7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3aabf566-9805-49b5-ab2a-623ea8897c7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3aabf566-9805-49b5-ab2a-623ea8897c7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove any bias within the dataset, it is randomly shuffled. This shuffled dataset is then split into the predictors and the target."
      ],
      "metadata": {
        "id": "eBk7xJt-Ag0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp.iloc[np.random.permutation(len(df_prcp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xT4g-UZFi01",
        "outputId": "9cb4395c-475e-443a-810d-2a248eeab541"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail\n",
            "1450    2  2019   5  28  0.00    1             1                 0     0\n",
            "2629    6  2014   9   6  0.01    1             0                 0     0\n",
            "2862    4  2018  10   4  0.04    0             0                 0     0\n",
            "1884    4  2019   7   4  0.00    1             0                 0     0\n",
            "3294    3  2017  11   8  0.37    0             1                 0     0\n",
            "2121    4  2014   7  10  0.01    1             1                 0     0\n",
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "1450    2  2019   5  28  0.00    1             1                 0     0   \n",
            "2629    6  2014   9   6  0.01    1             0                 0     0   \n",
            "2862    4  2018  10   4  0.04    0             0                 0     0   \n",
            "1884    4  2019   7   4  0.00    1             0                 0     0   \n",
            "3294    3  2017  11   8  0.37    0             1                 0     0   \n",
            "2121    4  2014   7  10  0.01    1             1                 0     0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "1450             654  \n",
            "2629             553  \n",
            "2862             717  \n",
            "1884             394  \n",
            "3294             662  \n",
            "2121             584  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xkzkVBQjL_",
        "outputId": "039fefe1-768b-42a6-8ac9-327893883872"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1450    654\n",
            "2629    553\n",
            "2862    717\n",
            "1884    394\n",
            "3294    662\n",
            "2121    584\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the accuracy/reliabilty of the trained linear regressor, the dataset has been split into a training and a testing dataset. As when compared with other machine learning datasets, this dataset is relatively small a larger portion is required for testing. 20% (511 rows) of the dataset has been reserved for testing, with the remaining 80% (2044 rows) used for training the dataset."
      ],
      "metadata": {
        "id": "RG7LYzfjBqnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 9\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "id": "NwSG_P_nRgPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd155feb-cf07-484a-9ea4-1e2c2e6951af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2031\n",
            "508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUKI-paISKeh",
        "outputId": "e1eb2d47-8f99-4ad0-fdd5-1a2b93d36399"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-f6acba22e49c>:7: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please convert numpy dtypes explicitly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:183: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/linear.py:740: regression_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.contrib.estimator.*_head.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3770ec34d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:From <ipython-input-8-f6acba22e49c>:7: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to the Estimator interface.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27354407, step = 1\n",
            "INFO:tensorflow:global_step/sec: 584.21\n",
            "INFO:tensorflow:loss = 0.005071222, step = 101 (0.174 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 101 vs previous value: 101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 762.361\n",
            "INFO:tensorflow:loss = 0.0070082378, step = 201 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 1002.11\n",
            "INFO:tensorflow:loss = 0.009041734, step = 301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.367\n",
            "INFO:tensorflow:loss = 0.0058775144, step = 401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.47\n",
            "INFO:tensorflow:loss = 0.007790626, step = 501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.963\n",
            "INFO:tensorflow:loss = 0.0077437176, step = 601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.71\n",
            "INFO:tensorflow:loss = 0.0064713806, step = 701 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.293\n",
            "INFO:tensorflow:loss = 0.0049265595, step = 801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.492\n",
            "INFO:tensorflow:loss = 0.009352726, step = 901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.084\n",
            "INFO:tensorflow:loss = 0.0069999937, step = 1001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.668\n",
            "INFO:tensorflow:loss = 0.0063805208, step = 1101 (0.119 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1101 vs previous value: 1101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 855.622\n",
            "INFO:tensorflow:loss = 0.007061491, step = 1201 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.225\n",
            "INFO:tensorflow:loss = 0.0056537176, step = 1301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.569\n",
            "INFO:tensorflow:loss = 0.007926329, step = 1401 (0.111 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1401 vs previous value: 1401. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 790.109\n",
            "INFO:tensorflow:loss = 0.009273037, step = 1501 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.76\n",
            "INFO:tensorflow:loss = 0.006425207, step = 1601 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.25\n",
            "INFO:tensorflow:loss = 0.005417606, step = 1701 (0.100 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1795 vs previous value: 1795. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 974.699\n",
            "INFO:tensorflow:loss = 0.008859936, step = 1801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1040.98\n",
            "INFO:tensorflow:loss = 0.0063872226, step = 1901 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.863\n",
            "INFO:tensorflow:loss = 0.009260079, step = 2001 (0.108 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2001 vs previous value: 2001. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 939.365\n",
            "INFO:tensorflow:loss = 0.0073845885, step = 2101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.752\n",
            "INFO:tensorflow:loss = 0.007509118, step = 2201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.51\n",
            "INFO:tensorflow:loss = 0.007792425, step = 2301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 383.309\n",
            "INFO:tensorflow:loss = 0.006035885, step = 2401 (0.246 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.752\n",
            "INFO:tensorflow:loss = 0.006145778, step = 2501 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.943\n",
            "INFO:tensorflow:loss = 0.007888547, step = 2601 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.63\n",
            "INFO:tensorflow:loss = 0.0062242644, step = 2701 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.821\n",
            "INFO:tensorflow:loss = 0.008422315, step = 2801 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.275\n",
            "INFO:tensorflow:loss = 0.0063634133, step = 2901 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.352\n",
            "INFO:tensorflow:loss = 0.007131967, step = 3001 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.063\n",
            "INFO:tensorflow:loss = 0.0061645824, step = 3101 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.877\n",
            "INFO:tensorflow:loss = 0.0054662265, step = 3201 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.983\n",
            "INFO:tensorflow:loss = 0.007357872, step = 3301 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.979\n",
            "INFO:tensorflow:loss = 0.0070346566, step = 3401 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.594\n",
            "INFO:tensorflow:loss = 0.006028331, step = 3501 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.738\n",
            "INFO:tensorflow:loss = 0.005663614, step = 3601 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.302\n",
            "INFO:tensorflow:loss = 0.006156386, step = 3701 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 489.882\n",
            "INFO:tensorflow:loss = 0.005299403, step = 3801 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.591\n",
            "INFO:tensorflow:loss = 0.0076813684, step = 3901 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.261\n",
            "INFO:tensorflow:loss = 0.0053189024, step = 4001 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.58\n",
            "INFO:tensorflow:loss = 0.0048975814, step = 4101 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 637.372\n",
            "INFO:tensorflow:loss = 0.00697013, step = 4201 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.774\n",
            "INFO:tensorflow:loss = 0.008490957, step = 4301 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.338\n",
            "INFO:tensorflow:loss = 0.0070231026, step = 4401 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.982\n",
            "INFO:tensorflow:loss = 0.0073182518, step = 4501 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.578\n",
            "INFO:tensorflow:loss = 0.0052869124, step = 4601 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.89\n",
            "INFO:tensorflow:loss = 0.007426598, step = 4701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.948\n",
            "INFO:tensorflow:loss = 0.006813232, step = 4801 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 326.186\n",
            "INFO:tensorflow:loss = 0.0075355223, step = 4901 (0.303 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.404\n",
            "INFO:tensorflow:loss = 0.007414148, step = 5001 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 381.039\n",
            "INFO:tensorflow:loss = 0.006352839, step = 5101 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.688\n",
            "INFO:tensorflow:loss = 0.0055832905, step = 5201 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 333.002\n",
            "INFO:tensorflow:loss = 0.008468852, step = 5301 (0.302 sec)\n",
            "INFO:tensorflow:global_step/sec: 495.365\n",
            "INFO:tensorflow:loss = 0.0062126713, step = 5401 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 530.447\n",
            "INFO:tensorflow:loss = 0.0064659845, step = 5501 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 385.2\n",
            "INFO:tensorflow:loss = 0.006443395, step = 5601 (0.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.795\n",
            "INFO:tensorflow:loss = 0.00817136, step = 5701 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.82\n",
            "INFO:tensorflow:loss = 0.0062141595, step = 5801 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.295\n",
            "INFO:tensorflow:loss = 0.0043606376, step = 5901 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 666.802\n",
            "INFO:tensorflow:loss = 0.005761112, step = 6001 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.469\n",
            "INFO:tensorflow:loss = 0.007936735, step = 6101 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.912\n",
            "INFO:tensorflow:loss = 0.009726308, step = 6201 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 449.982\n",
            "INFO:tensorflow:loss = 0.0063138977, step = 6301 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 378.937\n",
            "INFO:tensorflow:loss = 0.0075029554, step = 6401 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 447.698\n",
            "INFO:tensorflow:loss = 0.006339522, step = 6501 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 513.35\n",
            "INFO:tensorflow:loss = 0.007625269, step = 6601 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 614.366\n",
            "INFO:tensorflow:loss = 0.008951985, step = 6701 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.018\n",
            "INFO:tensorflow:loss = 0.007191481, step = 6801 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 369.212\n",
            "INFO:tensorflow:loss = 0.006972444, step = 6901 (0.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.467\n",
            "INFO:tensorflow:loss = 0.0065472173, step = 7001 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 390.733\n",
            "INFO:tensorflow:loss = 0.007496378, step = 7101 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.089\n",
            "INFO:tensorflow:loss = 0.005321037, step = 7201 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 591.145\n",
            "INFO:tensorflow:loss = 0.0058673336, step = 7301 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.735\n",
            "INFO:tensorflow:loss = 0.0061230315, step = 7401 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.14\n",
            "INFO:tensorflow:loss = 0.0067685083, step = 7501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.496\n",
            "INFO:tensorflow:loss = 0.00583887, step = 7601 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 608.496\n",
            "INFO:tensorflow:loss = 0.0056881984, step = 7701 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.377\n",
            "INFO:tensorflow:loss = 0.0067415563, step = 7801 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.874\n",
            "INFO:tensorflow:loss = 0.004823157, step = 7901 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.038\n",
            "INFO:tensorflow:loss = 0.0063174274, step = 8001 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.735\n",
            "INFO:tensorflow:loss = 0.006408529, step = 8101 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.318\n",
            "INFO:tensorflow:loss = 0.0057590525, step = 8201 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.57\n",
            "INFO:tensorflow:loss = 0.0051063616, step = 8301 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.615\n",
            "INFO:tensorflow:loss = 0.00589584, step = 8401 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.148\n",
            "INFO:tensorflow:loss = 0.0071407855, step = 8501 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 672.986\n",
            "INFO:tensorflow:loss = 0.0060495064, step = 8601 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.215\n",
            "INFO:tensorflow:loss = 0.006675645, step = 8701 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.571\n",
            "INFO:tensorflow:loss = 0.007011604, step = 8801 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.008\n",
            "INFO:tensorflow:loss = 0.0063135233, step = 8901 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.837\n",
            "INFO:tensorflow:loss = 0.0064807725, step = 9001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 953\n",
            "INFO:tensorflow:loss = 0.00609053, step = 9101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1002.41\n",
            "INFO:tensorflow:loss = 0.0053306306, step = 9201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.452\n",
            "INFO:tensorflow:loss = 0.0073230825, step = 9301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.782\n",
            "INFO:tensorflow:loss = 0.00578367, step = 9401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.889\n",
            "INFO:tensorflow:loss = 0.0066896873, step = 9501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.8\n",
            "INFO:tensorflow:loss = 0.0058924565, step = 9601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.09\n",
            "INFO:tensorflow:loss = 0.0072459197, step = 9701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.746\n",
            "INFO:tensorflow:loss = 0.008268984, step = 9801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.046\n",
            "INFO:tensorflow:loss = 0.005688408, step = 9901 (0.106 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0048314915.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 93.88877093431789\n",
            "Just using average = 598.9172821270311 has RMSE of 99.19843127180098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A number of learning rates between 0.0000001 and 0.1 were been used to determine a suitable learning rate for the dataset. As the learning rate decreases the the overall time to train the dataset increases. REF https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 17/11"
      ],
      "metadata": {
        "id": "3uYG_ScZR3yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fifEsTD98hy",
        "outputId": "035d4e6d-c4fe-458c-96d5-06298b72270b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769ba2d10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n",
            "[0.56210583 0.53232276 0.5297852  0.56320906 0.51793087 0.54421693\n",
            " 0.5189606  0.4885822  0.5629493  0.5060771  0.53323096 0.51482785\n",
            " 0.5022705  0.5319142  0.47829488 0.53492403 0.5072606  0.576366\n",
            " 0.53238267 0.53261167 0.52869445 0.5173587  0.52983093 0.49778187\n",
            " 0.5406675  0.5183686  0.5374979  0.5580212  0.56952655 0.5598694\n",
            " 0.55980873 0.56438035 0.47322932 0.5438664  0.49622232 0.51793635\n",
            " 0.51902276 0.51678294 0.5694296  0.49900186 0.5879601  0.53330916\n",
            " 0.52503353 0.5245062  0.5566466  0.5495427  0.54309195 0.49033862\n",
            " 0.5730556  0.4935801  0.50969756 0.5605255  0.4862097  0.55127734\n",
            " 0.53353316 0.5706239  0.5781539  0.5427215  0.5140844  0.54068124\n",
            " 0.5248328  0.50971925 0.5672169  0.5089115  0.5005322  0.48349398\n",
            " 0.57070464 0.504525   0.532006   0.51298404 0.51389897 0.5258202\n",
            " 0.5018219  0.48371303 0.5396503  0.52186316 0.5510464  0.5238042\n",
            " 0.5300965  0.5355549  0.53471196 0.5463198  0.5456771  0.5771824\n",
            " 0.50043696 0.54656464 0.50638044 0.52123314 0.47713768 0.54927194\n",
            " 0.52553606 0.5057271  0.47500598 0.5311891  0.49986565 0.5641113\n",
            " 0.4832902  0.4680908  0.55704296 0.55997807 0.51187503 0.54935265\n",
            " 0.53586215 0.5532048  0.5136325  0.5234321  0.560673   0.5414586\n",
            " 0.5489085  0.5511692  0.54209065 0.55918264 0.51553476 0.569113\n",
            " 0.55056626 0.49063012 0.5126858  0.53505397 0.5808576  0.5017658\n",
            " 0.503582   0.5132135  0.5418411  0.55402327 0.50093526 0.52489495\n",
            " 0.49664763 0.55203325 0.54357237 0.51157147 0.5513498  0.46837765\n",
            " 0.5091822  0.5205297  0.50092983 0.5470759  0.50868875 0.51365095\n",
            " 0.55500835 0.5567717  0.5694263  0.5474939  0.5603024  0.50462097\n",
            " 0.5695468  0.53576416 0.50153756 0.49721152 0.51031697 0.51844287\n",
            " 0.55873466 0.50705045 0.51025534 0.56719905 0.5535262  0.48003402\n",
            " 0.5023662  0.5155077  0.49435377 0.531184   0.54731697 0.5076785\n",
            " 0.4848712  0.5259979  0.5136811  0.56692773 0.54722357 0.55763346\n",
            " 0.5146475  0.544702   0.5221807  0.56275594 0.5178173  0.5270951\n",
            " 0.49467918 0.49403334 0.53615904 0.48075834 0.45965877 0.49089813\n",
            " 0.51314926 0.5037166  0.54708314 0.52181953 0.5468422  0.55661637\n",
            " 0.47673303 0.53445196 0.53719896 0.50808096 0.49144703 0.5181671\n",
            " 0.5272272  0.5618832  0.49676418 0.58153135 0.54661137 0.5152978\n",
            " 0.53402334 0.55792123 0.561639   0.48961106 0.55635846 0.49591222\n",
            " 0.47974443 0.53584516 0.54756933 0.5116114  0.5555237  0.5701156\n",
            " 0.5040571  0.5466041  0.5186988  0.4896127  0.5528852  0.5674802\n",
            " 0.57996374 0.47530788 0.504113   0.54741967 0.5349577  0.5007996\n",
            " 0.48612392 0.5327216  0.5324423  0.5093656  0.5068143  0.50575465\n",
            " 0.5205498  0.46728778 0.540966   0.53908646 0.56849027 0.5081517\n",
            " 0.49963346 0.4999903  0.5276057  0.48558548 0.4553169  0.53985006\n",
            " 0.5191842  0.56774837 0.5436111  0.5101858  0.4972111  0.55845845\n",
            " 0.5173878  0.47783738 0.5768361  0.49268326 0.55680525 0.50593156\n",
            " 0.5033949  0.5581925  0.54711133 0.54817885 0.5293302  0.49057317\n",
            " 0.5762931  0.48111537 0.53329366 0.48941824 0.56398    0.57169795\n",
            " 0.5078503  0.5783454  0.49814767 0.56334114 0.5795283  0.52984625\n",
            " 0.57749075 0.527534   0.50335985 0.5243651  0.5090154  0.45514262\n",
            " 0.5696605  0.53831726 0.5063747  0.52791685 0.54231286 0.51345295\n",
            " 0.53146034 0.5753272  0.5127463  0.47665283 0.49115077 0.4935746\n",
            " 0.5855843  0.5431792  0.49785995 0.5432227  0.50178915 0.49409065\n",
            " 0.543074   0.50825065 0.49859884 0.52295893 0.56536025 0.56426233\n",
            " 0.54839927 0.51256824 0.5563878  0.5404452  0.53839904 0.49805254\n",
            " 0.5070069  0.49698645 0.5736231  0.47685987 0.50684994 0.5500292\n",
            " 0.5055539  0.49645787 0.53329915 0.5222506  0.5265781  0.5516748\n",
            " 0.49702382 0.5112922  0.50125164 0.50445426 0.5416646  0.55354\n",
            " 0.5504717  0.57301825 0.57283914 0.55793786 0.5082903  0.50044143\n",
            " 0.52368367 0.55133927 0.54548985 0.5775195  0.4993942  0.5145246\n",
            " 0.5514995  0.5753615  0.5340909  0.47288424 0.558582   0.5690212\n",
            " 0.5539375  0.47953203 0.50496244 0.47666767 0.46962738 0.5132228\n",
            " 0.46150428 0.4632871  0.52877444 0.5641786  0.46460304 0.49656522\n",
            " 0.5486956  0.54252666 0.5224133  0.5553448  0.54028213 0.5352106\n",
            " 0.53218454 0.5107452  0.53884435 0.5134319  0.5546485  0.5065085\n",
            " 0.581316   0.56811136 0.5499103  0.52112734 0.48457667 0.5802584\n",
            " 0.5188884  0.55400735 0.59136105 0.5186402  0.5614113  0.58043873\n",
            " 0.5179494  0.48138016 0.53995746 0.5777567  0.56510746 0.5178695\n",
            " 0.49349612 0.53813463 0.50308686 0.53522694 0.5356966  0.51825583\n",
            " 0.51483095 0.51389927 0.46361476 0.5040234  0.4723413  0.55274576\n",
            " 0.5434357  0.5647881  0.53140676 0.53326404 0.5722185  0.52887267\n",
            " 0.58178914 0.5656445  0.5191975  0.53289497 0.5164872  0.50329244\n",
            " 0.5333717  0.5066831  0.5196404  0.52638006 0.49778125 0.5244965\n",
            " 0.5330801  0.5073654  0.5315143  0.51976967 0.5284939  0.474688\n",
            " 0.5274634  0.5331616  0.49924734 0.47342128 0.5151178  0.520054\n",
            " 0.48776636 0.5047684  0.48748645 0.47079134 0.54313457 0.52654433\n",
            " 0.54158586 0.5105461  0.54282624 0.5617176  0.47571042 0.5557094\n",
            " 0.555142   0.53886425 0.5235352  0.47129327 0.55397236 0.5844916\n",
            " 0.58362794 0.49318254 0.48835304 0.5603798  0.5370069  0.5626564\n",
            " 0.4915136  0.53095245 0.5402866  0.57168144 0.49058044 0.5010191\n",
            " 0.51015925 0.47259504 0.55233955 0.56569535 0.49087936 0.48985934\n",
            " 0.52453196 0.4927879  0.5301524  0.5759312  0.48271617 0.50153834\n",
            " 0.49797624 0.49061358 0.49455696 0.56640327 0.5129493  0.53588\n",
            " 0.5622269  0.5366737  0.5256597  0.5213886  0.52619046 0.53152776\n",
            " 0.5269208  0.5451855  0.5836839  0.47472304 0.46127748 0.46333557\n",
            " 0.49678588 0.52167916 0.55518806 0.45853534 0.47238016 0.5303327\n",
            " 0.5145059  0.5653787  0.5346734  0.50285786 0.56544596 0.49378088\n",
            " 0.5189114  0.5470409  0.4794217  0.5383596  0.5293373  0.54358035\n",
            " 0.5042823  0.58045727 0.5123307  0.5677361 ]\n",
            "[0.58742463 0.54091301 0.55900086 0.50559862 0.55297158 0.62015504\n",
            " 0.42291128 0.41515935 0.60034453 0.47114556 0.57881137 0.45564169\n",
            " 0.60637382 0.46080965 0.48148148 0.58570198 0.42463394 0.5503876\n",
            " 0.49440138 0.64685616 0.45822567 0.56330749 0.69164513 0.41602067\n",
            " 0.44702842 0.57622739 0.57795004 0.49440138 0.53229974 0.51076658\n",
            " 0.54349699 0.48320413 0.43927649 0.374677   0.4918174  0.53919035\n",
            " 0.56158484 0.53832903 0.56416882 0.5211025  0.5796727  0.41257537\n",
            " 0.55211025 0.63221361 0.58225668 0.62704565 0.53574505 0.55469423\n",
            " 0.52196382 0.46080965 0.46597761 0.59259259 0.51507321 0.61412575\n",
            " 0.53919035 0.52196382 0.5081826  0.63738157 0.44530577 0.54952627\n",
            " 0.63652024 0.42204996 0.57105943 0.50990525 0.6546081  0.50043066\n",
            " 0.49354005 0.51937984 0.68217054 0.34022394 0.43066322 0.53832903\n",
            " 0.44272179 0.48234281 0.55555556 0.45133506 0.42894057 0.60034453\n",
            " 0.64857881 0.32213609 0.16192937 0.59086994 0.52368648 0.40999139\n",
            " 0.45305771 0.58828596 0.5211025  0.40999139 0.53229974 0.51248923\n",
            " 0.53229974 0.49784668 0.43927649 0.6287683  0.57364341 0.5503876\n",
            " 0.50559862 0.43496985 0.50990525 0.62101637 0.40310078 0.2213609\n",
            " 0.43927649 0.5211025  0.53316107 0.43410853 0.4005168  0.60551249\n",
            " 0.49612403 0.57450474 0.5667528  0.52799311 0.56330749 0.374677\n",
            " 0.45305771 0.48492679 0.53660637 0.63049096 0.60465116 0.4039621\n",
            " 0.60292851 0.43755383 0.65374677 0.51076658 0.44444444 0.63996555\n",
            " 0.56933678 0.44272179 0.43755383 0.45564169 0.63824289 0.59689922\n",
            " 0.56244617 0.58225668 0.6124031  0.63565891 0.44702842 0.58053402\n",
            " 0.55469423 0.43583118 0.49009475 0.54263566 0.64771748 0.62273902\n",
            " 0.55555556 0.59431525 0.374677   0.52024117 0.59431525 0.55986219\n",
            " 0.51679587 0.63049096 0.5047373  0.52799311 0.62790698 0.41085271\n",
            " 0.44358312 0.57105943 0.51076658 0.59517657 0.55297158 0.51248923\n",
            " 0.47200689 0.55986219 0.58742463 0.50990525 0.45305771 0.55469423\n",
            " 0.51765719 0.50990525 0.59173127 0.49354005 0.45994832 0.52713178\n",
            " 0.50215332 0.5081826  0.42721792 0.47200689 0.42291128 0.45478036\n",
            " 0.59776055 0.42377261 0.53402239 0.46080965 0.61843239 0.50990525\n",
            " 0.39965547 0.46942291 0.54435831 0.50129199 0.56072351 0.625323\n",
            " 0.54349699 0.59086994 0.41602067 0.42807924 0.55986219 0.52799311\n",
            " 0.47459087 0.5667528  0.60120586 0.40999139 0.59862188 0.59431525\n",
            " 0.43841516 0.48148148 0.61757106 0.4461671  0.42291128 0.43927649\n",
            " 0.66838932 0.55641688 0.6873385  0.44788975 0.59862188 0.5211025\n",
            " 0.35400517 0.44013781 0.45047373 0.66838932 0.5538329  0.52540913\n",
            " 0.583118   0.51765719 0.52799311 0.49612403 0.55469423 0.65202412\n",
            " 0.54005168 0.38845823 0.54866494 0.59086994 0.69336779 0.44358312\n",
            " 0.47372954 0.45908699 0.57881137 0.40310078 0.34969854 0.42894057\n",
            " 0.43669251 0.50215332 0.52971576 0.46511628 0.44358312 0.58656331\n",
            " 0.65977606 0.51765719 0.60637382 0.41085271 0.58570198 0.36175711\n",
            " 0.46425495 0.52282515 0.54694229 0.6124031  0.53660637 0.43496985\n",
            " 0.66666667 0.46167097 0.52540913 0.45736434 0.63824289 0.53057709\n",
            " 0.64427218 0.4461671  0.58656331 0.61068045 0.60206718 0.51507321\n",
            " 0.53229974 0.4918174  0.36089578 0.51507321 0.64857881 0.44530577\n",
            " 0.61154177 0.34625323 0.43238587 0.61154177 0.45564169 0.45822567\n",
            " 0.56847545 0.51421189 0.53832903 0.35400517 0.44099914 0.48492679\n",
            " 0.625323   0.35658915 0.43066322 0.55986219 0.42894057 0.38673557\n",
            " 0.57536606 0.56847545 0.45822567 0.53660637 0.54091301 0.54263566\n",
            " 0.59259259 0.42635659 0.54694229 0.47459087 0.54349699 0.43238587\n",
            " 0.43755383 0.57278208 0.49956934 0.46080965 0.30749354 0.63049096\n",
            " 0.50645995 0.43410853 0.65202412 0.42894057 0.52196382 0.67011197\n",
            " 0.47803618 0.4788975  0.51593454 0.59345392 0.44875108 0.51593454\n",
            " 0.48148148 0.55813953 0.54263566 0.52971576 0.61154177 0.7002584\n",
            " 0.61929371 0.61757106 0.5994832  0.65288544 0.63393626 0.58656331\n",
            " 0.61929371 0.70542636 0.57536606 0.43927649 0.55900086 0.43927649\n",
            " 0.61584841 0.50129199 0.48923342 0.50732127 0.39276486 0.54263566\n",
            " 0.36692506 0.29371232 0.54349699 0.6287683  0.2962963  0.55900086\n",
            " 0.51421189 0.4918174  0.58484065 0.49698536 0.76055125 0.7329888\n",
            " 0.40826873 0.46339363 0.4918174  0.625323   0.57364341 0.48923342\n",
            " 0.4952627  0.61584841 0.46683893 0.50301464 0.47717485 0.61929371\n",
            " 0.55986219 0.82687339 0.52627046 0.55124892 0.55900086 0.50215332\n",
            " 0.66063738 0.41257537 0.59173127 0.60206718 0.50301464 0.38501292\n",
            " 0.44530577 0.55641688 0.54263566 0.51937984 0.54694229 0.6089578\n",
            " 0.47975883 0.43927649 0.32385874 0.31438415 0.36003445 0.45650301\n",
            " 0.51679587 0.53402239 0.54263566 0.59862188 0.49870801 0.54521964\n",
            " 0.48751077 0.56072351 0.58914729 0.35486649 0.56158484 0.56847545\n",
            " 0.56416882 0.43927649 0.42635659 0.54005168 0.41343669 0.63135228\n",
            " 0.72437554 0.4496124  0.46683893 0.51076658 0.51593454 0.42635659\n",
            " 0.53143842 0.50301464 0.40137812 0.45908699 0.60551249 0.48406546\n",
            " 0.53143842 0.62704565 0.37639966 0.40482343 0.37726098 0.55813953\n",
            " 0.62790698 0.41257537 0.46425495 0.60809647 0.44444444 0.54091301\n",
            " 0.53229974 0.54091301 0.43927649 0.41085271 0.38070629 0.51851852\n",
            " 0.62015504 0.61843239 0.5374677  0.53316107 0.58828596 0.53919035\n",
            " 0.4203273  0.56244617 0.5503876  0.52540913 0.59000861 0.46167097\n",
            " 0.61068045 0.33850129 0.36175711 0.51679587 0.55986219 0.47028424\n",
            " 0.53488372 0.3910422  0.63307494 0.45908699 0.40999139 0.47803618\n",
            " 0.55900086 0.41429802 0.46511628 0.51765719 0.54177433 0.34969854\n",
            " 0.56847545 0.54005168 0.39190353 0.55813953 0.60034453 0.5503876\n",
            " 0.60034453 0.52024117 0.53832903 0.45650301 0.43496985 0.42549526\n",
            " 0.45478036 0.56158484 0.6416882  0.36950904 0.40568475 0.47286822\n",
            " 0.51421189 0.65891473 0.60292851 0.4461671  0.44358312 0.44272179\n",
            " 0.55555556 0.53229974 0.55297158 0.48062016 0.57278208 0.47631352\n",
            " 0.50732127 0.57536606 0.4788975  0.3910422 ]\n",
            "The trained model has an aproximate error rate of -11.162235794691592 which equates to -2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two main tests have been applied to the model. The Route Mean Squared Error (RMSE) and a comparison between the target values in the testing dataset and the predicted values using the predictors in the testing dataset.\n",
        "\n",
        "Due to the nature of machine learning, different results are achieved with each run of the model.\n",
        "\n",
        "Predominantly the RMSE of the model is lower than that of the average. This indicates that the linear regression model makes more accurate predictions when compared to using just the average. The error rate of the model is aprocimately -2% when comparing the predicited and actual results."
      ],
      "metadata": {
        "id": "kKckcLmPVIMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n",
        "A relationship between dew point and the number of collisions was also uncovered in assignment 1. This linear relationship suggests that as the dew point increases the number of collisions increase. \n",
        "\n",
        "The process to produce the model follows the same process as the precipitation model above where the data is loaded, further cleaned, shuffled, split into a train and test dataset prior to training and tesing the model."
      ],
      "metadata": {
        "id": "RB0Zq1024UmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean.csv', index_col=0, )\n",
        "print(df_dewp[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85f71e7-0351-4cdd-980b-6564371353fe",
        "id": "d2NB6odM5G9I"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp = df_dewp.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] != 2012]\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] < 2020]\n",
        "cols = df_dewp['NUM_COLLISIONS']\n",
        "df_dewp = df_dewp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp[:6])\n",
        "df_dewp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "6cb8b926-279e-435a-f65d-906e919da2e1",
        "id": "WwtmLQ6a5rHs"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  24.4             681\n",
            "51    5  2014   1  17  35.8             589\n",
            "54    1  2016   1  25  21.2             658\n",
            "55    5  2016   1  29  36.8             645\n",
            "58    5  2017   1  20  32.5             605\n",
            "59    7  2013   1  13  44.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         dewp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.998434  2015.999217     6.524070    15.723679    44.163170   \n",
              "std       2.000391     2.000000     3.449676     8.801271    16.995303   \n",
              "min       1.000000  2013.000000     1.000000     1.000000    -6.700000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    32.150000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    45.300000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    58.500000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    74.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.109980  \n",
              "std        100.277185  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-374ae0ef-74c6-4592-8a9f-2d75a92ebd50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998434</td>\n",
              "      <td>2015.999217</td>\n",
              "      <td>6.524070</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000391</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.449676</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-374ae0ef-74c6-4592-8a9f-2d75a92ebd50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-374ae0ef-74c6-4592-8a9f-2d75a92ebd50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-374ae0ef-74c6-4592-8a9f-2d75a92ebd50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp.iloc[np.random.permutation(len(df_dewp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62643fc4-2e37-4ae2-dc52-b2ad8ed880b5",
        "id": "KXbAqzNN694C"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  dewp\n",
            "966     7  2015   4  26  34.0\n",
            "1292    4  2019   5  16  47.6\n",
            "3096    2  2016  11   1  37.1\n",
            "1472    3  2016   5  18  45.2\n",
            "2339    3  2015   8  26  68.1\n",
            "2121    4  2014   7  10  66.1\n",
            "      day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "966     7  2015   4  26  34.0             477\n",
            "1292    4  2019   5  16  47.6             642\n",
            "3096    2  2016  11   1  37.1             732\n",
            "1472    3  2016   5  18  45.2             676\n",
            "2339    3  2015   8  26  68.1             606\n",
            "2121    4  2014   7  10  66.1             584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69509144-b08a-4b3e-848f-c19e0c7e5fee",
        "id": "iGbT5sAJ7KTK"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "966     477\n",
            "1292    642\n",
            "3096    732\n",
            "1472    676\n",
            "2339    606\n",
            "2121    584\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4886e61-2017-4dba-9ade-962f5fc3128d",
        "id": "vpHbBnml7PZw"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3032167a-0b73-482c-df19-396f17023778",
        "id": "j4GKf5BL7WI3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d45d8d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.28104508, step = 1\n",
            "INFO:tensorflow:global_step/sec: 574.815\n",
            "INFO:tensorflow:loss = 0.007068947, step = 101 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 756.946\n",
            "INFO:tensorflow:loss = 0.007001254, step = 201 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.518\n",
            "INFO:tensorflow:loss = 0.0073363464, step = 301 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.974\n",
            "INFO:tensorflow:loss = 0.0068931864, step = 401 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1042.87\n",
            "INFO:tensorflow:loss = 0.0068940776, step = 501 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1041.25\n",
            "INFO:tensorflow:loss = 0.0067550116, step = 601 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.67\n",
            "INFO:tensorflow:loss = 0.008446421, step = 701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.393\n",
            "INFO:tensorflow:loss = 0.007492586, step = 801 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 935.471\n",
            "INFO:tensorflow:loss = 0.006796991, step = 901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.808\n",
            "INFO:tensorflow:loss = 0.0074080545, step = 1001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.109\n",
            "INFO:tensorflow:loss = 0.00913956, step = 1101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.753\n",
            "INFO:tensorflow:loss = 0.006702544, step = 1201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.513\n",
            "INFO:tensorflow:loss = 0.008467543, step = 1301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.832\n",
            "INFO:tensorflow:loss = 0.008090837, step = 1401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.95\n",
            "INFO:tensorflow:loss = 0.006771632, step = 1501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.89\n",
            "INFO:tensorflow:loss = 0.008153485, step = 1601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.79\n",
            "INFO:tensorflow:loss = 0.007998163, step = 1701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.977\n",
            "INFO:tensorflow:loss = 0.006166231, step = 1801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.054\n",
            "INFO:tensorflow:loss = 0.0057435227, step = 1901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.234\n",
            "INFO:tensorflow:loss = 0.0052710846, step = 2001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.62\n",
            "INFO:tensorflow:loss = 0.0062294044, step = 2101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.184\n",
            "INFO:tensorflow:loss = 0.0077252383, step = 2201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.396\n",
            "INFO:tensorflow:loss = 0.0077243466, step = 2301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.99\n",
            "INFO:tensorflow:loss = 0.006960836, step = 2401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.47\n",
            "INFO:tensorflow:loss = 0.006827412, step = 2501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.357\n",
            "INFO:tensorflow:loss = 0.007481108, step = 2601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.429\n",
            "INFO:tensorflow:loss = 0.005990659, step = 2701 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.348\n",
            "INFO:tensorflow:loss = 0.007176333, step = 2801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.995\n",
            "INFO:tensorflow:loss = 0.0068916176, step = 2901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.522\n",
            "INFO:tensorflow:loss = 0.0061269305, step = 3001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.08\n",
            "INFO:tensorflow:loss = 0.006875561, step = 3101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.19\n",
            "INFO:tensorflow:loss = 0.007013436, step = 3201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.417\n",
            "INFO:tensorflow:loss = 0.005429775, step = 3301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.08\n",
            "INFO:tensorflow:loss = 0.0072669564, step = 3401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.311\n",
            "INFO:tensorflow:loss = 0.0048722345, step = 3501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.819\n",
            "INFO:tensorflow:loss = 0.007307672, step = 3601 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.343\n",
            "INFO:tensorflow:loss = 0.005719696, step = 3701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.447\n",
            "INFO:tensorflow:loss = 0.008883127, step = 3801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.147\n",
            "INFO:tensorflow:loss = 0.006654145, step = 3901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.369\n",
            "INFO:tensorflow:loss = 0.0064436393, step = 4001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 915.351\n",
            "INFO:tensorflow:loss = 0.004445455, step = 4101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.49\n",
            "INFO:tensorflow:loss = 0.005897862, step = 4201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.24\n",
            "INFO:tensorflow:loss = 0.005490096, step = 4301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 968.299\n",
            "INFO:tensorflow:loss = 0.009112926, step = 4401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.512\n",
            "INFO:tensorflow:loss = 0.0074688187, step = 4501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.637\n",
            "INFO:tensorflow:loss = 0.009505941, step = 4601 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.606\n",
            "INFO:tensorflow:loss = 0.007148453, step = 4701 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.807\n",
            "INFO:tensorflow:loss = 0.007903065, step = 4801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1015.52\n",
            "INFO:tensorflow:loss = 0.0066872234, step = 4901 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.138\n",
            "INFO:tensorflow:loss = 0.0074227066, step = 5001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026.72\n",
            "INFO:tensorflow:loss = 0.0067078704, step = 5101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.634\n",
            "INFO:tensorflow:loss = 0.0072008404, step = 5201 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.641\n",
            "INFO:tensorflow:loss = 0.0066984454, step = 5301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.563\n",
            "INFO:tensorflow:loss = 0.0066998517, step = 5401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.563\n",
            "INFO:tensorflow:loss = 0.006970914, step = 5501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.067\n",
            "INFO:tensorflow:loss = 0.004977247, step = 5601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.578\n",
            "INFO:tensorflow:loss = 0.007267562, step = 5701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.865\n",
            "INFO:tensorflow:loss = 0.005892288, step = 5801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.276\n",
            "INFO:tensorflow:loss = 0.008045213, step = 5901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.91\n",
            "INFO:tensorflow:loss = 0.0065332227, step = 6001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.468\n",
            "INFO:tensorflow:loss = 0.007250309, step = 6101 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.194\n",
            "INFO:tensorflow:loss = 0.0060027177, step = 6201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.788\n",
            "INFO:tensorflow:loss = 0.008381891, step = 6301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 793.18\n",
            "INFO:tensorflow:loss = 0.0061286623, step = 6401 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.211\n",
            "INFO:tensorflow:loss = 0.005395264, step = 6501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.33\n",
            "INFO:tensorflow:loss = 0.0074085053, step = 6601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.553\n",
            "INFO:tensorflow:loss = 0.0054904064, step = 6701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.054\n",
            "INFO:tensorflow:loss = 0.0065083485, step = 6801 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 953.043\n",
            "INFO:tensorflow:loss = 0.006740502, step = 6901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.264\n",
            "INFO:tensorflow:loss = 0.006681816, step = 7001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 993.812\n",
            "INFO:tensorflow:loss = 0.0075597, step = 7101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.06\n",
            "INFO:tensorflow:loss = 0.0059816292, step = 7201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.445\n",
            "INFO:tensorflow:loss = 0.005964151, step = 7301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.382\n",
            "INFO:tensorflow:loss = 0.0059394133, step = 7401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.085\n",
            "INFO:tensorflow:loss = 0.0064059016, step = 7501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.996\n",
            "INFO:tensorflow:loss = 0.008311088, step = 7601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.199\n",
            "INFO:tensorflow:loss = 0.008126605, step = 7701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.524\n",
            "INFO:tensorflow:loss = 0.008718124, step = 7801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.814\n",
            "INFO:tensorflow:loss = 0.009040002, step = 7901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.032\n",
            "INFO:tensorflow:loss = 0.0064936215, step = 8001 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.933\n",
            "INFO:tensorflow:loss = 0.0054352083, step = 8101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1030.27\n",
            "INFO:tensorflow:loss = 0.005637536, step = 8201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.154\n",
            "INFO:tensorflow:loss = 0.0061090617, step = 8301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.601\n",
            "INFO:tensorflow:loss = 0.005604267, step = 8401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.206\n",
            "INFO:tensorflow:loss = 0.0056434576, step = 8501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 847.083\n",
            "INFO:tensorflow:loss = 0.0063393125, step = 8601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.544\n",
            "INFO:tensorflow:loss = 0.005327799, step = 8701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.235\n",
            "INFO:tensorflow:loss = 0.0054040225, step = 8801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.448\n",
            "INFO:tensorflow:loss = 0.0063558323, step = 8901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.065\n",
            "INFO:tensorflow:loss = 0.006293688, step = 9001 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.962\n",
            "INFO:tensorflow:loss = 0.0057717236, step = 9101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 918.498\n",
            "INFO:tensorflow:loss = 0.008644267, step = 9201 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.193\n",
            "INFO:tensorflow:loss = 0.0062779924, step = 9301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.785\n",
            "INFO:tensorflow:loss = 0.006671143, step = 9401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.14\n",
            "INFO:tensorflow:loss = 0.0067308797, step = 9501 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.402\n",
            "INFO:tensorflow:loss = 0.0073228683, step = 9601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.886\n",
            "INFO:tensorflow:loss = 0.009702179, step = 9701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.772\n",
            "INFO:tensorflow:loss = 0.005305926, step = 9801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.497\n",
            "INFO:tensorflow:loss = 0.007390837, step = 9901 (0.115 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.005900843.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 89.12176891716852\n",
            "Just using average = 598.116927592955 has RMSE of 96.06957549415907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82795524-c81f-49aa-ed19-c261ec078066",
        "id": "CmqqgLT09593"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376db950d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.5438962  0.4851647  0.5026894  0.52537984 0.5434492  0.50346303\n",
            " 0.56172353 0.5625193  0.49681592 0.57240576 0.5254114  0.488935\n",
            " 0.56094795 0.52711546 0.50265723 0.5588435  0.5061072  0.5043206\n",
            " 0.5340504  0.5491598  0.47387102 0.46157077 0.46813536 0.49736303\n",
            " 0.47743794 0.5085834  0.5020151  0.48627532 0.4823827  0.49648526\n",
            " 0.48104838 0.4809948  0.48267835 0.5071648  0.5249856  0.5351226\n",
            " 0.5253108  0.5200598  0.5222175  0.45756918 0.5137877  0.5308395\n",
            " 0.50849324 0.5040166  0.4915178  0.5117137  0.49590725 0.5124543\n",
            " 0.5316618  0.45008358 0.54385465 0.5225567  0.5478861  0.5719523\n",
            " 0.45546645 0.5359801  0.4767777  0.5228509  0.5373077  0.48635948\n",
            " 0.4684854  0.49111587 0.4698773  0.46211514 0.49908686 0.4597421\n",
            " 0.50924057 0.47888893 0.531697   0.5389135  0.5337841  0.47731683\n",
            " 0.5660528  0.5369272  0.5192258  0.49174914 0.55625176 0.49475342\n",
            " 0.5062241  0.5609131  0.49400234 0.46765593 0.5524619  0.4711879\n",
            " 0.48522684 0.5722692  0.47384644 0.53939754 0.48984057 0.5053063\n",
            " 0.518655   0.5442208  0.5016505  0.5210808  0.5204087  0.5199703\n",
            " 0.51002944 0.48382872 0.50158846 0.5703711  0.5240278  0.48387232\n",
            " 0.53775483 0.5511231  0.55657995 0.52542657 0.51980305 0.5316652\n",
            " 0.49931842 0.54080105 0.5547447  0.47723708 0.47920623 0.53608125\n",
            " 0.53499734 0.4861117  0.48927405 0.48150942 0.49591166 0.5279106\n",
            " 0.5333125  0.5551256  0.49729905 0.5095509  0.5329464  0.48927882\n",
            " 0.486937   0.54501843 0.54090506 0.48358628 0.4494454  0.5512293\n",
            " 0.54389507 0.56423134 0.54571855 0.49345005 0.54673576 0.5521956\n",
            " 0.51003146 0.45707572 0.52907115 0.5141955  0.48149934 0.51191336\n",
            " 0.5093522  0.49215657 0.5684109  0.5643687  0.5300378  0.523119\n",
            " 0.54983246 0.49238035 0.5275109  0.5001665  0.49127474 0.5248935\n",
            " 0.4807416  0.47853413 0.49350628 0.5075366  0.51326555 0.49801266\n",
            " 0.49064368 0.56391466 0.51100343 0.5060192  0.48953748 0.49344474\n",
            " 0.4734416  0.46422076 0.47325805 0.5052115  0.5166668  0.53212476\n",
            " 0.55129397 0.50988185 0.5059624  0.5143969  0.48653314 0.45247334\n",
            " 0.55317175 0.48918033 0.50460726 0.45544526 0.47768298 0.51301396\n",
            " 0.5187786  0.5320873  0.48021603 0.48198003 0.51417094 0.5572862\n",
            " 0.4698581  0.48718494 0.53919935 0.5218568  0.4659423  0.46821147\n",
            " 0.52669114 0.47464684 0.48632908 0.5416303  0.5163315  0.47035903\n",
            " 0.4633758  0.53225744 0.541894   0.48411882 0.52524275 0.5129559\n",
            " 0.47439045 0.45506576 0.52563876 0.51462203 0.5056897  0.4953041\n",
            " 0.4922888  0.5307164  0.4802221  0.5040734  0.4668045  0.4926458\n",
            " 0.57562095 0.49684018 0.5041088  0.5269666  0.49567214 0.47535297\n",
            " 0.52712816 0.5400813  0.5208568  0.48165855 0.49590984 0.5032631\n",
            " 0.49895936 0.52813363 0.5040415  0.53255796 0.5116953  0.4697306\n",
            " 0.48510224 0.44480765 0.4984019  0.4494402  0.48897433 0.5348551\n",
            " 0.5278462  0.54901445 0.451579   0.45502537 0.5522031  0.5002329\n",
            " 0.46979204 0.47724518 0.5135635  0.5401492  0.5393514  0.4570218\n",
            " 0.4825655  0.5599195  0.5356345  0.5560396  0.47360983 0.5414919\n",
            " 0.48926753 0.56314725 0.53117806 0.55161566 0.44072294 0.44759932\n",
            " 0.52932996 0.53553426 0.5241261  0.5089671  0.5211791  0.46678323\n",
            " 0.5166957  0.4947871  0.5374277  0.4966938  0.5405467  0.5238222\n",
            " 0.4791394  0.5572833  0.5471907  0.5380731  0.51004046 0.5097217\n",
            " 0.5647655  0.4959778  0.5067328  0.5048585  0.55935526 0.5237522\n",
            " 0.5029969  0.47549158 0.49202064 0.52462286 0.5474006  0.57877815\n",
            " 0.5153812  0.53275037 0.48589984 0.49106008 0.46117827 0.48779976\n",
            " 0.4823782  0.5031157  0.5455573  0.54712707 0.49629918 0.5312709\n",
            " 0.46728218 0.511899   0.5315195  0.53754634 0.5212774  0.5435156\n",
            " 0.50127524 0.53709316 0.48171416 0.5536789  0.49595797 0.49740914\n",
            " 0.5431782  0.49703458 0.4549506  0.5024975  0.47515216 0.45370856\n",
            " 0.5150979  0.52830577 0.43155333 0.45079213 0.4537019  0.49499318\n",
            " 0.5131456  0.50556    0.52154946 0.49608368 0.5664356  0.51787823\n",
            " 0.4789274  0.49789092 0.50258917 0.4365507  0.480243   0.5229733\n",
            " 0.4711325  0.53344643 0.5125006  0.5590429  0.54808867 0.48803306\n",
            " 0.48321748 0.50266486 0.507588   0.52145946 0.45851684 0.53092676\n",
            " 0.4433792  0.53744227 0.47400364 0.5480785  0.53895026 0.50693613\n",
            " 0.49731657 0.5059927  0.5361229  0.49858263 0.46783063 0.54177517\n",
            " 0.4891246  0.55833876 0.46373886 0.48046637 0.5104597  0.49021673\n",
            " 0.46707353 0.5441033  0.46454012 0.49131453 0.5435898  0.50145364\n",
            " 0.5450779  0.53841037 0.5375936  0.46188104 0.49457857 0.5199049\n",
            " 0.4654569  0.5156468  0.5455109  0.57597655 0.4746232  0.5172612\n",
            " 0.48630533 0.5545233  0.54318494 0.5340862  0.56275666 0.4558037\n",
            " 0.4679416  0.48268408 0.5150327  0.56510586 0.52772456 0.54433364\n",
            " 0.44021845 0.5445815  0.5720487  0.47838086 0.5471074  0.5168577\n",
            " 0.505306   0.5086559  0.48859248 0.5119329  0.49988157 0.51142585\n",
            " 0.47970724 0.49473807 0.5095364  0.52122694 0.50352645 0.5178613\n",
            " 0.5011762  0.54720855 0.45304063 0.4992605  0.48480523 0.5191401\n",
            " 0.53630674 0.46963072 0.5169683  0.4617665  0.49211532 0.50207746\n",
            " 0.57119596 0.5083861  0.48473516 0.5294196  0.57372564 0.5302686\n",
            " 0.5421235  0.46070805 0.5285963  0.44609216 0.5165758  0.55798614\n",
            " 0.5582465  0.52740866 0.5120622  0.5173248  0.5076166  0.5200223\n",
            " 0.48081052 0.51131976 0.5040798  0.5074398  0.52650446 0.50077665\n",
            " 0.50325227 0.51936626 0.5166274  0.57227844 0.56530434 0.52120954\n",
            " 0.55618507 0.49253732 0.49149945 0.49135643 0.5086663  0.525332\n",
            " 0.5173771  0.49563965 0.5312299  0.48582253 0.49842787 0.516431\n",
            " 0.5439542  0.5145613  0.49943903 0.5509883  0.50293505 0.4827914\n",
            " 0.5373683  0.49833444 0.5150856  0.48793405 0.51811    0.53126806\n",
            " 0.47455892 0.48963377 0.5364117  0.4836031  0.5199896  0.48775497\n",
            " 0.51078075 0.5458147  0.4639782  0.5674751  0.5104326  0.43726572\n",
            " 0.5129979  0.4930371  0.54833287 0.4938047  0.5372678  0.5112198\n",
            " 0.52864474]\n",
            "[0.53229974 0.44444444 0.41085271 0.60551249 0.53488372 0.6873385\n",
            " 0.53919035 0.60378984 0.57278208 0.50215332 0.44875108 0.5047373\n",
            " 0.4039621  0.62790698 0.57881137 0.45478036 0.66666667 0.36864772\n",
            " 0.54866494 0.51593454 0.4005168  0.51507321 0.45736434 0.56847545\n",
            " 0.43410853 0.46511628 0.56244617 0.43755383 0.43927649 0.53660637\n",
            " 0.42894057 0.40654608 0.36434109 0.46942291 0.46425495 0.59345392\n",
            " 0.6089578  0.6089578  0.64771748 0.39276486 0.48923342 0.5374677\n",
            " 0.54091301 0.54866494 0.4918174  0.61326443 0.39793282 0.61154177\n",
            " 0.38931955 0.38845823 0.42807924 0.43669251 0.54694229 0.61929371\n",
            " 0.33850129 0.37726098 0.60551249 0.6416882  0.6124031  0.51851852\n",
            " 0.41946598 0.43755383 0.45564169 0.44444444 0.39707149 0.49095607\n",
            " 0.44358312 0.4039621  0.4496124  0.50990525 0.64082687 0.44358312\n",
            " 0.43496985 0.62704565 0.65116279 0.44099914 0.60120586 0.5994832\n",
            " 0.62273902 0.57364341 0.48062016 0.32816537 0.57019811 0.51335056\n",
            " 0.51593454 0.53143842 0.41085271 0.62015504 0.44875108 0.50387597\n",
            " 0.6089578  0.53402239 0.41343669 0.69853575 0.625323   0.65374677\n",
            " 0.42807924 0.61843239 0.52627046 0.55469423 0.58570198 0.4332472\n",
            " 0.56933678 0.66149871 0.56416882 0.52540913 0.54694229 0.74677003\n",
            " 0.59173127 0.60465116 0.51248923 0.41774332 0.53057709 0.63738157\n",
            " 0.42894057 0.47975883 0.53574505 0.47286822 0.43583118 0.55727821\n",
            " 0.53488372 0.52713178 0.51937984 0.64082687 0.59173127 0.5796727\n",
            " 0.57019811 0.63996555 0.56589147 0.60120586 0.39276486 0.59345392\n",
            " 0.52799311 0.53919035 0.54091301 0.51421189 0.53402239 0.6744186\n",
            " 0.56761413 0.43927649 0.56158484 0.4918174  0.45391904 0.54521964\n",
            " 0.60292851 0.49870801 0.61929371 0.57019811 0.55986219 0.54435831\n",
            " 0.49354005 0.41860465 0.61757106 0.44444444 0.47114556 0.57622739\n",
            " 0.41860465 0.41946598 0.55555556 0.5503876  0.52799311 0.57708872\n",
            " 0.42118863 0.47286822 0.52971576 0.41343669 0.41429802 0.52799311\n",
            " 0.49354005 0.51765719 0.4005168  0.59259259 0.47286822 0.64685616\n",
            " 0.53229974 0.60981912 0.52971576 0.32213609 0.4461671  0.42291128\n",
            " 0.5538329  0.41774332 0.5503876  0.60551249 0.52024117 0.56761413\n",
            " 0.60981912 0.52971576 0.53919035 0.5374677  0.58397933 0.51162791\n",
            " 0.43496985 0.47803618 0.54952627 0.58139535 0.6416882  0.50215332\n",
            " 0.68217054 0.55900086 0.36089578 0.63824289 0.5245478  0.41429802\n",
            " 0.47372954 0.66666667 0.53574505 0.55124892 0.60034453 0.4754522\n",
            " 0.72265289 0.60465116 0.57795004 0.66925065 0.59776055 0.59000861\n",
            " 0.44875108 0.5667528  0.35228252 0.60465116 0.57364341 0.45736434\n",
            " 0.41774332 0.42807924 0.55124892 0.53057709 0.52971576 0.50904393\n",
            " 0.66063738 0.57536606 0.60034453 0.43583118 0.5960379  0.45478036\n",
            " 0.45305771 0.57536606 0.45219638 0.5667528  0.47459087 0.49009475\n",
            " 0.53574505 0.56416882 0.51162791 0.36003445 0.44530577 0.54694229\n",
            " 0.5667528  0.49612403 0.42549526 0.36434109 0.49440138 0.35486649\n",
            " 0.36434109 0.45564169 0.59000861 0.53402239 0.49267873 0.32385874\n",
            " 0.50559862 0.4754522  0.57450474 0.5503876  0.47372954 0.61670973\n",
            " 0.51248923 0.46942291 0.51679587 0.43583118 0.31093885 0.53488372\n",
            " 0.50990525 0.42463394 0.56761413 0.58656331 0.60465116 0.46339363\n",
            " 0.46339363 0.54694229 0.4788975  0.5047373  0.54694229 0.26098191\n",
            " 0.47114556 0.57450474 0.55469423 0.66666667 0.49612403 0.52713178\n",
            " 0.60120586 0.26873385 0.46942291 0.49354005 0.6089578  0.63049096\n",
            " 0.44530577 0.42635659 0.63996555 0.39534884 0.5667528  0.3712317\n",
            " 0.64944014 0.49784668 0.45908699 0.49009475 0.44013781 0.45822567\n",
            " 0.42463394 0.40310078 0.50215332 0.5667528  0.49095607 0.54694229\n",
            " 0.38673557 0.54780362 0.49612403 0.60034453 0.49440138 0.62790698\n",
            " 0.60292851 0.5994832  0.6089578  0.6416882  0.67355728 0.55813953\n",
            " 0.55641688 0.47459087 0.35400517 0.57536606 0.58656331 0.37209302\n",
            " 0.6089578  0.50990525 0.4005168  0.36950904 0.49698536 0.55986219\n",
            " 0.53919035 0.43927649 0.42894057 0.39276486 0.57536606 0.55986219\n",
            " 0.51851852 0.57105943 0.59689922 0.42635659 0.44013781 0.42980189\n",
            " 0.33074935 0.5667528  0.58742463 0.59517657 0.55727821 0.46770026\n",
            " 0.51937984 0.65202412 0.53488372 0.42291128 0.40310078 0.46683893\n",
            " 0.44530577 0.60981912 0.51507321 0.47631352 0.65633075 0.54349699\n",
            " 0.4754522  0.60034453 0.55641688 0.41171404 0.5245478  0.6124031\n",
            " 0.43927649 0.57105943 0.65116279 0.54005168 0.44702842 0.48664944\n",
            " 0.39793282 0.37295435 0.58570198 0.416882   0.52799311 0.63307494\n",
            " 0.58828596 0.50904393 0.61412575 0.50990525 0.52627046 0.63479759\n",
            " 0.41515935 0.46511628 0.35228252 0.60723514 0.58225668 0.52799311\n",
            " 0.49956934 0.65030146 0.49267873 0.59862188 0.50990525 0.51765719\n",
            " 0.47459087 0.53574505 0.49095607 0.57536606 0.48062016 0.60551249\n",
            " 0.36864772 0.64341085 0.5538329  0.44272179 0.55297158 0.51851852\n",
            " 0.51076658 0.49267873 0.5211025  0.49440138 0.48320413 0.59173127\n",
            " 0.5211025  0.56847545 0.55727821 0.58225668 0.55900086 0.6287683\n",
            " 0.48492679 0.60120586 0.36692506 0.46425495 0.44358312 0.58225668\n",
            " 0.5667528  0.62187769 0.45478036 0.49095607 0.47717485 0.51076658\n",
            " 0.53057709 0.66322136 0.4496124  0.4952627  0.53229974 0.59086994\n",
            " 0.58139535 0.60981912 0.52368648 0.46167097 0.50559862 0.49698536\n",
            " 0.66666667 0.58828596 0.5538329  0.51937984 0.57019811 0.54349699\n",
            " 0.43238587 0.53143842 0.59431525 0.66322136 0.46425495 0.4461671\n",
            " 0.62360034 0.60206718 0.58397933 0.58570198 0.56158484 0.57536606\n",
            " 0.62618432 0.53832903 0.38845823 0.50129199 0.54263566 0.56330749\n",
            " 0.47631352 0.54177433 0.54694229 0.43841516 0.45994832 0.49009475\n",
            " 0.4203273  0.70542636 0.625323   0.5538329  0.39018088 0.42204996\n",
            " 0.41515935 0.53057709 0.52196382 0.38070629 0.44013781 0.54780362\n",
            " 0.55297158 0.52885444 0.47803618 0.45305771 0.61154177 0.47286822\n",
            " 0.5047373  0.54177433 0.39534884 0.60292851 0.625323   0.38329027\n",
            " 0.42377261 0.55124892 0.56416882 0.50215332 0.53574505 0.66408269\n",
            " 0.64944014]\n",
            "The trained model has an aproximate error rate of 11.12673435717413 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RMSE for the dewp model is generally slightly lower in comparison to the model procued for precipitation. As the RMSE for the model is lower than the RMSE of the mean, it can be argued that the model has a higher level of accuracy in comparision to the using the mean."
      ],
      "metadata": {
        "id": "k_jNEZ_dS62M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visibility (visib)\n",
        "A further relationship was uncovered between visibilty and the number of collisions. This is a negative linear relationship where as the visibility increases the number of collisions decrease. \n",
        "\n",
        "The process to produce the model follows the same process as above where the data is loaded, further cleaned, shuffled, split into a train and test dataset prior to training and tesing the model."
      ],
      "metadata": {
        "id": "XfOkQa04Wgr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_visib = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/coldata.csv', index_col=0, )\n",
        "print(df_visib[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f678e5-e7c4-4fd1-da29-215979f11f57",
        "id": "tOsAfHzhWgr6"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_visib = df_visib.drop(columns=['collision_date', 'temp', 'prcp', 'slp','dewp','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_visib = df_visib.loc[df_visib[\"year\"] != 2012]\n",
        "df_visib = df_visib.loc[df_visib[\"year\"] < 2020]\n",
        "cols = df_visib['NUM_COLLISIONS']\n",
        "df_visib = df_visib.drop(columns=['NUM_COLLISIONS'])\n",
        "df_visib.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_visib[:6])\n",
        "df_visib.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b67b60c3-2908-44ab-f0d9-ac872bd2a1fd",
        "id": "pAaWfrlBWgr7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  visib  NUM_COLLISIONS\n",
            "49    4  2016   1  28   10.0             681\n",
            "51    5  2014   1  17    6.7             589\n",
            "54    1  2016   1  25   10.0             658\n",
            "55    5  2016   1  29   10.0             645\n",
            "58    5  2017   1  20   10.0             605\n",
            "59    7  2013   1  13    4.3             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day    year           mo           da        visib  \\\n",
              "count  2556.000000  2556.0  2556.000000  2556.000000  2556.000000   \n",
              "mean      3.999218  2016.0     6.524257    15.725743     8.295618   \n",
              "std       2.000391     2.0     3.449013     8.800168     2.207870   \n",
              "min       1.000000  2013.0     1.000000     1.000000     0.200000   \n",
              "25%       2.000000  2014.0     4.000000     8.000000     7.100000   \n",
              "50%       4.000000  2016.0     7.000000    16.000000     9.400000   \n",
              "75%       6.000000  2018.0    10.000000    23.000000    10.000000   \n",
              "max       7.000000  2019.0    12.000000    31.000000    10.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2556.000000  \n",
              "mean       599.118936  \n",
              "std        100.258581  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3a68a3c-6703-4395-951e-2b86179347ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>visib</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.0</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999218</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>6.524257</td>\n",
              "      <td>15.725743</td>\n",
              "      <td>8.295618</td>\n",
              "      <td>599.118936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000391</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.449013</td>\n",
              "      <td>8.800168</td>\n",
              "      <td>2.207870</td>\n",
              "      <td>100.258581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.100000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>9.400000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3a68a3c-6703-4395-951e-2b86179347ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3a68a3c-6703-4395-951e-2b86179347ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3a68a3c-6703-4395-951e-2b86179347ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_visib.iloc[np.random.permutation(len(df_visib))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf718b1-9bc5-4a27-82ed-d490babfa050",
        "id": "9VOgsgFJWgr7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  visib\n",
            "1615    5  2014   6  20    9.8\n",
            "385     7  2019   2  17   10.0\n",
            "3338    7  2016  11  20    5.5\n",
            "395     7  2016   2  21    8.4\n",
            "808     4  2015   3  12   10.0\n",
            "805     7  2019   3  24   10.0\n",
            "      day  year  mo  da  visib  NUM_COLLISIONS\n",
            "1615    5  2014   6  20    9.8             765\n",
            "385     7  2019   2  17   10.0             439\n",
            "3338    7  2016  11  20    5.5             529\n",
            "395     7  2016   2  21    8.4             481\n",
            "808     4  2015   3  12   10.0             586\n",
            "805     7  2019   3  24   10.0             466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaccb47-659e-4726-81e7-cde163c6b454",
        "id": "dx2bYy6zWgr7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1615    765\n",
            "385     439\n",
            "3338    529\n",
            "395     481\n",
            "808     586\n",
            "805     466\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d850775-b04a-40be-c772-54a0be8e5782",
        "id": "TKLeYNUiWgr7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_visib', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_visib', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70658a72-54b6-434c-e3ef-9f60b32ab75e",
        "id": "J24fhLeNWgr8"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d437490>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_visib', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_visib/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.28360546, step = 1\n",
            "INFO:tensorflow:global_step/sec: 838.916\n",
            "INFO:tensorflow:loss = 0.010178417, step = 101 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.157\n",
            "INFO:tensorflow:loss = 0.007864419, step = 201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.387\n",
            "INFO:tensorflow:loss = 0.006883328, step = 301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.726\n",
            "INFO:tensorflow:loss = 0.0055556484, step = 401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.529\n",
            "INFO:tensorflow:loss = 0.0057206037, step = 501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.836\n",
            "INFO:tensorflow:loss = 0.00718684, step = 601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.769\n",
            "INFO:tensorflow:loss = 0.0074655795, step = 701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.23\n",
            "INFO:tensorflow:loss = 0.0068270406, step = 801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.056\n",
            "INFO:tensorflow:loss = 0.006900786, step = 901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.306\n",
            "INFO:tensorflow:loss = 0.0060780896, step = 1001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 905.737\n",
            "INFO:tensorflow:loss = 0.0063431356, step = 1101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.688\n",
            "INFO:tensorflow:loss = 0.0077963034, step = 1201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.006\n",
            "INFO:tensorflow:loss = 0.008079399, step = 1301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.527\n",
            "INFO:tensorflow:loss = 0.007748372, step = 1401 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.682\n",
            "INFO:tensorflow:loss = 0.007018519, step = 1501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.318\n",
            "INFO:tensorflow:loss = 0.008800852, step = 1601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.449\n",
            "INFO:tensorflow:loss = 0.0074492716, step = 1701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.604\n",
            "INFO:tensorflow:loss = 0.007887766, step = 1801 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.59\n",
            "INFO:tensorflow:loss = 0.009365057, step = 1901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.057\n",
            "INFO:tensorflow:loss = 0.0078001805, step = 2001 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.784\n",
            "INFO:tensorflow:loss = 0.005546543, step = 2101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.016\n",
            "INFO:tensorflow:loss = 0.006159575, step = 2201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.382\n",
            "INFO:tensorflow:loss = 0.00742229, step = 2301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 854.143\n",
            "INFO:tensorflow:loss = 0.007874548, step = 2401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.034\n",
            "INFO:tensorflow:loss = 0.007851299, step = 2501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.817\n",
            "INFO:tensorflow:loss = 0.006767719, step = 2601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.857\n",
            "INFO:tensorflow:loss = 0.005571537, step = 2701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.23\n",
            "INFO:tensorflow:loss = 0.00812641, step = 2801 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.27\n",
            "INFO:tensorflow:loss = 0.006754013, step = 2901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.21\n",
            "INFO:tensorflow:loss = 0.0070064315, step = 3001 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.599\n",
            "INFO:tensorflow:loss = 0.0059451526, step = 3101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.53\n",
            "INFO:tensorflow:loss = 0.008839775, step = 3201 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.191\n",
            "INFO:tensorflow:loss = 0.006386717, step = 3301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.07\n",
            "INFO:tensorflow:loss = 0.0072391215, step = 3401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.217\n",
            "INFO:tensorflow:loss = 0.0068325447, step = 3501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.228\n",
            "INFO:tensorflow:loss = 0.0057795714, step = 3601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.971\n",
            "INFO:tensorflow:loss = 0.0049373168, step = 3701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.018\n",
            "INFO:tensorflow:loss = 0.006968986, step = 3801 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.117\n",
            "INFO:tensorflow:loss = 0.008385463, step = 3901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.261\n",
            "INFO:tensorflow:loss = 0.009719824, step = 4001 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.285\n",
            "INFO:tensorflow:loss = 0.006284872, step = 4101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.19\n",
            "INFO:tensorflow:loss = 0.007868463, step = 4201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.223\n",
            "INFO:tensorflow:loss = 0.0079681445, step = 4301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.339\n",
            "INFO:tensorflow:loss = 0.0056399577, step = 4401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.37\n",
            "INFO:tensorflow:loss = 0.0068901605, step = 4501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.194\n",
            "INFO:tensorflow:loss = 0.0070046023, step = 4601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.268\n",
            "INFO:tensorflow:loss = 0.008290043, step = 4701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.304\n",
            "INFO:tensorflow:loss = 0.0066585382, step = 4801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.515\n",
            "INFO:tensorflow:loss = 0.0051504467, step = 4901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.993\n",
            "INFO:tensorflow:loss = 0.0068585845, step = 5001 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.407\n",
            "INFO:tensorflow:loss = 0.00596833, step = 5101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 900.272\n",
            "INFO:tensorflow:loss = 0.005752094, step = 5201 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.734\n",
            "INFO:tensorflow:loss = 0.0059041497, step = 5301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.003\n",
            "INFO:tensorflow:loss = 0.0067345705, step = 5401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 861.614\n",
            "INFO:tensorflow:loss = 0.007922595, step = 5501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.332\n",
            "INFO:tensorflow:loss = 0.006772669, step = 5601 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.211\n",
            "INFO:tensorflow:loss = 0.0064061014, step = 5701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 817.963\n",
            "INFO:tensorflow:loss = 0.0061536753, step = 5801 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.337\n",
            "INFO:tensorflow:loss = 0.005826048, step = 5901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.102\n",
            "INFO:tensorflow:loss = 0.006381928, step = 6001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.92\n",
            "INFO:tensorflow:loss = 0.0075549306, step = 6101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.877\n",
            "INFO:tensorflow:loss = 0.0068405913, step = 6201 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.453\n",
            "INFO:tensorflow:loss = 0.0074987654, step = 6301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.686\n",
            "INFO:tensorflow:loss = 0.007230332, step = 6401 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.576\n",
            "INFO:tensorflow:loss = 0.0069372263, step = 6501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.161\n",
            "INFO:tensorflow:loss = 0.007789393, step = 6601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.815\n",
            "INFO:tensorflow:loss = 0.009014208, step = 6701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.877\n",
            "INFO:tensorflow:loss = 0.0069573754, step = 6801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.639\n",
            "INFO:tensorflow:loss = 0.0066204453, step = 6901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.523\n",
            "INFO:tensorflow:loss = 0.006974726, step = 7001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.982\n",
            "INFO:tensorflow:loss = 0.005362246, step = 7101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.329\n",
            "INFO:tensorflow:loss = 0.0073987283, step = 7201 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.525\n",
            "INFO:tensorflow:loss = 0.0066606714, step = 7301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 854.202\n",
            "INFO:tensorflow:loss = 0.006291269, step = 7401 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.699\n",
            "INFO:tensorflow:loss = 0.007650762, step = 7501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.172\n",
            "INFO:tensorflow:loss = 0.006845629, step = 7601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 820.492\n",
            "INFO:tensorflow:loss = 0.0072725858, step = 7701 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.246\n",
            "INFO:tensorflow:loss = 0.006989135, step = 7801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.341\n",
            "INFO:tensorflow:loss = 0.008139489, step = 7901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.853\n",
            "INFO:tensorflow:loss = 0.006711048, step = 8001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.216\n",
            "INFO:tensorflow:loss = 0.009756706, step = 8101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 897.35\n",
            "INFO:tensorflow:loss = 0.005791806, step = 8201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.978\n",
            "INFO:tensorflow:loss = 0.006694878, step = 8301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.387\n",
            "INFO:tensorflow:loss = 0.008019084, step = 8401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.589\n",
            "INFO:tensorflow:loss = 0.0052426667, step = 8501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.988\n",
            "INFO:tensorflow:loss = 0.0059493924, step = 8601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.402\n",
            "INFO:tensorflow:loss = 0.007266896, step = 8701 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.053\n",
            "INFO:tensorflow:loss = 0.008095212, step = 8801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.387\n",
            "INFO:tensorflow:loss = 0.0079228375, step = 8901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.916\n",
            "INFO:tensorflow:loss = 0.008511196, step = 9001 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.193\n",
            "INFO:tensorflow:loss = 0.005928871, step = 9101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.951\n",
            "INFO:tensorflow:loss = 0.008848152, step = 9201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.463\n",
            "INFO:tensorflow:loss = 0.006933124, step = 9301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.587\n",
            "INFO:tensorflow:loss = 0.0063842647, step = 9401 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.069\n",
            "INFO:tensorflow:loss = 0.0055227103, step = 9501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 860.09\n",
            "INFO:tensorflow:loss = 0.006898126, step = 9601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.571\n",
            "INFO:tensorflow:loss = 0.0068765935, step = 9701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.163\n",
            "INFO:tensorflow:loss = 0.008030507, step = 9801 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.372\n",
            "INFO:tensorflow:loss = 0.008249725, step = 9901 (0.116 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_visib/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0063181636.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_visib/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 93.73226959331255\n",
            "Just using average = 599.7318982387476 has RMSE of 99.26465090834728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_visib', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f8b599-b325-4792-8e22-12edc1a84f99",
        "id": "0ZA56JV3Wgr8"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d4e6350>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_visib', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_visib/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "512\n",
            "[0.5504391  0.55971545 0.5072935  0.5370538  0.53530693 0.52788585\n",
            " 0.5033492  0.4765762  0.50716877 0.488393   0.57567686 0.49013257\n",
            " 0.52894205 0.52244145 0.44171983 0.44245112 0.4876567  0.44718105\n",
            " 0.5308005  0.4446873  0.5322599  0.5281426  0.5290186  0.5027014\n",
            " 0.51841724 0.51194924 0.5305418  0.4955938  0.53985393 0.5401778\n",
            " 0.50576967 0.5347008  0.50672823 0.48755383 0.47575164 0.5361112\n",
            " 0.47728527 0.49773952 0.4604193  0.4720142  0.47537768 0.5618585\n",
            " 0.508723   0.55307496 0.5005889  0.5239452  0.5207383  0.49846473\n",
            " 0.51298594 0.46904385 0.47171193 0.53308374 0.5536526  0.49934363\n",
            " 0.45558247 0.5406828  0.53935236 0.47306567 0.5467636  0.51331216\n",
            " 0.51389056 0.5355613  0.49564394 0.53242517 0.5099678  0.47282064\n",
            " 0.5233621  0.5047874  0.4623144  0.47548264 0.55878884 0.49825191\n",
            " 0.48070648 0.5313772  0.519996   0.51458204 0.5574097  0.5179419\n",
            " 0.4957683  0.52014995 0.5503821  0.46177587 0.536709   0.5185787\n",
            " 0.54585165 0.55909115 0.4970733  0.52084327 0.48388833 0.5530719\n",
            " 0.54280037 0.55546546 0.52784616 0.51147366 0.51193976 0.52524316\n",
            " 0.5398112  0.4736408  0.5094755  0.52065533 0.50833505 0.4363572\n",
            " 0.5505957  0.51003176 0.52053416 0.5202754  0.46070486 0.45301703\n",
            " 0.4844222  0.5140177  0.50921726 0.5533858  0.45256937 0.50087434\n",
            " 0.49400663 0.47682467 0.48514503 0.5334372  0.48922706 0.48261636\n",
            " 0.513833   0.5060654  0.5324774  0.53256327 0.52162814 0.5113252\n",
            " 0.51224875 0.52408224 0.47960395 0.5445585  0.477903   0.46982047\n",
            " 0.5281403  0.49682516 0.52814287 0.5303626  0.4811902  0.49199265\n",
            " 0.45428905 0.51038903 0.50713205 0.44997323 0.4552649  0.5282342\n",
            " 0.48811698 0.5326613  0.498822   0.48125947 0.47485393 0.492621\n",
            " 0.4637224  0.5227999  0.5049862  0.49326947 0.4910063  0.46976477\n",
            " 0.5064119  0.48772037 0.4814336  0.4698584  0.47161496 0.48193294\n",
            " 0.5297132  0.5246313  0.5030906  0.45415503 0.4795493  0.51867825\n",
            " 0.48664623 0.5615679  0.51810384 0.48637772 0.5426591  0.46691304\n",
            " 0.46831018 0.50597864 0.5444904  0.4472857  0.5264675  0.5100135\n",
            " 0.48588437 0.5130958  0.535744   0.4941633  0.49093783 0.5384074\n",
            " 0.5074296  0.5007284  0.44493198 0.5125604  0.55998194 0.54601115\n",
            " 0.48810822 0.54026324 0.477964   0.50812423 0.46067888 0.4787128\n",
            " 0.509385   0.4767914  0.50402707 0.53038234 0.5253205  0.51298213\n",
            " 0.49241686 0.47854578 0.51500654 0.48639748 0.46296117 0.55494\n",
            " 0.5058142  0.5426548  0.52412593 0.5059711  0.46192643 0.48397988\n",
            " 0.49179167 0.5571298  0.5237275  0.4540527  0.5717774  0.49223125\n",
            " 0.49068868 0.54546314 0.53622884 0.50035775 0.46610758 0.49371296\n",
            " 0.48757848 0.5310076  0.49438623 0.54003125 0.49627665 0.5080267\n",
            " 0.46736068 0.5312722  0.53107506 0.52789575 0.45641494 0.5018739\n",
            " 0.4665553  0.54828256 0.5453812  0.51871246 0.4678632  0.4540303\n",
            " 0.54796535 0.47293526 0.533612   0.4703979  0.5415458  0.47709286\n",
            " 0.49581644 0.48562527 0.51148474 0.470415   0.5021694  0.49477598\n",
            " 0.46090686 0.5280532  0.5262115  0.4635225  0.53338104 0.5219931\n",
            " 0.534405   0.51743484 0.46768185 0.5499291  0.48542565 0.4908295\n",
            " 0.5467583  0.5344006  0.49306363 0.48603725 0.553256   0.4749567\n",
            " 0.47074234 0.5407754  0.47627974 0.5239707  0.5670122  0.50230724\n",
            " 0.48526454 0.5446956  0.47952986 0.49168962 0.47114706 0.5070699\n",
            " 0.5441112  0.48909703 0.47249418 0.49869692 0.46509367 0.4866562\n",
            " 0.5545855  0.46877712 0.45295826 0.5273613  0.53431094 0.55099684\n",
            " 0.5281653  0.45198148 0.5000053  0.45983103 0.47130966 0.5009733\n",
            " 0.52144337 0.5152395  0.50667065 0.48132294 0.4857918  0.54209244\n",
            " 0.54608923 0.4519837  0.5267598  0.54735273 0.52682537 0.5167121\n",
            " 0.4851818  0.46711957 0.516444   0.4978869  0.47926468 0.5395728\n",
            " 0.51250577 0.47473937 0.48199445 0.4801684  0.52348757 0.4926201\n",
            " 0.48025066 0.54673517 0.5142324  0.4925933  0.47911218 0.48159027\n",
            " 0.45955518 0.4971308  0.50657576 0.48986578 0.5322714  0.5146072\n",
            " 0.5327812  0.50238925 0.5133323  0.48089737 0.48087692 0.5382612\n",
            " 0.4883572  0.5450295  0.5000457  0.5260167  0.5238532  0.5531364\n",
            " 0.5540621  0.47914255 0.4916078  0.5148412  0.5547368  0.51038224\n",
            " 0.545721   0.46229354 0.4500628  0.50308377 0.52215505 0.55007905\n",
            " 0.48196578 0.5234922  0.55676204 0.48883826 0.5055029  0.45628396\n",
            " 0.49518055 0.50483865 0.5304513  0.45694932 0.50680435 0.535143\n",
            " 0.54604846 0.48791546 0.51695037 0.5212393  0.4963715  0.5284097\n",
            " 0.5426755  0.49283326 0.5161981  0.5386492  0.45422935 0.5107263\n",
            " 0.48085767 0.49515736 0.4869187  0.54286826 0.49135497 0.53855956\n",
            " 0.52185106 0.49626857 0.51003057 0.48893166 0.47384697 0.54059017\n",
            " 0.51906186 0.532432   0.48526925 0.53608763 0.55658704 0.52465427\n",
            " 0.46960014 0.46619368 0.54303855 0.5529296  0.47051156 0.45978665\n",
            " 0.5248403  0.5055241  0.4744714  0.476849   0.53275603 0.4501929\n",
            " 0.5101424  0.47509217 0.4864546  0.52991825 0.52269876 0.53045213\n",
            " 0.5265387  0.5266278  0.5243968  0.509995   0.52254766 0.47562188\n",
            " 0.49648064 0.50395155 0.5033843  0.5303484  0.4593662  0.5285908\n",
            " 0.54634714 0.5332879  0.48539937 0.506317   0.47576565 0.529068\n",
            " 0.4880916  0.5167766  0.52702713 0.5003054  0.514334   0.47592324\n",
            " 0.55743694 0.47532398 0.47243273 0.49670404 0.47972685 0.5032938\n",
            " 0.5503378  0.51222056 0.5253116  0.49985242 0.50524414 0.4648177\n",
            " 0.55705494 0.5421606  0.501391   0.50152534 0.52740574 0.47589725\n",
            " 0.4944135  0.5315693  0.5485056  0.5164718  0.5022322  0.5205649\n",
            " 0.48308706 0.51580924 0.543243   0.4823355  0.45231062 0.4933402\n",
            " 0.49540204 0.49545443 0.4631713  0.43923897 0.54236114 0.493348\n",
            " 0.5005211  0.5394022  0.52168596 0.51176864 0.47788936 0.49287933\n",
            " 0.48361507 0.5250069  0.44511715 0.56596434 0.5256622  0.4789778\n",
            " 0.4961092  0.4877405  0.5254294  0.52055764 0.5125534  0.51997787\n",
            " 0.48265737 0.54673594 0.48968557 0.4707719  0.5087794  0.47448903\n",
            " 0.4754874  0.49804938]\n",
            "[0.50990525 0.53660637 0.45219638 0.53316107 0.53229974 0.54866494\n",
            " 0.56847545 0.49095607 0.44358312 0.41602067 0.57450474 0.34969854\n",
            " 0.49354005 0.51593454 0.36692506 0.32213609 0.60292851 0.36692506\n",
            " 0.45908699 0.39534884 0.51507321 0.69681309 0.66063738 0.50215332\n",
            " 0.65719208 0.60723514 0.54608096 0.51162791 0.4039621  0.55555556\n",
            " 0.56158484 0.59862188 0.46511628 0.55297158 0.41257537 0.34969854\n",
            " 0.56589147 0.47717485 0.47803618 0.46942291 0.40740741 0.34453058\n",
            " 0.72437554 0.64254953 0.45908699 0.4625323  0.59259259 0.54091301\n",
            " 0.57795004 0.51593454 0.38501292 0.49612403 0.63049096 0.67786391\n",
            " 0.36347976 0.52885444 0.48406546 0.55469423 0.63824289 0.49267873\n",
            " 0.5047373  0.60378984 0.54694229 0.49698536 0.56330749 0.50215332\n",
            " 0.44702842 0.6873385  0.41171404 0.41171404 0.60809647 0.4952627\n",
            " 0.60551249 0.53143842 0.60034453 0.56244617 0.55124892 0.36864772\n",
            " 0.6546081  0.42894057 0.58914729 0.47372954 0.55641688 0.53488372\n",
            " 0.5994832  0.44702842 0.51937984 0.50904393 0.60551249 0.63824289\n",
            " 0.50215332 0.53229974 0.51421189 0.56503015 0.49956934 0.60378984\n",
            " 0.48148148 0.50387597 0.58914729 0.55555556 0.59173127 0.36864772\n",
            " 0.37898363 0.54780362 0.43755383 0.53229974 0.49009475 0.38156761\n",
            " 0.64427218 0.65202412 0.71576227 0.45908699 0.42635659 0.53488372\n",
            " 0.37984496 0.47459087 0.4005168  0.5667528  0.4005168  0.55469423\n",
            " 0.50301464 0.44875108 0.52971576 0.63738157 0.60809647 0.48664944\n",
            " 0.56933678 0.58828596 0.40826873 0.51162791 0.55813953 0.53402239\n",
            " 0.44186047 0.58139535 0.52971576 0.46080965 0.48492679 0.57278208\n",
            " 0.38845823 0.86046512 0.47286822 0.26614987 0.37812231 0.53229974\n",
            " 0.5047373  0.41515935 0.5538329  0.53316107 0.47717485 0.59776055\n",
            " 0.60465116 0.39018088 0.57708872 0.49612403 0.54263566 0.50559862\n",
            " 0.6287683  0.55986219 0.43755383 0.64685616 0.40999139 0.47114556\n",
            " 0.61412575 0.52627046 0.5211025  0.38329027 0.67355728 0.54005168\n",
            " 0.4461671  0.60206718 0.50301464 0.52885444 0.56072351 0.56847545\n",
            " 0.44099914 0.51248923 0.52196382 0.27562446 0.54866494 0.42980189\n",
            " 0.53574505 0.5245478  0.63910422 0.47459087 0.53832903 0.58828596\n",
            " 0.68217054 0.63652024 0.36778639 0.59431525 0.57622739 0.61154177\n",
            " 0.59000861 0.51765719 0.46339363 0.46080965 0.39793282 0.30060293\n",
            " 0.54091301 0.65202412 0.52540913 0.22739018 0.53574505 0.62618432\n",
            " 0.51937984 0.48492679 0.55124892 0.52540913 0.49698536 0.60292851\n",
            " 0.57364341 0.53057709 0.54177433 0.54780362 0.43669251 0.46511628\n",
            " 0.45391904 0.62618432 0.54694229 0.53229974 0.55469423 0.53316107\n",
            " 0.54263566 0.5047373  0.60723514 0.39190353 0.42635659 0.50129199\n",
            " 0.54349699 0.58225668 0.53832903 0.48406546 0.53574505 0.64685616\n",
            " 0.45736434 0.54263566 0.56933678 0.56933678 0.43927649 0.49440138\n",
            " 0.54005168 0.59173127 0.51162791 0.4918174  0.44788975 0.42980189\n",
            " 0.37898363 0.56589147 0.62015504 0.49784668 0.53229974 0.47372954\n",
            " 0.63479759 0.50215332 0.55986219 0.51248923 0.46167097 0.61068045\n",
            " 0.36864772 0.54091301 0.51765719 0.48837209 0.65374677 0.42204996\n",
            " 0.5503876  0.55986219 0.40999139 0.49009475 0.49784668 0.56330749\n",
            " 0.60120586 0.51851852 0.53660637 0.56847545 0.35400517 0.583118\n",
            " 0.47803618 0.58742463 0.34280792 0.61757106 0.58742463 0.54435831\n",
            " 0.48062016 0.42463394 0.47286822 0.42807924 0.56330749 0.60034453\n",
            " 0.4203273  0.48234281 0.42980189 0.70887166 0.43583118 0.33936262\n",
            " 0.33936262 0.52627046 0.33419466 0.56330749 0.49870801 0.51937984\n",
            " 0.49095607 0.43238587 0.41343669 0.49784668 0.52799311 0.63738157\n",
            " 0.5047373  0.57364341 0.55641688 0.5503876  0.39190353 0.52885444\n",
            " 0.49095607 0.39965547 0.52282515 0.55727821 0.48578811 0.30577089\n",
            " 0.5374677  0.43927649 0.52196382 0.39448751 0.43496985 0.65891473\n",
            " 0.59086994 0.5245478  0.50904393 0.36434109 0.53229974 0.4625323\n",
            " 0.36089578 0.52196382 0.50559862 0.49009475 0.60981912 0.42377261\n",
            " 0.44272179 0.39965547 0.45822567 0.53660637 0.4203273  0.63738157\n",
            " 0.5994832  0.54349699 0.55211025 0.40654608 0.4788975  0.54608096\n",
            " 0.35228252 0.48751077 0.58914729 0.62704565 0.52024117 0.40568475\n",
            " 0.5796727  0.51679587 0.52971576 0.49009475 0.52196382 0.63393626\n",
            " 0.59689922 0.47200689 0.34797588 0.6873385  0.49784668 0.55727821\n",
            " 0.46339363 0.56503015 0.52971576 0.51421189 0.54866494 0.33419466\n",
            " 0.51248923 0.46339363 0.55469423 0.41085271 0.60120586 0.49956934\n",
            " 0.60809647 0.56761413 0.48664944 0.64427218 0.54005168 0.59000861\n",
            " 0.54694229 0.49870801 0.54952627 0.52799311 0.42635659 0.63135228\n",
            " 0.56847545 0.43669251 0.5047373  0.39793282 0.4918174  0.63135228\n",
            " 0.56330749 0.34366925 0.6546081  0.41343669 0.43066322 0.54866494\n",
            " 0.52024117 0.54177433 0.59086994 0.58225668 0.60637382 0.48406546\n",
            " 0.44099914 0.53832903 0.56847545 0.45822567 0.39879414 0.44358312\n",
            " 0.63479759 0.53488372 0.39793282 0.53057709 0.44444444 0.48923342\n",
            " 0.52799311 0.43669251 0.5047373  0.52540913 0.61843239 0.52971576\n",
            " 0.56416882 0.47459087 0.4952627  0.60378984 0.48062016 0.51765719\n",
            " 0.52971576 0.44186047 0.50732127 0.5994832  0.36003445 0.51162791\n",
            " 0.46942291 0.51593454 0.56589147 0.54091301 0.44444444 0.46080965\n",
            " 0.59345392 0.53402239 0.61068045 0.41860465 0.51937984 0.43755383\n",
            " 0.48234281 0.55986219 0.58139535 0.59000861 0.42894057 0.55297158\n",
            " 0.61154177 0.61584841 0.51248923 0.56503015 0.53229974 0.41860465\n",
            " 0.51851852 0.46770026 0.45219638 0.53660637 0.54091301 0.50732127\n",
            " 0.34022394 0.61929371 0.60034453 0.5667528  0.41085271 0.62015504\n",
            " 0.48406546 0.58225668 0.5047373  0.51593454 0.45650301 0.66063738\n",
            " 0.49009475 0.54349699 0.47459087 0.36003445 0.4754522  0.65977606\n",
            " 0.36434109 0.54091301 0.67355728 0.65977606 0.56847545 0.6089578\n",
            " 0.41343669 0.51421189 0.37726098 0.6089578  0.35486649 0.374677\n",
            " 0.67183463 0.64857881 0.4952627  0.55727821 0.31007752 0.37639966\n",
            " 0.49956934 0.52540913 0.69336779 0.38070629 0.51765719 0.47372954\n",
            " 0.5081826  0.59173127]\n",
            "The trained model has an aproximate error rate of 9.503555105708074 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the difference RMSE between the mean and the model is less, it is arguable that the model is not as accurate when predicting the number of collisions given the level of visibity as a predictor.\n",
        "\n",
        "Although the RMSE value indicates a weaker model, the error rate is lower indicating that there is a significant error increasing the RMSE."
      ],
      "metadata": {
        "id": "ntCQ4Pkefi3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure (SLP) -REMOVE?\n"
      ],
      "metadata": {
        "id": "O60cqN0x90SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0756b741-8c73-428d-f5f0-b3abdc5f5022",
        "id": "u-aXoGB4_v4s"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp = df.drop(columns=['collision_date', 'temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] != 2012]\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] < 2020]\n",
        "cols = df_slp['NUM_COLLISIONS']\n",
        "df_slp = df_slp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp[:6])\n",
        "df_slp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "f017cfcf-f652-4e82-fca1-82db68596ef0",
        "id": "xsAYkW7-ATbk"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da     slp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  1016.1             681\n",
            "51    5  2014   1  17  1014.8             589\n",
            "54    1  2016   1  25  1021.4             658\n",
            "55    5  2016   1  29   999.4             645\n",
            "58    5  2017   1  20  1015.5             605\n",
            "59    7  2013   1  13  1020.7             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da          slp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.999217  2016.000391     6.522114    15.719765  1016.777221   \n",
              "std       2.000783     2.000294     3.447986     8.796698     7.628429   \n",
              "min       1.000000  2013.000000     1.000000     1.000000   989.500000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000  1012.200000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000  1016.700000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000  1021.700000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000  1044.200000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.147162  \n",
              "std        100.268048  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f97516b-8974-4734-b988-d231dbe21137\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999217</td>\n",
              "      <td>2016.000391</td>\n",
              "      <td>6.522114</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000783</td>\n",
              "      <td>2.000294</td>\n",
              "      <td>3.447986</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f97516b-8974-4734-b988-d231dbe21137')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f97516b-8974-4734-b988-d231dbe21137 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f97516b-8974-4734-b988-d231dbe21137');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp.iloc[np.random.permutation(len(df_slp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51079937-639a-4055-e659-9da5aaa22489",
        "id": "I-drEIq1ATbu"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da     slp\n",
            "1403    4  2019   5  23  1022.9\n",
            "3128    5  2016  11  25  1023.9\n",
            "129     4  2014   1  30  1027.2\n",
            "727     4  2015   3  26  1016.9\n",
            "2842    2  2015  10  13  1003.5\n",
            "1800    5  2017   6  16  1018.9\n",
            "      day  year  mo  da     slp  NUM_COLLISIONS\n",
            "1403    4  2019   5  23  1022.9             722\n",
            "3128    5  2016  11  25  1023.9             455\n",
            "129     4  2014   1  30  1027.2             657\n",
            "727     4  2015   3  26  1016.9             525\n",
            "2842    2  2015  10  13  1003.5             724\n",
            "1800    5  2017   6  16  1018.9             657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f22ce8-2ceb-492a-b624-7ce0c689429b",
        "id": "pq6BF1zLATbv"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1403    722\n",
            "3128    455\n",
            "129     657\n",
            "727     525\n",
            "2842    724\n",
            "1800    657\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8294ea-4684-43bf-dceb-ff8b894ea6c6",
        "id": "pVdp0YKeATbv"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36221069-adfd-4e16-a408-5ad987b609ba",
        "id": "twTCPAjdATbv"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376dae1890>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.26518497, step = 1\n",
            "INFO:tensorflow:global_step/sec: 803.716\n",
            "INFO:tensorflow:loss = 0.0068272715, step = 101 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.684\n",
            "INFO:tensorflow:loss = 0.006221723, step = 201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.837\n",
            "INFO:tensorflow:loss = 0.008020048, step = 301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.413\n",
            "INFO:tensorflow:loss = 0.0055495854, step = 401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.16\n",
            "INFO:tensorflow:loss = 0.007677032, step = 501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.286\n",
            "INFO:tensorflow:loss = 0.004667553, step = 601 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.237\n",
            "INFO:tensorflow:loss = 0.0077134697, step = 701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.309\n",
            "INFO:tensorflow:loss = 0.006589991, step = 801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.174\n",
            "INFO:tensorflow:loss = 0.0062193703, step = 901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.86\n",
            "INFO:tensorflow:loss = 0.007949725, step = 1001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 913.698\n",
            "INFO:tensorflow:loss = 0.006715299, step = 1101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.028\n",
            "INFO:tensorflow:loss = 0.005412845, step = 1201 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.735\n",
            "INFO:tensorflow:loss = 0.0068558753, step = 1301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.44\n",
            "INFO:tensorflow:loss = 0.0068300166, step = 1401 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.73\n",
            "INFO:tensorflow:loss = 0.007831175, step = 1501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.207\n",
            "INFO:tensorflow:loss = 0.006943042, step = 1601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.936\n",
            "INFO:tensorflow:loss = 0.008177599, step = 1701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.913\n",
            "INFO:tensorflow:loss = 0.0062287124, step = 1801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.79\n",
            "INFO:tensorflow:loss = 0.008832496, step = 1901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.77\n",
            "INFO:tensorflow:loss = 0.008703797, step = 2001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.95\n",
            "INFO:tensorflow:loss = 0.006695399, step = 2101 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.011\n",
            "INFO:tensorflow:loss = 0.005493979, step = 2201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.25\n",
            "INFO:tensorflow:loss = 0.005757856, step = 2301 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.597\n",
            "INFO:tensorflow:loss = 0.0076254867, step = 2401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.691\n",
            "INFO:tensorflow:loss = 0.0061264266, step = 2501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.912\n",
            "INFO:tensorflow:loss = 0.006668539, step = 2601 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.1\n",
            "INFO:tensorflow:loss = 0.0062073176, step = 2701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.349\n",
            "INFO:tensorflow:loss = 0.009401159, step = 2801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.171\n",
            "INFO:tensorflow:loss = 0.008957837, step = 2901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.048\n",
            "INFO:tensorflow:loss = 0.0049695433, step = 3001 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 864.844\n",
            "INFO:tensorflow:loss = 0.008299775, step = 3101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.156\n",
            "INFO:tensorflow:loss = 0.0059870384, step = 3201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.75\n",
            "INFO:tensorflow:loss = 0.0049743364, step = 3301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.113\n",
            "INFO:tensorflow:loss = 0.007905588, step = 3401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.682\n",
            "INFO:tensorflow:loss = 0.0065214434, step = 3501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.13\n",
            "INFO:tensorflow:loss = 0.01007566, step = 3601 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.617\n",
            "INFO:tensorflow:loss = 0.008289837, step = 3701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.682\n",
            "INFO:tensorflow:loss = 0.007816302, step = 3801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.233\n",
            "INFO:tensorflow:loss = 0.006013807, step = 3901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.421\n",
            "INFO:tensorflow:loss = 0.0058460124, step = 4001 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 868.361\n",
            "INFO:tensorflow:loss = 0.006158975, step = 4101 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.936\n",
            "INFO:tensorflow:loss = 0.005158253, step = 4201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.389\n",
            "INFO:tensorflow:loss = 0.005558285, step = 4301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.692\n",
            "INFO:tensorflow:loss = 0.006144513, step = 4401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.456\n",
            "INFO:tensorflow:loss = 0.0073893387, step = 4501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 968.921\n",
            "INFO:tensorflow:loss = 0.0077959453, step = 4601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.835\n",
            "INFO:tensorflow:loss = 0.0073652915, step = 4701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.037\n",
            "INFO:tensorflow:loss = 0.0072922865, step = 4801 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.858\n",
            "INFO:tensorflow:loss = 0.007096657, step = 4901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 953.123\n",
            "INFO:tensorflow:loss = 0.007460785, step = 5001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.116\n",
            "INFO:tensorflow:loss = 0.007622525, step = 5101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.459\n",
            "INFO:tensorflow:loss = 0.006512071, step = 5201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.221\n",
            "INFO:tensorflow:loss = 0.0053998083, step = 5301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.078\n",
            "INFO:tensorflow:loss = 0.0060798707, step = 5401 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.094\n",
            "INFO:tensorflow:loss = 0.0062694224, step = 5501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.251\n",
            "INFO:tensorflow:loss = 0.0066407877, step = 5601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.491\n",
            "INFO:tensorflow:loss = 0.0069179675, step = 5701 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.852\n",
            "INFO:tensorflow:loss = 0.009532673, step = 5801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.216\n",
            "INFO:tensorflow:loss = 0.0046628052, step = 5901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 953.242\n",
            "INFO:tensorflow:loss = 0.006238603, step = 6001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.092\n",
            "INFO:tensorflow:loss = 0.009459603, step = 6101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.584\n",
            "INFO:tensorflow:loss = 0.008153668, step = 6201 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.626\n",
            "INFO:tensorflow:loss = 0.006910852, step = 6301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.906\n",
            "INFO:tensorflow:loss = 0.008110482, step = 6401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.505\n",
            "INFO:tensorflow:loss = 0.008734699, step = 6501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.477\n",
            "INFO:tensorflow:loss = 0.008026516, step = 6601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 918.947\n",
            "INFO:tensorflow:loss = 0.004879243, step = 6701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.164\n",
            "INFO:tensorflow:loss = 0.006809076, step = 6801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.726\n",
            "INFO:tensorflow:loss = 0.00902164, step = 6901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.642\n",
            "INFO:tensorflow:loss = 0.006289358, step = 7001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.545\n",
            "INFO:tensorflow:loss = 0.009525912, step = 7101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 864.183\n",
            "INFO:tensorflow:loss = 0.006972595, step = 7201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.888\n",
            "INFO:tensorflow:loss = 0.0062893317, step = 7301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.361\n",
            "INFO:tensorflow:loss = 0.0066667465, step = 7401 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.542\n",
            "INFO:tensorflow:loss = 0.006082625, step = 7501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.249\n",
            "INFO:tensorflow:loss = 0.006825344, step = 7601 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.886\n",
            "INFO:tensorflow:loss = 0.008619172, step = 7701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.274\n",
            "INFO:tensorflow:loss = 0.006234711, step = 7801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.424\n",
            "INFO:tensorflow:loss = 0.008127531, step = 7901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.629\n",
            "INFO:tensorflow:loss = 0.0065310225, step = 8001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.588\n",
            "INFO:tensorflow:loss = 0.006693021, step = 8101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 842.14\n",
            "INFO:tensorflow:loss = 0.0062081027, step = 8201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 993.174\n",
            "INFO:tensorflow:loss = 0.005693787, step = 8301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.179\n",
            "INFO:tensorflow:loss = 0.007754346, step = 8401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.175\n",
            "INFO:tensorflow:loss = 0.011507501, step = 8501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.785\n",
            "INFO:tensorflow:loss = 0.009028345, step = 8601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.092\n",
            "INFO:tensorflow:loss = 0.0058726673, step = 8701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.291\n",
            "INFO:tensorflow:loss = 0.008602882, step = 8801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.103\n",
            "INFO:tensorflow:loss = 0.008075595, step = 8901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.978\n",
            "INFO:tensorflow:loss = 0.005987255, step = 9001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.7\n",
            "INFO:tensorflow:loss = 0.0056231115, step = 9101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 867.795\n",
            "INFO:tensorflow:loss = 0.008550084, step = 9201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.455\n",
            "INFO:tensorflow:loss = 0.0060705994, step = 9301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.816\n",
            "INFO:tensorflow:loss = 0.0068952693, step = 9401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.84\n",
            "INFO:tensorflow:loss = 0.0071844086, step = 9501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.817\n",
            "INFO:tensorflow:loss = 0.0085958615, step = 9601 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.449\n",
            "INFO:tensorflow:loss = 0.008454528, step = 9701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.771\n",
            "INFO:tensorflow:loss = 0.0054241694, step = 9801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.822\n",
            "INFO:tensorflow:loss = 0.007897022, step = 9901 (0.117 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0058629713.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 108.62709815916816\n",
            "Just using average = 598.8703522504892 has RMSE of 98.99650903705167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c651a2-5499-4fec-aef3-4d32aae81b2d",
        "id": "RJS2QjMpATbw"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376dae5a90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.47726688 0.48780158 0.522086   0.49973044 0.4978588  0.5165646\n",
            " 0.44908002 0.4564626  0.47302002 0.47140738 0.46855244 0.4466032\n",
            " 0.47461522 0.5117271  0.5168502  0.5141059  0.4404212  0.48614636\n",
            " 0.51015514 0.477686   0.40357786 0.46565765 0.468483   0.41868368\n",
            " 0.43016425 0.5318547  0.5164944  0.49383643 0.50359315 0.488932\n",
            " 0.48779833 0.42585146 0.51008487 0.48235023 0.43231213 0.42731687\n",
            " 0.5124463  0.47948673 0.46733722 0.48665735 0.50703335 0.43136793\n",
            " 0.4666833  0.44735178 0.5113628  0.44484374 0.46621126 0.41841972\n",
            " 0.51697284 0.46732652 0.46199858 0.4679905  0.49869782 0.41903698\n",
            " 0.50828815 0.468562   0.43574688 0.42282537 0.45909116 0.43675506\n",
            " 0.4719419  0.4227706  0.47365773 0.4684898  0.43784475 0.457773\n",
            " 0.43989468 0.51391447 0.44584534 0.45944256 0.5285387  0.45349595\n",
            " 0.4289899  0.52143    0.48769963 0.50172585 0.48161307 0.51627636\n",
            " 0.5176956  0.43718195 0.48795906 0.4930749  0.5087599  0.47241846\n",
            " 0.46075448 0.43325692 0.5088914  0.48820147 0.49211678 0.5319146\n",
            " 0.4964381  0.4594566  0.44501963 0.46363097 0.47406918 0.47704294\n",
            " 0.4996571  0.40823546 0.4602503  0.48787442 0.4482706  0.47958937\n",
            " 0.50222    0.4588441  0.4875979  0.4531279  0.44072896 0.48521256\n",
            " 0.48229685 0.44345412 0.51899934 0.4757497  0.47632876 0.48290128\n",
            " 0.47864756 0.4369237  0.50291693 0.4990177  0.42056635 0.42948782\n",
            " 0.42800227 0.42897564 0.45484912 0.4264944  0.46802083 0.48664564\n",
            " 0.42306003 0.41626757 0.5056964  0.47082865 0.4343675  0.4132998\n",
            " 0.42416108 0.44312716 0.4420887  0.4565679  0.4466352  0.44337952\n",
            " 0.48642904 0.47120866 0.4134847  0.43285382 0.43483204 0.4624834\n",
            " 0.46321467 0.4218726  0.46264407 0.43341747 0.46446246 0.42588085\n",
            " 0.44919398 0.46118033 0.43597406 0.48236114 0.45880282 0.45061454\n",
            " 0.46996695 0.5279265  0.50284374 0.45389935 0.532975   0.4942658\n",
            " 0.41558042 0.42995134 0.4966696  0.50794953 0.5130365  0.4823732\n",
            " 0.45197904 0.41438916 0.4992732  0.5172681  0.46610418 0.48884577\n",
            " 0.47903025 0.491942   0.4955288  0.49567243 0.5216643  0.51589406\n",
            " 0.46256614 0.5105702  0.47206253 0.42177927 0.470555   0.47048604\n",
            " 0.45818397 0.49843284 0.5080351  0.4675929  0.4663132  0.41391286\n",
            " 0.46581244 0.4959747  0.42800504 0.46687722 0.47303754 0.5079862\n",
            " 0.49665385 0.45250148 0.45741326 0.48586828 0.4986429  0.44727698\n",
            " 0.4732627  0.4806246  0.4562289  0.5062588  0.46388948 0.4343377\n",
            " 0.50175494 0.40815803 0.4295051  0.47357687 0.45056817 0.4440685\n",
            " 0.44407105 0.46791184 0.49994218 0.46387565 0.4263563  0.4772104\n",
            " 0.43910015 0.45733678 0.41305596 0.4766966  0.44812015 0.4822189\n",
            " 0.4364933  0.4567891  0.4871747  0.45489982 0.4535656  0.47538075\n",
            " 0.45317563 0.4908413  0.4998137  0.4272629  0.4573803  0.47575313\n",
            " 0.46299678 0.46450627 0.48625982 0.45218018 0.44419196 0.49138367\n",
            " 0.41063422 0.49338034 0.48818564 0.42322287 0.49682432 0.480566\n",
            " 0.4884189  0.46044517 0.4449252  0.49022144 0.4505958  0.52751154\n",
            " 0.47844332 0.44909856 0.42561498 0.52251035 0.4440708  0.44612825\n",
            " 0.46389604 0.48840111 0.40990967 0.43517035 0.4451091  0.4471615\n",
            " 0.5155989  0.4757137  0.4596515  0.48186517 0.5101483  0.4694049\n",
            " 0.49230796 0.4762251  0.47715908 0.5117984  0.47242162 0.42901993\n",
            " 0.48153046 0.45836493 0.4714367  0.48376742 0.5297699  0.4456145\n",
            " 0.4837618  0.492592   0.42936403 0.43670154 0.46089372 0.4727749\n",
            " 0.44552875 0.4470004  0.46746403 0.46719465 0.44107154 0.49996027\n",
            " 0.5325529  0.5019538  0.45513368 0.5269022  0.4263937  0.41549024\n",
            " 0.48334903 0.48118263 0.4726377  0.41669208 0.47156477 0.5074707\n",
            " 0.48612544 0.468518   0.47618124 0.46945736 0.48143876 0.44499624\n",
            " 0.49681535 0.45174846 0.49035615 0.47726268 0.50573623 0.4278573\n",
            " 0.4622298  0.45915836 0.48697135 0.45758653 0.45591715 0.4912083\n",
            " 0.4526657  0.4401476  0.50884396 0.48563534 0.5119315  0.46478257\n",
            " 0.49023184 0.48803246 0.43413714 0.43135974 0.4657177  0.42273274\n",
            " 0.5243037  0.41596803 0.4465533  0.50679576 0.46766713 0.4280098\n",
            " 0.46191105 0.46740785 0.46833697 0.51963055 0.49552333 0.4725356\n",
            " 0.46159968 0.4559606  0.4855057  0.50107366 0.52375066 0.481122\n",
            " 0.45779175 0.51166594 0.4484218  0.46729827 0.48318967 0.41299337\n",
            " 0.48052782 0.5274435  0.52000797 0.47541887 0.52364486 0.5207316\n",
            " 0.4948628  0.4789929  0.43456402 0.4696181  0.5178248  0.51416224\n",
            " 0.45335704 0.47280222 0.4584853  0.48679566 0.47053054 0.45902985\n",
            " 0.45681158 0.41493925 0.5033403  0.5137075  0.5153303  0.5018478\n",
            " 0.4418296  0.48939914 0.4124402  0.48865303 0.4401939  0.50165415\n",
            " 0.44752577 0.44674245 0.45582196 0.44057482 0.52313787 0.5188454\n",
            " 0.4529126  0.5293594  0.496714   0.4305195  0.51421016 0.44364563\n",
            " 0.4327908  0.46929318 0.44955635 0.4873818  0.45887896 0.44147792\n",
            " 0.5002361  0.49006656 0.45697385 0.40350512 0.45651627 0.4698795\n",
            " 0.45038155 0.49995208 0.49768353 0.47992572 0.46346727 0.48610657\n",
            " 0.45189598 0.448752   0.43653333 0.478127   0.49451402 0.50108343\n",
            " 0.44919908 0.4394815  0.5085879  0.50590974 0.4723248  0.44343653\n",
            " 0.47157496 0.50593275 0.48876822 0.47977495 0.47518212 0.51408684\n",
            " 0.46991026 0.46814346 0.48037982 0.48239607 0.45328283 0.4508763\n",
            " 0.51617706 0.4185928  0.5212941  0.4632023  0.44736898 0.43197533\n",
            " 0.5070622  0.4486493  0.50683427 0.46887007 0.5151215  0.44153392\n",
            " 0.47439942 0.466617   0.41950727 0.39640376 0.5133943  0.46114218\n",
            " 0.43345156 0.45848438 0.44632867 0.51947105 0.44581217 0.45329517\n",
            " 0.5101229  0.43124953 0.47646767 0.43380252 0.43365452 0.49788848\n",
            " 0.44348556 0.47629058 0.43239236 0.43742755 0.42360365 0.5053838\n",
            " 0.4759275  0.4978773  0.4746476  0.49581015 0.49046656 0.48086074\n",
            " 0.5088343  0.44812924 0.51019615 0.47847965 0.4852427  0.44646376\n",
            " 0.44308078 0.5020517  0.47520474 0.47642586 0.45940313 0.46515045\n",
            " 0.51257503 0.4934778  0.51407486 0.46382546 0.45535904 0.5270476\n",
            " 0.50147676]\n",
            "[0.48148148 0.52799311 0.52713178 0.5667528  0.57019811 0.63307494\n",
            " 0.59431525 0.46942291 0.58053402 0.37984496 0.52540913 0.6416882\n",
            " 0.58225668 0.5081826  0.43152455 0.6089578  0.4754522  0.51248923\n",
            " 0.63049096 0.54866494 0.40913006 0.50904393 0.58139535 0.38329027\n",
            " 0.56330749 0.40999139 0.5667528  0.60809647 0.44444444 0.52196382\n",
            " 0.59000861 0.40913006 0.46770026 0.52024117 0.44444444 0.41774332\n",
            " 0.37898363 0.55986219 0.6546081  0.48062016 0.58656331 0.55986219\n",
            " 0.61412575 0.51507321 0.60120586 0.55900086 0.40310078 0.44530577\n",
            " 0.49009475 0.5374677  0.46167097 0.52971576 0.65719208 0.38845823\n",
            " 0.45047373 0.59776055 0.47200689 0.46597761 0.58656331 0.43410853\n",
            " 0.38415159 0.47114556 0.5211025  0.44444444 0.36950904 0.48062016\n",
            " 0.57364341 0.54091301 0.42204996 0.64857881 0.43496985 0.46942291\n",
            " 0.5245478  0.50215332 0.54866494 0.44702842 0.46511628 0.56503015\n",
            " 0.55211025 0.51851852 0.48923342 0.44702842 0.47803618 0.48837209\n",
            " 0.47803618 0.44702842 0.59259259 0.66063738 0.4918174  0.55469423\n",
            " 0.54952627 0.56244617 0.5796727  0.39448751 0.51593454 0.53229974\n",
            " 0.40568475 0.35745047 0.46683893 0.65633075 0.54005168 0.64857881\n",
            " 0.61757106 0.56244617 0.46683893 0.69853575 0.5994832  0.56933678\n",
            " 0.67011197 0.56330749 0.49009475 0.65202412 1.         0.59173127\n",
            " 0.5538329  0.48492679 0.61670973 0.60981912 0.46770026 0.47028424\n",
            " 0.39534884 0.41085271 0.51421189 0.38587425 0.56933678 0.49095607\n",
            " 0.416882   0.41429802 0.49698536 0.66925065 0.40310078 0.35400517\n",
            " 0.42463394 0.49095607 0.41085271 0.46339363 0.55641688 0.3910422\n",
            " 0.59517657 0.5667528  0.43927649 0.45650301 0.49009475 0.4461671\n",
            " 0.57278208 0.37726098 0.49354005 0.51851852 0.43238587 0.47803618\n",
            " 0.39362618 0.55124892 0.64857881 0.52799311 0.42635659 0.43496985\n",
            " 0.46597761 0.6089578  0.69939707 0.5960379  0.56072351 0.46597761\n",
            " 0.44530577 0.60465116 0.54952627 0.44875108 0.64944014 0.53402239\n",
            " 0.56072351 0.41515935 0.59173127 0.51851852 0.4332472  0.47631352\n",
            " 0.71748493 0.46425495 0.67011197 0.49009475 0.55727821 0.55986219\n",
            " 0.47975883 0.55297158 0.60034453 0.39965547 0.53316107 0.74677003\n",
            " 0.68303187 0.60206718 0.38070629 0.46683893 0.66063738 0.38845823\n",
            " 0.65546942 0.56072351 0.50732127 0.55727821 0.53143842 0.59517657\n",
            " 0.54091301 0.53660637 0.60292851 0.51421189 0.63565891 0.44272179\n",
            " 0.52971576 0.70801034 0.4788975  0.63996555 0.50215332 0.53574505\n",
            " 0.49267873 0.36692506 0.39879414 0.47286822 0.4461671  0.5047373\n",
            " 0.50387597 0.57708872 0.41429802 0.39276486 0.45908699 0.62360034\n",
            " 0.44444444 0.38845823 0.32816537 0.33936262 0.4918174  0.45736434\n",
            " 0.47459087 0.48751077 0.42894057 0.49870801 0.5374677  0.48664944\n",
            " 0.55900086 0.47975883 0.5667528  0.45047373 0.42807924 0.47372954\n",
            " 0.64771748 0.6089578  0.5047373  0.52713178 0.54349699 0.47286822\n",
            " 0.3453919  0.52885444 0.47717485 0.43496985 0.56330749 0.70111972\n",
            " 0.32213609 0.49267873 0.3910422  0.16192937 0.39793282 0.60809647\n",
            " 0.48923342 0.4788975  0.58053402 0.49870801 0.44702842 0.36434109\n",
            " 0.63996555 0.54263566 0.46597761 0.43927649 0.46511628 0.41860465\n",
            " 0.61154177 0.59689922 0.50990525 0.44272179 0.52885444 0.60034453\n",
            " 0.53229974 0.4952627  0.55641688 0.60292851 0.51507321 0.43583118\n",
            " 0.53316107 0.49784668 0.56847545 0.55727821 0.44702842 0.48062016\n",
            " 0.48148148 0.5994832  0.48492679 0.53316107 0.56330749 0.43927649\n",
            " 0.43066322 0.51421189 0.49956934 0.60292851 0.47975883 0.4496124\n",
            " 0.65030146 0.67011197 0.53574505 0.57450474 0.46167097 0.37812231\n",
            " 0.61154177 0.59345392 0.57364341 0.40913006 0.50301464 0.51162791\n",
            " 0.48234281 0.48751077 0.46856158 0.59689922 0.58742463 0.66063738\n",
            " 0.63738157 0.53832903 0.63479759 0.44186047 0.50301464 0.47459087\n",
            " 0.56330749 0.45305771 0.58828596 0.5245478  0.41343669 0.48664944\n",
            " 0.66838932 0.58828596 0.68475452 0.63738157 0.56416882 0.63221361\n",
            " 0.48664944 0.58139535 0.4203273  0.56416882 0.54263566 0.40913006\n",
            " 0.55297158 0.35917313 0.53660637 0.54694229 0.62962963 0.44013781\n",
            " 0.54349699 0.57105943 0.51507321 0.51248923 0.53574505 0.63996555\n",
            " 0.50990525 0.72782084 0.59431525 0.49870801 0.33763997 0.59086994\n",
            " 0.62704565 0.4754522  0.44099914 0.4625323  0.46683893 0.41257537\n",
            " 0.53574505 0.45822567 0.56933678 0.55555556 0.5503876  0.6124031\n",
            " 0.58914729 0.62618432 0.46942291 0.45219638 0.53143842 0.55555556\n",
            " 0.44358312 0.57622739 0.67183463 0.62015504 0.56589147 0.44530577\n",
            " 0.65891473 0.44444444 0.61498708 0.54952627 0.48751077 0.55900086\n",
            " 0.50559862 0.55986219 0.40482343 0.42204996 0.4788975  0.55211025\n",
            " 0.53574505 0.41343669 0.59776055 0.46425495 0.51421189 0.62618432\n",
            " 0.7037037  0.63479759 0.34969854 0.45478036 0.54521964 0.45305771\n",
            " 0.51765719 0.43669251 0.41860465 0.51248923 0.51937984 0.43755383\n",
            " 0.61584841 0.39018088 0.55297158 0.39534884 0.50387597 0.53919035\n",
            " 0.64857881 0.58397933 0.50215332 0.50990525 0.60378984 0.59259259\n",
            " 0.5374677  0.47459087 0.39190353 0.48148148 0.52368648 0.47631352\n",
            " 0.56847545 0.47286822 0.47286822 0.44788975 0.58225668 0.60809647\n",
            " 0.52627046 0.49956934 0.58828596 0.51593454 0.52196382 0.56416882\n",
            " 0.49095607 0.37726098 0.65374677 0.63738157 0.49009475 0.55469423\n",
            " 0.52627046 0.3875969  0.46425495 0.45822567 0.56933678 0.52196382\n",
            " 0.52196382 0.33936262 0.5503876  0.54608096 0.4203273  0.48492679\n",
            " 0.59345392 0.42980189 0.47717485 0.36950904 0.53402239 0.70456503\n",
            " 0.4754522  0.53832903 0.44875108 0.55986219 0.53143842 0.48062016\n",
            " 0.45047373 0.54005168 0.51937984 0.58139535 0.43238587 0.48923342\n",
            " 0.38845823 0.53402239 0.47631352 0.62187769 0.37639966 0.49784668\n",
            " 0.46511628 0.52024117 0.58570198 0.51421189 0.49612403 0.59086994\n",
            " 0.56158484 0.45822567 0.47459087 0.6287683  0.64341085 0.5245478\n",
            " 0.51507321 0.55555556 0.48578811 0.45822567 0.66666667 0.48406546\n",
            " 0.53660637 0.36175711 0.38845823 0.54177433 0.54349699 0.54694229\n",
            " 0.5211025 ]\n",
            "The trained model has an aproximate error rate of 54.10823065695698 which equates to 9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust - Remove"
      ],
      "metadata": {
        "id": "zwAKPA36B8U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d0a195-e103-4588-de83-70de0e86c709",
        "id": "Wphmfh3WB4mu"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "3     5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "11    3  2020   1  15     2020-01-15             508  43.9  38.3  1019.4   \n",
            "12    5  2021   1   1     2021-01-01             257  39.6  29.3  1029.3   \n",
            "14    2  2022   1  25     2022-01-25             235  41.6  31.8  1013.2   \n",
            "18    7  2021   1   3     2021-01-03             186  41.1  32.3  1018.0   \n",
            "19    4  2020   1   2     2020-01-02             413  39.6  28.9  1011.8   \n",
            "\n",
            "    visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "3    10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "11    8.2  ...  51.1  35.1  0.02  999.9    1             0                 0   \n",
            "12   10.0  ...  54.0  33.1  0.13  999.9    0             0                 0   \n",
            "14   10.0  ...  48.9  30.0  0.00  999.9    0             0                 0   \n",
            "18   10.0  ...  53.1  39.0  0.00  999.9    0             0                 0   \n",
            "19   10.0  ...  46.0  33.1  0.01  999.9    0             0                 0   \n",
            "\n",
            "    hail  thunder  tornado_funnel_cloud  \n",
            "3      0        0                     0  \n",
            "11     0        0                 10000  \n",
            "12     0        0                     0  \n",
            "14     0        0                     0  \n",
            "18     0        0                     0  \n",
            "19     0        0                     0  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] != 2012]\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] < 2020]\n",
        "cols = df_gust['NUM_COLLISIONS']\n",
        "df_gust = df_gust.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust[:6])\n",
        "df_gust.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "05c89291-05ac-4f3f-ec02-0741c6091e2b",
        "id": "pzygHg-iB4mv"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  gust  NUM_COLLISIONS\n",
            "74    7  2016   1  17  18.1             451\n",
            "76    4  2014   1   9  20.0             561\n",
            "79    6  2019   1  19  21.0             479\n",
            "80    7  2015   1  11  17.1             341\n",
            "83    4  2015   1  29  20.0             519\n",
            "85    7  2019   1  13  15.9             374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day        year           mo           da         gust  \\\n",
              "count  1629.000000  1629.00000  1629.000000  1629.000000  1629.000000   \n",
              "mean      4.024555  2015.91283     6.278699    15.702885    27.511602   \n",
              "std       1.989070     2.01341     3.747683     8.667634     7.366770   \n",
              "min       1.000000  2013.00000     1.000000     1.000000    14.000000   \n",
              "25%       2.000000  2014.00000     3.000000     8.000000    22.000000   \n",
              "50%       4.000000  2016.00000     6.000000    16.000000    26.000000   \n",
              "75%       6.000000  2018.00000    10.000000    23.000000    31.100000   \n",
              "max       7.000000  2019.00000    12.000000    31.000000    71.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     1629.000000  \n",
              "mean       596.513198  \n",
              "std        104.479660  \n",
              "min        188.000000  \n",
              "25%        526.000000  \n",
              "50%        597.000000  \n",
              "75%        663.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b452d01b-0527-45fd-827d-dbc5bf1e17a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.024555</td>\n",
              "      <td>2015.91283</td>\n",
              "      <td>6.278699</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.989070</td>\n",
              "      <td>2.01341</td>\n",
              "      <td>3.747683</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.00000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b452d01b-0527-45fd-827d-dbc5bf1e17a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b452d01b-0527-45fd-827d-dbc5bf1e17a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b452d01b-0527-45fd-827d-dbc5bf1e17a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust.iloc[np.random.permutation(len(df_gust))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89646736-66fb-43a2-fb97-34362a4e28d9",
        "id": "GA5wbn0KB4mw"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  gust\n",
            "2982    5  2013  10  11  26.0\n",
            "2581    1  2013   9  16  15.0\n",
            "562     5  2013   2   1  38.1\n",
            "3543    6  2018  12  15  27.0\n",
            "1508    1  2016   5  16  36.9\n",
            "3254    6  2018  11  17  26.0\n",
            "      day  year  mo  da  gust  NUM_COLLISIONS\n",
            "2982    5  2013  10  11  26.0             621\n",
            "2581    1  2013   9  16  15.0             570\n",
            "562     5  2013   2   1  38.1             574\n",
            "3543    6  2018  12  15  27.0             628\n",
            "1508    1  2016   5  16  36.9             742\n",
            "3254    6  2018  11  17  26.0             559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ca1f88-9744-472f-84dd-063d10c665fe",
        "id": "iJRw3cGvB4mw"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2982    621\n",
            "2581    570\n",
            "562     574\n",
            "3543    628\n",
            "1508    742\n",
            "3254    559\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e11f2c-895a-40f0-e1a3-fb5107351327",
        "id": "aZCjpnyzB4mw"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1303\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822c2649-813b-4501-c6d7-3b566b80cdad",
        "id": "wrbkpT05B4mx"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d9be9d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27558836, step = 1\n",
            "INFO:tensorflow:global_step/sec: 903.246\n",
            "INFO:tensorflow:loss = 0.007470894, step = 101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.447\n",
            "INFO:tensorflow:loss = 0.0062682787, step = 201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.84\n",
            "INFO:tensorflow:loss = 0.0064116335, step = 301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.8\n",
            "INFO:tensorflow:loss = 0.007871846, step = 401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1062.38\n",
            "INFO:tensorflow:loss = 0.007480868, step = 501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.663\n",
            "INFO:tensorflow:loss = 0.006258694, step = 601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.035\n",
            "INFO:tensorflow:loss = 0.010331571, step = 701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.656\n",
            "INFO:tensorflow:loss = 0.007229491, step = 801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.663\n",
            "INFO:tensorflow:loss = 0.008765852, step = 901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.792\n",
            "INFO:tensorflow:loss = 0.004372893, step = 1001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.771\n",
            "INFO:tensorflow:loss = 0.008323256, step = 1101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.724\n",
            "INFO:tensorflow:loss = 0.008140288, step = 1201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1054.86\n",
            "INFO:tensorflow:loss = 0.0075136153, step = 1301 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.965\n",
            "INFO:tensorflow:loss = 0.006137577, step = 1401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.398\n",
            "INFO:tensorflow:loss = 0.0070282957, step = 1501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.345\n",
            "INFO:tensorflow:loss = 0.004829711, step = 1601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.691\n",
            "INFO:tensorflow:loss = 0.008967444, step = 1701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.158\n",
            "INFO:tensorflow:loss = 0.007264603, step = 1801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.951\n",
            "INFO:tensorflow:loss = 0.007023784, step = 1901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.8\n",
            "INFO:tensorflow:loss = 0.006740228, step = 2001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.345\n",
            "INFO:tensorflow:loss = 0.005623405, step = 2101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.771\n",
            "INFO:tensorflow:loss = 0.009534148, step = 2201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.435\n",
            "INFO:tensorflow:loss = 0.009295654, step = 2301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.298\n",
            "INFO:tensorflow:loss = 0.0067506954, step = 2401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.811\n",
            "INFO:tensorflow:loss = 0.008265687, step = 2501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 938.84\n",
            "INFO:tensorflow:loss = 0.0064832834, step = 2601 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.838\n",
            "INFO:tensorflow:loss = 0.00990185, step = 2701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.689\n",
            "INFO:tensorflow:loss = 0.008406195, step = 2801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.16\n",
            "INFO:tensorflow:loss = 0.009765004, step = 2901 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1055.37\n",
            "INFO:tensorflow:loss = 0.00622205, step = 3001 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.117\n",
            "INFO:tensorflow:loss = 0.009837591, step = 3101 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.621\n",
            "INFO:tensorflow:loss = 0.013230378, step = 3201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.515\n",
            "INFO:tensorflow:loss = 0.00617222, step = 3301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.19\n",
            "INFO:tensorflow:loss = 0.0055796513, step = 3401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.44\n",
            "INFO:tensorflow:loss = 0.0053715324, step = 3501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1064.63\n",
            "INFO:tensorflow:loss = 0.005821191, step = 3601 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.41\n",
            "INFO:tensorflow:loss = 0.005637561, step = 3701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.313\n",
            "INFO:tensorflow:loss = 0.007797161, step = 3801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.562\n",
            "INFO:tensorflow:loss = 0.009202038, step = 3901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.512\n",
            "INFO:tensorflow:loss = 0.0067554186, step = 4001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.382\n",
            "INFO:tensorflow:loss = 0.0063358545, step = 4101 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.041\n",
            "INFO:tensorflow:loss = 0.006259204, step = 4201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.94\n",
            "INFO:tensorflow:loss = 0.0034261188, step = 4301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.871\n",
            "INFO:tensorflow:loss = 0.0065611294, step = 4401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.456\n",
            "INFO:tensorflow:loss = 0.007401755, step = 4501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.929\n",
            "INFO:tensorflow:loss = 0.006998108, step = 4601 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.344\n",
            "INFO:tensorflow:loss = 0.007658053, step = 4701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.569\n",
            "INFO:tensorflow:loss = 0.008427399, step = 4801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.822\n",
            "INFO:tensorflow:loss = 0.006571062, step = 4901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.05\n",
            "INFO:tensorflow:loss = 0.007053672, step = 5001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.589\n",
            "INFO:tensorflow:loss = 0.0072366446, step = 5101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.582\n",
            "INFO:tensorflow:loss = 0.0071128006, step = 5201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.642\n",
            "INFO:tensorflow:loss = 0.0067223976, step = 5301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.565\n",
            "INFO:tensorflow:loss = 0.006595244, step = 5401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.838\n",
            "INFO:tensorflow:loss = 0.007065324, step = 5501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.656\n",
            "INFO:tensorflow:loss = 0.0069376575, step = 5601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.85\n",
            "INFO:tensorflow:loss = 0.01051192, step = 5701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 953.886\n",
            "INFO:tensorflow:loss = 0.00726334, step = 5801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.743\n",
            "INFO:tensorflow:loss = 0.009724457, step = 5901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 947.514\n",
            "INFO:tensorflow:loss = 0.0068779076, step = 6001 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.46\n",
            "INFO:tensorflow:loss = 0.0055828057, step = 6101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.993\n",
            "INFO:tensorflow:loss = 0.0077273017, step = 6201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.161\n",
            "INFO:tensorflow:loss = 0.0061955303, step = 6301 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014\n",
            "INFO:tensorflow:loss = 0.008006364, step = 6401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.706\n",
            "INFO:tensorflow:loss = 0.0070661823, step = 6501 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.859\n",
            "INFO:tensorflow:loss = 0.0074101966, step = 6601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.791\n",
            "INFO:tensorflow:loss = 0.0080761425, step = 6701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.2\n",
            "INFO:tensorflow:loss = 0.006601657, step = 6801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.443\n",
            "INFO:tensorflow:loss = 0.005697962, step = 6901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.59\n",
            "INFO:tensorflow:loss = 0.008612346, step = 7001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 900.796\n",
            "INFO:tensorflow:loss = 0.007175696, step = 7101 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.17\n",
            "INFO:tensorflow:loss = 0.0075305393, step = 7201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.662\n",
            "INFO:tensorflow:loss = 0.0071521373, step = 7301 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.933\n",
            "INFO:tensorflow:loss = 0.0067411633, step = 7401 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.503\n",
            "INFO:tensorflow:loss = 0.008642627, step = 7501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.971\n",
            "INFO:tensorflow:loss = 0.0043583782, step = 7601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.735\n",
            "INFO:tensorflow:loss = 0.00580257, step = 7701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1016.19\n",
            "INFO:tensorflow:loss = 0.00543046, step = 7801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.939\n",
            "INFO:tensorflow:loss = 0.006018004, step = 7901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 938.254\n",
            "INFO:tensorflow:loss = 0.0069521545, step = 8001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.39\n",
            "INFO:tensorflow:loss = 0.006418401, step = 8101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.721\n",
            "INFO:tensorflow:loss = 0.0069789607, step = 8201 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.059\n",
            "INFO:tensorflow:loss = 0.007831062, step = 8301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.614\n",
            "INFO:tensorflow:loss = 0.0065567037, step = 8401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.303\n",
            "INFO:tensorflow:loss = 0.0071967, step = 8501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1029.47\n",
            "INFO:tensorflow:loss = 0.0065564485, step = 8601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.728\n",
            "INFO:tensorflow:loss = 0.002839495, step = 8701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.26\n",
            "INFO:tensorflow:loss = 0.0074307555, step = 8801 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.994\n",
            "INFO:tensorflow:loss = 0.0060974853, step = 8901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.503\n",
            "INFO:tensorflow:loss = 0.006318315, step = 9001 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.707\n",
            "INFO:tensorflow:loss = 0.0059175924, step = 9101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.112\n",
            "INFO:tensorflow:loss = 0.0062494148, step = 9201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 897.742\n",
            "INFO:tensorflow:loss = 0.007903458, step = 9301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.971\n",
            "INFO:tensorflow:loss = 0.006867126, step = 9401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.873\n",
            "INFO:tensorflow:loss = 0.0073884344, step = 9501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.368\n",
            "INFO:tensorflow:loss = 0.006208919, step = 9601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.505\n",
            "INFO:tensorflow:loss = 0.0068699582, step = 9701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.43\n",
            "INFO:tensorflow:loss = 0.010818249, step = 9801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.863\n",
            "INFO:tensorflow:loss = 0.006359162, step = 9901 (0.110 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0067077037.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 96.1909809048557\n",
            "Just using average = 597.1795855717575 has RMSE of 104.64943497722479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84167a1f-0ddc-4038-c3f2-7c615cc510ba",
        "id": "ByVWr9PzB4mx"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d886f50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n",
            "[0.53027797 0.55667156 0.5268498  0.4796487  0.4919054  0.45642954\n",
            " 0.54545987 0.48299107 0.50824106 0.46172765 0.4393457  0.5505935\n",
            " 0.47052285 0.47876197 0.5632546  0.4459617  0.51585454 0.49418178\n",
            " 0.49783853 0.49893972 0.52450716 0.5533219  0.48155633 0.4886029\n",
            " 0.505243   0.48405573 0.53516006 0.5608611  0.4821213  0.46209013\n",
            " 0.5043734  0.5131686  0.46321678 0.45029688 0.560638   0.56349164\n",
            " 0.4477465  0.52382034 0.5538842  0.5271109  0.5515485  0.4847281\n",
            " 0.5729917  0.48477197 0.5097766  0.47860855 0.522906   0.51747274\n",
            " 0.5238774  0.5658221  0.5389643  0.44475475 0.5190162  0.47534493\n",
            " 0.5306313  0.5361755  0.4994357  0.48724335 0.49996743 0.5183902\n",
            " 0.5628576  0.52602375 0.52806336 0.49655616 0.49452692 0.48214036\n",
            " 0.5811218  0.5414016  0.5192875  0.57000774 0.49954674 0.5249707\n",
            " 0.5323206  0.48616177 0.5296994  0.49164206 0.5160071  0.502125\n",
            " 0.4567406  0.4957736  0.5515521  0.56200147 0.5380638  0.45850673\n",
            " 0.5174355  0.50018156 0.5272658  0.49663797 0.55507964 0.4846953\n",
            " 0.5020305  0.5061655  0.5189823  0.48917207 0.5591547  0.45828003\n",
            " 0.53535146 0.5730398  0.5331031  0.44454038 0.5691319  0.53577757\n",
            " 0.46892774 0.46987396 0.43301585 0.56205815 0.45953602 0.5115662\n",
            " 0.51936346 0.47061566 0.5321694  0.51899505 0.53231573 0.5211824\n",
            " 0.53839135 0.4945016  0.5242626  0.5087382  0.46422195 0.558712\n",
            " 0.4699358  0.48401105 0.54469585 0.5301407  0.49117142 0.5492872\n",
            " 0.5013996  0.45137078 0.4577232  0.47556406 0.48497707 0.47209874\n",
            " 0.54418236 0.5657627  0.51525253 0.49426126 0.542999   0.48922876\n",
            " 0.5397842  0.5406068  0.4755727  0.5231304  0.55302584 0.51800644\n",
            " 0.5568315  0.55248696 0.5080832  0.4798383  0.55594134 0.45113176\n",
            " 0.5416601  0.5447673  0.56439066 0.45727724 0.4535882  0.44596264\n",
            " 0.48446473 0.5634962  0.4724238  0.5770331  0.50983685 0.5174945\n",
            " 0.46461743 0.53171474 0.5203348  0.54112893 0.53535783 0.53502965\n",
            " 0.55990666 0.5389683  0.52345914 0.4983209  0.47014478 0.5344836\n",
            " 0.5713428  0.53254735 0.46284488 0.4830373  0.5593751  0.5134359\n",
            " 0.505442   0.5595512  0.45064884 0.4751181  0.4925721  0.5779094\n",
            " 0.5432509  0.5087653  0.53157103 0.5329545  0.45902807 0.5131563\n",
            " 0.55184054 0.52350825 0.5473563  0.53238684 0.4665547  0.48530003\n",
            " 0.48055345 0.516736   0.48730764 0.48729637 0.4432944  0.53501886\n",
            " 0.4830715  0.4888392  0.4814767  0.5423217  0.56574225 0.5075328\n",
            " 0.57272196 0.46432    0.55765796 0.56803256 0.5215873  0.5012108\n",
            " 0.50209606 0.5105099  0.50400215 0.54604155 0.55498874 0.5082776\n",
            " 0.49122414 0.5332759  0.48941526 0.5202973  0.49727395 0.4983185\n",
            " 0.5324335  0.48833278 0.49086148 0.51002717 0.5049243  0.48259786\n",
            " 0.560162   0.5431036  0.45944738 0.4944375  0.4656907  0.54616505\n",
            " 0.5223909  0.5527404  0.5392617  0.54389066 0.5070984  0.51107883\n",
            " 0.45381558 0.5182199  0.52089417 0.52395415 0.5557861  0.52984214\n",
            " 0.5220294  0.51934266 0.5588697  0.55964965 0.52083415 0.4990922\n",
            " 0.4860542  0.5033646  0.45124325 0.5509838  0.5629951  0.54424214\n",
            " 0.49398404 0.5170839  0.5066882  0.555607   0.5252931  0.5112879\n",
            " 0.51377714 0.545934   0.5369252  0.5022031  0.45361573 0.5218657\n",
            " 0.55508727 0.5131447  0.4732863  0.458568   0.5253424  0.53861165\n",
            " 0.49097243 0.5274446  0.5495784  0.5512626  0.5021504  0.5247857\n",
            " 0.54284906 0.49492642 0.5358202  0.56260175 0.481115   0.5281439\n",
            " 0.5029959  0.5573152  0.47962022 0.48704073 0.5010309  0.54694927\n",
            " 0.5090458  0.5257369  0.45368075 0.5107185  0.48693073 0.51534915\n",
            " 0.48906338 0.49277097 0.46568668 0.52417475 0.4559076  0.5215479\n",
            " 0.52192473 0.569622   0.5197788  0.50797254 0.49617937 0.5152583\n",
            " 0.546777   0.56706756 0.5364109  0.46910146 0.45009583 0.5341854\n",
            " 0.56645685 0.5760163 ]\n",
            "[0.59259259 0.46856158 0.59173127 0.52282515 0.42894057 0.35486649\n",
            " 0.38070629 0.58828596 0.5503876  0.34797588 0.34969854 0.58225668\n",
            " 0.43669251 0.54091301 0.67700258 0.37639966 0.57364341 0.55641688\n",
            " 0.51593454 0.46942291 0.416882   0.59345392 0.44186047 0.65030146\n",
            " 0.56847545 0.62704565 0.57364341 0.6089578  0.45564169 0.4005168\n",
            " 0.51937984 0.70111972 0.41257537 0.34453058 0.63824289 0.45047373\n",
            " 0.45305771 0.63738157 0.54952627 0.44013781 0.58828596 0.51507321\n",
            " 0.51507321 0.44702842 0.56761413 0.54005168 0.63307494 0.57278208\n",
            " 0.64685616 0.5503876  0.52799311 0.38587425 0.50301464 0.50301464\n",
            " 0.34280792 0.44702842 0.75107666 0.4918174  0.48664944 0.63824289\n",
            " 0.56933678 0.48492679 0.47459087 0.50129199 0.62704565 0.43496985\n",
            " 0.42807924 0.53660637 0.54005168 0.63996555 0.66322136 0.44186047\n",
            " 0.58828596 0.49095607 0.50215332 0.4788975  0.49784668 0.54177433\n",
            " 0.45650301 0.44099914 0.59862188 0.48406546 0.57019811 0.43927649\n",
            " 0.52540913 0.60034453 0.53402239 0.44186047 0.48406546 0.44702842\n",
            " 0.5960379  0.4788975  0.60120586 0.59000861 0.52282515 0.39362618\n",
            " 0.42291128 0.5960379  0.49440138 0.33850129 0.46339363 0.5374677\n",
            " 0.50559862 0.34366925 0.44530577 0.5994832  0.37209302 0.51162791\n",
            " 0.36950904 0.64685616 0.6089578  0.52799311 0.91731266 0.55211025\n",
            " 0.51421189 0.44358312 0.37726098 0.52799311 0.52196382 0.44358312\n",
            " 0.50990525 0.47459087 0.51593454 0.51679587 0.5047373  0.52971576\n",
            " 0.5245478  0.40482343 0.40568475 0.51765719 0.46339363 0.46511628\n",
            " 0.58484065 0.57450474 0.43238587 0.68303187 0.57536606 0.39448751\n",
            " 0.44875108 0.58397933 0.44272179 0.58914729 0.55727821 0.59689922\n",
            " 0.44875108 0.45908699 0.45736434 0.51593454 0.42549526 0.40137812\n",
            " 0.43152455 0.49354005 0.53402239 0.36692506 0.34711456 0.34969854\n",
            " 0.47286822 0.60551249 0.43410853 0.44702842 0.71490095 0.5667528\n",
            " 0.44788975 0.42204996 0.53057709 0.47631352 0.60809647 0.4788975\n",
            " 0.47286822 0.38931955 0.52971576 0.46942291 0.55900086 0.48751077\n",
            " 0.60637382 0.54866494 0.42980189 0.47717485 0.59086994 0.55641688\n",
            " 0.48148148 0.51248923 0.38845823 0.5374677  0.49956934 0.53229974\n",
            " 0.52368648 0.56847545 0.64513351 0.45391904 0.42635659 0.54521964\n",
            " 0.64771748 0.54091301 0.64513351 0.61154177 0.2962963  0.51851852\n",
            " 0.59259259 0.5538329  0.4788975  0.39793282 0.39965547 0.53574505\n",
            " 0.40826873 0.53919035 0.57536606 0.47459087 0.40999139 0.50215332\n",
            " 0.47631352 0.53402239 0.62446167 0.54694229 0.48148148 0.50645995\n",
            " 0.4788975  0.52196382 0.48923342 0.56933678 0.55641688 0.64341085\n",
            " 0.49870801 0.43927649 0.51421189 0.45822567 0.54349699 0.54349699\n",
            " 0.6416882  0.44875108 0.47286822 0.45219638 0.59259259 0.44444444\n",
            " 0.6089578  0.48923342 0.38587425 0.54177433 0.46770026 0.58139535\n",
            " 0.51248923 0.70542636 0.57622739 0.45650301 0.43238587 0.62015504\n",
            " 0.36864772 0.49440138 0.58656331 0.57450474 0.5796727  0.59086994\n",
            " 0.46080965 0.67011197 0.59862188 0.55211025 0.45478036 0.52971576\n",
            " 0.45391904 0.5047373  0.37726098 0.5211025  0.51248923 0.2213609\n",
            " 0.5047373  0.46080965 0.64341085 0.4039621  0.56761413 0.7329888\n",
            " 0.49009475 0.60292851 0.52024117 0.5538329  0.34022394 0.47803618\n",
            " 0.51851852 0.41343669 0.49698536 0.31093885 0.60292851 0.51765719\n",
            " 0.53057709 0.43669251 0.55297158 0.51765719 0.70456503 0.64082687\n",
            " 0.54694229 0.54005168 0.43152455 0.61154177 0.41774332 0.41429802\n",
            " 0.63910422 0.64944014 0.37984496 0.65202412 0.58742463 0.61068045\n",
            " 0.45564169 0.59086994 0.40913006 0.49784668 0.41085271 0.70542636\n",
            " 0.42894057 0.63824289 0.41774332 0.55555556 0.33850129 0.54435831\n",
            " 0.53574505 0.57364341 0.50732127 0.4496124  0.42635659 0.66838932\n",
            " 0.59259259 0.58570198 0.37639966 0.36003445 0.42549526 0.42894057\n",
            " 0.55986219 0.47028424]\n",
            "The trained model has an aproximate error rate of -1.9899369828357287 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (Mxpsd) -Remove"
      ],
      "metadata": {
        "id": "OxfDYOaGD1Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803ae559-1994-4de0-be63-7cee0846b16c",
        "id": "9TtzqELvE-aN"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','gust','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] != 2012]\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] < 2020]\n",
        "cols = df_mxpsd['NUM_COLLISIONS']\n",
        "df_mxpsd = df_mxpsd.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd[:6])\n",
        "df_mxpsd.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b696b867-23c6-4957-85bd-5602952871c9",
        "id": "O_x5VoUvE-aO"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "49    4  2016   1  28    8.9             681\n",
            "51    5  2014   1  17    8.9             589\n",
            "54    1  2016   1  25    8.9             658\n",
            "55    5  2016   1  29    9.9             645\n",
            "58    5  2017   1  20    9.9             605\n",
            "59    7  2013   1  13    9.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da        mxpsd  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      3.999608  2016.001567     6.520564    15.737172    17.240110   \n",
              "std       2.001469     2.000587     3.449204     8.797367     5.858333   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     5.100000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    13.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    15.900000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    20.000000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    49.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2553.000000  \n",
              "mean       599.033686  \n",
              "std        100.284761  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a069b0c3-8119-41f2-b8af-a2e75d99519d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999608</td>\n",
              "      <td>2016.001567</td>\n",
              "      <td>6.520564</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.001469</td>\n",
              "      <td>2.000587</td>\n",
              "      <td>3.449204</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a069b0c3-8119-41f2-b8af-a2e75d99519d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a069b0c3-8119-41f2-b8af-a2e75d99519d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a069b0c3-8119-41f2-b8af-a2e75d99519d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd.iloc[np.random.permutation(len(df_mxpsd))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8fa352-6459-4c42-f001-79491a5720cc",
        "id": "oEaMTRvFE-aO"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  mxpsd\n",
            "3592    7  2018  12  23   20.0\n",
            "3649    4  2014  12  11   26.0\n",
            "1905    6  2018   7  14    9.9\n",
            "2353    3  2016   8   3   14.0\n",
            "3427    7  2019  12  29   11.1\n",
            "754     7  2019   3  31   18.1\n",
            "      day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "3592    7  2018  12  23   20.0             505\n",
            "3649    4  2014  12  11   26.0             669\n",
            "1905    6  2018   7  14    9.9             577\n",
            "2353    3  2016   8   3   14.0             687\n",
            "3427    7  2019  12  29   11.1             384\n",
            "754     7  2019   3  31   18.1             450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2480d0a5-caf6-4fdd-bb58-c8deb1cc0095",
        "id": "2IjTestaE-aO"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3592    505\n",
            "3649    669\n",
            "1905    577\n",
            "2353    687\n",
            "3427    384\n",
            "754     450\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0da267-8fb0-46bb-c6f2-15745cf7272c",
        "id": "ihCguRRBE-aO"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2042\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16dedebc-2ced-4b0b-88d7-c18a5abbb2c2",
        "id": "-hI4RaYNE-aP"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d9b71d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.26268768, step = 1\n",
            "INFO:tensorflow:global_step/sec: 892.168\n",
            "INFO:tensorflow:loss = 0.006546222, step = 101 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.994\n",
            "INFO:tensorflow:loss = 0.007897538, step = 201 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.597\n",
            "INFO:tensorflow:loss = 0.0066778315, step = 301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.81\n",
            "INFO:tensorflow:loss = 0.0064044576, step = 401 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.345\n",
            "INFO:tensorflow:loss = 0.008723834, step = 501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.9\n",
            "INFO:tensorflow:loss = 0.005981509, step = 601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.405\n",
            "INFO:tensorflow:loss = 0.005915991, step = 701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 918.748\n",
            "INFO:tensorflow:loss = 0.0072831833, step = 801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.869\n",
            "INFO:tensorflow:loss = 0.008279091, step = 901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.768\n",
            "INFO:tensorflow:loss = 0.005608087, step = 1001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.062\n",
            "INFO:tensorflow:loss = 0.006798896, step = 1101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.235\n",
            "INFO:tensorflow:loss = 0.0071546785, step = 1201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.891\n",
            "INFO:tensorflow:loss = 0.006549864, step = 1301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.748\n",
            "INFO:tensorflow:loss = 0.0068046534, step = 1401 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.924\n",
            "INFO:tensorflow:loss = 0.008331898, step = 1501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.401\n",
            "INFO:tensorflow:loss = 0.009167654, step = 1601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.499\n",
            "INFO:tensorflow:loss = 0.0068994113, step = 1701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.037\n",
            "INFO:tensorflow:loss = 0.007182869, step = 1801 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.243\n",
            "INFO:tensorflow:loss = 0.0061426833, step = 1901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.368\n",
            "INFO:tensorflow:loss = 0.007931329, step = 2001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.305\n",
            "INFO:tensorflow:loss = 0.006997345, step = 2101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.884\n",
            "INFO:tensorflow:loss = 0.006423476, step = 2201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.666\n",
            "INFO:tensorflow:loss = 0.008079788, step = 2301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.853\n",
            "INFO:tensorflow:loss = 0.007194587, step = 2401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.934\n",
            "INFO:tensorflow:loss = 0.009706568, step = 2501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.587\n",
            "INFO:tensorflow:loss = 0.0063240346, step = 2601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.559\n",
            "INFO:tensorflow:loss = 0.0060525383, step = 2701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.825\n",
            "INFO:tensorflow:loss = 0.0057820436, step = 2801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.946\n",
            "INFO:tensorflow:loss = 0.007583695, step = 2901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.945\n",
            "INFO:tensorflow:loss = 0.005942163, step = 3001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.403\n",
            "INFO:tensorflow:loss = 0.008137941, step = 3101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.639\n",
            "INFO:tensorflow:loss = 0.0062538553, step = 3201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.952\n",
            "INFO:tensorflow:loss = 0.00641933, step = 3301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.135\n",
            "INFO:tensorflow:loss = 0.007062899, step = 3401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 918.002\n",
            "INFO:tensorflow:loss = 0.0062094526, step = 3501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.797\n",
            "INFO:tensorflow:loss = 0.005903562, step = 3601 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 838.17\n",
            "INFO:tensorflow:loss = 0.006216595, step = 3701 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 1021.26\n",
            "INFO:tensorflow:loss = 0.0057010464, step = 3801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.166\n",
            "INFO:tensorflow:loss = 0.00535996, step = 3901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.15\n",
            "INFO:tensorflow:loss = 0.0057067806, step = 4001 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.748\n",
            "INFO:tensorflow:loss = 0.0074858535, step = 4101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.73\n",
            "INFO:tensorflow:loss = 0.007567904, step = 4201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.815\n",
            "INFO:tensorflow:loss = 0.005117761, step = 4301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.579\n",
            "INFO:tensorflow:loss = 0.0047466354, step = 4401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.689\n",
            "INFO:tensorflow:loss = 0.005949651, step = 4501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.866\n",
            "INFO:tensorflow:loss = 0.0062347157, step = 4601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.835\n",
            "INFO:tensorflow:loss = 0.007415131, step = 4701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.111\n",
            "INFO:tensorflow:loss = 0.005631056, step = 4801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.063\n",
            "INFO:tensorflow:loss = 0.0061459295, step = 4901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 834.755\n",
            "INFO:tensorflow:loss = 0.007546136, step = 5001 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 858.834\n",
            "INFO:tensorflow:loss = 0.0057442146, step = 5101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.001\n",
            "INFO:tensorflow:loss = 0.006206177, step = 5201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.415\n",
            "INFO:tensorflow:loss = 0.0063183513, step = 5301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.399\n",
            "INFO:tensorflow:loss = 0.0067721885, step = 5401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.177\n",
            "INFO:tensorflow:loss = 0.008517642, step = 5501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 934.678\n",
            "INFO:tensorflow:loss = 0.009056281, step = 5601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.702\n",
            "INFO:tensorflow:loss = 0.00756969, step = 5701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1030.79\n",
            "INFO:tensorflow:loss = 0.0068251817, step = 5801 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.866\n",
            "INFO:tensorflow:loss = 0.0067017814, step = 5901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.38\n",
            "INFO:tensorflow:loss = 0.005550414, step = 6001 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 839.294\n",
            "INFO:tensorflow:loss = 0.006368121, step = 6101 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.876\n",
            "INFO:tensorflow:loss = 0.005539944, step = 6201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 900.819\n",
            "INFO:tensorflow:loss = 0.0051452187, step = 6301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.268\n",
            "INFO:tensorflow:loss = 0.008302239, step = 6401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.885\n",
            "INFO:tensorflow:loss = 0.008347798, step = 6501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.031\n",
            "INFO:tensorflow:loss = 0.0073040416, step = 6601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.83\n",
            "INFO:tensorflow:loss = 0.0067514274, step = 6701 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.265\n",
            "INFO:tensorflow:loss = 0.005740881, step = 6801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.839\n",
            "INFO:tensorflow:loss = 0.0064105787, step = 6901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.802\n",
            "INFO:tensorflow:loss = 0.005130865, step = 7001 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.609\n",
            "INFO:tensorflow:loss = 0.0066127274, step = 7101 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1044.82\n",
            "INFO:tensorflow:loss = 0.006006087, step = 7201 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.251\n",
            "INFO:tensorflow:loss = 0.00702502, step = 7301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.762\n",
            "INFO:tensorflow:loss = 0.006215384, step = 7401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.045\n",
            "INFO:tensorflow:loss = 0.006247554, step = 7501 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.56\n",
            "INFO:tensorflow:loss = 0.005362872, step = 7601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.354\n",
            "INFO:tensorflow:loss = 0.0067122644, step = 7701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.2\n",
            "INFO:tensorflow:loss = 0.0069476217, step = 7801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.425\n",
            "INFO:tensorflow:loss = 0.006085987, step = 7901 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.492\n",
            "INFO:tensorflow:loss = 0.0077877743, step = 8001 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.172\n",
            "INFO:tensorflow:loss = 0.006701583, step = 8101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.925\n",
            "INFO:tensorflow:loss = 0.006248279, step = 8201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.018\n",
            "INFO:tensorflow:loss = 0.006694734, step = 8301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.849\n",
            "INFO:tensorflow:loss = 0.0062842844, step = 8401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.117\n",
            "INFO:tensorflow:loss = 0.0060966536, step = 8501 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.337\n",
            "INFO:tensorflow:loss = 0.0049521755, step = 8601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.517\n",
            "INFO:tensorflow:loss = 0.0049549914, step = 8701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.37\n",
            "INFO:tensorflow:loss = 0.0056213667, step = 8801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.309\n",
            "INFO:tensorflow:loss = 0.007647814, step = 8901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.039\n",
            "INFO:tensorflow:loss = 0.006550201, step = 9001 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.242\n",
            "INFO:tensorflow:loss = 0.0062841354, step = 9101 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.946\n",
            "INFO:tensorflow:loss = 0.0055569015, step = 9201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.934\n",
            "INFO:tensorflow:loss = 0.006348942, step = 9301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 835.81\n",
            "INFO:tensorflow:loss = 0.0059895148, step = 9401 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 852.702\n",
            "INFO:tensorflow:loss = 0.005003015, step = 9501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 838.973\n",
            "INFO:tensorflow:loss = 0.0065116147, step = 9601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.69\n",
            "INFO:tensorflow:loss = 0.0071990606, step = 9701 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.937\n",
            "INFO:tensorflow:loss = 0.007553482, step = 9801 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.878\n",
            "INFO:tensorflow:loss = 0.0068961885, step = 9901 (0.170 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0075411582.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 95.30998043856992\n",
            "Just using average = 598.2497551420176 has RMSE of 101.1332211003221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d6b075-9278-4851-d839-87dfefd36c87",
        "id": "5vjLYo0bE-aP"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d7b3fd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.46929494 0.5535782  0.504074   0.50088716 0.54088485 0.52802414\n",
            " 0.49843335 0.47779536 0.46176493 0.46753922 0.46982074 0.55190325\n",
            " 0.5332394  0.48021474 0.57670665 0.5505148  0.469271   0.5289351\n",
            " 0.5497735  0.5438908  0.5024241  0.5511826  0.46666533 0.4874362\n",
            " 0.49522007 0.5541924  0.54036695 0.52826285 0.46978033 0.4796929\n",
            " 0.5125896  0.5242982  0.54819345 0.4941268  0.54023015 0.55189484\n",
            " 0.50629956 0.51655275 0.5460638  0.54543924 0.49801502 0.45472208\n",
            " 0.5175781  0.5161395  0.5336684  0.52472425 0.54972374 0.49095833\n",
            " 0.4694322  0.56089324 0.5287943  0.56115973 0.5616587  0.47398034\n",
            " 0.50847274 0.52560717 0.5587235  0.5016224  0.49427867 0.53114593\n",
            " 0.49907002 0.5219892  0.53754854 0.51120335 0.55048096 0.5239389\n",
            " 0.52057016 0.49541116 0.5153777  0.5062313  0.5143223  0.5338858\n",
            " 0.5157742  0.52124745 0.53872955 0.55237836 0.5151989  0.48618677\n",
            " 0.5027288  0.4810264  0.5093205  0.5363649  0.5293767  0.5532737\n",
            " 0.55572975 0.50312304 0.5529075  0.54079115 0.51762027 0.5269719\n",
            " 0.5303929  0.49060217 0.49977335 0.5340985  0.5083534  0.4673975\n",
            " 0.5190285  0.5370044  0.48304814 0.49092814 0.4931497  0.4954547\n",
            " 0.56986916 0.51802397 0.5610897  0.52949375 0.56043136 0.5271325\n",
            " 0.5287149  0.54283583 0.48666605 0.48403206 0.5148468  0.52337074\n",
            " 0.5340076  0.5188808  0.49598357 0.5130302  0.5056879  0.4981047\n",
            " 0.53316945 0.46715048 0.4736231  0.54403824 0.49961847 0.56076163\n",
            " 0.5266835  0.5151437  0.51269346 0.5632877  0.54602116 0.46967673\n",
            " 0.5300969  0.5402304  0.49077916 0.560853   0.51681054 0.50033224\n",
            " 0.551223   0.4677258  0.46536097 0.49312192 0.5035405  0.52693987\n",
            " 0.5065856  0.5171786  0.4958571  0.5453938  0.5427003  0.5078184\n",
            " 0.46182826 0.5372708  0.5235877  0.5502931  0.5302252  0.5632477\n",
            " 0.5153637  0.47686085 0.5388529  0.45543197 0.4898972  0.52894586\n",
            " 0.54727495 0.4921372  0.5147955  0.498662   0.459263   0.54854053\n",
            " 0.48986223 0.4989513  0.46498138 0.51763886 0.5332178  0.49558637\n",
            " 0.4908808  0.51093936 0.49500614 0.52291435 0.48017246 0.54894346\n",
            " 0.53626287 0.50169235 0.4933585  0.5629322  0.5039119  0.5357831\n",
            " 0.4983216  0.49024117 0.5604075  0.4974918  0.48931238 0.53338045\n",
            " 0.50888395 0.5141836  0.5156343  0.54901356 0.5419193  0.53400457\n",
            " 0.46676466 0.57585764 0.562158   0.51108885 0.5332521  0.461413\n",
            " 0.5223918  0.48326352 0.523384   0.48484364 0.5451402  0.5084936\n",
            " 0.48787102 0.5639765  0.570961   0.5271591  0.54607    0.5692956\n",
            " 0.53329915 0.5137192  0.4868562  0.51677835 0.495848   0.4801384\n",
            " 0.5009226  0.5269996  0.5411512  0.49693623 0.5412072  0.5364233\n",
            " 0.49938312 0.49045724 0.51915777 0.5176554  0.48227844 0.5168391\n",
            " 0.50083566 0.4975011  0.47474036 0.47116244 0.5092224  0.46585134\n",
            " 0.48905817 0.51689327 0.49140346 0.48866218 0.47974843 0.49302173\n",
            " 0.52264035 0.52336824 0.5247736  0.49072984 0.4657324  0.5518646\n",
            " 0.5571007  0.53772646 0.54820406 0.5650596  0.48797148 0.49629042\n",
            " 0.5197675  0.5226285  0.5278224  0.54330957 0.5198629  0.5493789\n",
            " 0.5226967  0.49099416 0.549201   0.5143205  0.4820017  0.5014395\n",
            " 0.5176296  0.4989834  0.52789795 0.47327468 0.51408523 0.5272997\n",
            " 0.5129707  0.48670465 0.5223763  0.47413948 0.5004878  0.5081204\n",
            " 0.5228714  0.5206257  0.4804901  0.54742354 0.53781235 0.47799563\n",
            " 0.55241245 0.5310685  0.53983814 0.5396972  0.5111351  0.49699935\n",
            " 0.47522324 0.51268125 0.4996818  0.5495599  0.4667195  0.50874704\n",
            " 0.5129438  0.4671926  0.48175153 0.4870091  0.5481423  0.5294168\n",
            " 0.49004313 0.49024844 0.49788728 0.5056613  0.52592397 0.51047605\n",
            " 0.5263002  0.48413408 0.4893395  0.4810167  0.5101024  0.52290905\n",
            " 0.5068644  0.4870205  0.5494768  0.5163667  0.5619556  0.5577128\n",
            " 0.4966828  0.5127933  0.5516964  0.5040695  0.51082176 0.47186098\n",
            " 0.47725803 0.5453106  0.5496644  0.53383416 0.52039665 0.51311755\n",
            " 0.5307318  0.51383233 0.5463385  0.5313098  0.5328532  0.49459112\n",
            " 0.4943926  0.48936346 0.52892375 0.53274095 0.5722254  0.48789868\n",
            " 0.49975356 0.5340348  0.50506806 0.5191453  0.563407   0.53723466\n",
            " 0.534026   0.50106376 0.4977186  0.4838479  0.56770146 0.5323666\n",
            " 0.57800925 0.5610408  0.5159979  0.53390646 0.555772   0.5186062\n",
            " 0.49668178 0.47350132 0.51195574 0.47025356 0.49151433 0.5418317\n",
            " 0.49584267 0.5206207  0.5749838  0.49491858 0.5428842  0.52826345\n",
            " 0.48733053 0.50447714 0.5172835  0.47448185 0.48817006 0.52698237\n",
            " 0.5345295  0.5517237  0.47910282 0.5612672  0.51532215 0.46348497\n",
            " 0.470634   0.48832786 0.45618227 0.5268161  0.47451398 0.5094039\n",
            " 0.5632325  0.5003917  0.5620512  0.5217207  0.53917545 0.4991363\n",
            " 0.54927796 0.47251117 0.53103495 0.4612929  0.516241   0.5158419\n",
            " 0.5367959  0.52884346 0.52817583 0.50503534 0.5372411  0.5747835\n",
            " 0.5218268  0.45998913 0.48520312 0.52986264 0.5199572  0.4609197\n",
            " 0.4536003  0.49219933 0.52174324 0.521043   0.46912804 0.5380212\n",
            " 0.5450849  0.49854568 0.5336594  0.54690146 0.47942513 0.5194887\n",
            " 0.49117532 0.48353204 0.5424282  0.5252257  0.5326423  0.56942326\n",
            " 0.52692366 0.5150046  0.50732726 0.50770384 0.49246037 0.51100993\n",
            " 0.5382409  0.51693827 0.5098498  0.5015672  0.46503112 0.5308774\n",
            " 0.47439218 0.5306594  0.54057807 0.5223216  0.5391758  0.5414819\n",
            " 0.5115617  0.4949557  0.52626127 0.55214846 0.53402644 0.5208526\n",
            " 0.46087936 0.52643865 0.5420783  0.5196592  0.52291256 0.48193476\n",
            " 0.49583426 0.52327794 0.5260054  0.5338213  0.5692489  0.5157948\n",
            " 0.47025815 0.51546246 0.5279288  0.5119857  0.49089792 0.4809088\n",
            " 0.513037   0.51337355 0.48341355 0.51300806 0.47718707 0.5169255\n",
            " 0.55954385 0.555768   0.523886   0.49712804 0.5152794  0.5353544\n",
            " 0.5143113  0.546132   0.50700974 0.50972086 0.5014666  0.48804078\n",
            " 0.5435732  0.48521855 0.49579757 0.5378428  0.50120276 0.49727294\n",
            " 0.49484926 0.5190254  0.48194402 0.55702436 0.5394926  0.5120646\n",
            " 0.5683402 ]\n",
            "[0.41515935 0.44358312 0.38587425 0.64427218 0.53574505 0.26098191\n",
            " 0.42894057 0.39276486 0.41257537 0.38587425 0.44875108 0.4754522\n",
            " 0.46425495 0.50904393 0.68130922 0.46770026 0.77174849 0.64341085\n",
            " 0.6416882  0.56589147 0.46597761 0.46856158 0.51937984 0.55469423\n",
            " 0.35745047 0.62360034 0.56847545 0.47459087 0.49354005 0.41946598\n",
            " 0.67183463 0.72437554 0.4039621  0.52713178 0.54521964 0.54866494\n",
            " 0.57708872 0.46080965 0.55469423 0.54091301 0.53574505 0.39793282\n",
            " 0.56330749 0.57795004 0.4625323  0.64082687 0.56503015 0.6089578\n",
            " 0.4005168  0.55555556 0.38931955 0.52024117 0.53229974 0.39190353\n",
            " 0.52540913 0.4039621  0.53660637 0.45478036 0.42291128 0.51593454\n",
            " 0.26873385 0.57881137 0.56503015 0.65891473 0.48406546 0.63738157\n",
            " 0.53660637 0.35228252 0.68044789 0.66322136 0.5211025  0.59776055\n",
            " 0.50301464 0.56761413 0.63221361 0.44788975 0.54521964 0.40999139\n",
            " 0.48664944 0.45736434 0.56761413 0.34969854 0.5047373  0.55211025\n",
            " 0.54866494 0.43755383 0.65288544 0.58484065 0.50559862 0.64944014\n",
            " 0.49009475 0.52713178 0.55555556 0.64599483 0.62790698 0.4918174\n",
            " 0.52799311 0.59259259 0.4754522  0.31438415 0.43496985 0.60465116\n",
            " 0.49956934 0.54349699 0.53143842 0.54866494 0.36089578 0.43669251\n",
            " 0.59086994 0.58914729 0.49956934 0.50990525 0.54177433 0.50990525\n",
            " 0.58139535 0.50559862 0.46425495 0.45219638 0.52713178 0.64857881\n",
            " 0.5994832  0.43927649 0.56933678 0.6124031  0.72265289 0.59173127\n",
            " 0.48664944 0.39190353 0.48492679 0.47631352 0.59259259 0.49784668\n",
            " 0.46683893 0.56158484 0.47459087 0.50990525 0.5211025  0.64857881\n",
            " 0.50990525 0.40826873 0.38329027 0.4754522  0.5245478  0.65719208\n",
            " 0.62187769 0.50387597 0.6546081  0.44444444 0.61670973 0.61412575\n",
            " 0.37812231 0.62015504 0.5211025  0.52885444 0.91731266 0.55986219\n",
            " 0.46683893 0.44702842 0.51507321 0.38587425 0.44702842 0.53229974\n",
            " 0.58828596 0.43410853 0.53488372 0.54263566 0.41429802 0.55469423\n",
            " 0.5796727  0.60378984 0.47803618 0.44186047 0.53574505 0.83893196\n",
            " 0.48320413 0.72782084 0.5047373  0.55813953 0.52713178 0.5503876\n",
            " 0.45305771 0.51421189 0.34797588 0.54694229 0.47717485 0.5667528\n",
            " 0.46080965 0.48492679 0.53143842 0.49956934 0.4788975  0.55813953\n",
            " 0.45736434 0.54780362 0.50904393 0.54349699 0.58139535 0.55813953\n",
            " 0.42118863 0.57536606 0.43927649 0.5538329  0.47114556 0.39534884\n",
            " 0.56416882 0.64857881 0.56847545 0.38587425 0.61068045 0.46856158\n",
            " 0.41085271 0.52627046 0.57278208 0.49612403 0.63135228 0.4952627\n",
            " 0.63910422 0.66063738 0.48492679 0.6580534  0.44875108 0.48148148\n",
            " 0.4788975  0.61498708 0.61757106 0.49267873 0.54866494 0.6709733\n",
            " 0.59086994 0.63824289 0.42291128 0.57278208 0.40913006 0.43066322\n",
            " 0.46167097 0.40568475 0.51076658 0.45650301 0.59431525 0.42204996\n",
            " 0.40999139 0.6873385  0.5047373  0.49354005 0.46167097 0.44875108\n",
            " 0.66925065 0.57364341 0.55986219 0.66838932 0.374677   0.47459087\n",
            " 0.56416882 0.47114556 0.43927649 0.63996555 0.44530577 0.44358312\n",
            " 0.55211025 0.57105943 0.61412575 0.56761413 0.4625323  0.57881137\n",
            " 0.47803618 0.50215332 0.51421189 0.50990525 0.45047373 0.51507321\n",
            " 0.56847545 0.55641688 0.31007752 0.37812231 0.58225668 0.5538329\n",
            " 0.49267873 0.52282515 0.44186047 0.41946598 0.52713178 0.42807924\n",
            " 0.41602067 0.52971576 0.42204996 0.59345392 0.44702842 0.45564169\n",
            " 0.42549526 0.42204996 0.62101637 0.5667528  0.55555556 0.50990525\n",
            " 0.46511628 0.40999139 0.50301464 0.59259259 0.48751077 0.47286822\n",
            " 0.71231697 0.34797588 0.43669251 0.52024117 0.59000861 0.69681309\n",
            " 0.5081826  0.59431525 0.45305771 0.54091301 0.53229974 0.55727821\n",
            " 0.50559862 0.47975883 0.5245478  0.45305771 0.53057709 0.53402239\n",
            " 0.51076658 0.59431525 0.49870801 0.49440138 0.55727821 0.56589147\n",
            " 0.50129199 0.46425495 0.5081826  0.48062016 0.63221361 0.41429802\n",
            " 0.54005168 0.48751077 0.52885444 0.61154177 0.5667528  0.53229974\n",
            " 0.46511628 0.60292851 0.53919035 0.49267873 0.51162791 0.45564169\n",
            " 0.49870801 0.52799311 0.55555556 0.59431525 0.65374677 0.51851852\n",
            " 0.46683893 0.55641688 0.65202412 0.50732127 0.60723514 0.43755383\n",
            " 0.47631352 0.50215332 0.52971576 0.44358312 0.3712317  0.58570198\n",
            " 0.58914729 0.49095607 0.60034453 0.4788975  0.4952627  0.59689922\n",
            " 0.37812231 0.48406546 0.51851852 0.41257537 0.45822567 0.46597761\n",
            " 0.47114556 0.64944014 0.49956934 0.54608096 0.36175711 0.66494401\n",
            " 0.38070629 0.54521964 0.63652024 0.45391904 0.60292851 0.56933678\n",
            " 0.56158484 0.56416882 0.51507321 0.57881137 0.57450474 0.41774332\n",
            " 0.50732127 0.56589147 0.44702842 0.61584841 0.41085271 0.41085271\n",
            " 0.52540913 0.61068045 0.60637382 0.65202412 0.5374677  0.56847545\n",
            " 0.52282515 0.37553833 0.35658915 0.32385874 0.67355728 0.40999139\n",
            " 0.56503015 0.54263566 0.46339363 0.48923342 0.54952627 0.52799311\n",
            " 0.3255814  0.39276486 0.56847545 0.55727821 0.49956934 0.27562446\n",
            " 0.3910422  0.65030146 0.51421189 0.46511628 0.40654608 0.53660637\n",
            " 0.62618432 0.45219638 0.48664944 0.45908699 0.48492679 0.58225668\n",
            " 0.65891473 0.43583118 0.60378984 0.60120586 0.6416882  0.51076658\n",
            " 0.44013781 0.50387597 0.49612403 0.4754522  0.7002584  0.53919035\n",
            " 0.48406546 0.49784668 0.53143842 0.39879414 0.33850129 0.56416882\n",
            " 0.38845823 0.52282515 0.52024117 0.65719208 0.55813953 0.35228252\n",
            " 0.56158484 0.36089578 0.5796727  0.50301464 0.58570198 0.67011197\n",
            " 0.40913006 0.65977606 0.52368648 0.6287683  0.55641688 0.51593454\n",
            " 0.47975883 0.37037037 0.53919035 0.5081826  0.53229974 0.58656331\n",
            " 0.39534884 0.48751077 0.60120586 0.37639966 0.43066322 0.60551249\n",
            " 0.54608096 0.583118   0.44358312 0.48578811 0.38845823 0.65374677\n",
            " 0.57450474 0.57019811 0.42894057 0.51162791 0.48923342 0.51593454\n",
            " 0.49440138 0.61843239 0.55986219 0.49870801 0.47975883 0.45994832\n",
            " 0.60292851 0.55986219 0.43410853 0.59689922 0.49354005 0.625323\n",
            " 0.40740741 0.60120586 0.48578811 0.54091301 0.48664944 0.625323\n",
            " 0.6089578 ]\n",
            "The trained model has an aproximate error rate of 3.9255952049491474 which equates to 1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Neural Network (DNN)\n",
        "Although the primary outcome of assignment 1 was uncovering linear relationships, it was also shown there was more complex relationships which cannot be predicted using a linear regressor. In this case a Deep Learning Neural Network (DNN) can be used.\n",
        "\n",
        "A Deep Learning Neural Network (DNN) is a form of unsupervised learning, where a number of hidden layers containg nodes are used to uncover non linear relationships. Karhunen, Raiko and Cho (2015) infer that deep learning neural networks work in a similar way to the human brain. REF https://doi.org/10.1016/B978-0-12-802806-3.00007-5 19/11 This is where both the relationship between the input and output data is explored, as well as the relationship between the underying data."
      ],
      "metadata": {
        "id": "xIvme6cXJF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precipition (prcp)"
      ],
      "metadata": {
        "id": "cixnwflQxNZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd622355-2c20-491c-fc64-66d0cb75a961",
        "id": "F9YrBdhTYhyE"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp_dnn = df.drop(columns=['temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] != 2012]\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] < 2020]\n",
        "cols = df_prcp_dnn['NUM_COLLISIONS']\n",
        "df_prcp_dnn = df_prcp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp_dnn.insert(loc=25, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp_dnn[:6])\n",
        "df_prcp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "jw5elr-LbRBF",
        "outputId": "b68af7c9-fe9c-45b4-a5fe-390571df279f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  Dec  \\\n",
            "49  2016  28  0.09    0             0                 0     0    0    0    0   \n",
            "51  2014  17  0.00    1             0                 0     0    0    0    0   \n",
            "54  2016  25  0.02    0             0                 0     0    0    0    0   \n",
            "55  2016  29  0.00    0             0                 0     0    0    0    0   \n",
            "58  2017  20  0.00    0             0                 0     0    0    0    0   \n",
            "59  2013  13  0.01    1             0                 0     0    0    0    0   \n",
            "\n",
            "    ...  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49  ...    0    0    0    0    0    0    0    0    1             681  \n",
            "51  ...    0    0    0    0    0    0    1    0    0             589  \n",
            "54  ...    0    0    0    0    0    1    0    0    0             658  \n",
            "55  ...    0    0    0    0    0    0    1    0    0             645  \n",
            "58  ...    0    0    0    0    0    0    1    0    0             605  \n",
            "59  ...    0    0    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         prcp          fog  rain_drizzle  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000   2539.000000   \n",
              "mean   2015.989366    15.745569     0.122588     0.253249      0.375345   \n",
              "std       1.996126     8.803199     0.329143     0.434958      0.484307   \n",
              "min    2013.000000     1.000000     0.000000     0.000000      0.000000   \n",
              "25%    2014.000000     8.000000     0.000000     0.000000      0.000000   \n",
              "50%    2016.000000    16.000000     0.000000     0.000000      0.000000   \n",
              "75%    2018.000000    23.000000     0.060000     1.000000      1.000000   \n",
              "max    2019.000000    31.000000     3.760000     1.000000      1.000000   \n",
              "\n",
              "       snow_ice_pellets         hail          Apr          Aug          Dec  \\\n",
              "count       2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean           0.085467     0.000394     0.082316     0.083497     0.085467   \n",
              "std            0.279630     0.019846     0.274899     0.276687     0.279630   \n",
              "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "       ...          Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  ...  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean   ...     0.082710     0.085467     0.079953     0.142970     0.143364   \n",
              "std    ...     0.275497     0.279630     0.271273     0.350111     0.350512   \n",
              "min    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max    ...     1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000     2539.000000  \n",
              "mean      0.143757     0.142182     0.142576     0.142182      599.135093  \n",
              "std       0.350913     0.349305     0.349709     0.349305      100.299164  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bc0cff9-490c-4574-b2e8-f7dcfcfe2a96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.989366</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.082316</td>\n",
              "      <td>0.083497</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.079953</td>\n",
              "      <td>0.142970</td>\n",
              "      <td>0.143364</td>\n",
              "      <td>0.143757</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>0.142576</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.996126</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>0.274899</td>\n",
              "      <td>0.276687</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275497</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.271273</td>\n",
              "      <td>0.350111</td>\n",
              "      <td>0.350512</td>\n",
              "      <td>0.350913</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>0.349709</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bc0cff9-490c-4574-b2e8-f7dcfcfe2a96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bc0cff9-490c-4574-b2e8-f7dcfcfe2a96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bc0cff9-490c-4574-b2e8-f7dcfcfe2a96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp_dnn.iloc[np.random.permutation(len(df_prcp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpJi3P_8YcIq",
        "outputId": "aea60409-0a80-4477-fe35-cc178b875866"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  \\\n",
            "1739  2015   9  0.00    0             1                 0     0    0    0   \n",
            "3211  2017  18  0.00    0             0                 0     0    0    0   \n",
            "1000  2014  27  0.37    0             1                 0     0    1    0   \n",
            "3119  2014  12  0.00    1             0                 0     0    0    0   \n",
            "143   2016  26  0.00    0             0                 0     0    0    0   \n",
            "270   2019  20  0.61    1             1                 0     0    0    0   \n",
            "\n",
            "      Dec  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1739    0  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3211    0  ...    0    1    0    0    0    0    0    0    0    0  \n",
            "1000    0  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "3119    0  ...    0    1    0    0    0    0    0    0    1    0  \n",
            "143     0  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "270     0  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "\n",
            "[6 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_A0EFvxZpeA",
        "outputId": "70388504-2c07-4fea-9f11-5d2c3ebbd0e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1739    652\n",
            "3211    629\n",
            "1000    449\n",
            "3119    546\n",
            "143     734\n",
            "270     411\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdO_kvOZZuii",
        "outputId": "54a7ea77-8175-4078-9858-e52d9723f01b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaTuIaPRfMxx",
        "outputId": "741a8ca9-7716-4993-aade-da06f12e06ce"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d87d290>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 798834.44, step = 1\n",
            "INFO:tensorflow:global_step/sec: 370.077\n",
            "INFO:tensorflow:loss = 1.2682004, step = 101 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.386\n",
            "INFO:tensorflow:loss = 0.011474724, step = 201 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 473.048\n",
            "INFO:tensorflow:loss = 0.015397187, step = 301 (0.213 sec)\n",
            "INFO:tensorflow:global_step/sec: 475.113\n",
            "INFO:tensorflow:loss = 0.013880731, step = 401 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 473.828\n",
            "INFO:tensorflow:loss = 0.0142975, step = 501 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 463.716\n",
            "INFO:tensorflow:loss = 0.013032053, step = 601 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.908\n",
            "INFO:tensorflow:loss = 0.013579579, step = 701 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.774\n",
            "INFO:tensorflow:loss = 0.0136835445, step = 801 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.833\n",
            "INFO:tensorflow:loss = 0.014060046, step = 901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.261\n",
            "INFO:tensorflow:loss = 0.012807634, step = 1001 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.621\n",
            "INFO:tensorflow:loss = 0.011883773, step = 1101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.317\n",
            "INFO:tensorflow:loss = 0.012568977, step = 1201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 734.504\n",
            "INFO:tensorflow:loss = 0.016141608, step = 1301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.919\n",
            "INFO:tensorflow:loss = 0.010246929, step = 1401 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.998\n",
            "INFO:tensorflow:loss = 0.009099813, step = 1501 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.404\n",
            "INFO:tensorflow:loss = 0.01009549, step = 1601 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.591\n",
            "INFO:tensorflow:loss = 0.014071004, step = 1701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.897\n",
            "INFO:tensorflow:loss = 0.010923181, step = 1801 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.527\n",
            "INFO:tensorflow:loss = 0.009205714, step = 1901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.876\n",
            "INFO:tensorflow:loss = 0.009339069, step = 2001 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.991\n",
            "INFO:tensorflow:loss = 0.009474289, step = 2101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.305\n",
            "INFO:tensorflow:loss = 0.009447392, step = 2201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.136\n",
            "INFO:tensorflow:loss = 0.0079272995, step = 2301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.049\n",
            "INFO:tensorflow:loss = 0.010243344, step = 2401 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.771\n",
            "INFO:tensorflow:loss = 0.009172513, step = 2501 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.547\n",
            "INFO:tensorflow:loss = 0.009644677, step = 2601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.372\n",
            "INFO:tensorflow:loss = 0.0070529645, step = 2701 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.681\n",
            "INFO:tensorflow:loss = 0.008547591, step = 2801 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.314\n",
            "INFO:tensorflow:loss = 0.008785406, step = 2901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.567\n",
            "INFO:tensorflow:loss = 0.008285804, step = 3001 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 710.574\n",
            "INFO:tensorflow:loss = 0.0065002097, step = 3101 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.553\n",
            "INFO:tensorflow:loss = 0.009565406, step = 3201 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.714\n",
            "INFO:tensorflow:loss = 0.0076703453, step = 3301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.455\n",
            "INFO:tensorflow:loss = 0.0072908225, step = 3401 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.396\n",
            "INFO:tensorflow:loss = 0.0055073863, step = 3501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 727.546\n",
            "INFO:tensorflow:loss = 0.00662507, step = 3601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.032\n",
            "INFO:tensorflow:loss = 0.008300027, step = 3701 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.741\n",
            "INFO:tensorflow:loss = 0.0063039074, step = 3801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.307\n",
            "INFO:tensorflow:loss = 0.0059312033, step = 3901 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.538\n",
            "INFO:tensorflow:loss = 0.0051446827, step = 4001 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.166\n",
            "INFO:tensorflow:loss = 0.008310014, step = 4101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.453\n",
            "INFO:tensorflow:loss = 0.006174654, step = 4201 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.858\n",
            "INFO:tensorflow:loss = 0.006498403, step = 4301 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.492\n",
            "INFO:tensorflow:loss = 0.0065852255, step = 4401 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.132\n",
            "INFO:tensorflow:loss = 0.008890552, step = 4501 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.722\n",
            "INFO:tensorflow:loss = 0.0070508583, step = 4601 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.16\n",
            "INFO:tensorflow:loss = 0.0047512306, step = 4701 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.05\n",
            "INFO:tensorflow:loss = 0.007879116, step = 4801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.868\n",
            "INFO:tensorflow:loss = 0.0048119444, step = 4901 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.673\n",
            "INFO:tensorflow:loss = 0.0061138254, step = 5001 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.92\n",
            "INFO:tensorflow:loss = 0.004362501, step = 5101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.847\n",
            "INFO:tensorflow:loss = 0.0042644907, step = 5201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.235\n",
            "INFO:tensorflow:loss = 0.0058259666, step = 5301 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.806\n",
            "INFO:tensorflow:loss = 0.006617657, step = 5401 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.269\n",
            "INFO:tensorflow:loss = 0.006231977, step = 5501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.021\n",
            "INFO:tensorflow:loss = 0.005756508, step = 5601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.485\n",
            "INFO:tensorflow:loss = 0.0071556997, step = 5701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.526\n",
            "INFO:tensorflow:loss = 0.00489778, step = 5801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.106\n",
            "INFO:tensorflow:loss = 0.006668807, step = 5901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.971\n",
            "INFO:tensorflow:loss = 0.0047350912, step = 6001 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.716\n",
            "INFO:tensorflow:loss = 0.0029147672, step = 6101 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.51\n",
            "INFO:tensorflow:loss = 0.012226197, step = 6201 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.247\n",
            "INFO:tensorflow:loss = 0.0087176105, step = 6301 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.745\n",
            "INFO:tensorflow:loss = 0.007299904, step = 6401 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.612\n",
            "INFO:tensorflow:loss = 0.0043988116, step = 6501 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.851\n",
            "INFO:tensorflow:loss = 0.034871254, step = 6601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.587\n",
            "INFO:tensorflow:loss = 0.0038735922, step = 6701 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.691\n",
            "INFO:tensorflow:loss = 0.009024849, step = 6801 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.661\n",
            "INFO:tensorflow:loss = 0.0069879224, step = 6901 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.943\n",
            "INFO:tensorflow:loss = 0.018080313, step = 7001 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.578\n",
            "INFO:tensorflow:loss = 0.04694294, step = 7101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.405\n",
            "INFO:tensorflow:loss = 0.25087744, step = 7201 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.958\n",
            "INFO:tensorflow:loss = 0.099842, step = 7301 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.398\n",
            "INFO:tensorflow:loss = 0.0062780958, step = 7401 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.283\n",
            "INFO:tensorflow:loss = 1.4536906, step = 7501 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.969\n",
            "INFO:tensorflow:loss = 0.012085656, step = 7601 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 657.312\n",
            "INFO:tensorflow:loss = 0.018267408, step = 7701 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.901\n",
            "INFO:tensorflow:loss = 0.012004645, step = 7801 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.473\n",
            "INFO:tensorflow:loss = 8.545756, step = 7901 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.542\n",
            "INFO:tensorflow:loss = 0.0046213167, step = 8001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.064\n",
            "INFO:tensorflow:loss = 0.0704205, step = 8101 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.348\n",
            "INFO:tensorflow:loss = 0.0047004786, step = 8201 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.169\n",
            "INFO:tensorflow:loss = 0.13811776, step = 8301 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.187\n",
            "INFO:tensorflow:loss = 0.08640508, step = 8401 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.06\n",
            "INFO:tensorflow:loss = 0.008620715, step = 8501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.626\n",
            "INFO:tensorflow:loss = 0.27057534, step = 8601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.408\n",
            "INFO:tensorflow:loss = 0.30042118, step = 8701 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.925\n",
            "INFO:tensorflow:loss = 0.0067375074, step = 8801 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.772\n",
            "INFO:tensorflow:loss = 0.0065748137, step = 8901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.385\n",
            "INFO:tensorflow:loss = 0.010834018, step = 9001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.607\n",
            "INFO:tensorflow:loss = 0.029446537, step = 9101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.267\n",
            "INFO:tensorflow:loss = 0.049916953, step = 9201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.387\n",
            "INFO:tensorflow:loss = 0.032452345, step = 9301 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 681.513\n",
            "INFO:tensorflow:loss = 0.30602583, step = 9401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.009\n",
            "INFO:tensorflow:loss = 0.049295068, step = 9501 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.905\n",
            "INFO:tensorflow:loss = 0.012821615, step = 9601 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.774\n",
            "INFO:tensorflow:loss = 0.24208578, step = 9701 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.09\n",
            "INFO:tensorflow:loss = 1.308434, step = 9801 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.467\n",
            "INFO:tensorflow:loss = 1.888778, step = 9901 (0.141 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0049710497.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 93.74234685742437\n",
            "Just using average = 598.4101427868045 has RMSE of 101.82952104666464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba870a5e-1310-4107-a377-a65b4a8594f4",
        "id": "Ld6baV60hPOs"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d56c850>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5060923  0.538975   0.47282818 0.48340252 0.46773174 0.43765667\n",
            " 0.5395014  0.5066798  0.5112803  0.5418589  0.5222285  0.4821284\n",
            " 0.42439678 0.41203716 0.52383065 0.4757655  0.4636729  0.36477306\n",
            " 0.5360911  0.4059184  0.53789926 0.5042994  0.4580424  0.5297816\n",
            " 0.44667462 0.49606732 0.54194283 0.4320872  0.53397775 0.50532174\n",
            " 0.47484997 0.5465815  0.5064738  0.5018656  0.45395306 0.47741345\n",
            " 0.5022776  0.38008526 0.47395733 0.47222546 0.4066508  0.4102519\n",
            " 0.48615673 0.50441384 0.5299494  0.508549   0.47113445 0.45838574\n",
            " 0.364445   0.40471295 0.49804333 0.40429333 0.46783856 0.4934428\n",
            " 0.5055506  0.48051098 0.5210917  0.44837597 0.54478097 0.4823039\n",
            " 0.3907664  0.46537426 0.48817852 0.47753552 0.5804026  0.48899487\n",
            " 0.39472607 0.4747813  0.49580792 0.48545483 0.49438885 0.5286524\n",
            " 0.49016216 0.4905055  0.48656872 0.50091195 0.4608195  0.4020503\n",
            " 0.5043452  0.48201397 0.4982722  0.50270486 0.45720318 0.5168574\n",
            " 0.52118325 0.47367504 0.47772625 0.4924586  0.5228312  0.48710278\n",
            " 0.40017346 0.51173806 0.51466775 0.50965524 0.47625378 0.5277064\n",
            " 0.50472665 0.4724467  0.48463085 0.49693707 0.50267434 0.43742016\n",
            " 0.4862788  0.4732249  0.5121882  0.4710963  0.39802197 0.43252972\n",
            " 0.47788647 0.40564373 0.49255016 0.5041926  0.5052912  0.50509286\n",
            " 0.46152142 0.447346   0.50626016 0.5141566  0.5192988  0.48658398\n",
            " 0.47411755 0.45830944 0.5081217  0.45626476 0.48582104 0.47718456\n",
            " 0.4878581  0.46680096 0.47331646 0.50591683 0.54327035 0.4820216\n",
            " 0.4657481  0.46836498 0.53851724 0.54225564 0.4995387  0.49148205\n",
            " 0.43761852 0.48382977 0.5224116  0.5167811  0.49626568 0.4662135\n",
            " 0.5443919  0.4327586  0.46400097 0.38672283 0.48166302 0.40803936\n",
            " 0.5106394  0.40638378 0.5187113  0.49625042 0.5031855  0.47761944\n",
            " 0.51381326 0.47059277 0.36295727 0.49082592 0.5379374  0.51082253\n",
            " 0.550648   0.51461434 0.49296215 0.50836587 0.46282604 0.50262094\n",
            " 0.47140148 0.47992352 0.49425152 0.48932293 0.4898341  0.46142223\n",
            " 0.4538081  0.43621472 0.5160563  0.45970562 0.48831585 0.52224374\n",
            " 0.46161297 0.5044291  0.5084727  0.36454418 0.4685481  0.44660595\n",
            " 0.46178845 0.54371285 0.4573405  0.5241053  0.5152552  0.45000866\n",
            " 0.4784892  0.51861215 0.4838374  0.46322277 0.54498696 0.39001873\n",
            " 0.5051005  0.4674571  0.51794076 0.4838908  0.4409068  0.5302317\n",
            " 0.4769862  0.5071833  0.49418285 0.50041604 0.5059931  0.39864758\n",
            " 0.51755166 0.48991802 0.43463543 0.47743633 0.4035304  0.5449183\n",
            " 0.50688577 0.4119151  0.50690866 0.47349194 0.48800305 0.38557842\n",
            " 0.48093823 0.41114452 0.5235102  0.54226327 0.46786907 0.36975506\n",
            " 0.45019177 0.50576425 0.4634364  0.49406078 0.46749523 0.46927288\n",
            " 0.4768336  0.48927715 0.46682385 0.48693493 0.502476   0.5352366\n",
            " 0.45821026 0.42895153 0.45838574 0.5369456  0.44909313 0.40662792\n",
            " 0.4496577  0.58002114 0.50169015 0.5064509  0.5082133  0.47066906\n",
            " 0.4630473  0.5073283  0.4941447  0.49203137 0.49457195 0.5402949\n",
            " 0.5012553  0.53792214 0.405041   0.4920924  0.533375   0.48564556\n",
            " 0.50561166 0.5323374  0.46986035 0.54801583 0.4792445  0.4306834\n",
            " 0.4323161  0.39049175 0.47181347 0.5030329  0.5308573  0.4737666\n",
            " 0.4647868  0.5021479  0.49217632 0.52336526 0.48738506 0.36382702\n",
            " 0.51272225 0.49757794 0.49124554 0.40297344 0.4784663  0.51816964\n",
            " 0.49000958 0.52821755 0.57475686 0.51695657 0.43339947 0.5103266\n",
            " 0.5079844  0.4520152  0.5218699  0.49246624 0.49521282 0.49043682\n",
            " 0.50444436 0.50127053 0.5130198  0.51194406 0.46295574 0.46999767\n",
            " 0.5620692  0.41958264 0.46729687 0.4004481  0.5182383  0.38511303\n",
            " 0.43053845 0.53231454 0.4526408  0.51046395 0.50555825 0.52528024\n",
            " 0.4902003  0.49006298 0.5099375  0.39760235 0.5070002  0.4830592\n",
            " 0.39599255 0.5134928  0.52941537 0.4606059  0.51895547 0.48745373\n",
            " 0.46399334 0.53047585 0.4599345  0.43520764 0.40761974 0.49545696\n",
            " 0.4244807  0.47490337 0.50792336 0.37064007 0.4877055  0.4665492\n",
            " 0.49646404 0.4941981  0.51663613 0.4968913  0.46146038 0.5298655\n",
            " 0.5005381  0.5190623  0.5041239  0.50152993 0.46768597 0.5191233\n",
            " 0.4910014  0.50861    0.47015026 0.46469525 0.5158045  0.4606059\n",
            " 0.50752664 0.5407069  0.45563915 0.41255596 0.46197918 0.46468762\n",
            " 0.39882305 0.4117625  0.40942028 0.50591683 0.4738963  0.46113995\n",
            " 0.46393993 0.49119213 0.5609553  0.43290356 0.47954205 0.54480386\n",
            " 0.47359112 0.5036738  0.3903773  0.51385903 0.5200083  0.46251324\n",
            " 0.5150416  0.5007212  0.4049113  0.45317486 0.49959973 0.45167187\n",
            " 0.49119976 0.42074993 0.5616114  0.48786572 0.54851174 0.47060803\n",
            " 0.36691692 0.510586   0.4875758  0.5118449  0.47068432 0.4905894\n",
            " 0.40767315 0.46472576 0.4733012  0.4795039  0.5366328  0.49596813\n",
            " 0.43619946 0.5022242  0.37134197 0.5106242  0.43998364 0.46107128\n",
            " 0.45055798 0.42815807 0.44881848 0.51204324 0.50189614 0.51087594\n",
            " 0.49815777 0.37681988 0.4963496  0.47616985 0.51225686 0.5347407\n",
            " 0.53039956 0.50735116 0.46327618 0.5410807  0.47591808 0.40276745\n",
            " 0.51007485 0.49820355 0.49834087 0.49285534 0.5046122  0.5250666\n",
            " 0.48610333 0.49368694 0.5044062  0.50867105 0.47912243 0.47320965\n",
            " 0.50594735 0.5070002  0.368336   0.43305615 0.46933392 0.46445873\n",
            " 0.47443035 0.37606457 0.39894512 0.48821667 0.47046307 0.51669717\n",
            " 0.5750468  0.41314343 0.49693707 0.5403178  0.41416577 0.47986248\n",
            " 0.50973916 0.56331277 0.50088143 0.44919994 0.466244   0.5396311\n",
            " 0.5197871  0.497494   0.4530528  0.4946101  0.5117686  0.47240093\n",
            " 0.5362971  0.46090344 0.54116464 0.4655955  0.37891033 0.5261576\n",
            " 0.47312573 0.49153545 0.45648602 0.49467114 0.5018885  0.4873164\n",
            " 0.5075114  0.52385354 0.5048106  0.5574839  0.49137524 0.49626568\n",
            " 0.5284388  0.3767207  0.52409005 0.54482675 0.40146282 0.44820812\n",
            " 0.4543269  0.50667214 0.54191995 0.4802287  0.46635845 0.56499887\n",
            " 0.5375788  0.52180886 0.5246546  0.40551403]\n",
            "[0.60206718 0.625323   0.68217054 0.4625323  0.59259259 0.49354005\n",
            " 0.55986219 0.4952627  0.52971576 0.583118   0.57536606 0.50301464\n",
            " 0.40913006 0.42721792 0.62790698 0.4203273  0.59173127 0.37726098\n",
            " 0.60378984 0.40826873 0.49870801 0.50215332 0.51593454 0.60378984\n",
            " 0.52971576 0.5081826  0.55297158 0.46080965 0.38242894 0.59862188\n",
            " 0.43066322 0.48492679 0.50732127 0.61412575 0.4005168  0.5047373\n",
            " 0.52024117 0.42291128 0.48062016 0.48406546 0.40999139 0.41257537\n",
            " 0.57536606 0.63393626 0.64685616 0.55727821 0.44702842 0.40826873\n",
            " 0.29371232 0.47975883 0.54005168 0.49267873 0.53832903 0.33763997\n",
            " 0.56761413 0.60378984 0.61757106 0.45736434 0.69939707 0.45219638\n",
            " 0.4332472  0.4203273  0.48664944 0.56072351 0.68303187 0.4203273\n",
            " 0.38587425 0.51937984 0.43496985 0.54349699 0.60637382 0.72437554\n",
            " 0.54866494 0.44788975 0.45736434 0.52282515 0.52799311 0.44530577\n",
            " 0.51851852 0.53919035 0.60981912 0.55641688 0.46942291 0.66149871\n",
            " 0.62962963 0.49956934 0.53488372 0.55555556 0.55986219 0.54521964\n",
            " 0.42204996 0.63049096 0.65202412 0.63910422 0.60034453 0.68217054\n",
            " 0.63565891 0.49095607 0.52282515 0.45219638 0.74677003 0.50904393\n",
            " 0.60809647 0.46683893 0.56933678 0.39276486 0.48492679 0.44788975\n",
            " 0.57536606 0.40482343 0.55986219 0.47975883 0.5667528  0.61068045\n",
            " 0.51421189 0.50301464 0.55813953 0.51507321 0.49095607 0.48664944\n",
            " 0.51162791 0.47631352 0.45564169 0.5374677  0.46080965 0.7166236\n",
            " 0.54435831 0.41257537 0.46425495 0.53660637 0.58656331 0.53402239\n",
            " 0.44272179 0.41946598 0.75107666 0.63738157 0.45564169 0.39276486\n",
            " 0.43841516 0.60292851 0.61757106 0.47975883 0.60120586 0.47717485\n",
            " 0.57278208 0.49095607 0.43927649 0.35486649 0.5796727  0.41085271\n",
            " 0.68475452 0.52024117 0.62015504 0.51851852 0.43496985 0.49612403\n",
            " 0.51679587 0.47114556 0.44702842 0.50990525 0.50904393 0.54952627\n",
            " 0.5538329  0.46856158 0.5211025  0.32213609 0.53832903 0.55813953\n",
            " 0.34022394 0.37984496 0.53143842 0.46770026 0.62618432 0.44444444\n",
            " 0.50215332 0.41085271 0.52799311 0.40310078 0.63049096 0.40999139\n",
            " 0.48578811 0.57364341 0.53660637 0.32213609 0.32816537 0.34625323\n",
            " 0.4332472  0.58225668 0.45047373 0.52799311 0.58225668 0.52024117\n",
            " 0.42894057 0.63738157 0.5503876  0.51765719 0.52971576 0.43066322\n",
            " 0.53229974 0.55641688 0.5081826  0.55555556 0.45305771 0.4754522\n",
            " 0.47372954 0.56503015 0.68217054 0.45305771 0.57881137 0.41515935\n",
            " 0.59345392 0.49095607 0.51937984 0.68130922 0.42463394 0.58742463\n",
            " 0.50904393 0.40740741 0.50559862 0.58484065 0.59345392 0.34366925\n",
            " 0.46339363 0.43496985 0.46339363 0.55986219 0.51765719 0.31093885\n",
            " 0.50559862 0.583118   0.52540913 0.26098191 0.50990525 0.39965547\n",
            " 0.53919035 0.53057709 0.55813953 0.47631352 0.48406546 0.63910422\n",
            " 0.42635659 0.40913006 0.56847545 0.5667528  0.34280792 0.46942291\n",
            " 0.60809647 0.62704565 0.50129199 0.63824289 0.5667528  0.53660637\n",
            " 0.55124892 0.5667528  0.59259259 0.48234281 0.52799311 0.625323\n",
            " 0.52196382 0.60551249 0.46511628 0.48492679 0.54349699 0.54005168\n",
            " 0.65891473 0.59086994 0.43755383 0.60292851 0.39362618 0.35486649\n",
            " 0.34625323 0.47114556 0.47803618 0.42291128 0.58570198 0.60809647\n",
            " 0.46856158 0.48062016 0.53402239 0.80878553 0.57450474 0.39965547\n",
            " 0.44530577 0.56244617 0.48751077 0.44099914 0.60378984 0.55211025\n",
            " 0.5374677  0.52368648 0.62704565 0.51937984 0.50559862 0.54694229\n",
            " 0.51593454 0.4203273  0.56330749 0.55986219 0.50129199 0.44788975\n",
            " 0.51248923 0.48320413 0.71576227 0.55555556 0.53143842 0.54608096\n",
            " 0.62273902 0.48492679 0.49870801 0.41774332 0.60034453 0.33936262\n",
            " 0.47803618 0.56761413 0.39018088 0.56503015 0.52971576 0.48664944\n",
            " 0.52368648 0.5994832  0.54349699 0.39534884 0.48406546 0.42549526\n",
            " 0.42635659 0.58484065 0.63910422 0.41085271 0.45650301 0.52885444\n",
            " 0.66925065 0.6089578  0.56503015 0.36520241 0.36089578 0.76055125\n",
            " 0.46770026 0.4918174  0.44444444 0.39793282 0.4039621  0.44702842\n",
            " 0.63910422 0.59431525 0.62360034 0.50215332 0.51937984 0.59776055\n",
            " 0.53919035 0.48320413 0.38070629 0.52024117 0.47717485 0.60292851\n",
            " 0.61929371 0.53488372 0.28682171 0.3910422  0.61843239 0.63738157\n",
            " 0.52196382 0.64082687 0.43496985 0.4788975  0.59173127 0.54091301\n",
            " 0.47717485 0.4625323  0.43066322 0.58225668 0.53919035 0.51507321\n",
            " 0.42894057 0.54866494 0.625323   0.47717485 0.47028424 0.60551249\n",
            " 0.49870801 0.54091301 0.39276486 0.5211025  0.5503876  0.44358312\n",
            " 0.59862188 0.46339363 0.40740741 0.48923342 0.51593454 0.44530577\n",
            " 0.55297158 0.43927649 0.60292851 0.57881137 0.46597761 0.48148148\n",
            " 0.38501292 0.46339363 0.50904393 0.53316107 0.42894057 0.62446167\n",
            " 0.41429802 0.52713178 0.48751077 0.55469423 0.55813953 0.59259259\n",
            " 0.46511628 0.56072351 0.50387597 0.54435831 0.60465116 0.48751077\n",
            " 0.47975883 0.416882   0.50732127 0.5538329  0.51421189 0.46425495\n",
            " 0.57450474 0.40482343 0.60292851 0.60292851 0.68647717 0.56244617\n",
            " 0.53402239 0.56158484 0.56589147 0.583118   0.44702842 0.49354005\n",
            " 0.47286822 0.57881137 0.52282515 0.55813953 0.35228252 0.63824289\n",
            " 0.52540913 0.36950904 0.46942291 0.54694229 0.58914729 0.48406546\n",
            " 0.50990525 0.5667528  0.41257537 0.50904393 0.49354005 0.49956934\n",
            " 0.4918174  0.43496985 0.46339363 0.51507321 0.49095607 0.55555556\n",
            " 0.64427218 0.46167097 0.53229974 0.63479759 0.46339363 0.39793282\n",
            " 0.53402239 0.7037037  0.60809647 0.49009475 0.4788975  0.66666667\n",
            " 0.61843239 0.71490095 0.49612403 0.53574505 0.61584841 0.43927649\n",
            " 0.6873385  0.52627046 0.60120586 0.45391904 0.37639966 0.51937984\n",
            " 0.4754522  0.43583118 0.58570198 0.49009475 0.64513351 0.48751077\n",
            " 0.54694229 0.52282515 0.58139535 0.55986219 0.56589147 0.54177433\n",
            " 0.70542636 0.37898363 0.50387597 0.66494401 0.42118863 0.58397933\n",
            " 0.43410853 0.62962963 0.57622739 0.49784668 1.         0.55297158\n",
            " 0.56416882 0.63049096 0.59862188 0.42377261]\n",
            "The trained model has an aproximate error rate of 42.350708288529255 which equates to 7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "yT91LA7Xx77W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5e6c01-a3d7-49cc-b472-00d854ae2f4d",
        "id": "zQAH_kVzyAOD"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp_dnn = df.drop(columns=['temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] != 2012]\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] < 2020]\n",
        "cols = df_dewp_dnn['NUM_COLLISIONS']\n",
        "df_dewp_dnn = df_dewp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp_dnn[:6])\n",
        "df_dewp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "6b89aa16-f103-4b70-d265-19d60d46f819",
        "id": "_qzGppAwyAOE"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  24.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  35.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  21.2    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29  36.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  32.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  44.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         dewp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2015.999217    15.723679    44.163170     0.082192     0.084932   \n",
              "std       2.000000     8.801271    16.995303     0.274710     0.278834   \n",
              "min    2013.000000     1.000000    -6.700000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    32.150000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    45.300000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    58.500000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    74.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084932     0.077104     0.084932     0.084540     0.082192  ...   \n",
              "std       0.278834     0.266808     0.278834     0.278251     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142857      599.109980  \n",
              "std       0.349996     0.349996     0.349996     0.349996      100.277185  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bfd5da66-6fc3-4226-8bb8-9762a84ae776\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.999217</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfd5da66-6fc3-4226-8bb8-9762a84ae776')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bfd5da66-6fc3-4226-8bb8-9762a84ae776 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bfd5da66-6fc3-4226-8bb8-9762a84ae776');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp_dnn.iloc[np.random.permutation(len(df_dewp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e72843-4e8f-4e66-a247-4037444da5ca",
        "id": "z5-eCTxByAOE"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "1792  2013  19  54.4    0    0    0    0    0    0    1  ...    0    0    0   \n",
            "1078  2016   6  15.3    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "2670  2016  27  59.8    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2411  2018  23  61.6    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "1104  2017  22  44.5    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "763   2013  13  41.1    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1792    0    0    0    0    0    1    0  \n",
            "1078    0    0    0    0    0    1    0  \n",
            "2670    1    1    0    0    0    0    0  \n",
            "2411    0    0    0    0    0    0    1  \n",
            "1104    0    0    0    0    0    0    0  \n",
            "763     0    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44069227-b50a-44c0-ef72-e3ec1f189337",
        "id": "BbrOrPfQyAOE"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1792    622\n",
            "1078    628\n",
            "2670    659\n",
            "2411    610\n",
            "1104    551\n",
            "763     636\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9d2ff9-7355-4c84-ef79-9486223b7329",
        "id": "T2v7BylMyAOE"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41bcfc2-0898-4bdf-ff02-6e63d9bd77dd",
        "id": "HMGFsHtkyAOE"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d471450>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 1425.2727, step = 1\n",
            "INFO:tensorflow:global_step/sec: 542.081\n",
            "INFO:tensorflow:loss = 0.14055178, step = 101 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 718.322\n",
            "INFO:tensorflow:loss = 0.014738771, step = 201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.321\n",
            "INFO:tensorflow:loss = 0.009598495, step = 301 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.42\n",
            "INFO:tensorflow:loss = 0.018171396, step = 401 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.589\n",
            "INFO:tensorflow:loss = 0.014854988, step = 501 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.739\n",
            "INFO:tensorflow:loss = 0.0065168226, step = 601 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 666.368\n",
            "INFO:tensorflow:loss = 0.15406677, step = 701 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.082\n",
            "INFO:tensorflow:loss = 0.6826425, step = 801 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.901\n",
            "INFO:tensorflow:loss = 0.12235515, step = 901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.971\n",
            "INFO:tensorflow:loss = 0.050239168, step = 1001 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.42\n",
            "INFO:tensorflow:loss = 0.113397434, step = 1101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.499\n",
            "INFO:tensorflow:loss = 0.053266883, step = 1201 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.79\n",
            "INFO:tensorflow:loss = 0.11964263, step = 1301 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.893\n",
            "INFO:tensorflow:loss = 0.0039314013, step = 1401 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.972\n",
            "INFO:tensorflow:loss = 0.10632688, step = 1501 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.828\n",
            "INFO:tensorflow:loss = 0.004373833, step = 1601 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.612\n",
            "INFO:tensorflow:loss = 0.039271574, step = 1701 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.524\n",
            "INFO:tensorflow:loss = 0.0054240813, step = 1801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.982\n",
            "INFO:tensorflow:loss = 0.0040322435, step = 1901 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.916\n",
            "INFO:tensorflow:loss = 0.025701161, step = 2001 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.525\n",
            "INFO:tensorflow:loss = 1.3447139, step = 2101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.599\n",
            "INFO:tensorflow:loss = 0.006724174, step = 2201 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.667\n",
            "INFO:tensorflow:loss = 0.059453778, step = 2301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.817\n",
            "INFO:tensorflow:loss = 0.13097522, step = 2401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.123\n",
            "INFO:tensorflow:loss = 0.011358665, step = 2501 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.969\n",
            "INFO:tensorflow:loss = 0.0051325215, step = 2601 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.325\n",
            "INFO:tensorflow:loss = 0.5680485, step = 2701 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.918\n",
            "INFO:tensorflow:loss = 0.35451275, step = 2801 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 603.374\n",
            "INFO:tensorflow:loss = 0.020912558, step = 2901 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.1\n",
            "INFO:tensorflow:loss = 0.018289689, step = 3001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.434\n",
            "INFO:tensorflow:loss = 0.20641062, step = 3101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.463\n",
            "INFO:tensorflow:loss = 0.07692193, step = 3201 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.193\n",
            "INFO:tensorflow:loss = 0.011065905, step = 3301 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.538\n",
            "INFO:tensorflow:loss = 0.00531449, step = 3401 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.371\n",
            "INFO:tensorflow:loss = 0.023799744, step = 3501 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 449.4\n",
            "INFO:tensorflow:loss = 0.104495615, step = 3601 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.498\n",
            "INFO:tensorflow:loss = 0.006383184, step = 3701 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 477.406\n",
            "INFO:tensorflow:loss = 0.09185425, step = 3801 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.31\n",
            "INFO:tensorflow:loss = 0.07248607, step = 3901 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.751\n",
            "INFO:tensorflow:loss = 0.047584236, step = 4001 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.854\n",
            "INFO:tensorflow:loss = 0.004865801, step = 4101 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.761\n",
            "INFO:tensorflow:loss = 0.004534849, step = 4201 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 463.435\n",
            "INFO:tensorflow:loss = 0.011929104, step = 4301 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.308\n",
            "INFO:tensorflow:loss = 0.0066012535, step = 4401 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 457.384\n",
            "INFO:tensorflow:loss = 0.0048099626, step = 4501 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.336\n",
            "INFO:tensorflow:loss = 0.004850348, step = 4601 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 499.478\n",
            "INFO:tensorflow:loss = 0.0051906994, step = 4701 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 471.254\n",
            "INFO:tensorflow:loss = 0.024971649, step = 4801 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.329\n",
            "INFO:tensorflow:loss = 0.009483796, step = 4901 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 474.181\n",
            "INFO:tensorflow:loss = 0.0078100264, step = 5001 (0.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.485\n",
            "INFO:tensorflow:loss = 0.0039057618, step = 5101 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 421.86\n",
            "INFO:tensorflow:loss = 0.014801262, step = 5201 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.188\n",
            "INFO:tensorflow:loss = 0.04246202, step = 5301 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.073\n",
            "INFO:tensorflow:loss = 0.0039869975, step = 5401 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.87\n",
            "INFO:tensorflow:loss = 0.0098828655, step = 5501 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.336\n",
            "INFO:tensorflow:loss = 0.0201036, step = 5601 (0.214 sec)\n",
            "INFO:tensorflow:global_step/sec: 474.512\n",
            "INFO:tensorflow:loss = 0.006655297, step = 5701 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.066\n",
            "INFO:tensorflow:loss = 0.00574382, step = 5801 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.68\n",
            "INFO:tensorflow:loss = 0.0050312337, step = 5901 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 644.891\n",
            "INFO:tensorflow:loss = 0.004572062, step = 6001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.819\n",
            "INFO:tensorflow:loss = 0.007581152, step = 6101 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.325\n",
            "INFO:tensorflow:loss = 0.007750991, step = 6201 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.168\n",
            "INFO:tensorflow:loss = 0.0059668007, step = 6301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.799\n",
            "INFO:tensorflow:loss = 0.006140395, step = 6401 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.854\n",
            "INFO:tensorflow:loss = 0.007447508, step = 6501 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.851\n",
            "INFO:tensorflow:loss = 0.0039174515, step = 6601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.144\n",
            "INFO:tensorflow:loss = 0.005609312, step = 6701 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.428\n",
            "INFO:tensorflow:loss = 0.007991575, step = 6801 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 380.131\n",
            "INFO:tensorflow:loss = 0.0061073964, step = 6901 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.993\n",
            "INFO:tensorflow:loss = 0.007209321, step = 7001 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 380.424\n",
            "INFO:tensorflow:loss = 0.026019398, step = 7101 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.652\n",
            "INFO:tensorflow:loss = 0.004837913, step = 7201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.606\n",
            "INFO:tensorflow:loss = 0.0059177233, step = 7301 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.501\n",
            "INFO:tensorflow:loss = 0.0076554012, step = 7401 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.309\n",
            "INFO:tensorflow:loss = 0.007356509, step = 7501 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.485\n",
            "INFO:tensorflow:loss = 0.026321277, step = 7601 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.959\n",
            "INFO:tensorflow:loss = 0.0048084715, step = 7701 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.75\n",
            "INFO:tensorflow:loss = 0.004813702, step = 7801 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.457\n",
            "INFO:tensorflow:loss = 0.008065864, step = 7901 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.937\n",
            "INFO:tensorflow:loss = 0.0077028032, step = 8001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.51\n",
            "INFO:tensorflow:loss = 0.0070338636, step = 8101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.916\n",
            "INFO:tensorflow:loss = 0.004313115, step = 8201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 676.39\n",
            "INFO:tensorflow:loss = 0.0059379535, step = 8301 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.88\n",
            "INFO:tensorflow:loss = 0.006405825, step = 8401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.338\n",
            "INFO:tensorflow:loss = 0.008534401, step = 8501 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.35\n",
            "INFO:tensorflow:loss = 0.0041646333, step = 8601 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.997\n",
            "INFO:tensorflow:loss = 0.0038866538, step = 8701 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.072\n",
            "INFO:tensorflow:loss = 0.004273356, step = 8801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.799\n",
            "INFO:tensorflow:loss = 0.0042668497, step = 8901 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.598\n",
            "INFO:tensorflow:loss = 0.0052781412, step = 9001 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.185\n",
            "INFO:tensorflow:loss = 0.006708084, step = 9101 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.001\n",
            "INFO:tensorflow:loss = 0.010722933, step = 9201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.877\n",
            "INFO:tensorflow:loss = 0.0056642853, step = 9301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.246\n",
            "INFO:tensorflow:loss = 0.0076710545, step = 9401 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.571\n",
            "INFO:tensorflow:loss = 0.0048483945, step = 9501 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.281\n",
            "INFO:tensorflow:loss = 0.0037010051, step = 9601 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.885\n",
            "INFO:tensorflow:loss = 0.0056360047, step = 9701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.026\n",
            "INFO:tensorflow:loss = 0.008353794, step = 9801 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.646\n",
            "INFO:tensorflow:loss = 0.0042100484, step = 9901 (0.153 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0050785826.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 84.64368559494616\n",
            "Just using average = 598.6658512720156 has RMSE of 103.34538537526363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0752503e-13e2-4873-83fa-e62e506c3f48",
        "id": "NCz6izVhyAOF"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769d797d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.50938684 0.5685225  0.52078134 0.45745307 0.5399669  0.57444793\n",
            " 0.5135475  0.4295686  0.49450785 0.5433124  0.46798426 0.41196185\n",
            " 0.56338316 0.59016114 0.5576928  0.523501   0.5205563  0.5279618\n",
            " 0.5717111  0.51154953 0.5779994  0.4795633  0.5155359  0.54478365\n",
            " 0.5579539  0.4238928  0.53108174 0.45774823 0.5030342  0.49067026\n",
            " 0.5140694  0.46040016 0.54991347 0.46243793 0.5543969  0.5496438\n",
            " 0.5213857  0.52973515 0.4575017  0.43058544 0.54442626 0.53164417\n",
            " 0.42636138 0.50159365 0.49674374 0.50797707 0.51341397 0.5596636\n",
            " 0.5031479  0.5341795  0.4909659  0.4913934  0.50866395 0.5129605\n",
            " 0.57824856 0.54449826 0.47830206 0.5375739  0.53882295 0.50271994\n",
            " 0.41915804 0.48136622 0.5187419  0.43378216 0.5232864  0.52376515\n",
            " 0.48984224 0.49395853 0.4776786  0.5253554  0.43987757 0.4111541\n",
            " 0.50176436 0.5331188  0.5518082  0.50495774 0.55524355 0.5566426\n",
            " 0.50909644 0.5162104  0.56643206 0.44025284 0.48248154 0.4977978\n",
            " 0.55446845 0.48001963 0.5105465  0.45642215 0.43339306 0.41568977\n",
            " 0.4238742  0.4771288  0.5408519  0.5171352  0.4911309  0.5622788\n",
            " 0.5124188  0.5723832  0.54909974 0.49609357 0.55504876 0.54221565\n",
            " 0.52472335 0.5548654  0.41354162 0.55492646 0.4964648  0.4398473\n",
            " 0.5395945  0.5863288  0.52552587 0.5377653  0.5269845  0.5135165\n",
            " 0.5063465  0.49159437 0.53245264 0.51065165 0.4976743  0.5460411\n",
            " 0.55732614 0.5021027  0.46561104 0.454346   0.5511189  0.48570925\n",
            " 0.5186499  0.5507005  0.53882676 0.52023536 0.51490456 0.45012432\n",
            " 0.48672754 0.5257521  0.5294557  0.5258208  0.48878533 0.57059985\n",
            " 0.5555416  0.5206302  0.53985745 0.48727137 0.5195454  0.547972\n",
            " 0.43019253 0.5095094  0.5645054  0.48244482 0.5191651  0.5168763\n",
            " 0.5662814  0.5345734  0.4198342  0.50788337 0.51774365 0.49136835\n",
            " 0.5137084  0.5884264  0.43777806 0.49570042 0.5435663  0.5431579\n",
            " 0.51920944 0.4637223  0.46843225 0.5916403  0.5497981  0.5854538\n",
            " 0.52118427 0.518024   0.5560091  0.51444036 0.61435395 0.53647023\n",
            " 0.52011925 0.4992593  0.5774513  0.5334919  0.50109154 0.44715363\n",
            " 0.41509277 0.49780017 0.4958995  0.5008822  0.51991636 0.4999023\n",
            " 0.5549391  0.49546033 0.5378886  0.44488794 0.49738532 0.52919084\n",
            " 0.5133007  0.529661   0.5780001  0.48561746 0.48499233 0.4201315\n",
            " 0.46394926 0.45873863 0.4160083  0.50763375 0.5382708  0.45497257\n",
            " 0.5369893  0.53631    0.57534987 0.4056936  0.53354746 0.4968372\n",
            " 0.49745995 0.40898377 0.54180723 0.5462437  0.54512674 0.43450457\n",
            " 0.50535876 0.4799295  0.49224907 0.5358344  0.56891924 0.5380972\n",
            " 0.56081516 0.5422953  0.5204833  0.45134693 0.52037984 0.48767954\n",
            " 0.47504073 0.5468901  0.48670536 0.59124833 0.44864708 0.57790047\n",
            " 0.53061277 0.41644436 0.5038131  0.4846738  0.48935157 0.55241567\n",
            " 0.48056608 0.48359257 0.5373686  0.45717078 0.54218704 0.50864154\n",
            " 0.5066305  0.5300513  0.49849302 0.5057407  0.53766733 0.5241652\n",
            " 0.42042238 0.45637494 0.53994876 0.5228606  0.54580075 0.40036255\n",
            " 0.5242403  0.5690008  0.4363547  0.5280672  0.51072127 0.5353375\n",
            " 0.59826714 0.53656465 0.46556646 0.5142053  0.53078395 0.5847264\n",
            " 0.4273203  0.40512878 0.49000007 0.55321604 0.5374096  0.54409415\n",
            " 0.54430014 0.49237067 0.6057723  0.45525938 0.51604253 0.45503622\n",
            " 0.46540123 0.42505747 0.510511   0.5489555  0.39322883 0.56994325\n",
            " 0.41742927 0.57893044 0.58515745 0.58024555 0.54322964 0.48169476\n",
            " 0.5178924  0.52270156 0.5269492  0.5095878  0.5612672  0.57741886\n",
            " 0.5077608  0.52727705 0.49087983 0.5524047  0.54477125 0.53275734\n",
            " 0.51113707 0.4846676  0.5494109  0.55613786 0.5885454  0.3927701\n",
            " 0.5260077  0.50448304 0.552923   0.5393134  0.5363353  0.5036064\n",
            " 0.5393296  0.5336779  0.46496326 0.5134285  0.52664334 0.4181121\n",
            " 0.5420504  0.51488644 0.55631953 0.5688208  0.60469943 0.5445331\n",
            " 0.50659996 0.49957496 0.48071867 0.4445005  0.51216036 0.5538414\n",
            " 0.5436891  0.41619903 0.5179382  0.53210264 0.5090986  0.52352935\n",
            " 0.5408862  0.48329622 0.5268627  0.49028164 0.52863747 0.5478154\n",
            " 0.52500707 0.47717172 0.55527955 0.4437223  0.5729409  0.5564528\n",
            " 0.5372866  0.5116268  0.4466899  0.5268057  0.48118716 0.52252465\n",
            " 0.59788615 0.49155384 0.52917105 0.55589825 0.5366252  0.55456597\n",
            " 0.5716024  0.5091725  0.55749184 0.5252724  0.4743603  0.53054553\n",
            " 0.49145156 0.48915035 0.43423945 0.5957113  0.54011136 0.50777894\n",
            " 0.5730069  0.5202299  0.5586129  0.5247155  0.5474492  0.52643377\n",
            " 0.57247907 0.5771881  0.4994195  0.46693665 0.5191913  0.55258113\n",
            " 0.5492745  0.4462989  0.43854028 0.5426617  0.58672243 0.5495673\n",
            " 0.4982682  0.53741866 0.5217238  0.5800398  0.5394972  0.5036996\n",
            " 0.57444483 0.5453022  0.51713735 0.5790828  0.52709395 0.40138632\n",
            " 0.5097371  0.48338443 0.54343444 0.5130723  0.4775527  0.503238\n",
            " 0.53069836 0.5507265  0.60624534 0.5370043  0.49691063 0.4706846\n",
            " 0.56402    0.57186633 0.55931455 0.54432017 0.51109844 0.5912164\n",
            " 0.49309117 0.55926496 0.4340902  0.5199111  0.5307508  0.52122194\n",
            " 0.5406807  0.5199912  0.5391932  0.4953311  0.52056795 0.5193849\n",
            " 0.5443912  0.5584248  0.47468644 0.47166806 0.4801491  0.49769217\n",
            " 0.44556457 0.4755054  0.5569194  0.48384959 0.54683    0.42453462\n",
            " 0.5348123  0.39865476 0.54049546 0.5037859  0.44557577 0.47871953\n",
            " 0.5290459  0.5359693  0.49944478 0.555322   0.5021613  0.46620804\n",
            " 0.52785784 0.5557113  0.50295335 0.39867288 0.5475164  0.5019205\n",
            " 0.5628391  0.49662167 0.5446635  0.38890487 0.5500136  0.53897554\n",
            " 0.45064884 0.5298684  0.53170925 0.51400024 0.5636247  0.5054031\n",
            " 0.5126422  0.5727914  0.53776294 0.44536287 0.47473413 0.46298558\n",
            " 0.5417877  0.5053082  0.42890698 0.51067835 0.51608235 0.53042537\n",
            " 0.56830126 0.5299755  0.53137404 0.53625137 0.5509091  0.48598534\n",
            " 0.5844403  0.48041397 0.5755103  0.52929765 0.5590723  0.5540803\n",
            " 0.503717   0.469338   0.5363365  0.56287533 0.53409916 0.51953894\n",
            " 0.41992933]\n",
            "[0.39793282 0.57105943 0.5081826  0.38587425 0.64857881 0.66494401\n",
            " 0.45478036 0.42463394 0.47286822 0.60723514 0.49784668 0.43927649\n",
            " 0.65116279 0.62704565 0.60120586 0.52282515 0.50129199 0.53402239\n",
            " 0.69164513 0.60551249 0.61068045 0.42291128 0.62962963 0.4788975\n",
            " 0.6124031  0.37639966 0.65374677 0.47803618 0.4788975  0.42204996\n",
            " 0.57622739 0.47114556 0.5667528  0.41946598 0.53919035 0.4952627\n",
            " 0.52540913 0.57622739 0.39276486 0.52196382 0.46511628 0.60551249\n",
            " 0.33936262 0.47114556 0.4754522  0.49870801 0.52196382 0.5374677\n",
            " 0.49784668 0.51248923 0.50990525 0.51507321 0.45822567 0.54349699\n",
            " 0.5211025  0.63049096 0.36434109 0.54263566 0.34453058 0.37898363\n",
            " 0.4005168  0.49870801 0.49009475 0.36950904 0.55813953 0.52196382\n",
            " 0.51593454 0.46339363 0.42635659 0.51593454 0.41774332 0.32816537\n",
            " 0.50990525 0.52282515 0.5374677  0.44702842 0.53919035 0.54091301\n",
            " 0.4005168  0.5047373  0.64254953 0.43927649 0.45564169 0.47975883\n",
            " 0.72437554 0.38673557 0.58053402 0.33419466 0.40999139 0.36089578\n",
            " 0.35228252 0.54263566 0.63479759 0.56847545 0.41946598 0.52799311\n",
            " 0.60378984 0.45133506 0.58225668 0.48664944 0.46080965 0.59689922\n",
            " 0.48751077 0.53402239 0.416882   0.5081826  0.42291128 0.41774332\n",
            " 0.54952627 0.55297158 0.59173127 0.58570198 0.49956934 0.51679587\n",
            " 0.49009475 0.43755383 0.55555556 0.52627046 0.4918174  0.5503876\n",
            " 0.53057709 0.37037037 0.46597761 0.4788975  0.57622739 0.55297158\n",
            " 0.47975883 0.54349699 0.50129199 0.57450474 0.54435831 0.45822567\n",
            " 0.43238587 0.62618432 0.57536606 0.48062016 0.30577089 0.50904393\n",
            " 0.6089578  0.57536606 0.54263566 0.52971576 0.50990525 0.5667528\n",
            " 0.48492679 0.44875108 0.53919035 0.51076658 0.46511628 0.47631352\n",
            " 0.62015504 0.56072351 0.41515935 0.48406546 0.56416882 0.51421189\n",
            " 0.28251507 0.67355728 0.48751077 0.53660637 0.47975883 0.55727821\n",
            " 0.47286822 0.45564169 0.46770026 0.71490095 0.5994832  0.63824289\n",
            " 0.48492679 0.42807924 0.6416882  0.50645995 0.64685616 0.60465116\n",
            " 0.43755383 0.49440138 0.58225668 0.56589147 0.49698536 0.40740741\n",
            " 0.48923342 0.52971576 0.50990525 0.53488372 0.50732127 0.5503876\n",
            " 0.55727821 0.49956934 0.63393626 0.44099914 0.48751077 0.59689922\n",
            " 0.5503876  0.53229974 0.55727821 0.56158484 0.44444444 0.35745047\n",
            " 0.44444444 0.43669251 0.42291128 0.47459087 0.61498708 0.5047373\n",
            " 0.46942291 0.53143842 0.66838932 0.38329027 0.45305771 0.68217054\n",
            " 0.45564169 0.41085271 0.56933678 0.53229974 0.59259259 0.42118863\n",
            " 0.51937984 0.53402239 0.44099914 0.50387597 0.63307494 0.49354005\n",
            " 0.65202412 0.62015504 0.42549526 0.51421189 0.5667528  0.43152455\n",
            " 0.48148148 0.4952627  0.52627046 0.68561585 0.43152455 0.49612403\n",
            " 0.55986219 0.42118863 0.56158484 0.44358312 0.55124892 0.56416882\n",
            " 0.57019811 0.57708872 0.54694229 0.52024117 0.50732127 0.47459087\n",
            " 0.4952627  0.50732127 0.4918174  0.54091301 0.36864772 0.50732127\n",
            " 0.43066322 0.37984496 0.58570198 0.43066322 0.54349699 0.38501292\n",
            " 0.51851852 0.6089578  0.44099914 0.53919035 0.68130922 0.91731266\n",
            " 0.68303187 0.44444444 0.41860465 0.57278208 0.51421189 0.6546081\n",
            " 0.42549526 0.33936262 0.47717485 0.37639966 0.49009475 0.50215332\n",
            " 0.55900086 0.45391904 0.66322136 0.36003445 0.50990525 0.40740741\n",
            " 0.44013781 0.41429802 0.64513351 0.59000861 0.34969854 0.68044789\n",
            " 0.40654608 0.69939707 0.6580534  0.583118   0.44788975 1.\n",
            " 0.47631352 0.55900086 0.4496124  0.47631352 0.69595177 0.57622739\n",
            " 0.49009475 0.59000861 0.32816537 0.55813953 0.44702842 0.61412575\n",
            " 0.63049096 0.52971576 0.58484065 0.47459087 0.6873385  0.82773471\n",
            " 0.56589147 0.51679587 0.5245478  0.56589147 0.5667528  0.5047373\n",
            " 0.53402239 0.54177433 0.38673557 0.50990525 0.49095607 0.41257537\n",
            " 0.50559862 0.56933678 0.54263566 0.66666667 0.62704565 0.51937984\n",
            " 0.39879414 0.4918174  0.58656331 0.39362618 0.50129199 0.52368648\n",
            " 0.64427218 0.39793282 0.60378984 0.62187769 0.55641688 0.54866494\n",
            " 0.43152455 0.49612403 0.58570198 0.58914729 0.5047373  0.57881137\n",
            " 0.76055125 0.46080965 0.52971576 0.44875108 0.57364341 0.68217054\n",
            " 0.57622739 0.48406546 0.45478036 0.47803618 0.47803618 0.48923342\n",
            " 0.62704565 0.5047373  0.58914729 0.57019811 0.47631352 0.5538329\n",
            " 0.39190353 0.42291128 0.69681309 0.51593454 0.39448751 0.51507321\n",
            " 0.5374677  0.4754522  0.38845823 0.61154177 0.46339363 0.54521964\n",
            " 0.56847545 0.44788975 0.60981912 0.49267873 0.54780362 0.55813953\n",
            " 0.57450474 0.58656331 0.61670973 0.51937984 0.55469423 0.49440138\n",
            " 0.51248923 0.3712317  0.42463394 0.32213609 0.60981912 0.45822567\n",
            " 0.46597761 0.4918174  0.54952627 0.6287683  0.54694229 0.45305771\n",
            " 0.58656331 0.53229974 0.60637382 0.63221361 0.74677003 0.35400517\n",
            " 0.48664944 0.43066322 0.61068045 0.49956934 0.42894057 0.51421189\n",
            " 0.48751077 0.55469423 0.68130922 0.49784668 0.53660637 0.46511628\n",
            " 0.5081826  0.62790698 0.49440138 0.38070629 0.46770026 0.66838932\n",
            " 0.51765719 0.51076658 0.42204996 0.43583118 0.55986219 0.60206718\n",
            " 0.57278208 0.53832903 0.55641688 0.61154177 0.5211025  0.53143842\n",
            " 0.60981912 0.5667528  0.48148148 0.52024117 0.42204996 0.63738157\n",
            " 0.40826873 0.48923342 0.68475452 0.59173127 0.52713178 0.37812231\n",
            " 0.43152455 0.35486649 0.58139535 0.4461671  0.49440138 0.34280792\n",
            " 0.54435831 0.65030146 0.5081826  0.61843239 0.43927649 0.50559862\n",
            " 0.51248923 0.59862188 0.5994832  0.39793282 0.46339363 0.42894057\n",
            " 0.61757106 0.47372954 0.47975883 0.44530577 0.56330749 0.45822567\n",
            " 0.43927649 0.45047373 0.57364341 0.4625323  0.625323   0.44702842\n",
            " 0.54694229 0.59173127 0.5667528  0.49354005 0.47975883 0.49095607\n",
            " 0.55297158 0.55813953 0.3875969  0.49095607 0.72265289 0.52627046\n",
            " 0.56330749 0.35486649 0.61068045 0.53402239 0.41860465 0.53660637\n",
            " 0.57364341 0.43841516 0.54694229 0.47200689 0.54091301 0.58742463\n",
            " 0.62187769 0.30577089 0.52799311 0.52713178 0.60637382 0.45564169\n",
            " 0.42894057]\n",
            "The trained model has an aproximate error rate of 4.374949752118023 which equates to 1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure(slp)\n"
      ],
      "metadata": {
        "id": "eJ4eYJryNjGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14126fb1-b478-4e56-9d49-0b46b8e4da4d",
        "id": "mhUamxCZOTrA"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] != 2012]\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] < 2020]\n",
        "cols = df_slp_dnn['NUM_COLLISIONS']\n",
        "df_slp_dnn = df_slp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp_dnn[:6])\n",
        "df_slp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "41c52f48-bd5c-4a2b-fbb0-256d8553c51c",
        "id": "P-VEj2lxOTrB"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  1016.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  1014.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  1021.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29   999.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  1015.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  1020.7    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da          slp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2016.000391    15.719765  1016.777221     0.082192     0.084932   \n",
              "std       2.000294     8.796698     7.628429     0.274710     0.278834   \n",
              "min    2013.000000     1.000000   989.500000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000  1012.200000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000  1016.700000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000  1021.700000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000  1044.200000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084540     0.077104     0.084932     0.084932     0.082192  ...   \n",
              "std       0.278251     0.266808     0.278834     0.278834     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142466      599.147162  \n",
              "std       0.349996     0.349996     0.349996     0.349596      100.268048  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c429bf2-046c-4d36-81b8-5032e00ee318\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.000391</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142466</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000294</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349596</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c429bf2-046c-4d36-81b8-5032e00ee318')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c429bf2-046c-4d36-81b8-5032e00ee318 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c429bf2-046c-4d36-81b8-5032e00ee318');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp_dnn.iloc[np.random.permutation(len(df_slp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5e4510-8fb7-44e7-df6b-52a952e25a72",
        "id": "EG2EcMoLOTrC"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "228   2015  25  1000.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "225   2019   7  1024.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "812   2019  16  1009.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2514  2013  25  1012.3    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "3100  2015  18  1034.9    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "187   2014  23  1014.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "228     0    0    1    0    0    0    0  \n",
            "225     0    0    0    1    0    0    0  \n",
            "812     0    0    0    0    0    0    0  \n",
            "2514    1    0    0    0    0    1    0  \n",
            "3100    0    0    0    0    0    1    0  \n",
            "187     0    0    0    0    0    0    1  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f5a9ad-5eaf-414e-b766-b3bd4dfb7bb0",
        "id": "pd6Uk6a9OTrC"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "228     406\n",
            "225     584\n",
            "812     550\n",
            "2514    539\n",
            "3100    628\n",
            "187     612\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80077da7-e01d-4cf0-d80f-bc003e05b171",
        "id": "QX1fWn3jOTrC"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99885d1c-728e-48ee-c20b-29ea7a3fbcc5",
        "id": "C9uL6bF7OTrD"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769de7c50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 519866.2, step = 1\n",
            "INFO:tensorflow:global_step/sec: 375.058\n",
            "INFO:tensorflow:loss = 0.007341341, step = 101 (0.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 477.047\n",
            "INFO:tensorflow:loss = 0.007361635, step = 201 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.452\n",
            "INFO:tensorflow:loss = 0.0076099923, step = 301 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 405.061\n",
            "INFO:tensorflow:loss = 0.008149187, step = 401 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 460.494\n",
            "INFO:tensorflow:loss = 0.007698088, step = 501 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 389.37\n",
            "INFO:tensorflow:loss = 0.0068629105, step = 601 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.192\n",
            "INFO:tensorflow:loss = 0.0063186768, step = 701 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.423\n",
            "INFO:tensorflow:loss = 0.007199923, step = 801 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 401.071\n",
            "INFO:tensorflow:loss = 0.007154421, step = 901 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 352.785\n",
            "INFO:tensorflow:loss = 0.00941618, step = 1001 (0.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.206\n",
            "INFO:tensorflow:loss = 0.0071733985, step = 1101 (0.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.352\n",
            "INFO:tensorflow:loss = 0.008179106, step = 1201 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.505\n",
            "INFO:tensorflow:loss = 0.0066835186, step = 1301 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.718\n",
            "INFO:tensorflow:loss = 0.0064155376, step = 1401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.874\n",
            "INFO:tensorflow:loss = 0.009530839, step = 1501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.514\n",
            "INFO:tensorflow:loss = 0.010960236, step = 1601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 625.7\n",
            "INFO:tensorflow:loss = 0.009649563, step = 1701 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 638.296\n",
            "INFO:tensorflow:loss = 0.009156121, step = 1801 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.226\n",
            "INFO:tensorflow:loss = 0.008383161, step = 1901 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.224\n",
            "INFO:tensorflow:loss = 0.0070120078, step = 2001 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.793\n",
            "INFO:tensorflow:loss = 0.007986804, step = 2101 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.439\n",
            "INFO:tensorflow:loss = 0.0064311204, step = 2201 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.103\n",
            "INFO:tensorflow:loss = 0.009248378, step = 2301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 662.751\n",
            "INFO:tensorflow:loss = 0.007913995, step = 2401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.562\n",
            "INFO:tensorflow:loss = 0.0066349832, step = 2501 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.93\n",
            "INFO:tensorflow:loss = 0.009127185, step = 2601 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.672\n",
            "INFO:tensorflow:loss = 0.009191401, step = 2701 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.057\n",
            "INFO:tensorflow:loss = 0.007171122, step = 2801 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.082\n",
            "INFO:tensorflow:loss = 0.006385364, step = 2901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.487\n",
            "INFO:tensorflow:loss = 0.005899122, step = 3001 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.06\n",
            "INFO:tensorflow:loss = 0.0073453886, step = 3101 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.174\n",
            "INFO:tensorflow:loss = 0.010100739, step = 3201 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.684\n",
            "INFO:tensorflow:loss = 0.0068510547, step = 3301 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.495\n",
            "INFO:tensorflow:loss = 0.008578805, step = 3401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.518\n",
            "INFO:tensorflow:loss = 0.007849358, step = 3501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.722\n",
            "INFO:tensorflow:loss = 0.007654249, step = 3601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.546\n",
            "INFO:tensorflow:loss = 0.008043668, step = 3701 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.005\n",
            "INFO:tensorflow:loss = 0.009838961, step = 3801 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.938\n",
            "INFO:tensorflow:loss = 0.0074500665, step = 3901 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.095\n",
            "INFO:tensorflow:loss = 0.006607913, step = 4001 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.408\n",
            "INFO:tensorflow:loss = 0.007843617, step = 4101 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.677\n",
            "INFO:tensorflow:loss = 0.0083784275, step = 4201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.059\n",
            "INFO:tensorflow:loss = 0.008556696, step = 4301 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 662.211\n",
            "INFO:tensorflow:loss = 0.0079665985, step = 4401 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.918\n",
            "INFO:tensorflow:loss = 0.006891963, step = 4501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.225\n",
            "INFO:tensorflow:loss = 0.0058552283, step = 4601 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.7\n",
            "INFO:tensorflow:loss = 0.010168761, step = 4701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.186\n",
            "INFO:tensorflow:loss = 0.0099713635, step = 4801 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.75\n",
            "INFO:tensorflow:loss = 0.0061887703, step = 4901 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.133\n",
            "INFO:tensorflow:loss = 0.007800729, step = 5001 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.88\n",
            "INFO:tensorflow:loss = 0.006698651, step = 5101 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.709\n",
            "INFO:tensorflow:loss = 0.0074429037, step = 5201 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.266\n",
            "INFO:tensorflow:loss = 0.00786602, step = 5301 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.523\n",
            "INFO:tensorflow:loss = 0.00908776, step = 5401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.197\n",
            "INFO:tensorflow:loss = 0.007356336, step = 5501 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.092\n",
            "INFO:tensorflow:loss = 0.0063912375, step = 5601 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.215\n",
            "INFO:tensorflow:loss = 0.0068679005, step = 5701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.011\n",
            "INFO:tensorflow:loss = 0.00717062, step = 5801 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.648\n",
            "INFO:tensorflow:loss = 0.0068142675, step = 5901 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.129\n",
            "INFO:tensorflow:loss = 0.007416384, step = 6001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.079\n",
            "INFO:tensorflow:loss = 0.007644195, step = 6101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.103\n",
            "INFO:tensorflow:loss = 0.0076593766, step = 6201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.821\n",
            "INFO:tensorflow:loss = 0.009069411, step = 6301 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.712\n",
            "INFO:tensorflow:loss = 0.006748761, step = 6401 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.211\n",
            "INFO:tensorflow:loss = 0.007141436, step = 6501 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.401\n",
            "INFO:tensorflow:loss = 0.007497927, step = 6601 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.059\n",
            "INFO:tensorflow:loss = 0.0076280273, step = 6701 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 443.02\n",
            "INFO:tensorflow:loss = 0.010817209, step = 6801 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.112\n",
            "INFO:tensorflow:loss = 0.00781465, step = 6901 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.78\n",
            "INFO:tensorflow:loss = 0.0076046446, step = 7001 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.379\n",
            "INFO:tensorflow:loss = 0.010236029, step = 7101 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 452.73\n",
            "INFO:tensorflow:loss = 0.008059582, step = 7201 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 444.727\n",
            "INFO:tensorflow:loss = 0.00935003, step = 7301 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 440.194\n",
            "INFO:tensorflow:loss = 0.0116317645, step = 7401 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 411.548\n",
            "INFO:tensorflow:loss = 0.0061974074, step = 7501 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 406.935\n",
            "INFO:tensorflow:loss = 0.008335031, step = 7601 (0.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 431.322\n",
            "INFO:tensorflow:loss = 0.007439731, step = 7701 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 403.057\n",
            "INFO:tensorflow:loss = 0.007996088, step = 7801 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 492.629\n",
            "INFO:tensorflow:loss = 0.007333626, step = 7901 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.01\n",
            "INFO:tensorflow:loss = 0.007855314, step = 8001 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.145\n",
            "INFO:tensorflow:loss = 0.008996054, step = 8101 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 437.153\n",
            "INFO:tensorflow:loss = 0.006186885, step = 8201 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 444.654\n",
            "INFO:tensorflow:loss = 0.005522522, step = 8301 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 436.313\n",
            "INFO:tensorflow:loss = 0.007827906, step = 8401 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 429.315\n",
            "INFO:tensorflow:loss = 0.0067899227, step = 8501 (0.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 430.778\n",
            "INFO:tensorflow:loss = 0.0063655134, step = 8601 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 444.023\n",
            "INFO:tensorflow:loss = 0.0070089228, step = 8701 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 433.61\n",
            "INFO:tensorflow:loss = 0.0046774065, step = 8801 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.022\n",
            "INFO:tensorflow:loss = 0.007822428, step = 8901 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.584\n",
            "INFO:tensorflow:loss = 0.010539986, step = 9001 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.058\n",
            "INFO:tensorflow:loss = 0.0067732558, step = 9101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.843\n",
            "INFO:tensorflow:loss = 0.010699691, step = 9201 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.184\n",
            "INFO:tensorflow:loss = 0.0078167785, step = 9301 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.079\n",
            "INFO:tensorflow:loss = 0.0093641775, step = 9401 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.218\n",
            "INFO:tensorflow:loss = 0.0072650337, step = 9501 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.178\n",
            "INFO:tensorflow:loss = 0.0059588687, step = 9601 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.418\n",
            "INFO:tensorflow:loss = 0.013143143, step = 9701 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.857\n",
            "INFO:tensorflow:loss = 0.007878922, step = 9801 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.512\n",
            "INFO:tensorflow:loss = 0.0071252235, step = 9901 (0.153 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.008632779.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 98.13725359927167\n",
            "Just using average = 598.7358121330724 has RMSE of 97.87847068956921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad99ad9-4a0a-429c-b938-b033662408dc",
        "id": "mTPv83i1OTrD"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d57aa90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5103302  0.5103613  0.5070674  0.5101613  0.5117183  0.5106867\n",
            " 0.51136506 0.50941634 0.5122532  0.5127063  0.5136491  0.5104544\n",
            " 0.5118376  0.50883657 0.51362693 0.5069814  0.5093562  0.51150364\n",
            " 0.50702965 0.51470375 0.5106147  0.5130661  0.51313823 0.5104656\n",
            " 0.5071237  0.5116045  0.5127797  0.510921   0.5125872  0.50882584\n",
            " 0.51363844 0.50987655 0.51251173 0.5134938  0.5119828  0.50744396\n",
            " 0.51180637 0.5075727  0.509041   0.5104948  0.51262087 0.5057461\n",
            " 0.5077617  0.507396   0.5068128  0.5083421  0.5093314  0.509261\n",
            " 0.50928867 0.5083774  0.51099455 0.5086956  0.50989336 0.51573765\n",
            " 0.5075681  0.50841147 0.5132842  0.5070294  0.508884   0.5094659\n",
            " 0.51422936 0.51165926 0.5093714  0.5123597  0.50597346 0.5075458\n",
            " 0.5059107  0.5125922  0.51007754 0.5091987  0.51165634 0.51374996\n",
            " 0.5067896  0.5112192  0.51398325 0.50664747 0.50909543 0.5069687\n",
            " 0.5133351  0.5100954  0.5138924  0.51175046 0.5086712  0.5089871\n",
            " 0.5086598  0.510961   0.5077289  0.50772566 0.5069042  0.50754243\n",
            " 0.5134667  0.5138316  0.5103418  0.5079931  0.51408714 0.5089938\n",
            " 0.5077756  0.50771075 0.50620365 0.51346    0.51383436 0.5086966\n",
            " 0.5089833  0.5114029  0.51177067 0.5137     0.5095158  0.50532633\n",
            " 0.5141954  0.50732976 0.5072987  0.50836974 0.5117251  0.5084915\n",
            " 0.50928396 0.5083495  0.51112163 0.5114167  0.50917155 0.50760263\n",
            " 0.50975186 0.5129154  0.51067907 0.512944   0.5105658  0.5126772\n",
            " 0.5108151  0.5135519  0.5088517  0.5139287  0.51059073 0.5093731\n",
            " 0.5080455  0.5108273  0.51059073 0.5087968  0.505403   0.5088505\n",
            " 0.5122814  0.5089545  0.5124927  0.51115    0.5109937  0.5121261\n",
            " 0.5131044  0.51139456 0.5113417  0.5080644  0.50699896 0.5106928\n",
            " 0.511332   0.50777704 0.50826263 0.50996745 0.50748736 0.50766164\n",
            " 0.5111174  0.5061828  0.5108394  0.5133136  0.50971854 0.51212853\n",
            " 0.5072563  0.51225203 0.5055551  0.5134529  0.5128496  0.50895256\n",
            " 0.5125524  0.5093399  0.513844   0.5128355  0.5073734  0.5139116\n",
            " 0.5124788  0.5096367  0.5140165  0.5096579  0.5110736  0.5124503\n",
            " 0.5092164  0.50577205 0.5089232  0.51314366 0.50816774 0.5053234\n",
            " 0.51041436 0.5110889  0.50984526 0.5117396  0.5118691  0.50896114\n",
            " 0.51335883 0.505873   0.5100205  0.5094384  0.5120087  0.5142684\n",
            " 0.51345956 0.50963736 0.50977933 0.51276815 0.5118569  0.5093046\n",
            " 0.5107411  0.5082604  0.5116043  0.50940424 0.50983757 0.5098875\n",
            " 0.5107562  0.5103142  0.51102185 0.5123733  0.51424795 0.5060432\n",
            " 0.50957036 0.50797725 0.5093549  0.507866   0.514465   0.5085919\n",
            " 0.50651276 0.51031476 0.5060861  0.51027685 0.5116173  0.5128911\n",
            " 0.51454145 0.50756115 0.510999   0.5106558  0.51071    0.5086325\n",
            " 0.5091367  0.5097514  0.51052696 0.51269686 0.51185966 0.51195633\n",
            " 0.5087073  0.5086566  0.51477945 0.5112268  0.50680256 0.5120547\n",
            " 0.51543874 0.51156646 0.5064078  0.5094746  0.50736576 0.5122502\n",
            " 0.5056578  0.50979006 0.51167804 0.509493   0.5129838  0.51330453\n",
            " 0.5150213  0.51312464 0.5098655  0.51039195 0.5149425  0.51480645\n",
            " 0.50706905 0.5091261  0.5065904  0.51176155 0.51309    0.50764304\n",
            " 0.509136   0.5121739  0.50895673 0.5068418  0.51174086 0.5115583\n",
            " 0.51203173 0.51084995 0.5067872  0.508096   0.51336586 0.51067525\n",
            " 0.5064129  0.50884855 0.5099935  0.5109986  0.5131132  0.5104385\n",
            " 0.51156753 0.5128356  0.5073838  0.5076399  0.50827354 0.51125646\n",
            " 0.5111702  0.5084307  0.50971574 0.5109143  0.5101821  0.50943196\n",
            " 0.509827   0.5098347  0.5065407  0.5107228  0.5095996  0.51283\n",
            " 0.5124088  0.5130395  0.51240957 0.5126634  0.50836426 0.508836\n",
            " 0.50830275 0.5065063  0.50845677 0.51159257 0.508564   0.50719726\n",
            " 0.5130963  0.511207   0.5078058  0.50957257 0.50608987 0.51310676\n",
            " 0.51030266 0.5096769  0.50657314 0.507302   0.509574   0.5128336\n",
            " 0.50867903 0.5119424  0.51372594 0.5102678  0.5116146  0.51312786\n",
            " 0.50736535 0.5062667  0.51238436 0.5146393  0.51147574 0.5120154\n",
            " 0.5095995  0.509302   0.50824857 0.5132623  0.5135453  0.5109939\n",
            " 0.5094333  0.5111238  0.51447004 0.5124011  0.50987446 0.51003176\n",
            " 0.51294076 0.50790685 0.5098602  0.50664073 0.5117672  0.50834596\n",
            " 0.5119015  0.50955737 0.5099287  0.5082683  0.5141022  0.51182455\n",
            " 0.5099877  0.51387024 0.5090856  0.51393604 0.5130931  0.5097181\n",
            " 0.5093777  0.5093053  0.5105371  0.5127222  0.5120568  0.50740725\n",
            " 0.514873   0.505402   0.50907755 0.5119077  0.512883   0.5088913\n",
            " 0.5096506  0.51385784 0.51153004 0.50810575 0.51374704 0.50749993\n",
            " 0.5134138  0.511396   0.51317793 0.5074615  0.5071538  0.5119156\n",
            " 0.51203895 0.5134144  0.5146396  0.5121893  0.50868315 0.5127579\n",
            " 0.50799626 0.5140115  0.50558287 0.5077382  0.51283365 0.5125421\n",
            " 0.5076386  0.5130848  0.51233715 0.50646037 0.5092541  0.50771266\n",
            " 0.50758624 0.5113138  0.50759184 0.51234406 0.51190174 0.51129544\n",
            " 0.5135697  0.51141715 0.5097987  0.5092939  0.5148107  0.5122306\n",
            " 0.5152159  0.50921375 0.5065587  0.5112767  0.5083118  0.5084334\n",
            " 0.51074284 0.5102866  0.5089375  0.50962627 0.5086357  0.51441765\n",
            " 0.5118194  0.51200414 0.51114273 0.5081004  0.5124843  0.509252\n",
            " 0.50749344 0.5079059  0.5113058  0.50981814 0.51153004 0.51234984\n",
            " 0.5119318  0.5064375  0.51214856 0.5108792  0.5145095  0.5074881\n",
            " 0.51073956 0.5139395  0.5081498  0.5151041  0.5118525  0.5079707\n",
            " 0.5110418  0.50936675 0.5124547  0.5093012  0.50847703 0.51332366\n",
            " 0.5144726  0.5080813  0.5121336  0.51090664 0.5114133  0.5081679\n",
            " 0.5073435  0.50695354 0.50988567 0.5124051  0.51180387 0.50770956\n",
            " 0.5138476  0.511186   0.5110569  0.5129181  0.5132642  0.50688094\n",
            " 0.5069923  0.510884   0.508378   0.51448    0.51341337 0.51267844\n",
            " 0.5105178  0.5127124  0.51021314 0.50671774 0.5117933  0.50767964\n",
            " 0.51441824 0.5073707  0.50879574 0.5115809  0.51046675 0.50926566\n",
            " 0.51038164 0.5138459  0.51086074 0.5086688  0.5105985  0.5133889\n",
            " 0.51160395]\n",
            "[0.6709733  0.51076658 0.58914729 0.49698536 0.59345392 0.56589147\n",
            " 0.45047373 0.54091301 0.67011197 0.52713178 0.49612403 0.55727821\n",
            " 0.52713178 0.47114556 0.62962963 0.55900086 0.52282515 0.57622739\n",
            " 0.4952627  0.54005168 0.51937984 0.58828596 0.45305771 0.56158484\n",
            " 0.43755383 0.63738157 0.45650301 0.4203273  0.66322136 0.49612403\n",
            " 0.47631352 0.55555556 0.41343669 0.54694229 0.55813953 0.57881137\n",
            " 0.48492679 0.51507321 0.54349699 0.53660637 0.53229974 0.41946598\n",
            " 0.53229974 0.43066322 0.60034453 0.58053402 0.45047373 0.54866494\n",
            " 0.62790698 0.55727821 0.47286822 0.40913006 0.44444444 0.55469423\n",
            " 0.58397933 0.40740741 0.7166236  0.57105943 0.46339363 0.50129199\n",
            " 0.45564169 0.65891473 0.59776055 0.40999139 0.49956934 0.50215332\n",
            " 0.47286822 0.60465116 0.36692506 0.49956934 0.43496985 0.50990525\n",
            " 0.52799311 0.50559862 0.69164513 0.44444444 0.47459087 0.43496985\n",
            " 0.60120586 0.53488372 0.58656331 0.54780362 0.54866494 0.44013781\n",
            " 0.47286822 0.40568475 0.59776055 0.54005168 0.47286822 0.6124031\n",
            " 0.45391904 0.64771748 0.51679587 0.55555556 0.33936262 0.43496985\n",
            " 0.61584841 0.44099914 0.45822567 0.52024117 0.64857881 0.47372954\n",
            " 0.64944014 0.51593454 0.56589147 0.54349699 0.59259259 0.40826873\n",
            " 0.55900086 0.41429802 0.62273902 0.63393626 0.66666667 0.60981912\n",
            " 0.55986219 0.46597761 0.53402239 0.48234281 0.51851852 0.39190353\n",
            " 0.60034453 0.44099914 0.54177433 0.40999139 0.57019811 0.36864772\n",
            " 0.52713178 0.60551249 0.45908699 0.40999139 0.56158484 0.60378984\n",
            " 0.46597761 0.45219638 0.53574505 0.45478036 0.35745047 0.62015504\n",
            " 0.49956934 0.42980189 0.28682171 0.44530577 0.43755383 0.56244617\n",
            " 0.55986219 0.44702842 0.43669251 0.57536606 0.45736434 0.55124892\n",
            " 0.51507321 0.57881137 0.54521964 0.44444444 0.33419466 0.51507321\n",
            " 0.47372954 0.50732127 0.71748493 0.45564169 0.56761413 0.60206718\n",
            " 0.50129199 0.36175711 0.41257537 0.4039621  0.42894057 0.49354005\n",
            " 0.61154177 0.54091301 0.55124892 0.54177433 0.36003445 0.57536606\n",
            " 0.61929371 0.63738157 0.53919035 0.63652024 0.50043066 0.4918174\n",
            " 0.53488372 0.43496985 0.56847545 0.4918174  0.53919035 0.32816537\n",
            " 0.49095607 0.36089578 0.52713178 0.52713178 0.44530577 0.62618432\n",
            " 0.35228252 0.53919035 0.32213609 0.53057709 0.58656331 0.53229974\n",
            " 0.55727821 0.70801034 0.56416882 0.4918174  0.39793282 0.45908699\n",
            " 0.4203273  0.45908699 0.4203273  0.61498708 0.53660637 0.38845823\n",
            " 0.48923342 0.50215332 0.5245478  0.51765719 0.4005168  0.55813953\n",
            " 0.48320413 0.56158484 0.32730405 0.50215332 0.56416882 0.5960379\n",
            " 0.60723514 0.52196382 0.63221361 0.54177433 0.69939707 0.53574505\n",
            " 0.51507321 0.5667528  0.5047373  0.416882   0.62015504 0.374677\n",
            " 0.64685616 0.53832903 0.59086994 0.58656331 0.41515935 0.37984496\n",
            " 0.46942291 0.56330749 0.54263566 0.53488372 0.53402239 0.40826873\n",
            " 0.53057709 0.59862188 0.45391904 0.54177433 0.53143842 0.56847545\n",
            " 0.47631352 0.57450474 0.53143842 0.51593454 0.38070629 0.40568475\n",
            " 0.56158484 0.56847545 0.59689922 0.50387597 0.72782084 0.65546942\n",
            " 0.51937984 0.51507321 0.76141258 0.58656331 0.46425495 0.54694229\n",
            " 0.56847545 0.60292851 0.4461671  0.6089578  0.36864772 0.50990525\n",
            " 0.43927649 0.61929371 0.52799311 0.56416882 0.39534884 0.48923342\n",
            " 0.48062016 0.43669251 0.4332472  0.41085271 0.4754522  0.55297158\n",
            " 0.63135228 0.54263566 0.5503876  0.53660637 0.5796727  0.44875108\n",
            " 0.54005168 0.54435831 0.48578811 0.4918174  0.59086994 0.53229974\n",
            " 0.49440138 0.57795004 0.58570198 0.46167097 0.57364341 0.59000861\n",
            " 0.67786391 0.36692506 0.48923342 0.48406546 0.54091301 0.62790698\n",
            " 0.52368648 0.53919035 0.55555556 0.62015504 0.58139535 0.6873385\n",
            " 0.54091301 0.60120586 0.61068045 0.70456503 0.43669251 0.69939707\n",
            " 0.49784668 0.76055125 0.45564169 0.49354005 0.52971576 0.50301464\n",
            " 0.36778639 0.64599483 0.49698536 0.55469423 0.44530577 0.51593454\n",
            " 0.51765719 0.47459087 0.6089578  0.51937984 0.49009475 0.62187769\n",
            " 0.45564169 0.59345392 0.53057709 0.43496985 0.40223945 0.5503876\n",
            " 0.26614987 0.37209302 0.61584841 0.48062016 0.49698536 0.44875108\n",
            " 0.52885444 0.47286822 0.45047373 0.52196382 0.54608096 0.53057709\n",
            " 0.63652024 0.60809647 0.55813953 0.56503015 0.52540913 0.52971576\n",
            " 0.63565891 0.46683893 0.44358312 0.45305771 0.56330749 0.44875108\n",
            " 0.57881137 0.61670973 0.48406546 0.50990525 0.62962963 0.51593454\n",
            " 0.5503876  0.41343669 0.44875108 0.44444444 0.44530577 0.48062016\n",
            " 0.56847545 0.57019811 0.53488372 0.4754522  0.65374677 0.48751077\n",
            " 0.5245478  0.64427218 0.69681309 0.47631352 0.37726098 0.33936262\n",
            " 0.54005168 0.45822567 0.49956934 0.49354005 0.54435831 0.40137812\n",
            " 0.50129199 0.47975883 0.55727821 0.51421189 0.42807924 0.74677003\n",
            " 0.34453058 0.4496124  0.45391904 0.44272179 0.63824289 0.68130922\n",
            " 0.5245478  0.38242894 0.45047373 0.69422911 0.49956934 0.63135228\n",
            " 0.59000861 0.63824289 0.53660637 0.60465116 0.48406546 0.31438415\n",
            " 0.53660637 0.48837209 0.60292851 0.48578811 0.53057709 0.46856158\n",
            " 0.57192076 0.49095607 0.52799311 0.4918174  0.52627046 0.58656331\n",
            " 0.56933678 0.40913006 0.52799311 0.48923342 0.59431525 0.29371232\n",
            " 0.39276486 0.3910422  0.59259259 0.34280792 0.43410853 0.58570198\n",
            " 0.51765719 0.48751077 0.49354005 0.60292851 0.5245478  0.57536606\n",
            " 0.59086994 0.42118863 0.52713178 0.5503876  0.43755383 0.65288544\n",
            " 0.47028424 0.71490095 0.49354005 0.47803618 0.53402239 0.33936262\n",
            " 0.56589147 0.45650301 0.57881137 0.37209302 0.44702842 0.58484065\n",
            " 0.51851852 0.37037037 0.46683893 0.47114556 0.50129199 0.66925065\n",
            " 0.38156761 0.55986219 0.47803618 0.66063738 0.52885444 0.56847545\n",
            " 0.47717485 0.46080965 0.52540913 0.56503015 0.5667528  0.374677\n",
            " 0.49870801 0.39879414 0.4754522  0.63738157 0.44013781 0.52799311\n",
            " 0.44530577 0.54091301 0.54177433 0.34969854 0.54091301 0.41085271\n",
            " 0.42894057 0.46339363 0.58914729 0.5245478  0.5081826  0.48062016\n",
            " 0.61843239]\n",
            "The trained model has an aproximate error rate of 8.212774415772019 which equates to 1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "lHX1HoDlQwJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e3f599-a105-40f5-b87e-a3e61729ed20",
        "id": "Tf_wMIWXQ7zg"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd  gust  \\\n",
            "3   2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0  20.0   \n",
            "11  2020  15             508  43.9  38.3  1019.4    8.2   5.4   14.0  15.0   \n",
            "12  2021   1             257  39.6  29.3  1029.3   10.0   7.6   14.0  20.0   \n",
            "14  2022  25             235  41.6  31.8  1013.2   10.0   9.6   15.0  19.0   \n",
            "18  2021   3             186  41.1  32.3  1018.0   10.0  10.3   19.0  27.0   \n",
            "19  2020   2             413  39.6  28.9  1011.8   10.0  13.0   19.0  26.0   \n",
            "\n",
            "    ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "11  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "12  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "14  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "18  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "19  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] != 2012]\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] < 2020]\n",
        "cols = df_gust_dnn['NUM_COLLISIONS']\n",
        "df_gust_dnn = df_gust_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust_dnn[:6])\n",
        "df_gust_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "dc91e1f6-1fdc-4ced-d3a9-827c798e8946",
        "id": "oTbpzolhQ7zh"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "74  2016  17  18.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "76  2014   9  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "79  2019  19  21.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "80  2015  11  17.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "83  2015  29  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "85  2019  13  15.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "74    0    1    0    0    0    0             451  \n",
            "76    0    0    0    0    0    1             561  \n",
            "79    0    0    0    0    0    0             479  \n",
            "80    0    1    0    0    0    0             341  \n",
            "83    0    0    0    0    0    1             519  \n",
            "85    0    1    0    0    0    0             374  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             year           da         gust          Apr          Aug  \\\n",
              "count  1629.00000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean   2015.91283    15.702885    27.511602     0.095764     0.042357   \n",
              "std       2.01341     8.667634     7.366770     0.294358     0.201465   \n",
              "min    2013.00000     1.000000    14.000000     0.000000     0.000000   \n",
              "25%    2014.00000     8.000000    22.000000     0.000000     0.000000   \n",
              "50%    2016.00000    16.000000    26.000000     0.000000     0.000000   \n",
              "75%    2018.00000    23.000000    31.100000     0.000000     0.000000   \n",
              "max    2019.00000    31.000000    71.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000  ...   \n",
              "mean      0.104359     0.095150     0.108656     0.046041     0.061387  ...   \n",
              "std       0.305819     0.293513     0.311302     0.209637     0.240113  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean      0.096378     0.087784     0.071209     0.139963     0.141191   \n",
              "std       0.295200     0.283067     0.257253     0.347055     0.348325   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000     1629.000000  \n",
              "mean      0.139963     0.151627     0.138122     0.145488      596.513198  \n",
              "std       0.347055     0.358769     0.345133     0.352700      104.479660  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      526.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      597.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      663.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c92041ed-363f-4d6e-855d-84734de686b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.91283</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>0.095764</td>\n",
              "      <td>0.042357</td>\n",
              "      <td>0.104359</td>\n",
              "      <td>0.095150</td>\n",
              "      <td>0.108656</td>\n",
              "      <td>0.046041</td>\n",
              "      <td>0.061387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096378</td>\n",
              "      <td>0.087784</td>\n",
              "      <td>0.071209</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.141191</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.151627</td>\n",
              "      <td>0.138122</td>\n",
              "      <td>0.145488</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.01341</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>0.294358</td>\n",
              "      <td>0.201465</td>\n",
              "      <td>0.305819</td>\n",
              "      <td>0.293513</td>\n",
              "      <td>0.311302</td>\n",
              "      <td>0.209637</td>\n",
              "      <td>0.240113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.295200</td>\n",
              "      <td>0.283067</td>\n",
              "      <td>0.257253</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.348325</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.358769</td>\n",
              "      <td>0.345133</td>\n",
              "      <td>0.352700</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.00000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.00000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.00000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.00000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c92041ed-363f-4d6e-855d-84734de686b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c92041ed-363f-4d6e-855d-84734de686b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c92041ed-363f-4d6e-855d-84734de686b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust_dnn.iloc[np.random.permutation(len(df_gust_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b143fe85-b50a-4361-b445-e94da453dff7",
        "id": "a_4bidEbQ7zi"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "281   2018   6  39.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "977   2018  23  15.9    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "3204  2013  15  20.0    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "3013  2014  24  29.9    0    0    0    0    0    0    0  ...    0    0    1   \n",
            "2433  2018  31  24.1    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "3601  2019  20  28.9    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "281     0    0    0    0    0    0    0  \n",
            "977     0    0    0    1    0    0    0  \n",
            "3204    0    0    0    0    1    0    0  \n",
            "3013    0    0    0    0    1    0    0  \n",
            "2433    0    0    0    0    1    0    0  \n",
            "3601    0    0    0    0    1    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f6bd0f-1508-4e2b-d21e-39a83cf7eb36",
        "id": "l4TDr10XQ7zi"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "281     599\n",
            "977     675\n",
            "3204    617\n",
            "3013    669\n",
            "2433    623\n",
            "3601    698\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc0d87c-3284-444a-ea7a-5769d10ee264",
        "id": "75AKCrz7Q7zi"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32ee6c4-b887-4a09-9546-c3118a9c93d7",
        "id": "v4av8KXMQ7zj"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376dac9a90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 18673.336, step = 1\n",
            "INFO:tensorflow:global_step/sec: 484.729\n",
            "INFO:tensorflow:loss = 0.70193785, step = 101 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.397\n",
            "INFO:tensorflow:loss = 0.8945514, step = 201 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.731\n",
            "INFO:tensorflow:loss = 1.0413133, step = 301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.419\n",
            "INFO:tensorflow:loss = 0.9185983, step = 401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.568\n",
            "INFO:tensorflow:loss = 0.6781819, step = 501 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.529\n",
            "INFO:tensorflow:loss = 0.47958076, step = 601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.243\n",
            "INFO:tensorflow:loss = 0.49747148, step = 701 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.253\n",
            "INFO:tensorflow:loss = 0.56368184, step = 801 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.624\n",
            "INFO:tensorflow:loss = 0.45416635, step = 901 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.677\n",
            "INFO:tensorflow:loss = 0.7625744, step = 1001 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.445\n",
            "INFO:tensorflow:loss = 0.415617, step = 1101 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 672.222\n",
            "INFO:tensorflow:loss = 0.3730878, step = 1201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.13\n",
            "INFO:tensorflow:loss = 0.29410893, step = 1301 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.733\n",
            "INFO:tensorflow:loss = 0.3086409, step = 1401 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.583\n",
            "INFO:tensorflow:loss = 0.21529213, step = 1501 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 644.592\n",
            "INFO:tensorflow:loss = 0.26920673, step = 1601 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 310.319\n",
            "INFO:tensorflow:loss = 0.27754658, step = 1701 (0.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.836\n",
            "INFO:tensorflow:loss = 0.20628981, step = 1801 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.754\n",
            "INFO:tensorflow:loss = 0.18240294, step = 1901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.583\n",
            "INFO:tensorflow:loss = 0.10389, step = 2001 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.048\n",
            "INFO:tensorflow:loss = 0.17433618, step = 2101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.564\n",
            "INFO:tensorflow:loss = 0.11899939, step = 2201 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.148\n",
            "INFO:tensorflow:loss = 0.07353995, step = 2301 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.415\n",
            "INFO:tensorflow:loss = 0.059818484, step = 2401 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.715\n",
            "INFO:tensorflow:loss = 0.061366893, step = 2501 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.278\n",
            "INFO:tensorflow:loss = 0.048497755, step = 2601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.514\n",
            "INFO:tensorflow:loss = 0.041565917, step = 2701 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.73\n",
            "INFO:tensorflow:loss = 0.028508138, step = 2801 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.746\n",
            "INFO:tensorflow:loss = 0.032048322, step = 2901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.416\n",
            "INFO:tensorflow:loss = 0.025731692, step = 3001 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.491\n",
            "INFO:tensorflow:loss = 0.018195234, step = 3101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.031\n",
            "INFO:tensorflow:loss = 0.02009888, step = 3201 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.124\n",
            "INFO:tensorflow:loss = 0.017514288, step = 3301 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.571\n",
            "INFO:tensorflow:loss = 0.015429909, step = 3401 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 666.406\n",
            "INFO:tensorflow:loss = 0.013547147, step = 3501 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.19\n",
            "INFO:tensorflow:loss = 0.01302237, step = 3601 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.09\n",
            "INFO:tensorflow:loss = 0.010096358, step = 3701 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.998\n",
            "INFO:tensorflow:loss = 0.009635253, step = 3801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.707\n",
            "INFO:tensorflow:loss = 0.0073199198, step = 3901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.297\n",
            "INFO:tensorflow:loss = 0.0076338463, step = 4001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.164\n",
            "INFO:tensorflow:loss = 0.0088734515, step = 4101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.963\n",
            "INFO:tensorflow:loss = 0.005867481, step = 4201 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.603\n",
            "INFO:tensorflow:loss = 0.006705656, step = 4301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.184\n",
            "INFO:tensorflow:loss = 0.00947777, step = 4401 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.957\n",
            "INFO:tensorflow:loss = 0.0060048313, step = 4501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.135\n",
            "INFO:tensorflow:loss = 0.012684831, step = 4601 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.876\n",
            "INFO:tensorflow:loss = 0.0068772016, step = 4701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.943\n",
            "INFO:tensorflow:loss = 0.0062021464, step = 4801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.947\n",
            "INFO:tensorflow:loss = 0.0066352147, step = 4901 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.469\n",
            "INFO:tensorflow:loss = 0.0062752645, step = 5001 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.351\n",
            "INFO:tensorflow:loss = 0.0051950123, step = 5101 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.255\n",
            "INFO:tensorflow:loss = 0.009340225, step = 5201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.215\n",
            "INFO:tensorflow:loss = 0.0066435672, step = 5301 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.634\n",
            "INFO:tensorflow:loss = 0.005338735, step = 5401 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 666.595\n",
            "INFO:tensorflow:loss = 0.005302998, step = 5501 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.075\n",
            "INFO:tensorflow:loss = 0.11127514, step = 5601 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.817\n",
            "INFO:tensorflow:loss = 0.07140973, step = 5701 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.403\n",
            "INFO:tensorflow:loss = 2.6822453, step = 5801 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.591\n",
            "INFO:tensorflow:loss = 0.11159591, step = 5901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.083\n",
            "INFO:tensorflow:loss = 0.35331622, step = 6001 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.995\n",
            "INFO:tensorflow:loss = 0.01962588, step = 6101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 681.104\n",
            "INFO:tensorflow:loss = 1.2752035, step = 6201 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 666.21\n",
            "INFO:tensorflow:loss = 0.0560956, step = 6301 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 589.364\n",
            "INFO:tensorflow:loss = 0.026735486, step = 6401 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.391\n",
            "INFO:tensorflow:loss = 0.025348796, step = 6501 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.104\n",
            "INFO:tensorflow:loss = 0.22204752, step = 6601 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.717\n",
            "INFO:tensorflow:loss = 0.01171908, step = 6701 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.523\n",
            "INFO:tensorflow:loss = 0.01978841, step = 6801 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.346\n",
            "INFO:tensorflow:loss = 0.1655911, step = 6901 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 607.866\n",
            "INFO:tensorflow:loss = 0.020654775, step = 7001 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.882\n",
            "INFO:tensorflow:loss = 0.025291584, step = 7101 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.658\n",
            "INFO:tensorflow:loss = 0.22352934, step = 7201 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.094\n",
            "INFO:tensorflow:loss = 0.09228887, step = 7301 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.947\n",
            "INFO:tensorflow:loss = 0.13442531, step = 7401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.631\n",
            "INFO:tensorflow:loss = 0.21895084, step = 7501 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.029\n",
            "INFO:tensorflow:loss = 0.06312843, step = 7601 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.306\n",
            "INFO:tensorflow:loss = 0.003787713, step = 7701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.274\n",
            "INFO:tensorflow:loss = 1.6083025, step = 7801 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.244\n",
            "INFO:tensorflow:loss = 0.03597509, step = 7901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.463\n",
            "INFO:tensorflow:loss = 0.075209275, step = 8001 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.006\n",
            "INFO:tensorflow:loss = 0.009834706, step = 8101 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.781\n",
            "INFO:tensorflow:loss = 0.21339767, step = 8201 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.301\n",
            "INFO:tensorflow:loss = 0.07718846, step = 8301 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.943\n",
            "INFO:tensorflow:loss = 0.6693057, step = 8401 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.561\n",
            "INFO:tensorflow:loss = 0.018416494, step = 8501 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 676.363\n",
            "INFO:tensorflow:loss = 0.005700861, step = 8601 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.803\n",
            "INFO:tensorflow:loss = 0.24004243, step = 8701 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 644.704\n",
            "INFO:tensorflow:loss = 0.44804114, step = 8801 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.326\n",
            "INFO:tensorflow:loss = 0.029313333, step = 8901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.165\n",
            "INFO:tensorflow:loss = 0.041390017, step = 9001 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.556\n",
            "INFO:tensorflow:loss = 0.01570971, step = 9101 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.93\n",
            "INFO:tensorflow:loss = 0.013633769, step = 9201 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.183\n",
            "INFO:tensorflow:loss = 0.5801587, step = 9301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.877\n",
            "INFO:tensorflow:loss = 0.1323193, step = 9401 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.379\n",
            "INFO:tensorflow:loss = 0.09747846, step = 9501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.162\n",
            "INFO:tensorflow:loss = 0.13551947, step = 9601 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 614.268\n",
            "INFO:tensorflow:loss = 0.2235672, step = 9701 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.521\n",
            "INFO:tensorflow:loss = 0.0027549441, step = 9801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.04\n",
            "INFO:tensorflow:loss = 0.28524226, step = 9901 (0.141 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.5716943.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 783.970272843907\n",
            "Just using average = 595.926323867997 has RMSE of 101.48823712997127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a67fc3b-982f-4581-af2f-9bbb4bbf735f",
        "id": "NdhxllohQ7zj"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d471990>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.194391  1.1049479 1.1870203 1.1668943 1.040269  1.2286973 1.1888226\n",
            " 1.1962259 1.1737874 1.2359042 1.1580473 1.1932143 1.2301476 1.2346545\n",
            " 1.1978226 1.2110507 1.1685503 1.1629512 1.1595644 1.2015777 1.2566245\n",
            " 1.1113663 1.2151308 1.1686465 1.1777934 1.1814578 1.2395948 1.2205758\n",
            " 1.2178271 1.248207  1.255397  1.2170538 1.0982308 1.2237643 1.1792283\n",
            " 1.2652383 1.2003063 1.2181973 1.2123485 1.2580627 1.1765153 1.1615616\n",
            " 1.1081288 1.1996698 1.2026939 1.1819993 1.0811682 1.1169717 1.2250378\n",
            " 1.1825143 1.1953098 1.235367  1.2359048 1.0429051 1.2581121 1.2040194\n",
            " 1.185708  1.1822493 1.2109714 1.2125906 1.2258148 1.162156  1.1766837\n",
            " 1.1772774 1.2070302 1.2180911 1.1651337 1.0667249 1.2588843 1.0631528\n",
            " 1.2660097 1.1971905 1.2171893 1.1015611 1.2190645 1.1461064 1.2118015\n",
            " 1.1074315 1.2208043 1.1856952 1.2239091 1.2390212 1.1499317 1.2235936\n",
            " 1.231025  1.1850772 1.1791211 1.1788703 1.2213271 1.2305825 1.2382582\n",
            " 1.1406907 1.1323256 1.182517  1.0485505 1.2256471 1.2203379 1.0457914\n",
            " 1.2185743 1.1515418 1.2134819 1.152345  1.1944004 1.1714753 1.2210833\n",
            " 1.2073425 1.2141763 1.1946999 1.146339  1.2183473 1.2085645 1.1916578\n",
            " 1.1034359 1.2291759 1.080249  1.1649609 1.1885694 1.1485007 1.0979493\n",
            " 1.2148013 1.2173237 1.1162425 1.1842129 1.2530997 1.186016  1.1575941\n",
            " 1.2224696 1.1814457 1.1426573 1.0613999 1.108804  1.2151856 1.153504\n",
            " 1.1874524 1.2199124 1.1600158 1.2003143 1.2463511 1.1653472 1.1992264\n",
            " 1.207089  1.1810818 1.173153  1.2177492 1.1954116 1.1791478 1.21451\n",
            " 1.1934868 1.2241579 1.2131319 1.1793349 1.2250286 1.0715783 1.2299606\n",
            " 1.2084923 1.1851044 1.2999864 1.2561333 1.2568636 1.2154527 1.1664492\n",
            " 1.2649233 1.1391459 1.2021347 1.2075852 1.2308218 1.2208977 1.2246106\n",
            " 1.2290043 1.2615589 1.1597786 1.2623559 1.1923282 1.183577  1.1728116\n",
            " 1.1430498 1.2239566 1.1930258 1.2587963 1.2079947 1.0785232 1.2118369\n",
            " 1.1601275 1.1747583 1.2175618 1.1688739 1.1539463 1.1842718 1.218486\n",
            " 1.0990229 1.1641835 1.1757575 1.1638584 1.2189722 1.1607063 1.1945539\n",
            " 1.2209784 1.1869966 1.1766559 1.2162535 1.2471778 1.1980681 1.2244055\n",
            " 1.0762321 1.1448818 1.1595812 1.1751311 1.153023  1.0655618 1.1754905\n",
            " 1.2127402 1.1573577 1.1623273 1.215209  1.1295271 1.2244984 1.199443\n",
            " 1.2238997 1.2501895 1.1773185 1.2135503 1.2278086 1.2660725 1.2235334\n",
            " 1.2128048 1.1637393 1.1110969 1.2087365 1.0800127 1.1067591 1.1146919\n",
            " 1.1624478 1.2636936 1.1488633 1.26315   1.2261426 1.0744796 1.2327305\n",
            " 1.2298058 1.1809509 1.2152425 1.1844038 1.2995684 1.1766827 1.1460685\n",
            " 1.2153585 1.0663904 1.2119483 1.1721872 1.226331  1.1061729 1.2334951\n",
            " 1.1957107 1.231998  1.2609475 1.2080771 1.1541085 1.2954713 1.2064571\n",
            " 1.1197846 1.1889224 1.0698956 1.2050204 1.2413205 1.2206045 1.2155449\n",
            " 1.207119  1.1866305 1.1015476 1.159003  1.1460919 1.2349122 1.0756668\n",
            " 1.2196816 1.2126756 1.2159154 1.1654795 1.1883123 1.1335232 1.1943465\n",
            " 1.2405335 1.2619729 1.2205211 1.0711744 1.2125908 1.2531809 1.1753769\n",
            " 1.1839198 1.2591331 1.063835  1.0504093 1.2993727 1.2227616 1.1838284\n",
            " 1.1646812 1.2133588 1.0651374 1.3021344 1.1636658 1.1836892 1.2219787\n",
            " 1.1649597 1.157823  1.2557107 1.2027465 1.210401  1.2025712 1.222747\n",
            " 1.2182429 1.1468141 1.1934754 1.2160784 1.1603279 1.2976013 1.2158046\n",
            " 1.1100581 1.0771599 1.173172  1.2597258 1.1857262 1.2194425 1.1077116\n",
            " 1.1650443 1.1787114 1.1623846 1.1752108]\n",
            "[0.55555556 0.36606374 0.47803618 0.56158484 0.45822567 0.59862188\n",
            " 0.48234281 0.5081826  0.46597761 0.59776055 0.52713178 0.54263566\n",
            " 0.62790698 0.59086994 0.48492679 0.52971576 0.4625323  0.51421189\n",
            " 0.53143842 0.33850129 0.6873385  0.43152455 0.61584841 0.46511628\n",
            " 0.47372954 0.5796727  0.51076658 0.52024117 0.58914729 0.49612403\n",
            " 0.55813953 0.45822567 0.44099914 0.31955211 0.64599483 0.6873385\n",
            " 0.4005168  0.43152455 0.44186047 0.52885444 0.52799311 0.51765719\n",
            " 0.4461671  0.48751077 0.55124892 0.47975883 0.42118863 0.48492679\n",
            " 0.60809647 0.48062016 0.51851852 0.60378984 0.57536606 0.33850129\n",
            " 0.53488372 0.63049096 0.47028424 0.53574505 0.5374677  0.62187769\n",
            " 0.49267873 0.48320413 0.55297158 0.46770026 0.50559862 0.52282515\n",
            " 0.49354005 0.41257537 0.64685616 0.37639966 0.71748493 0.39362618\n",
            " 0.51421189 0.50387597 0.60206718 0.46080965 0.62962963 0.4788975\n",
            " 0.64857881 0.49009475 0.52368648 0.55900086 0.45736434 0.46080965\n",
            " 0.49870801 0.51765719 0.44358312 0.57105943 0.54694229 0.60034453\n",
            " 0.61412575 0.48837209 0.44444444 0.56503015 0.33419466 0.5994832\n",
            " 0.53832903 0.82773471 0.57105943 0.48664944 0.60637382 0.45736434\n",
            " 0.55986219 0.34280792 0.57622739 0.47631352 0.49009475 0.54349699\n",
            " 0.42635659 0.47028424 0.49870801 0.54177433 0.42204996 0.49440138\n",
            " 0.48923342 0.44444444 0.55986219 0.44013781 0.42463394 0.64771748\n",
            " 0.60378984 0.33505599 0.53143842 0.55813953 0.47114556 0.56589147\n",
            " 0.80878553 0.44875108 0.3910422  0.36692506 0.38242894 0.50129199\n",
            " 0.31007752 0.5245478  0.54263566 0.50301464 0.62187769 0.86046512\n",
            " 0.46339363 0.55813953 0.44702842 0.48406546 0.41085271 0.5245478\n",
            " 0.50215332 0.45564169 0.49784668 0.48234281 0.5667528  0.60637382\n",
            " 0.51421189 0.42980189 0.44444444 0.52368648 0.58570198 0.41085271\n",
            " 0.64513351 0.7329888  0.53488372 0.52282515 0.44272179 0.56847545\n",
            " 0.47372954 0.38070629 0.43669251 0.56761413 0.63049096 0.54091301\n",
            " 0.55555556 0.56158484 0.34022394 0.59345392 0.51507321 0.48664944\n",
            " 0.50387597 0.50990525 0.32213609 0.51593454 0.58742463 0.26098191\n",
            " 0.44875108 0.55813953 0.47286822 0.45478036 0.54091301 0.48751077\n",
            " 0.42807924 0.54349699 0.65891473 0.39190353 0.50990525 0.50990525\n",
            " 0.48923342 0.71576227 0.52540913 0.51248923 0.45305771 0.6546081\n",
            " 0.45564169 0.52196382 0.59862188 0.52024117 0.54866494 0.33850129\n",
            " 0.44272179 0.52971576 0.46080965 0.41085271 0.36692506 0.47372954\n",
            " 0.56761413 0.40913006 0.51162791 0.54866494 0.46511628 0.44530577\n",
            " 0.60637382 0.69853575 0.58570198 0.52799311 0.59086994 0.5503876\n",
            " 0.58828596 0.58484065 0.63996555 0.47803618 0.4461671  0.49095607\n",
            " 0.38156761 0.45908699 0.42635659 0.58914729 0.48492679 0.47975883\n",
            " 0.60809647 0.51593454 0.37639966 0.63135228 0.6089578  0.50043066\n",
            " 0.57364341 0.51248923 0.68130922 0.52971576 0.47459087 0.60206718\n",
            " 0.38501292 0.54263566 0.46167097 0.63824289 0.41085271 0.48234281\n",
            " 0.48234281 0.53919035 0.66925065 0.63479759 0.4918174  0.68303187\n",
            " 0.49009475 0.45391904 0.57278208 0.36089578 0.53143842 0.60378984\n",
            " 0.66063738 0.63135228 0.56589147 0.53316107 0.4203273  0.43496985\n",
            " 0.51076658 0.58225668 0.39793282 0.51421189 0.5994832  0.55986219\n",
            " 0.56330749 0.48492679 0.45650301 0.49267873 0.55986219 0.54608096\n",
            " 0.53143842 0.42635659 0.51679587 0.56158484 0.65202412 0.47975883\n",
            " 0.60034453 0.3453919  0.40913006 0.59689922 0.51937984 0.56244617\n",
            " 0.54177433 0.51507321 0.3875969  0.66322136 0.42291128 0.5047373\n",
            " 0.6089578  0.48837209 0.50990525 0.64341085 0.4461671  0.58570198\n",
            " 0.51507321 0.37295435 0.57881137 0.54952627 0.56244617 0.65202412\n",
            " 0.39018088 0.60981912 0.54005168 0.45305771 0.38587425 0.52885444\n",
            " 0.56503015 0.53057709 0.46856158 0.44444444 0.5211025  0.4918174\n",
            " 0.53057709 0.5047373 ]\n",
            "The trained model has an aproximate error rate of -779.7476145094889 which equates to -130%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (mxpsd)"
      ],
      "metadata": {
        "id": "tKaVpVT8T55I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc1e437-11ef-4910-f1a4-dbc2642ec29c",
        "id": "_LbHDT1WUbJs"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] != 2012]\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] < 2020]\n",
        "cols = df_mxpsd_dnn['NUM_COLLISIONS']\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd_dnn[:6])\n",
        "df_mxpsd_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "4bcc4e41-00c6-49b2-f900-2fcef3352671",
        "id": "OoJSkkKHUbJ3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da        mxpsd          Apr          Aug  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean   2016.001567    15.737172    17.240110     0.082256     0.084998   \n",
              "std       2.000587     8.797367     5.858333     0.274808     0.278933   \n",
              "min    2013.000000     1.000000     5.100000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    13.000000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    15.900000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    20.000000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    49.000000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000  ...   \n",
              "mean      0.084998     0.077164     0.084998     0.084998     0.082256  ...   \n",
              "std       0.278933     0.266904     0.278933     0.278933     0.274808  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      0.081864     0.084998     0.081473     0.143361     0.142969   \n",
              "std       0.274212     0.278933     0.273613     0.350509     0.350110   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000     2553.000000  \n",
              "mean      0.142969     0.142969     0.142577     0.142186      599.033686  \n",
              "std       0.350110     0.350110     0.349710     0.349309      100.284761  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab677bf3-29a5-4dd1-9443-d4b60ccf4670\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.001567</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.077164</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081864</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.081473</td>\n",
              "      <td>0.143361</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142577</td>\n",
              "      <td>0.142186</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000587</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.266904</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274212</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.273613</td>\n",
              "      <td>0.350509</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.349710</td>\n",
              "      <td>0.349309</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab677bf3-29a5-4dd1-9443-d4b60ccf4670')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab677bf3-29a5-4dd1-9443-d4b60ccf4670 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab677bf3-29a5-4dd1-9443-d4b60ccf4670');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd_dnn.iloc[np.random.permutation(len(df_mxpsd_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2713e512-67a8-48f3-e146-eee5e2302b60",
        "id": "z1Ut5LCiUbJ4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "1687  2019  10   14.0    0    0    0    0    0    0    1  ...    0    0    0   \n",
            "1463  2019  29   20.0    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "2638  2014  24   15.0    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2641  2018  18   15.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "1093  2017  21   19.0    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "238   2015   4   22.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1687    0    0    0    1    0    0    0  \n",
            "1463    0    0    0    0    0    1    0  \n",
            "2638    1    0    0    0    0    1    0  \n",
            "2641    1    1    0    0    0    0    0  \n",
            "1093    0    0    0    0    1    0    0  \n",
            "238     0    0    1    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4785ab-875c-46bc-b398-d21fb83a7f15",
        "id": "QibQoUleUbJ4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1687    613\n",
            "1463    601\n",
            "2638    566\n",
            "2641    740\n",
            "1093    690\n",
            "238     381\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad46eed-e232-4b3e-b254-734f5b843509",
        "id": "-C_tqkmUUbJ5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_mxpsd', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f5b0c4-dd82-4355-a557-49a8a28ff363",
        "id": "C3O7xWeMUbJ5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f376d97df10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 75043.26, step = 1\n",
            "INFO:tensorflow:global_step/sec: 519.179\n",
            "INFO:tensorflow:loss = 1.2256441, step = 101 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.017\n",
            "INFO:tensorflow:loss = 0.0972218, step = 201 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.309\n",
            "INFO:tensorflow:loss = 0.10458371, step = 301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.582\n",
            "INFO:tensorflow:loss = 0.08815303, step = 401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 736.557\n",
            "INFO:tensorflow:loss = 0.08122876, step = 501 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.304\n",
            "INFO:tensorflow:loss = 0.0872712, step = 601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.915\n",
            "INFO:tensorflow:loss = 0.09661062, step = 701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 718.027\n",
            "INFO:tensorflow:loss = 0.080243245, step = 801 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.437\n",
            "INFO:tensorflow:loss = 0.06925833, step = 901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.854\n",
            "INFO:tensorflow:loss = 0.07288408, step = 1001 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.166\n",
            "INFO:tensorflow:loss = 0.07770621, step = 1101 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.019\n",
            "INFO:tensorflow:loss = 0.066153996, step = 1201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.517\n",
            "INFO:tensorflow:loss = 0.0824205, step = 1301 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.63\n",
            "INFO:tensorflow:loss = 0.067098156, step = 1401 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.809\n",
            "INFO:tensorflow:loss = 0.067228064, step = 1501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.466\n",
            "INFO:tensorflow:loss = 0.05060573, step = 1601 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.483\n",
            "INFO:tensorflow:loss = 0.059742242, step = 1701 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.051\n",
            "INFO:tensorflow:loss = 0.05639551, step = 1801 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.838\n",
            "INFO:tensorflow:loss = 0.047132127, step = 1901 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.733\n",
            "INFO:tensorflow:loss = 0.04557778, step = 2001 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.268\n",
            "INFO:tensorflow:loss = 0.048354723, step = 2101 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.475\n",
            "INFO:tensorflow:loss = 0.04133435, step = 2201 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.14\n",
            "INFO:tensorflow:loss = 0.044150125, step = 2301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.57\n",
            "INFO:tensorflow:loss = 0.035576493, step = 2401 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.892\n",
            "INFO:tensorflow:loss = 0.034666643, step = 2501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.55\n",
            "INFO:tensorflow:loss = 0.029443147, step = 2601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.42\n",
            "INFO:tensorflow:loss = 0.030608244, step = 2701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.82\n",
            "INFO:tensorflow:loss = 0.027130533, step = 2801 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.211\n",
            "INFO:tensorflow:loss = 0.019926753, step = 2901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.435\n",
            "INFO:tensorflow:loss = 0.030919885, step = 3001 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.348\n",
            "INFO:tensorflow:loss = 0.020433374, step = 3101 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.384\n",
            "INFO:tensorflow:loss = 0.021214448, step = 3201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.442\n",
            "INFO:tensorflow:loss = 0.016327253, step = 3301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.403\n",
            "INFO:tensorflow:loss = 0.016393272, step = 3401 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.37\n",
            "INFO:tensorflow:loss = 0.017332977, step = 3501 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 676.932\n",
            "INFO:tensorflow:loss = 0.0196079, step = 3601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.763\n",
            "INFO:tensorflow:loss = 0.014781702, step = 3701 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 662.093\n",
            "INFO:tensorflow:loss = 0.012443613, step = 3801 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.871\n",
            "INFO:tensorflow:loss = 0.011289902, step = 3901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.323\n",
            "INFO:tensorflow:loss = 0.011993761, step = 4001 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.021\n",
            "INFO:tensorflow:loss = 0.010895876, step = 4101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.968\n",
            "INFO:tensorflow:loss = 0.008202232, step = 4201 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.707\n",
            "INFO:tensorflow:loss = 0.010492079, step = 4301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.035\n",
            "INFO:tensorflow:loss = 0.008878835, step = 4401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.863\n",
            "INFO:tensorflow:loss = 0.007845951, step = 4501 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 726.371\n",
            "INFO:tensorflow:loss = 0.0074567767, step = 4601 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.419\n",
            "INFO:tensorflow:loss = 0.009002838, step = 4701 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.876\n",
            "INFO:tensorflow:loss = 0.007357738, step = 4801 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 726.617\n",
            "INFO:tensorflow:loss = 0.009176947, step = 4901 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.887\n",
            "INFO:tensorflow:loss = 0.007162171, step = 5001 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.407\n",
            "INFO:tensorflow:loss = 0.0080480715, step = 5101 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.017\n",
            "INFO:tensorflow:loss = 0.008039063, step = 5201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.251\n",
            "INFO:tensorflow:loss = 0.005003849, step = 5301 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.59\n",
            "INFO:tensorflow:loss = 0.0040761735, step = 5401 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 737.384\n",
            "INFO:tensorflow:loss = 0.007747478, step = 5501 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.348\n",
            "INFO:tensorflow:loss = 0.0053968085, step = 5601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.552\n",
            "INFO:tensorflow:loss = 0.0061450005, step = 5701 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 703\n",
            "INFO:tensorflow:loss = 0.005364887, step = 5801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.323\n",
            "INFO:tensorflow:loss = 0.0038247362, step = 5901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 681.677\n",
            "INFO:tensorflow:loss = 0.006180803, step = 6001 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.592\n",
            "INFO:tensorflow:loss = 0.005319276, step = 6101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.837\n",
            "INFO:tensorflow:loss = 0.0036310581, step = 6201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.575\n",
            "INFO:tensorflow:loss = 0.0076344246, step = 6301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.102\n",
            "INFO:tensorflow:loss = 0.005003808, step = 6401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.226\n",
            "INFO:tensorflow:loss = 0.0065231435, step = 6501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.944\n",
            "INFO:tensorflow:loss = 0.0043341145, step = 6601 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.234\n",
            "INFO:tensorflow:loss = 0.004994784, step = 6701 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.201\n",
            "INFO:tensorflow:loss = 0.006448638, step = 6801 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.378\n",
            "INFO:tensorflow:loss = 0.005521392, step = 6901 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.47\n",
            "INFO:tensorflow:loss = 0.011766525, step = 7001 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.598\n",
            "INFO:tensorflow:loss = 0.0069769816, step = 7101 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.065\n",
            "INFO:tensorflow:loss = 0.0067844004, step = 7201 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.442\n",
            "INFO:tensorflow:loss = 0.0037604193, step = 7301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 672.581\n",
            "INFO:tensorflow:loss = 0.005217638, step = 7401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.969\n",
            "INFO:tensorflow:loss = 0.009422189, step = 7501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.683\n",
            "INFO:tensorflow:loss = 0.016662193, step = 7601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.061\n",
            "INFO:tensorflow:loss = 0.0047825207, step = 7701 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.927\n",
            "INFO:tensorflow:loss = 0.03201012, step = 7801 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.368\n",
            "INFO:tensorflow:loss = 0.0052011255, step = 7901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.315\n",
            "INFO:tensorflow:loss = 0.605196, step = 8001 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 637.018\n",
            "INFO:tensorflow:loss = 0.85058045, step = 8101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.744\n",
            "INFO:tensorflow:loss = 0.1524032, step = 8201 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.818\n",
            "INFO:tensorflow:loss = 0.00343804, step = 8301 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.69\n",
            "INFO:tensorflow:loss = 0.050115895, step = 8401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.819\n",
            "INFO:tensorflow:loss = 0.15166247, step = 8501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.738\n",
            "INFO:tensorflow:loss = 0.45712814, step = 8601 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.539\n",
            "INFO:tensorflow:loss = 0.022724882, step = 8701 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.733\n",
            "INFO:tensorflow:loss = 0.07597601, step = 8801 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.156\n",
            "INFO:tensorflow:loss = 0.010327946, step = 8901 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.838\n",
            "INFO:tensorflow:loss = 0.064667664, step = 9001 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.326\n",
            "INFO:tensorflow:loss = 0.08387163, step = 9101 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.031\n",
            "INFO:tensorflow:loss = 0.012697916, step = 9201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.091\n",
            "INFO:tensorflow:loss = 0.029674884, step = 9301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.804\n",
            "INFO:tensorflow:loss = 0.19230658, step = 9401 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 710.293\n",
            "INFO:tensorflow:loss = 0.020899635, step = 9501 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.975\n",
            "INFO:tensorflow:loss = 0.0044858335, step = 9601 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.33\n",
            "INFO:tensorflow:loss = 0.019869711, step = 9701 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.718\n",
            "INFO:tensorflow:loss = 0.6116721, step = 9801 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.982\n",
            "INFO:tensorflow:loss = 0.045440525, step = 9901 (0.142 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.12494253.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 325.02715731466327\n",
            "Just using average = 599.409402546523 has RMSE of 99.09077607746597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65ff9fa-b9cd-4c1b-b877-98e54ef16e10",
        "id": "NoIPNaT3UbJ5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769d79ad0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2263927 1.2170818 1.0584722 1.1852422 1.1661063 1.1748366 1.2012285\n",
            " 1.178473  1.2680091 1.2221575 1.0659655 1.1959519 1.1773702 1.2193909\n",
            " 1.2336334 1.1494995 1.2073975 1.2269225 1.2524749 1.2465492 1.229411\n",
            " 1.1820177 1.1855026 1.0962048 1.0685252 1.1862141 1.0814662 1.0950677\n",
            " 1.1849273 1.1802145 1.2982483 1.1581275 1.2647433 1.2013193 1.2174778\n",
            " 1.2352034 1.2129431 1.2095284 1.2132425 1.1360071 1.1800944 1.1714175\n",
            " 1.1665826 1.1960803 1.2428361 1.187238  1.1788391 1.2150774 1.1497185\n",
            " 1.1874614 1.1245847 1.2578483 1.1063821 1.2624607 1.222285  1.2148796\n",
            " 1.1466534 1.2918048 1.2542887 1.1718099 1.1917621 1.2107341 1.2685\n",
            " 1.2201335 1.167687  1.2410557 1.1757586 1.1785907 1.206907  1.2020473\n",
            " 1.1034303 1.1578122 1.1723841 1.2535187 1.261845  1.2066633 1.2123258\n",
            " 1.159027  1.251253  1.2218122 1.2133411 1.2161541 1.2226516 1.1764673\n",
            " 1.184691  1.2148336 1.245418  1.0459018 1.2188526 1.2181077 1.0419735\n",
            " 1.0776452 1.1763196 1.1878501 1.183058  1.2241572 1.2212137 1.0787548\n",
            " 1.2965889 1.1330868 1.1688303 1.2569761 1.2408139 1.2598141 1.2098143\n",
            " 1.2090278 1.1710448 1.0917835 1.2148895 1.0992119 1.2139816 1.1758434\n",
            " 1.1880515 1.1764625 1.2038869 1.1741527 1.2121159 1.2118142 1.0749066\n",
            " 1.2589892 1.1857266 1.2232004 1.2396002 1.1740359 1.0786207 1.1205819\n",
            " 1.2072227 1.2140223 1.2263088 1.2916695 1.1850677 1.2253935 1.0794351\n",
            " 1.1958687 1.0397968 1.2421889 1.2120643 1.211047  1.2161576 1.1711125\n",
            " 1.2019454 1.1752234 1.1771364 1.2126098 1.2136931 1.240004  1.1318678\n",
            " 1.2151461 1.1875114 1.0972991 1.1583897 1.2206483 1.2618171 1.2158164\n",
            " 1.2309102 1.1203997 1.1428989 1.189424  1.1886027 1.2650692 1.2129753\n",
            " 1.0444891 1.2183731 1.1921055 1.1766034 1.1392577 1.219663  1.158055\n",
            " 1.1564456 1.206678  1.1937267 1.2558558 1.0912557 1.1523247 1.1525018\n",
            " 1.0620042 1.2159761 1.2305095 1.1934028 1.0813096 1.233724  1.0629077\n",
            " 1.232453  1.1482137 1.173382  1.2020646 1.1902059 1.1741418 1.1667928\n",
            " 1.2756194 1.1781794 1.2333853 1.0614884 1.2052562 1.1534336 1.2343347\n",
            " 1.2319793 1.0685953 1.0662884 1.2076035 1.263082  1.2394066 1.1817356\n",
            " 1.0987713 1.2196636 1.1835338 1.2302718 1.2072811 1.2070978 1.1054124\n",
            " 1.1912471 1.1667359 1.2479568 1.1678534 1.1583452 1.2206717 1.2542678\n",
            " 1.0464432 1.1555257 1.2081735 1.1688964 1.2146904 1.1768516 1.1797965\n",
            " 1.1897202 1.1766363 1.2228471 1.2008921 1.0715078 1.1706271 1.2282041\n",
            " 1.2338214 1.1592486 1.2245421 1.1733336 1.2022457 1.2157904 1.1008961\n",
            " 1.2191341 1.2085627 1.2316357 1.2123483 1.2164208 1.2606993 1.1048765\n",
            " 1.2288065 1.1667558 1.249333  1.1862688 1.1409827 1.0915885 1.2577888\n",
            " 1.2620023 1.09584   1.2143252 1.2241449 1.2132443 1.2173955 1.2087823\n",
            " 1.1639454 1.1436615 1.2313117 1.1875291 1.2027272 1.1904291 1.1396476\n",
            " 1.1944195 1.0769385 1.149398  1.109187  1.2138519 1.1062683 1.1928461\n",
            " 1.2149495 1.1018306 1.1374136 1.2026446 1.1793947 1.1876284 1.1131316\n",
            " 1.1775146 1.2929395 1.1079868 1.1548369 1.1812358 1.2983456 1.2548593\n",
            " 1.1924269 1.1899773 1.2107863 1.1866062 1.1692773 1.2096422 1.2160138\n",
            " 1.223714  1.1808145 1.1946081 1.1624676 1.2075453 1.2151577 1.1888973\n",
            " 1.2117102 1.1765026 1.1435696 1.1692184 1.2534797 1.2091911 1.2140533\n",
            " 1.1884665 1.2077639 1.2224092 1.2156962 1.0948526 1.2288848 1.2709508\n",
            " 1.0788944 1.218644  1.2181273 1.2583789 1.227535  1.2579832 1.2331085\n",
            " 1.2654523 1.2943376 1.2671006 1.2217573 1.2370561 1.2168515 1.206634\n",
            " 1.2083143 1.1473014 1.1673795 1.2992611 1.1998281 1.1840936 1.1545315\n",
            " 1.2142227 1.21774   1.1478071 1.2666247 1.2014415 1.1880084 1.1024084\n",
            " 1.1869147 1.2225162 1.0885185 1.2090687 1.1887498 1.2061937 1.2325637\n",
            " 1.2567174 1.1221437 1.2517258 1.2560935 1.2574227 1.215871  1.0977409\n",
            " 1.2390511 1.1910784 1.1650697 1.2066758 1.2189254 1.0497574 1.2272699\n",
            " 1.1784302 1.2138081 1.0636376 1.2160219 1.2917141 1.220291  1.1628934\n",
            " 1.2006549 1.1030138 1.230346  1.122075  1.0999205 1.2702606 1.2640343\n",
            " 1.2123439 1.2325457 1.2458978 1.2136415 1.219357  1.2516358 1.1772932\n",
            " 1.242456  1.1877004 1.2516351 1.2115583 1.2079872 1.1813612 1.1454021\n",
            " 1.2183625 1.1669757 1.1092002 1.1789479 1.1741511 1.2239656 1.2195826\n",
            " 1.2083871 1.0748667 1.2052    1.2632252 1.1597694 1.1717799 1.2407259\n",
            " 1.1940424 1.2173942 1.2323307 1.1746697 1.190367  1.0761279 1.2016702\n",
            " 1.2006749 1.2108047 1.1890746 1.0736712 1.0634254 1.2159549 1.2182157\n",
            " 1.2587891 1.0796958 1.1319344 1.0659021 1.0701456 1.2035522 1.2218837\n",
            " 1.1931664 1.1115019 1.2081646 1.1625576 1.0442278 1.1613582 1.2199496\n",
            " 1.2039132 1.1801746 1.2118849 1.0952678 1.2253991 1.2159016 1.192654\n",
            " 1.1859589 1.173106  1.2137841 1.2291892 1.2276027 1.1051843 1.1386501\n",
            " 1.2616667 1.2129169 1.0694765 1.223648  1.1543522 1.1764991 1.2077385\n",
            " 1.2011166 1.194418  1.0808066 1.220972  1.2154913 1.0762204 1.1795312\n",
            " 1.2154135 1.0730159 1.1586204 1.1328185 1.2048647 1.1879466 1.1644055\n",
            " 1.1304705 1.1791526 1.1914754 1.2194606 1.2588183 1.1448743 1.1840326\n",
            " 1.2703943 1.2339988 1.1555731 1.258837  1.2246562 1.0783359 1.107227\n",
            " 1.099561  1.184623  1.2107297 1.1862773 1.1555291 1.2337272 1.1849958\n",
            " 1.2497727 1.264602  1.0590423 1.2111616 1.1898649 1.1887705 1.2144161\n",
            " 1.0735067 1.0794367 1.2460916 1.2651829 1.0721095 1.1871115 1.1648817\n",
            " 1.1668804 1.214394  1.2975329 1.1860495 1.1640822 1.1506923 1.2002662]\n",
            "[0.42721792 0.51507321 0.36692506 0.42291128 0.33936262 0.4788975\n",
            " 0.51076658 0.4918174  0.60292851 0.51679587 0.37209302 0.48492679\n",
            " 0.57795004 0.52799311 0.58225668 0.30577089 0.64341085 0.4496124\n",
            " 0.55813953 0.52540913 0.46339363 0.39793282 0.39534884 0.41774332\n",
            " 0.43066322 0.53143842 0.36347976 0.43755383 0.5503876  0.40310078\n",
            " 0.5960379  0.43583118 0.49009475 0.5503876  0.71576227 0.55727821\n",
            " 0.45650301 0.6416882  0.58914729 0.45650301 0.50301464 0.45564169\n",
            " 0.49698536 0.47459087 0.62015504 0.49267873 0.5503876  0.52282515\n",
            " 0.40137812 0.57795004 0.44875108 0.5538329  0.51335056 0.48492679\n",
            " 0.45564169 0.5994832  0.48923342 0.64341085 0.6873385  0.34625323\n",
            " 0.51593454 0.63996555 0.62273902 0.46425495 0.52713178 0.51507321\n",
            " 0.59086994 0.55555556 0.53143842 0.62187769 0.36950904 0.50215332\n",
            " 0.4918174  0.70542636 0.66408269 0.5667528  0.61068045 0.52540913\n",
            " 0.63996555 0.47717485 0.55900086 0.54694229 0.69853575 0.52799311\n",
            " 0.6546081  0.58656331 0.72437554 0.42549526 0.53402239 0.63565891\n",
            " 0.32127476 0.44875108 0.52799311 0.5374677  0.48406546 0.54263566\n",
            " 0.32213609 0.48923342 0.62704565 0.46511628 0.52971576 0.67011197\n",
            " 0.57536606 0.66838932 0.57881137 0.56416882 0.45219638 0.31438415\n",
            " 0.47028424 0.39018088 0.54091301 0.52971576 0.48234281 0.68217054\n",
            " 0.48751077 0.48923342 0.5667528  0.60206718 0.39362618 0.57105943\n",
            " 0.47803618 0.45650301 0.56244617 0.48406546 0.42980189 0.44358312\n",
            " 0.60292851 0.42291128 0.63824289 0.66666667 0.51248923 0.38070629\n",
            " 0.39362618 0.52540913 0.36950904 0.50215332 0.52627046 0.60120586\n",
            " 0.5667528  0.50215332 0.4918174  0.45736434 0.47459087 0.54694229\n",
            " 0.56330749 0.38415159 0.45650301 0.53402239 0.52799311 0.44099914\n",
            " 0.41946598 0.63049096 0.71490095 0.54694229 0.50990525 0.47286822\n",
            " 0.42204996 0.54866494 0.49440138 0.59086994 0.52282515 0.29371232\n",
            " 0.6744186  0.52540913 0.4625323  0.48837209 0.54694229 0.53143842\n",
            " 0.38845823 0.374677   0.46511628 0.40999139 0.45391904 0.42721792\n",
            " 0.55900086 0.43496985 0.64427218 0.60292851 0.4625323  0.49354005\n",
            " 0.63049096 0.36778639 0.48234281 0.49354005 0.56847545 0.60292851\n",
            " 0.49009475 0.54091301 0.43410853 0.66838932 0.51507321 0.52368648\n",
            " 0.37639966 0.49698536 0.46856158 0.51937984 0.6416882  0.43238587\n",
            " 0.3910422  0.47459087 0.68561585 0.60378984 0.56072351 0.40913006\n",
            " 0.57881137 0.51937984 0.47459087 0.50904393 0.46683893 0.41343669\n",
            " 0.44444444 0.46511628 0.60378984 0.49095607 0.34022394 0.61757106\n",
            " 0.63910422 0.35400517 0.46942291 0.54952627 0.55641688 0.4754522\n",
            " 0.43152455 0.55469423 0.48923342 0.45564169 0.45047373 0.56847545\n",
            " 0.39534884 0.4005168  0.44358312 0.54263566 0.55813953 0.49267873\n",
            " 0.54177433 0.50559862 0.6287683  0.44358312 0.66666667 0.59173127\n",
            " 0.65288544 0.65374677 0.49354005 0.57450474 0.43238587 0.52196382\n",
            " 0.50990525 0.65202412 0.52627046 0.41085271 0.43496985 0.51076658\n",
            " 0.55986219 0.44099914 0.54005168 0.57019811 0.57536606 0.43152455\n",
            " 0.53488372 0.48062016 0.5211025  0.53574505 0.46942291 0.44358312\n",
            " 0.53057709 0.43238587 0.55986219 0.40568475 0.52024117 0.35745047\n",
            " 0.61584841 0.48148148 0.4496124  0.53229974 0.51421189 0.49698536\n",
            " 0.53919035 0.53488372 0.50732127 0.45650301 0.59259259 0.69853575\n",
            " 0.45305771 0.51335056 0.48062016 0.70456503 0.65633075 0.51679587\n",
            " 0.50990525 0.53402239 0.54263566 0.52713178 0.5796727  0.50990525\n",
            " 0.55727821 0.44875108 0.48234281 0.51421189 0.56072351 0.34453058\n",
            " 0.53574505 0.49354005 0.55124892 0.50129199 0.36864772 0.45478036\n",
            " 0.47975883 0.49870801 0.53229974 0.5211025  0.60206718 0.53660637\n",
            " 0.39276486 0.49095607 0.53057709 0.3910422  0.51076658 0.66063738\n",
            " 0.55211025 0.57622739 0.69939707 0.67011197 0.56416882 0.62704565\n",
            " 0.57536606 0.51248923 0.61412575 0.42463394 0.50215332 0.51593454\n",
            " 0.44013781 0.48751077 0.63221361 0.47717485 0.52627046 0.57708872\n",
            " 0.65202412 0.54521964 0.51593454 0.62618432 0.62446167 0.52196382\n",
            " 0.45478036 0.55986219 0.31955211 0.30060293 0.53229974 0.50215332\n",
            " 0.58914729 0.56072351 0.6287683  0.42635659 0.43238587 0.63738157\n",
            " 0.58656331 0.45908699 0.38587425 0.40999139 0.51679587 0.56158484\n",
            " 0.60120586 0.63652024 0.39793282 0.59345392 0.42118863 0.52971576\n",
            " 0.34453058 0.55555556 0.64857881 0.65202412 0.53660637 0.5538329\n",
            " 0.4788975  0.76141258 0.44272179 0.42463394 0.50301464 0.65719208\n",
            " 0.58914729 0.5503876  0.60120586 0.51937984 0.56072351 0.55813953\n",
            " 0.4918174  0.56158484 0.43927649 0.61584841 0.51679587 0.51162791\n",
            " 0.51421189 0.45908699 0.53229974 0.4952627  0.43927649 0.55555556\n",
            " 0.4788975  0.42980189 0.54694229 0.66666667 0.40999139 0.60206718\n",
            " 0.56847545 0.48320413 0.51507321 0.58053402 0.51248923 0.43927649\n",
            " 0.58914729 0.50990525 0.46511628 0.40654608 0.5211025  0.4461671\n",
            " 0.52627046 0.5081826  0.45478036 0.26614987 0.4918174  0.55727821\n",
            " 0.59517657 0.46511628 0.47286822 0.41257537 0.37898363 0.59431525\n",
            " 0.54091301 0.54435831 0.41257537 0.52540913 0.55641688 0.32730405\n",
            " 0.5374677  0.45305771 0.5503876  0.53574505 0.48234281 0.44875108\n",
            " 0.58397933 0.53057709 0.53832903 0.59345392 0.5047373  0.54608096\n",
            " 0.51765719 0.59086994 0.41602067 0.46425495 0.58225668 0.64427218\n",
            " 0.44444444 0.63307494 0.45047373 0.58828596 0.60034453 0.64685616\n",
            " 0.51593454 0.416882   0.58828596 0.53919035 0.41429802 0.46339363\n",
            " 0.48751077 0.42118863 0.16192937 0.45994832 0.60809647 0.5047373\n",
            " 0.5081826  0.44444444 0.47114556 0.47631352 0.68647717 0.60120586\n",
            " 0.51076658 0.44702842 0.625323   0.59086994 0.50990525 0.61757106\n",
            " 0.54521964 0.48492679 0.42721792 0.41515935 0.48492679 0.43496985\n",
            " 0.45047373 0.58570198 0.60378984 0.4496124  0.57622739 0.4754522\n",
            " 0.41429802 0.60292851 0.53832903 0.51162791 0.48148148 0.42377261\n",
            " 0.38845823 0.65374677 0.66494401 0.48148148 0.41257537 0.42204996\n",
            " 0.49440138 0.52196382 0.59689922 0.50215332 0.47803618 0.51421189\n",
            " 0.62015504]\n",
            "The trained model has an aproximate error rate of -781.1964083905323 which equates to -131%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others"
      ],
      "metadata": {
        "id": "Dzn0MDJwVdg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/datadnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8875cbaf-e11b-4d48-9cdd-6a2b07a06c02",
        "id": "r3AhZB1bVybn"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn = df.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "dnn = dnn.loc[dnn[\"year\"] != 2012]\n",
        "dnn = dnn.loc[dnn[\"year\"] < 2020]\n",
        "cols = dnn['NUM_COLLISIONS']\n",
        "dnn = dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "dnn.insert(loc=26, column='NUM_COLLISIONS', value=cols)\n",
        "print(dnn[:6])\n",
        "dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "14f51e9b-3bcf-463c-f20f-7042a2022406",
        "id": "izyExLEIVybo"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  Nov  Oct  \\\n",
            "49  2016  28  35.0   10.0   4.3  46.0  23.0  999.9    0    0  ...    0    0   \n",
            "51  2014  17  38.6    6.7   3.7  44.1  32.0  999.9    0    0  ...    0    0   \n",
            "54  2016  25  33.5   10.0   6.5  37.9  30.0  999.9    0    0  ...    0    0   \n",
            "55  2016  29  41.3   10.0   5.9  45.0  23.0  999.9    0    0  ...    0    0   \n",
            "58  2017  20  39.9   10.0   4.3  45.0  37.0  999.9    0    0  ...    0    0   \n",
            "59  2013  13  45.4    4.3   5.8  46.9  44.1  999.9    0    0  ...    0    0   \n",
            "\n",
            "    Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    0    1             681  \n",
            "51    0    0    0    0    1    0    0             589  \n",
            "54    0    0    0    1    0    0    0             658  \n",
            "55    0    0    0    0    1    0    0             645  \n",
            "58    0    0    0    0    1    0    0             605  \n",
            "59    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 27 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         year           da         temp        visib         wdsp  \\\n",
              "count  2556.0  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean   2016.0    15.725743    51.487715     8.295618    10.682864   \n",
              "std       2.0     8.800168    14.162738     2.207870     4.242687   \n",
              "min    2013.0     1.000000     5.800000     0.200000     2.700000   \n",
              "25%    2014.0     8.000000    40.400000     7.100000     7.700000   \n",
              "50%    2016.0    16.000000    52.000000     9.400000    10.100000   \n",
              "75%    2018.0    23.000000    63.900000    10.000000    13.000000   \n",
              "max    2019.0    31.000000    77.500000    10.000000    39.300000   \n",
              "\n",
              "               max          min          sndp          Apr          Aug  ...  \\\n",
              "count  2556.000000  2556.000000  2.556000e+03  2556.000000  2556.000000  ...   \n",
              "mean     59.564280    43.869757  9.999000e+02     0.082160     0.084898  ...   \n",
              "std      14.279867    14.722751  2.274182e-13     0.274661     0.278785  ...   \n",
              "min      18.000000    -2.000000  9.999000e+02     0.000000     0.000000  ...   \n",
              "25%      48.000000    33.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "50%      60.100000    44.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "75%      72.000000    55.900000  9.999000e+02     0.000000     0.000000  ...   \n",
              "max      90.000000    71.600000  9.999000e+02     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean      0.082160     0.084898     0.082160     0.143192     0.142801   \n",
              "std       0.274661     0.278785     0.274661     0.350338     0.349939   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000     2556.000000  \n",
              "mean      0.142801     0.142801     0.142801     0.142801      599.118936  \n",
              "std       0.349939     0.349939     0.349939     0.349939      100.258581  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f782f302-dcd1-4a7d-9097-8f3328fa8643\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2556.0</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2.556000e+03</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>15.725743</td>\n",
              "      <td>51.487715</td>\n",
              "      <td>8.295618</td>\n",
              "      <td>10.682864</td>\n",
              "      <td>59.564280</td>\n",
              "      <td>43.869757</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.143192</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>599.118936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.0</td>\n",
              "      <td>8.800168</td>\n",
              "      <td>14.162738</td>\n",
              "      <td>2.207870</td>\n",
              "      <td>4.242687</td>\n",
              "      <td>14.279867</td>\n",
              "      <td>14.722751</td>\n",
              "      <td>2.274182e-13</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.350338</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>100.258581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>40.400000</td>\n",
              "      <td>7.100000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>33.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>9.400000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>60.100000</td>\n",
              "      <td>44.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.0</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>63.900000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>55.900000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.0</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>39.300000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>71.600000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f782f302-dcd1-4a7d-9097-8f3328fa8643')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f782f302-dcd1-4a7d-9097-8f3328fa8643 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f782f302-dcd1-4a7d-9097-8f3328fa8643');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = dnn.iloc[np.random.permutation(len(dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01111b51-b86f-44cc-cb04-abb602510ce6",
        "id": "pOJhsz3dVybo"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  May  Nov  \\\n",
            "1702  2019  30  69.3    8.3   9.4  77.0  61.0  999.9    0    0  ...    0    0   \n",
            "2054  2013  27  69.1    9.6   8.8  73.9  64.9  999.9    0    0  ...    0    0   \n",
            "730   2016   7  36.8    9.9   9.0  46.0  23.0  999.9    0    0  ...    0    0   \n",
            "2625  2014   9  64.6   10.0  10.9  73.0  55.9  999.9    0    0  ...    0    0   \n",
            "1340  2014  18  56.5    9.8   8.5  63.0  50.0  999.9    0    0  ...    1    0   \n",
            "1961  2015   2  68.7    7.1   7.0  77.0  61.0  999.9    0    0  ...    0    0   \n",
            "\n",
            "      Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1702    0    0    0    1    0    0    0    0  \n",
            "2054    0    0    0    0    0    0    0    0  \n",
            "730     0    0    0    0    1    0    0    0  \n",
            "2625    0    1    1    0    0    0    0    0  \n",
            "1340    0    0    0    1    0    0    0    0  \n",
            "1961    0    0    0    0    0    0    0    1  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2d489b-0be5-44f3-ca95-28a501d138ee",
        "id": "iOF3gP_XVybp"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1702    569\n",
            "2054    516\n",
            "730     571\n",
            "2625    592\n",
            "1340    507\n",
            "1961    603\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9f9cad-9896-446a-bb6c-0e0d6e9c9b89",
        "id": "7XAEry6uVybp"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a78267-9033-4f6e-b227-68686d227d83",
        "id": "sb-ehJ2tVybq"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769e0d210>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 30449.512, step = 1\n",
            "INFO:tensorflow:global_step/sec: 495.037\n",
            "INFO:tensorflow:loss = 0.25086278, step = 101 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.691\n",
            "INFO:tensorflow:loss = 0.23757836, step = 201 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.943\n",
            "INFO:tensorflow:loss = 0.25054714, step = 301 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.635\n",
            "INFO:tensorflow:loss = 0.25477073, step = 401 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.086\n",
            "INFO:tensorflow:loss = 0.23914641, step = 501 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.407\n",
            "INFO:tensorflow:loss = 0.24517313, step = 601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.553\n",
            "INFO:tensorflow:loss = 0.23849544, step = 701 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.927\n",
            "INFO:tensorflow:loss = 0.23393875, step = 801 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 677.751\n",
            "INFO:tensorflow:loss = 0.22429407, step = 901 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.782\n",
            "INFO:tensorflow:loss = 0.22326359, step = 1001 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.5\n",
            "INFO:tensorflow:loss = 0.20744796, step = 1101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.463\n",
            "INFO:tensorflow:loss = 0.21083015, step = 1201 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.446\n",
            "INFO:tensorflow:loss = 0.20482619, step = 1301 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.678\n",
            "INFO:tensorflow:loss = 0.19867963, step = 1401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.316\n",
            "INFO:tensorflow:loss = 0.18389711, step = 1501 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.992\n",
            "INFO:tensorflow:loss = 0.17623025, step = 1601 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.868\n",
            "INFO:tensorflow:loss = 0.17223616, step = 1701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.093\n",
            "INFO:tensorflow:loss = 0.16654387, step = 1801 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.91\n",
            "INFO:tensorflow:loss = 0.14778103, step = 1901 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 638.217\n",
            "INFO:tensorflow:loss = 0.14315209, step = 2001 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.162\n",
            "INFO:tensorflow:loss = 0.14755854, step = 2101 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 718.605\n",
            "INFO:tensorflow:loss = 0.1442214, step = 2201 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.433\n",
            "INFO:tensorflow:loss = 0.139377, step = 2301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.855\n",
            "INFO:tensorflow:loss = 0.13926455, step = 2401 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 662.434\n",
            "INFO:tensorflow:loss = 0.114559695, step = 2501 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 676.963\n",
            "INFO:tensorflow:loss = 0.10425152, step = 2601 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.745\n",
            "INFO:tensorflow:loss = 0.11452116, step = 2701 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.461\n",
            "INFO:tensorflow:loss = 0.09785995, step = 2801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.884\n",
            "INFO:tensorflow:loss = 0.0893707, step = 2901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.05\n",
            "INFO:tensorflow:loss = 0.07749305, step = 3001 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.165\n",
            "INFO:tensorflow:loss = 0.07696898, step = 3101 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.479\n",
            "INFO:tensorflow:loss = 0.061650205, step = 3201 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 696.39\n",
            "INFO:tensorflow:loss = 0.07360089, step = 3301 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.522\n",
            "INFO:tensorflow:loss = 0.0602992, step = 3401 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.967\n",
            "INFO:tensorflow:loss = 0.05161022, step = 3501 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.056\n",
            "INFO:tensorflow:loss = 0.04627998, step = 3601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.298\n",
            "INFO:tensorflow:loss = 0.039775208, step = 3701 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.793\n",
            "INFO:tensorflow:loss = 0.04080929, step = 3801 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.326\n",
            "INFO:tensorflow:loss = 0.032178957, step = 3901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.721\n",
            "INFO:tensorflow:loss = 0.029602837, step = 4001 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.304\n",
            "INFO:tensorflow:loss = 0.02423637, step = 4101 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.508\n",
            "INFO:tensorflow:loss = 0.022220548, step = 4201 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 627.588\n",
            "INFO:tensorflow:loss = 0.019075923, step = 4301 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.142\n",
            "INFO:tensorflow:loss = 0.021845441, step = 4401 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.352\n",
            "INFO:tensorflow:loss = 0.016730744, step = 4501 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.34\n",
            "INFO:tensorflow:loss = 0.01305908, step = 4601 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.552\n",
            "INFO:tensorflow:loss = 0.0149013465, step = 4701 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.171\n",
            "INFO:tensorflow:loss = 0.016186288, step = 4801 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.21\n",
            "INFO:tensorflow:loss = 0.010697478, step = 4901 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.031\n",
            "INFO:tensorflow:loss = 0.0121748, step = 5001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.922\n",
            "INFO:tensorflow:loss = 0.010251671, step = 5101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.566\n",
            "INFO:tensorflow:loss = 0.009470027, step = 5201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 672.777\n",
            "INFO:tensorflow:loss = 0.009290161, step = 5301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 561.01\n",
            "INFO:tensorflow:loss = 0.008322063, step = 5401 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.913\n",
            "INFO:tensorflow:loss = 0.007463026, step = 5501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.656\n",
            "INFO:tensorflow:loss = 0.008654625, step = 5601 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.918\n",
            "INFO:tensorflow:loss = 0.0066266153, step = 5701 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.443\n",
            "INFO:tensorflow:loss = 0.005853649, step = 5801 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 665.544\n",
            "INFO:tensorflow:loss = 0.007863674, step = 5901 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.387\n",
            "INFO:tensorflow:loss = 0.0067504323, step = 6001 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.378\n",
            "INFO:tensorflow:loss = 0.006926375, step = 6101 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.249\n",
            "INFO:tensorflow:loss = 0.0076441476, step = 6201 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 429.208\n",
            "INFO:tensorflow:loss = 0.008132109, step = 6301 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 436.863\n",
            "INFO:tensorflow:loss = 0.007067818, step = 6401 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 427.988\n",
            "INFO:tensorflow:loss = 0.007771345, step = 6501 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 428.702\n",
            "INFO:tensorflow:loss = 0.008571635, step = 6601 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.173\n",
            "INFO:tensorflow:loss = 0.006370264, step = 6701 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 437.038\n",
            "INFO:tensorflow:loss = 0.007902211, step = 6801 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 436.403\n",
            "INFO:tensorflow:loss = 0.007571103, step = 6901 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.842\n",
            "INFO:tensorflow:loss = 0.007701074, step = 7001 (0.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 433.946\n",
            "INFO:tensorflow:loss = 0.0069748363, step = 7101 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 435.377\n",
            "INFO:tensorflow:loss = 0.006078242, step = 7201 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 432.866\n",
            "INFO:tensorflow:loss = 0.006920199, step = 7301 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.928\n",
            "INFO:tensorflow:loss = 0.0069835032, step = 7401 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 428.295\n",
            "INFO:tensorflow:loss = 0.008342635, step = 7501 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 413.828\n",
            "INFO:tensorflow:loss = 0.0075705536, step = 7601 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 424.022\n",
            "INFO:tensorflow:loss = 0.009770835, step = 7701 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 404.565\n",
            "INFO:tensorflow:loss = 0.007863128, step = 7801 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.21\n",
            "INFO:tensorflow:loss = 0.0064819586, step = 7901 (0.240 sec)\n",
            "INFO:tensorflow:global_step/sec: 420.341\n",
            "INFO:tensorflow:loss = 0.008122636, step = 8001 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 428.897\n",
            "INFO:tensorflow:loss = 0.0079645105, step = 8101 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 429.681\n",
            "INFO:tensorflow:loss = 0.008810185, step = 8201 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.679\n",
            "INFO:tensorflow:loss = 0.0063357507, step = 8301 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 637.078\n",
            "INFO:tensorflow:loss = 0.0073736934, step = 8401 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.25\n",
            "INFO:tensorflow:loss = 0.007868772, step = 8501 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.225\n",
            "INFO:tensorflow:loss = 0.0060352013, step = 8601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.256\n",
            "INFO:tensorflow:loss = 0.007032373, step = 8701 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.445\n",
            "INFO:tensorflow:loss = 0.0073649846, step = 8801 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.713\n",
            "INFO:tensorflow:loss = 0.0066415877, step = 8901 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 592.367\n",
            "INFO:tensorflow:loss = 0.0062673464, step = 9001 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.903\n",
            "INFO:tensorflow:loss = 0.0063018356, step = 9101 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.025\n",
            "INFO:tensorflow:loss = 0.005963506, step = 9201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 657.42\n",
            "INFO:tensorflow:loss = 0.008365927, step = 9301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.257\n",
            "INFO:tensorflow:loss = 0.007223002, step = 9401 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.937\n",
            "INFO:tensorflow:loss = 0.005691554, step = 9501 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.111\n",
            "INFO:tensorflow:loss = 0.006352528, step = 9601 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 606.31\n",
            "INFO:tensorflow:loss = 0.006287248, step = 9701 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.285\n",
            "INFO:tensorflow:loss = 0.008145567, step = 9801 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.504\n",
            "INFO:tensorflow:loss = 0.0067370157, step = 9901 (0.148 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00694375.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 98.91113368524593\n",
            "Just using average = 598.5019569471624 has RMSE of 98.91593876158191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05665a74-6289-467e-a5cb-0e39d011b0a5",
        "id": "ssTSefyuVybq"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37698d21d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.014e+03 2.100e+01 6.610e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
            " [2.014e+03 1.700e+01 6.830e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
            " [2.018e+03 2.400e+01 6.710e+01 ... 1.000e+00 0.000e+00 0.000e+00]\n",
            " ...\n",
            " [2.018e+03 5.000e+00 6.220e+01 ... 1.000e+00 0.000e+00 0.000e+00]\n",
            " [2.016e+03 2.000e+00 6.130e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
            " [2.015e+03 1.900e+01 7.140e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642\n",
            " 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642 0.515642]\n",
            "[0.44358312 0.40999139 0.58656331 0.54349699 0.4918174  0.57105943\n",
            " 0.51765719 0.75107666 0.47372954 0.62101637 0.53402239 0.44186047\n",
            " 0.39276486 0.46080965 0.44444444 0.45994832 0.47631352 0.49267873\n",
            " 0.47459087 0.60034453 0.43066322 0.62187769 0.45994832 0.54521964\n",
            " 0.63049096 0.53402239 0.58225668 0.41171404 0.62273902 0.48492679\n",
            " 0.42635659 0.4461671  0.54866494 0.55900086 0.56847545 0.45564169\n",
            " 0.6546081  0.54005168 0.52627046 0.53143842 0.51421189 0.38673557\n",
            " 0.48578811 0.47200689 0.44875108 0.51593454 0.5211025  0.55641688\n",
            " 0.55986219 0.53660637 0.53316107 0.51162791 0.36003445 0.39879414\n",
            " 0.59259259 0.44358312 0.5667528  0.44358312 0.55641688 0.58570198\n",
            " 0.42635659 0.4754522  0.51507321 0.44702842 0.63738157 0.60034453\n",
            " 0.50301464 0.37209302 0.55900086 0.54177433 0.59345392 0.45305771\n",
            " 0.54177433 0.56072351 0.45564169 0.45219638 0.63996555 0.56589147\n",
            " 0.48578811 0.54349699 0.44702842 0.51421189 0.4952627  0.63479759\n",
            " 0.54866494 0.52971576 0.50387597 0.38156761 0.55900086 0.39534884\n",
            " 0.45822567 0.7166236  0.48062016 0.44530577 0.43496985 0.57536606\n",
            " 0.5245478  0.60809647 0.63135228 0.53057709 0.54694229 0.41085271\n",
            " 0.49784668 0.4039621  0.48148148 0.57536606 0.49440138 0.4918174\n",
            " 0.54091301 0.62101637 0.56847545 0.47459087 0.47028424 0.62015504\n",
            " 0.53229974 0.46339363 0.42894057 0.47803618 0.56589147 0.37898363\n",
            " 0.46511628 0.45822567 0.42291128 0.63221361 0.70542636 0.47200689\n",
            " 0.42204996 0.56330749 0.50732127 0.47631352 0.45564169 0.42118863\n",
            " 0.61068045 0.51593454 0.47803618 0.6089578  0.63996555 0.57019811\n",
            " 0.64082687 0.69939707 0.51679587 0.42377261 0.7002584  0.63652024\n",
            " 0.54608096 0.32816537 0.46511628 0.60465116 0.5374677  0.38931955\n",
            " 0.5047373  0.55555556 0.42894057 0.52627046 0.50387597 0.60465116\n",
            " 0.6124031  0.5538329  0.65202412 0.46167097 0.56330749 0.57450474\n",
            " 0.45478036 0.48492679 0.58828596 0.4788975  0.37898363 0.52971576\n",
            " 0.57105943 0.60292851 0.48664944 0.54349699 0.47200689 0.49095607\n",
            " 0.4625323  0.40568475 0.34022394 0.53402239 0.63135228 0.59345392\n",
            " 0.55211025 0.59259259 0.58656331 0.51507321 0.64513351 0.54608096\n",
            " 0.46942291 0.47200689 0.35745047 0.52540913 0.66838932 0.65891473\n",
            " 0.50559862 0.52713178 0.51765719 0.59431525 0.5960379  0.55900086\n",
            " 0.54091301 0.49095607 0.4918174  0.50732127 0.52196382 0.50215332\n",
            " 0.52196382 0.47028424 0.60723514 0.4918174  0.64513351 0.5211025\n",
            " 0.40826873 0.57364341 0.57708872 0.60551249 0.47372954 0.31093885\n",
            " 0.36778639 0.53574505 0.63910422 0.37898363 0.60551249 0.51937984\n",
            " 0.59689922 0.68475452 0.54694229 0.65977606 0.60120586 0.54521964\n",
            " 0.53488372 0.58828596 0.41515935 0.60378984 0.48234281 0.82773471\n",
            " 0.37209302 0.47717485 0.37553833 0.58484065 0.63996555 0.65202412\n",
            " 0.58397933 0.49956934 0.55555556 0.54866494 0.52540913 0.45650301\n",
            " 0.60981912 0.45822567 0.52971576 0.64599483 0.52368648 0.49956934\n",
            " 0.58914729 0.50129199 0.58053402 0.76055125 0.34280792 0.45650301\n",
            " 0.45564169 0.5503876  0.43669251 0.63652024 0.60292851 0.39534884\n",
            " 0.54349699 0.49870801 0.49698536 0.47975883 0.26614987 0.54349699\n",
            " 0.56158484 0.49354005 0.49612403 0.37984496 0.50990525 0.42204996\n",
            " 0.49870801 0.4788975  0.41429802 0.62187769 0.66494401 0.50301464\n",
            " 0.30060293 0.51076658 0.39276486 0.44013781 0.50387597 0.33850129\n",
            " 0.55900086 0.36864772 0.64857881 0.53229974 0.36864772 0.52627046\n",
            " 0.49095607 0.51248923 0.52971576 0.60981912 0.38070629 0.40137812\n",
            " 0.50301464 0.63910422 0.41257537 0.56589147 0.5211025  0.52885444\n",
            " 0.65202412 0.57622739 0.5503876  0.5538329  0.6124031  0.54263566\n",
            " 0.58570198 0.33074935 0.38070629 0.54866494 0.51248923 0.54694229\n",
            " 0.44530577 0.50990525 0.54694229 0.51679587 0.42721792 0.54349699\n",
            " 0.60378984 0.43152455 0.50387597 0.54263566 0.59086994 0.48664944\n",
            " 0.51421189 0.54091301 0.61154177 0.65030146 0.58225668 0.43066322\n",
            " 0.57536606 0.42463394 0.39362618 0.64254953 0.51765719 0.59431525\n",
            " 0.64771748 0.56933678 0.47975883 0.49009475 0.53574505 0.52799311\n",
            " 0.38845823 0.58053402 0.57019811 0.68130922 0.45391904 0.416882\n",
            " 0.63221361 0.46511628 0.55813953 0.37037037 0.51162791 0.34625323\n",
            " 0.58139535 0.51076658 0.5211025  0.55297158 0.47631352 0.62790698\n",
            " 0.34366925 0.52885444 0.56761413 0.49784668 0.62187769 0.49784668\n",
            " 0.45564169 0.51765719 0.53660637 0.57622739 0.43066322 0.38845823\n",
            " 0.48406546 0.42980189 0.47631352 0.65202412 0.60809647 0.47717485\n",
            " 0.41085271 0.65202412 0.50215332 0.42894057 0.44875108 0.43927649\n",
            " 0.47028424 0.60723514 0.62618432 0.50129199 0.47717485 0.43669251\n",
            " 0.41257537 0.68217054 0.57019811 0.51593454 0.50043066 0.51507321\n",
            " 0.55469423 0.46511628 0.57536606 0.60809647 0.48148148 0.44013781\n",
            " 0.43496985 0.33936262 0.42894057 0.36606374 0.59862188 0.5211025\n",
            " 0.51937984 0.6089578  0.41085271 0.4952627  0.5245478  0.30577089\n",
            " 0.40740741 0.46080965 0.52971576 0.57019811 0.51335056 0.78466839\n",
            " 0.55813953 0.6873385  0.52282515 0.55986219 0.5667528  0.5211025\n",
            " 0.46425495 0.416882   0.53402239 0.48664944 0.38931955 0.52024117\n",
            " 0.56330749 0.47459087 0.60120586 0.39276486 0.4918174  0.49009475\n",
            " 0.53402239 0.50904393 0.63479759 0.4005168  0.48751077 0.45994832\n",
            " 0.47286822 0.43496985 0.54521964 0.63393626 0.61757106 0.44530577\n",
            " 0.45822567 0.55986219 0.6709733  0.44099914 0.39534884 0.54866494\n",
            " 0.58656331 0.57019811 0.6546081  0.53057709 0.50301464 0.57450474\n",
            " 0.50215332 0.53919035 0.43066322 0.62015504 0.53574505 0.42291128\n",
            " 0.51851852 0.61498708 0.60809647 0.50215332 0.55641688 0.53919035\n",
            " 0.57364341 0.66063738 0.66925065 0.63652024 0.57364341 0.56072351\n",
            " 0.43496985 0.60120586 0.39793282 0.6089578  0.51593454 0.56330749\n",
            " 0.50990525 0.39965547 0.66149871 0.40913006 0.416882   0.39793282\n",
            " 0.60120586 0.41946598 0.49267873 0.55555556 0.46856158 0.49009475\n",
            " 0.50904393 0.35400517 0.36175711 0.61498708 0.53143842 0.68561585\n",
            " 0.41860465 0.46942291]\n",
            "The trained model has an aproximate error rate of 2.9216839671134966 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location\n"
      ],
      "metadata": {
        "id": "kHVsefPtZOYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REF https://www.geeksforgeeks.org/read-a-zipped-file-as-a-pandas-dataframe/ 12/11"
      ],
      "metadata": {
        "id": "rxJZ5Y6xoUBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/locdnn.zip', index_col=0,compression='zip' )\n",
        "print(df_loc[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d863d6f0-0004-488c-aaf7-9e85a7a84197",
        "id": "d4VkDncof2uz"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS   latitude  longitude  temp  dewp     slp  visib  \\\n",
            "1  2018   2               1  40.681750 -73.967480  14.7   2.0  1024.9   10.0   \n",
            "2  2018   2               1  40.645370 -73.945110  14.7   2.0  1024.9   10.0   \n",
            "3  2018   2               1  40.614830 -73.998380  14.7   2.0  1024.9   10.0   \n",
            "4  2018   2               1  40.592190 -74.087395  14.7   2.0  1024.9   10.0   \n",
            "5  2018   2               1  40.769817 -73.782370  14.7   2.0  1024.9   10.0   \n",
            "6  2018   2               1  40.660175 -73.928200  14.7   2.0  1024.9   10.0   \n",
            "\n",
            "   wdsp  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "2  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "4  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "5  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc_dnn = df_loc.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] != 2012]\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] < 2020]\n",
        "cols = df_loc_dnn['NUM_COLLISIONS']\n",
        "df_loc_dnn = df_loc_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_loc_dnn.insert(loc=28, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_loc_dnn[:6])\n",
        "df_loc_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "03eae3e8-8cbe-412e-c330-ab080c8dbcf8",
        "id": "hl2pF-yuf2u0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da   latitude  longitude  temp  visib  wdsp   max  min   sndp  ...  \\\n",
            "1  2018   2  40.681750 -73.967480  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "2  2018   2  40.645370 -73.945110  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "3  2018   2  40.614830 -73.998380  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "4  2018   2  40.592190 -74.087395  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "5  2018   2  40.769817 -73.782370  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "6  2018   2  40.660175 -73.928200  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "\n",
            "   Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "1    0    0    0    1    0    0    0    0    0               1  \n",
            "2    0    0    0    1    0    0    0    0    0               1  \n",
            "3    0    0    0    1    0    0    0    0    0               1  \n",
            "4    0    0    0    1    0    0    0    0    0               1  \n",
            "5    0    0    0    1    0    0    0    0    0               1  \n",
            "6    0    0    0    1    0    0    0    0    0               1  \n",
            "\n",
            "[6 rows x 29 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               year            da      latitude     longitude          temp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   2.016149e+03  1.566747e+01  4.072402e+01 -7.392063e+01  5.203610e+01   \n",
              "std    1.970562e+00  8.752606e+00  7.845179e-02  8.651997e-02  1.410404e+01   \n",
              "min    2.013000e+03  1.000000e+00  4.049895e+01 -7.425453e+01  5.800000e+00   \n",
              "25%    2.014000e+03  8.000000e+00  4.066895e+01 -7.397644e+01  4.100000e+01   \n",
              "50%    2.016000e+03  1.600000e+01  4.072234e+01 -7.392891e+01  5.300000e+01   \n",
              "75%    2.018000e+03  2.300000e+01  4.076838e+01 -7.386641e+01  6.430000e+01   \n",
              "max    2.019000e+03  3.100000e+01  4.091288e+01 -7.366301e+01  7.750000e+01   \n",
              "\n",
              "              visib          wdsp           max           min          sndp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   8.262622e+00  1.066076e+01  6.012695e+01  4.442143e+01  9.999000e+02   \n",
              "std    2.220453e+00  4.197681e+00  1.424273e+01  1.468128e+01  2.842172e-12   \n",
              "min    2.000000e-01  2.700000e+00  1.800000e+01 -2.000000e+00  9.999000e+02   \n",
              "25%    7.000000e+00  7.700000e+00  4.890000e+01  3.310000e+01  9.999000e+02   \n",
              "50%    9.300000e+00  1.010000e+01  6.100000e+01  4.500000e+01  9.999000e+02   \n",
              "75%    1.000000e+01  1.290000e+01  7.300000e+01  5.700000e+01  9.999000e+02   \n",
              "max    1.000000e+01  3.930000e+01  9.000000e+01  7.160000e+01  9.999000e+02   \n",
              "\n",
              "       ...           Nov           Oct           Sep           Mon  \\\n",
              "count  ...  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   ...  8.510758e-02  8.877971e-02  8.587448e-02  1.487626e-01   \n",
              "std    ...  2.790418e-01  2.844256e-01  2.801787e-01  3.558544e-01   \n",
              "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "                Sat           Sun           Thu           Tue           Wed  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   1.173738e-01  1.438814e-01  1.597195e-01  1.467698e-01  1.505395e-01   \n",
              "std    3.218653e-01  3.509695e-01  3.663458e-01  3.538765e-01  3.575996e-01   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count    1.311775e+06  \n",
              "mean     1.026582e+00  \n",
              "std      1.784039e-01  \n",
              "min      1.000000e+00  \n",
              "25%      1.000000e+00  \n",
              "50%      1.000000e+00  \n",
              "75%      1.000000e+00  \n",
              "max      1.100000e+01  \n",
              "\n",
              "[8 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4e7b022-77a8-43a4-86e7-8f6bb3aa21a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.016149e+03</td>\n",
              "      <td>1.566747e+01</td>\n",
              "      <td>4.072402e+01</td>\n",
              "      <td>-7.392063e+01</td>\n",
              "      <td>5.203610e+01</td>\n",
              "      <td>8.262622e+00</td>\n",
              "      <td>1.066076e+01</td>\n",
              "      <td>6.012695e+01</td>\n",
              "      <td>4.442143e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>8.510758e-02</td>\n",
              "      <td>8.877971e-02</td>\n",
              "      <td>8.587448e-02</td>\n",
              "      <td>1.487626e-01</td>\n",
              "      <td>1.173738e-01</td>\n",
              "      <td>1.438814e-01</td>\n",
              "      <td>1.597195e-01</td>\n",
              "      <td>1.467698e-01</td>\n",
              "      <td>1.505395e-01</td>\n",
              "      <td>1.026582e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.970562e+00</td>\n",
              "      <td>8.752606e+00</td>\n",
              "      <td>7.845179e-02</td>\n",
              "      <td>8.651997e-02</td>\n",
              "      <td>1.410404e+01</td>\n",
              "      <td>2.220453e+00</td>\n",
              "      <td>4.197681e+00</td>\n",
              "      <td>1.424273e+01</td>\n",
              "      <td>1.468128e+01</td>\n",
              "      <td>2.842172e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>2.790418e-01</td>\n",
              "      <td>2.844256e-01</td>\n",
              "      <td>2.801787e-01</td>\n",
              "      <td>3.558544e-01</td>\n",
              "      <td>3.218653e-01</td>\n",
              "      <td>3.509695e-01</td>\n",
              "      <td>3.663458e-01</td>\n",
              "      <td>3.538765e-01</td>\n",
              "      <td>3.575996e-01</td>\n",
              "      <td>1.784039e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.013000e+03</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>4.049895e+01</td>\n",
              "      <td>-7.425453e+01</td>\n",
              "      <td>5.800000e+00</td>\n",
              "      <td>2.000000e-01</td>\n",
              "      <td>2.700000e+00</td>\n",
              "      <td>1.800000e+01</td>\n",
              "      <td>-2.000000e+00</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.014000e+03</td>\n",
              "      <td>8.000000e+00</td>\n",
              "      <td>4.066895e+01</td>\n",
              "      <td>-7.397644e+01</td>\n",
              "      <td>4.100000e+01</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>7.700000e+00</td>\n",
              "      <td>4.890000e+01</td>\n",
              "      <td>3.310000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.016000e+03</td>\n",
              "      <td>1.600000e+01</td>\n",
              "      <td>4.072234e+01</td>\n",
              "      <td>-7.392891e+01</td>\n",
              "      <td>5.300000e+01</td>\n",
              "      <td>9.300000e+00</td>\n",
              "      <td>1.010000e+01</td>\n",
              "      <td>6.100000e+01</td>\n",
              "      <td>4.500000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.018000e+03</td>\n",
              "      <td>2.300000e+01</td>\n",
              "      <td>4.076838e+01</td>\n",
              "      <td>-7.386641e+01</td>\n",
              "      <td>6.430000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.290000e+01</td>\n",
              "      <td>7.300000e+01</td>\n",
              "      <td>5.700000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.019000e+03</td>\n",
              "      <td>3.100000e+01</td>\n",
              "      <td>4.091288e+01</td>\n",
              "      <td>-7.366301e+01</td>\n",
              "      <td>7.750000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>3.930000e+01</td>\n",
              "      <td>9.000000e+01</td>\n",
              "      <td>7.160000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.100000e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4e7b022-77a8-43a4-86e7-8f6bb3aa21a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4e7b022-77a8-43a4-86e7-8f6bb3aa21a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4e7b022-77a8-43a4-86e7-8f6bb3aa21a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_loc_dnn.iloc[np.random.permutation(len(df_loc_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46241575-c62e-4e4d-c84b-d402d4420f41",
        "id": "cPhXM4DJf2u1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         year  da   latitude  longitude  temp  visib  wdsp   max   min   sndp  \\\n",
            "267720   2014   2  40.543154 -74.173772  39.3    5.5   8.7  45.0  30.9  999.9   \n",
            "1016167  2019   9  40.801125 -73.929880  73.7    8.9  10.4  78.1  69.1  999.9   \n",
            "856337   2017   2  40.756490 -73.997765  58.0   10.0   9.3  66.0  48.0  999.9   \n",
            "1075328  2015  23  40.709821 -73.940196  71.9   10.0   8.3  80.1  63.0  999.9   \n",
            "267094   2018  26  40.704200 -73.964290  36.4   10.0  19.4  45.0  32.0  999.9   \n",
            "185493   2014  28  40.719607 -73.730291  41.2    8.9  13.7  48.0  34.0  999.9   \n",
            "\n",
            "         ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "267720   ...    0    0    0    0    0    1    0    0    0    0  \n",
            "1016167  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "856337   ...    0    0    1    0    0    0    1    0    0    0  \n",
            "1075328  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "267094   ...    0    0    0    0    0    0    1    0    0    0  \n",
            "185493   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "\n",
            "[6 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e74bb9-54ef-498f-a636-b5f89eb5550f",
        "id": "sol3wzXcf2u2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "267720     1\n",
            "1016167    2\n",
            "856337     1\n",
            "1075328    1\n",
            "267094     1\n",
            "185493     1\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897130cc-5724-422a-a633-58f12f8ad2ea",
        "id": "_fZ3mPt-f2u2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_loc', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f04cf05-7ddd-4df0-fdcb-afa8983cd340",
        "id": "FEMInwLGf2u3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3769a7ae10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:loss = 156.72598, step = 1\n",
            "INFO:tensorflow:global_step/sec: 555.74\n",
            "INFO:tensorflow:loss = 0.0012783015, step = 101 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.757\n",
            "INFO:tensorflow:loss = 0.000111670524, step = 201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.106\n",
            "INFO:tensorflow:loss = 4.7639082e-06, step = 301 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.023\n",
            "INFO:tensorflow:loss = 9.176864e-08, step = 401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.376\n",
            "INFO:tensorflow:loss = 2.3307496e-08, step = 501 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.66\n",
            "INFO:tensorflow:loss = 1.1534198e-08, step = 601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 679.172\n",
            "INFO:tensorflow:loss = 1.1537262e-08, step = 701 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.105\n",
            "INFO:tensorflow:loss = 1.7013965e-08, step = 801 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 700.573\n",
            "INFO:tensorflow:loss = 3.9554433e-08, step = 901 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.939\n",
            "INFO:tensorflow:loss = 1.6982176e-08, step = 1001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.85\n",
            "INFO:tensorflow:loss = 1.1604843e-08, step = 1101 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.665\n",
            "INFO:tensorflow:loss = 3.897115e-08, step = 1201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 681.582\n",
            "INFO:tensorflow:loss = 5.9984684e-09, step = 1301 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.57\n",
            "INFO:tensorflow:loss = 3.952812e-08, step = 1401 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.976\n",
            "INFO:tensorflow:loss = 1.6980366e-08, step = 1501 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.641\n",
            "INFO:tensorflow:loss = 6.0608154e-09, step = 1601 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.612\n",
            "INFO:tensorflow:loss = 2.7975108e-08, step = 1701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.806\n",
            "INFO:tensorflow:loss = 1.6981023e-08, step = 1801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.051\n",
            "INFO:tensorflow:loss = 1.6982701e-08, step = 1901 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.896\n",
            "INFO:tensorflow:loss = 1.6994324e-08, step = 2001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.925\n",
            "INFO:tensorflow:loss = 1.1480926e-08, step = 2101 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.516\n",
            "INFO:tensorflow:loss = 1.6995468e-08, step = 2201 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 737.194\n",
            "INFO:tensorflow:loss = 2.2470026e-08, step = 2301 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 723.089\n",
            "INFO:tensorflow:loss = 2.2460268e-08, step = 2401 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.79\n",
            "INFO:tensorflow:loss = 2.8009065e-08, step = 2501 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 726.286\n",
            "INFO:tensorflow:loss = 2.2459437e-08, step = 2601 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.96\n",
            "INFO:tensorflow:loss = 2.2492205e-08, step = 2701 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.96\n",
            "INFO:tensorflow:loss = 2.786239e-08, step = 2801 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.733\n",
            "INFO:tensorflow:loss = 1.7096754e-08, step = 2901 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.159\n",
            "INFO:tensorflow:loss = 1.6995324e-08, step = 3001 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.273\n",
            "INFO:tensorflow:loss = 4.5048864e-08, step = 3101 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.579\n",
            "INFO:tensorflow:loss = 5.5121385e-10, step = 3201 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.168\n",
            "INFO:tensorflow:loss = 5.796117e-09, step = 3301 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.489\n",
            "INFO:tensorflow:loss = 5.968578e-09, step = 3401 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.905\n",
            "INFO:tensorflow:loss = 1.14443175e-08, step = 3501 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.687\n",
            "INFO:tensorflow:loss = 2.2538373e-08, step = 3601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.972\n",
            "INFO:tensorflow:loss = 2.2505231e-08, step = 3701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.666\n",
            "INFO:tensorflow:loss = 2.7894686e-08, step = 3801 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.287\n",
            "INFO:tensorflow:loss = 1.69855e-08, step = 3901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.832\n",
            "INFO:tensorflow:loss = 1.1439143e-08, step = 4001 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.492\n",
            "INFO:tensorflow:loss = 1.7008015e-08, step = 4101 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.256\n",
            "INFO:tensorflow:loss = 4.5121904e-08, step = 4201 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.921\n",
            "INFO:tensorflow:loss = 2.7945784e-08, step = 4301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.622\n",
            "INFO:tensorflow:loss = 1.7015063e-08, step = 4401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.8\n",
            "INFO:tensorflow:loss = 2.2495243e-08, step = 4501 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.427\n",
            "INFO:tensorflow:loss = 1.6992377e-08, step = 4601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 710.872\n",
            "INFO:tensorflow:loss = 2.7994924e-08, step = 4701 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.055\n",
            "INFO:tensorflow:loss = 6.1269048e-09, step = 4801 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 761.565\n",
            "INFO:tensorflow:loss = 1.2565072e-09, step = 4901 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.615\n",
            "INFO:tensorflow:loss = 1.7053743e-08, step = 5001 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.836\n",
            "INFO:tensorflow:loss = 2.7857203e-08, step = 5101 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.204\n",
            "INFO:tensorflow:loss = 5.999001e-09, step = 5201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.077\n",
            "INFO:tensorflow:loss = 4.069793e-10, step = 5301 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.399\n",
            "INFO:tensorflow:loss = 6.243038e-09, step = 5401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.409\n",
            "INFO:tensorflow:loss = 2.2575355e-08, step = 5501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.22\n",
            "INFO:tensorflow:loss = 2.8664779e-08, step = 5601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.265\n",
            "INFO:tensorflow:loss = 3.9444217e-08, step = 5701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.091\n",
            "INFO:tensorflow:loss = 5.751146e-09, step = 5801 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.184\n",
            "INFO:tensorflow:loss = 2.247306e-08, step = 5901 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 744.94\n",
            "INFO:tensorflow:loss = 1.7018357e-08, step = 6001 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 750.478\n",
            "INFO:tensorflow:loss = 3.9766917e-08, step = 6101 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 729.279\n",
            "INFO:tensorflow:loss = 1.6058799e-10, step = 6201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.575\n",
            "INFO:tensorflow:loss = 2.8229055e-08, step = 6301 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.342\n",
            "INFO:tensorflow:loss = 5.8625993e-09, step = 6401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.038\n",
            "INFO:tensorflow:loss = 1.1454159e-08, step = 6501 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.331\n",
            "INFO:tensorflow:loss = 2.317117e-08, step = 6601 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.83\n",
            "INFO:tensorflow:loss = 1.1469249e-08, step = 6701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.321\n",
            "INFO:tensorflow:loss = 1.698036e-08, step = 6801 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.914\n",
            "INFO:tensorflow:loss = 2.8600605e-08, step = 6901 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.493\n",
            "INFO:tensorflow:loss = 6.1037384e-09, step = 7001 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.761\n",
            "INFO:tensorflow:loss = 2.2459481e-08, step = 7101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.337\n",
            "INFO:tensorflow:loss = 2.557432e-10, step = 7201 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 734.113\n",
            "INFO:tensorflow:loss = 5.9265335e-09, step = 7301 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.619\n",
            "INFO:tensorflow:loss = 2.7937562e-08, step = 7401 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.055\n",
            "INFO:tensorflow:loss = 2.245968e-08, step = 7501 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 693.52\n",
            "INFO:tensorflow:loss = 2.2703334e-08, step = 7601 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 714.983\n",
            "INFO:tensorflow:loss = 7.171971e-09, step = 7701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 727.779\n",
            "INFO:tensorflow:loss = 2.2461531e-08, step = 7801 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.713\n",
            "INFO:tensorflow:loss = 1.7060506e-08, step = 7901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.419\n",
            "INFO:tensorflow:loss = 1.836826e-08, step = 8001 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.563\n",
            "INFO:tensorflow:loss = 4.3715968e-08, step = 8101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.094\n",
            "INFO:tensorflow:loss = 1.1494547e-08, step = 8201 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 723.711\n",
            "INFO:tensorflow:loss = 3.4051826e-08, step = 8301 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.418\n",
            "INFO:tensorflow:loss = 3.4554798e-08, step = 8401 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.569\n",
            "INFO:tensorflow:loss = 2.2579604e-08, step = 8501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.986\n",
            "INFO:tensorflow:loss = 3.17406e-07, step = 8601 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 736.557\n",
            "INFO:tensorflow:loss = 1.7062154e-08, step = 8701 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.223\n",
            "INFO:tensorflow:loss = 1.7301003e-08, step = 8801 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.031\n",
            "INFO:tensorflow:loss = 1.1634326e-08, step = 8901 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.907\n",
            "INFO:tensorflow:loss = 3.3164646e-08, step = 9001 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.67\n",
            "INFO:tensorflow:loss = 1.1738505e-08, step = 9101 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 752.606\n",
            "INFO:tensorflow:loss = 6.196182e-08, step = 9201 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 671.338\n",
            "INFO:tensorflow:loss = 4.3485905e-08, step = 9301 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 747.191\n",
            "INFO:tensorflow:loss = 2.415043e-08, step = 9401 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 729.693\n",
            "INFO:tensorflow:loss = 1.7222062e-08, step = 9501 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 701.624\n",
            "INFO:tensorflow:loss = 1.1621119e-08, step = 9601 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.183\n",
            "INFO:tensorflow:loss = 1.7131473e-08, step = 9701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 724.03\n",
            "INFO:tensorflow:loss = 1.7009837e-08, step = 9801 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 706.989\n",
            "INFO:tensorflow:loss = 1.1427293e-08, step = 9901 (0.143 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.7292086e-08.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 0.18042282495495965\n",
            "Just using average = 1.026596596215052 has RMSE of 0.17866213280656273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ea3c67-cff7-47c6-af1b-dbf489e8899b",
        "id": "UoS_IOAwf2u5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37711e7a90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.01300000e+03 2.30000000e+01 4.07660506e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01800000e+03 1.30000000e+01 4.07310680e+01 ... 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01700000e+03 1.00000000e+00 4.05991440e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.01800000e+03 4.00000000e+00 4.07605360e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [2.01800000e+03 1.40000000e+01 4.07658650e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [2.01700000e+03 1.60000000e+01 4.07441000e+01 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00086252 0.00086252 0.00086252 ... 0.00086252 0.00086252 0.00086252]\n",
            "[0.00086133 0.00086133 0.00086133 ... 0.00086133 0.00086133 0.00086133]\n",
            "The trained model has an aproximate error rate of 0.025144460959203294 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcyH_em_Vg-s"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}