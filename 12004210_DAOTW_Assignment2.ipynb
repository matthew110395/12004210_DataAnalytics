{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew110395/12004210_DataAnalytics/blob/main/12004210_DAOTW_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "This assignment builds on the New York taxi problem identified in assignment 1, where the City of New York is looking for a way to accurately predict the number of collisions on a particular day of the week. Within this document two different types of machine learning models will be utilised to predict the number of collisions. The linear relationships identified in assignment 1 will be used to create a linear regression model. A Deep Learning Neural Network (DNN) will be used to predict the number of collisions where the relationship is not linear. "
      ],
      "metadata": {
        "id": "-MIIDzFYc6Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n",
        "To prepare data for machine learning the pandas package has been used. The numpy package has been used to aid with mathematical functions.\n",
        "\n",
        "As within part 1 of this assignment the data file containing location data exceeds the size limit for hosting within github. To overcome this the file was zipped. In order to extract the data from the zip file the zipfile package has been used.\n",
        "\n",
        "Within this document, Tensorflow has been used for machine learning, with both a linear regression model and a Deep Neural Network model. Tensorflow version 1 is unsupported within Google Colab, therefore must be installed using the pip package manager.\n",
        "\n",
        "Shutil has also been imported to allow for easy file management, in particular the removal of saved models."
      ],
      "metadata": {
        "id": "PP_OBRRWE3tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "\n",
        "!pip install tensorflow==1.15.2\n",
        "import tensorflow as tf\n",
        "# needed for high-level file management\n",
        "import shutil  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Vp_IJAE49d",
        "outputId": "f5fa5453-20e5-4695-e468-f929835b5cb0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.38.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.19.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regressor\n",
        "Throughout assignment 1 a number of linear relationships were uncovered within the dataset. These relationsips form the basis of the linear regression models below.\n",
        "\n",
        "A linear regressor is used to predict an output variable based on one or more input variables. REF https://www.ibm.com/uk-en/topics/linear-regression 17/11\n",
        "\n"
      ],
      "metadata": {
        "id": "SxU2LFGDjN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the accuracy of the model the target values are scaled or standarised. This reduces the range of collisions from 188-1161 to 0.1619... - 1, this allows for quicker training of the model. https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0 REF 17.11"
      ],
      "metadata": {
        "id": "vKNbHovb6hXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_COLLISIONS=1161"
      ],
      "metadata": {
        "id": "k-aEMQX9EuJJ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Precipition\n",
        "As uncovered in assignment 1, as the volume of precipition increases, the number of collisions increase. \n",
        "\n",
        "The cleaned datafile produced in assignment is imported."
      ],
      "metadata": {
        "id": "VtJ7HqhA3bIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean.csv', index_col=0, )\n",
        "print(df_prcp[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0ljmfBkDwE",
        "outputId": "9ec5bfcc-b32e-4d22-885e-2e84a7b59cd3"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create the linear regression model, extra columns are removed to simplify the model with the aim of reducing error values within the model.\n",
        "\n",
        "The incomplete years (2012 and 2022) are removed, along with the eronious data for 2020 and 2021.\n",
        "\n",
        "To aid with the production of the model the target, in this case the number of collisions is moved to the end of the data table.\n",
        "\n",
        "The summary below shows there is 2539 rows within the dataset after the data has been cleansed further, as this count exists for each column each column or predcictor contains data."
      ],
      "metadata": {
        "id": "NnrlNKFM0R-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = df_prcp.drop(columns=['collision_date', 'temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] != 2012]\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] < 2020]\n",
        "cols = df_prcp['NUM_COLLISIONS']\n",
        "df_prcp = df_prcp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp.insert(loc=9, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp[:6])\n",
        "df_prcp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "nRcEly727YQb",
        "outputId": "f096eeb5-be8f-40e6-9e7b-779a0e701d4f"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "49    4  2016   1  28  0.09    0             0                 0     0   \n",
            "51    5  2014   1  17  0.00    1             0                 0     0   \n",
            "54    1  2016   1  25  0.02    0             0                 0     0   \n",
            "55    5  2016   1  29  0.00    0             0                 0     0   \n",
            "58    5  2017   1  20  0.00    0             0                 0     0   \n",
            "59    7  2013   1  13  0.01    1             0                 0     0   \n",
            "\n",
            "    NUM_COLLISIONS  \n",
            "49             681  \n",
            "51             589  \n",
            "54             658  \n",
            "55             645  \n",
            "58             605  \n",
            "59             373  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         prcp  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean      3.998425  2015.989366     6.518708    15.745569     0.122588   \n",
              "std       2.003542     1.996126     3.455211     8.803199     0.329143   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     0.000000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000     0.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000     0.000000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000     0.060000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000     3.760000   \n",
              "\n",
              "               fog  rain_drizzle  snow_ice_pellets         hail  \\\n",
              "count  2539.000000   2539.000000       2539.000000  2539.000000   \n",
              "mean      0.253249      0.375345          0.085467     0.000394   \n",
              "std       0.434958      0.484307          0.279630     0.019846   \n",
              "min       0.000000      0.000000          0.000000     0.000000   \n",
              "25%       0.000000      0.000000          0.000000     0.000000   \n",
              "50%       0.000000      0.000000          0.000000     0.000000   \n",
              "75%       1.000000      1.000000          0.000000     0.000000   \n",
              "max       1.000000      1.000000          1.000000     1.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2539.000000  \n",
              "mean       599.135093  \n",
              "std        100.299164  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf6851b2-6d0e-48f2-af85-f08b33d022b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998425</td>\n",
              "      <td>2015.989366</td>\n",
              "      <td>6.518708</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.003542</td>\n",
              "      <td>1.996126</td>\n",
              "      <td>3.455211</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf6851b2-6d0e-48f2-af85-f08b33d022b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf6851b2-6d0e-48f2-af85-f08b33d022b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf6851b2-6d0e-48f2-af85-f08b33d022b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove any bias within the dataset, it is randomly shuffled. This shuffled dataset is then split into the predictors and the target."
      ],
      "metadata": {
        "id": "eBk7xJt-Ag0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp.iloc[np.random.permutation(len(df_prcp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xT4g-UZFi01",
        "outputId": "8857708f-6eec-42f5-8dd0-ffb49df36b4b"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail\n",
            "3265    7  2018  11   4  0.45    0             0                 0     0\n",
            "3614    6  2017  12  16  0.08    0             0                 1     0\n",
            "429     6  2015   2  21  0.00    0             1                 0     0\n",
            "1291    4  2014   5   8  0.00    0             1                 0     0\n",
            "1022    5  2016   4  22  0.00    1             1                 0     0\n",
            "2192    5  2015   8  14  0.00    0             0                 0     0\n",
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "3265    7  2018  11   4  0.45    0             0                 0     0   \n",
            "3614    6  2017  12  16  0.08    0             0                 1     0   \n",
            "429     6  2015   2  21  0.00    0             1                 0     0   \n",
            "1291    4  2014   5   8  0.00    0             1                 0     0   \n",
            "1022    5  2016   4  22  0.00    1             1                 0     0   \n",
            "2192    5  2015   8  14  0.00    0             0                 0     0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "3265             502  \n",
            "3614             661  \n",
            "429              517  \n",
            "1291             596  \n",
            "1022             741  \n",
            "2192             729  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xkzkVBQjL_",
        "outputId": "216453c1-30d4-47d0-f51e-ebdab32d8efd"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3265    502\n",
            "3614    661\n",
            "429     517\n",
            "1291    596\n",
            "1022    741\n",
            "2192    729\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the accuracy/reliabilty of the trained linear regressor, the dataset has been split into a training and a testing dataset. As when compared with other machine learning datasets, this dataset is relatively small a larger portion is required for testing. 20% (511 rows) of the dataset has been reserved for testing, with the remaining 80% (2044 rows) used for training the dataset."
      ],
      "metadata": {
        "id": "RG7LYzfjBqnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 9\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "id": "NwSG_P_nRgPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24d5bdc-6359-44b1-f605-2401181d4bf3"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2031\n",
            "508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUKI-paISKeh",
        "outputId": "d655570b-d829-48e2-dc17-9c99390068c0"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c4c06d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.25712004, step = 1\n",
            "INFO:tensorflow:global_step/sec: 360.846\n",
            "INFO:tensorflow:loss = 0.0077984757, step = 101 (0.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 471.679\n",
            "INFO:tensorflow:loss = 0.005989602, step = 201 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.663\n",
            "INFO:tensorflow:loss = 0.0062982165, step = 301 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.284\n",
            "INFO:tensorflow:loss = 0.007830896, step = 401 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.446\n",
            "INFO:tensorflow:loss = 0.0070295143, step = 501 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.379\n",
            "INFO:tensorflow:loss = 0.0074045705, step = 601 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 795.578\n",
            "INFO:tensorflow:loss = 0.0066971965, step = 701 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 774.118\n",
            "INFO:tensorflow:loss = 0.0076593, step = 801 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 776.94\n",
            "INFO:tensorflow:loss = 0.006712172, step = 901 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.21\n",
            "INFO:tensorflow:loss = 0.0066762096, step = 1001 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 759.3\n",
            "INFO:tensorflow:loss = 0.0070536286, step = 1101 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.592\n",
            "INFO:tensorflow:loss = 0.0057173707, step = 1201 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.979\n",
            "INFO:tensorflow:loss = 0.0051599275, step = 1301 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 735.316\n",
            "INFO:tensorflow:loss = 0.0082608145, step = 1401 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.508\n",
            "INFO:tensorflow:loss = 0.007113132, step = 1501 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.859\n",
            "INFO:tensorflow:loss = 0.0071582412, step = 1601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 728.413\n",
            "INFO:tensorflow:loss = 0.0064016627, step = 1701 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.471\n",
            "INFO:tensorflow:loss = 0.0070564877, step = 1801 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 839.222\n",
            "INFO:tensorflow:loss = 0.005552577, step = 1901 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.071\n",
            "INFO:tensorflow:loss = 0.0075477995, step = 2001 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.621\n",
            "INFO:tensorflow:loss = 0.005818517, step = 2101 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.351\n",
            "INFO:tensorflow:loss = 0.0052909595, step = 2201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 724.792\n",
            "INFO:tensorflow:loss = 0.007102285, step = 2301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.583\n",
            "INFO:tensorflow:loss = 0.006063782, step = 2401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.607\n",
            "INFO:tensorflow:loss = 0.0070324205, step = 2501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.243\n",
            "INFO:tensorflow:loss = 0.007566843, step = 2601 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 772.507\n",
            "INFO:tensorflow:loss = 0.0054235617, step = 2701 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 838.812\n",
            "INFO:tensorflow:loss = 0.005675372, step = 2801 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.922\n",
            "INFO:tensorflow:loss = 0.005725488, step = 2901 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 878.839\n",
            "INFO:tensorflow:loss = 0.007714181, step = 3001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 792.22\n",
            "INFO:tensorflow:loss = 0.005837217, step = 3101 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 765.311\n",
            "INFO:tensorflow:loss = 0.004626683, step = 3201 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.863\n",
            "INFO:tensorflow:loss = 0.0059246244, step = 3301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 861.519\n",
            "INFO:tensorflow:loss = 0.0070081665, step = 3401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 752.736\n",
            "INFO:tensorflow:loss = 0.0063732537, step = 3501 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.945\n",
            "INFO:tensorflow:loss = 0.007572012, step = 3601 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.148\n",
            "INFO:tensorflow:loss = 0.005331916, step = 3701 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 842.401\n",
            "INFO:tensorflow:loss = 0.0050683166, step = 3801 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.998\n",
            "INFO:tensorflow:loss = 0.0077469684, step = 3901 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.318\n",
            "INFO:tensorflow:loss = 0.00621238, step = 4001 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 783.066\n",
            "INFO:tensorflow:loss = 0.0069361655, step = 4101 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 831.809\n",
            "INFO:tensorflow:loss = 0.0068397685, step = 4201 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 740.815\n",
            "INFO:tensorflow:loss = 0.008405516, step = 4301 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.692\n",
            "INFO:tensorflow:loss = 0.0072728377, step = 4401 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 788.619\n",
            "INFO:tensorflow:loss = 0.005010645, step = 4501 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.003\n",
            "INFO:tensorflow:loss = 0.0060479026, step = 4601 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.237\n",
            "INFO:tensorflow:loss = 0.0057616793, step = 4701 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 783.44\n",
            "INFO:tensorflow:loss = 0.006517249, step = 4801 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.418\n",
            "INFO:tensorflow:loss = 0.007322573, step = 4901 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 813.45\n",
            "INFO:tensorflow:loss = 0.007035657, step = 5001 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 752.363\n",
            "INFO:tensorflow:loss = 0.007138291, step = 5101 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.04\n",
            "INFO:tensorflow:loss = 0.0066877557, step = 5201 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.06\n",
            "INFO:tensorflow:loss = 0.0058956626, step = 5301 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 830.146\n",
            "INFO:tensorflow:loss = 0.0055920165, step = 5401 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 729.618\n",
            "INFO:tensorflow:loss = 0.0058149854, step = 5501 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 795.121\n",
            "INFO:tensorflow:loss = 0.00898945, step = 5601 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.186\n",
            "INFO:tensorflow:loss = 0.0050012595, step = 5701 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 821.178\n",
            "INFO:tensorflow:loss = 0.0053496566, step = 5801 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 719.167\n",
            "INFO:tensorflow:loss = 0.006118209, step = 5901 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.739\n",
            "INFO:tensorflow:loss = 0.005009223, step = 6001 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.637\n",
            "INFO:tensorflow:loss = 0.0054481598, step = 6101 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.974\n",
            "INFO:tensorflow:loss = 0.004796425, step = 6201 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 637.455\n",
            "INFO:tensorflow:loss = 0.007950958, step = 6301 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 795.919\n",
            "INFO:tensorflow:loss = 0.008348122, step = 6401 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 759.081\n",
            "INFO:tensorflow:loss = 0.007752786, step = 6501 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 799.037\n",
            "INFO:tensorflow:loss = 0.006146772, step = 6601 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 750.531\n",
            "INFO:tensorflow:loss = 0.0065741194, step = 6701 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.495\n",
            "INFO:tensorflow:loss = 0.0071890308, step = 6801 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 858.338\n",
            "INFO:tensorflow:loss = 0.0063590426, step = 6901 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.022\n",
            "INFO:tensorflow:loss = 0.0070876656, step = 7001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.268\n",
            "INFO:tensorflow:loss = 0.006708773, step = 7101 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 861.064\n",
            "INFO:tensorflow:loss = 0.007383666, step = 7201 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 878.055\n",
            "INFO:tensorflow:loss = 0.008612771, step = 7301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 809.96\n",
            "INFO:tensorflow:loss = 0.0065578604, step = 7401 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 722.559\n",
            "INFO:tensorflow:loss = 0.005702663, step = 7501 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 711.065\n",
            "INFO:tensorflow:loss = 0.0056765205, step = 7601 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 778.476\n",
            "INFO:tensorflow:loss = 0.0074159363, step = 7701 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 601.738\n",
            "INFO:tensorflow:loss = 0.007095259, step = 7801 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.809\n",
            "INFO:tensorflow:loss = 0.006431045, step = 7901 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 824.046\n",
            "INFO:tensorflow:loss = 0.0057518315, step = 8001 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 787.088\n",
            "INFO:tensorflow:loss = 0.0052697062, step = 8101 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.96\n",
            "INFO:tensorflow:loss = 0.006199975, step = 8201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.303\n",
            "INFO:tensorflow:loss = 0.0076053515, step = 8301 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.523\n",
            "INFO:tensorflow:loss = 0.0061915857, step = 8401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.287\n",
            "INFO:tensorflow:loss = 0.0059761265, step = 8501 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 781.35\n",
            "INFO:tensorflow:loss = 0.0057332655, step = 8601 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 718.329\n",
            "INFO:tensorflow:loss = 0.0060553984, step = 8701 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 817.707\n",
            "INFO:tensorflow:loss = 0.0065987194, step = 8801 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 744.815\n",
            "INFO:tensorflow:loss = 0.006934729, step = 8901 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 813.051\n",
            "INFO:tensorflow:loss = 0.0062237973, step = 9001 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 727.015\n",
            "INFO:tensorflow:loss = 0.0048401114, step = 9101 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.947\n",
            "INFO:tensorflow:loss = 0.010389652, step = 9201 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.774\n",
            "INFO:tensorflow:loss = 0.0067426497, step = 9301 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.487\n",
            "INFO:tensorflow:loss = 0.007814026, step = 9401 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 816.497\n",
            "INFO:tensorflow:loss = 0.007614771, step = 9501 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 775.441\n",
            "INFO:tensorflow:loss = 0.0058407066, step = 9601 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 801.602\n",
            "INFO:tensorflow:loss = 0.005744276, step = 9701 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.778\n",
            "INFO:tensorflow:loss = 0.006154378, step = 9801 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 801.06\n",
            "INFO:tensorflow:loss = 0.006982035, step = 9901 (0.128 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.008221562.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 95.58101427895235\n",
            "Just using average = 599.2466765140325 has RMSE of 102.9435177029394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A number of learning rates between 0.0000001 and 0.1 were been used to determine a suitable learning rate for the dataset. As the learning rate decreases the the overall time to train the dataset increases. REF https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 17/11"
      ],
      "metadata": {
        "id": "3uYG_ScZR3yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fifEsTD98hy",
        "outputId": "ccd87bed-ac9b-47f4-f332-deb6a7b6e8e8"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99bb8ed90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n",
            "[0.5022183  0.5353434  0.5206734  0.51581275 0.49316192 0.5045507\n",
            " 0.5338893  0.5600685  0.5156957  0.47868684 0.54077893 0.5094852\n",
            " 0.5023002  0.5001617  0.51542366 0.53891903 0.5187261  0.50646764\n",
            " 0.5721638  0.5590161  0.5000497  0.51368797 0.5334992  0.5064387\n",
            " 0.5656928  0.4869912  0.52234983 0.50545883 0.505442   0.46662718\n",
            " 0.52208596 0.48154363 0.5144106  0.5650267  0.514192   0.55520886\n",
            " 0.51249874 0.5646579  0.4953361  0.5488128  0.5561887  0.5030065\n",
            " 0.5198123  0.51275915 0.4894435  0.4967429  0.5206874  0.56131625\n",
            " 0.47201726 0.52059764 0.5433232  0.47994936 0.5841606  0.50721246\n",
            " 0.54845345 0.5150955  0.51543826 0.57824385 0.5476491  0.5040895\n",
            " 0.51481164 0.55049896 0.52189547 0.49409357 0.57436055 0.53269124\n",
            " 0.51624864 0.48820978 0.53704005 0.4861982  0.57216585 0.55831945\n",
            " 0.5313755  0.5380306  0.5825255  0.54509604 0.55864084 0.5512855\n",
            " 0.59518117 0.4968579  0.5170377  0.53704613 0.5274571  0.5502619\n",
            " 0.50967604 0.54330957 0.51766753 0.53065425 0.52828157 0.48775116\n",
            " 0.48216155 0.56290156 0.5596642  0.47279775 0.55465376 0.5016017\n",
            " 0.5515007  0.5686014  0.4942088  0.5346775  0.512212   0.48703608\n",
            " 0.5149052  0.5279058  0.5838939  0.5088948  0.5182113  0.5681853\n",
            " 0.52188474 0.53171223 0.52490306 0.47517955 0.5510886  0.5581606\n",
            " 0.56971145 0.5559426  0.55144596 0.51404816 0.54943556 0.53592896\n",
            " 0.52852046 0.5248314  0.5106723  0.56514806 0.55569917 0.591945\n",
            " 0.502844   0.55349755 0.53269273 0.48837605 0.5297943  0.5622229\n",
            " 0.5073188  0.5562912  0.4687575  0.5344072  0.51770115 0.52734196\n",
            " 0.5061218  0.5036945  0.47313356 0.5431585  0.5208798  0.50804704\n",
            " 0.5038304  0.52332485 0.545093   0.5541503  0.52661765 0.5172719\n",
            " 0.5335169  0.51381457 0.5161042  0.49334696 0.56386846 0.49959266\n",
            " 0.6019914  0.5043001  0.4891266  0.5038846  0.4615726  0.54282445\n",
            " 0.54660714 0.46985593 0.46219337 0.5267862  0.5258497  0.44694498\n",
            " 0.48501298 0.5007488  0.5167782  0.5506996  0.5475088  0.5291453\n",
            " 0.5774038  0.57964087 0.48168597 0.53429216 0.5397319  0.5235124\n",
            " 0.47226873 0.5169398  0.50923145 0.4731721  0.5842068  0.5131806\n",
            " 0.50049424 0.4619433  0.5726539  0.5389874  0.51492923 0.57007474\n",
            " 0.54840463 0.49258086 0.5485397  0.5098917  0.5139909  0.47863725\n",
            " 0.55531126 0.5029092  0.5503365  0.55846226 0.4822774  0.50553215\n",
            " 0.510893   0.5616     0.54823667 0.52001977 0.53741753 0.5492165\n",
            " 0.52952284 0.55843514 0.54153085 0.5754554  0.54835165 0.49843925\n",
            " 0.5220581  0.5576787  0.47937495 0.5025158  0.5296182  0.5029853\n",
            " 0.5464482  0.49664593 0.4939837  0.5596523  0.5257073  0.5031184\n",
            " 0.5838576  0.50948393 0.5308597  0.5488533  0.5529771  0.5695608\n",
            " 0.52955925 0.56433755 0.5476541  0.5113977  0.50119096 0.5311243\n",
            " 0.526359   0.5368926  0.5535708  0.5464004  0.5024737  0.5485636\n",
            " 0.5129497  0.51957446 0.51731426 0.4838649  0.46704543 0.5284886\n",
            " 0.4689297  0.50399214 0.5669209  0.4715806  0.56158286 0.5628479\n",
            " 0.5501016  0.47836918 0.5325736  0.48199156 0.50910366 0.5202376\n",
            " 0.5060469  0.5064854  0.51899654 0.55694145 0.50687456 0.52385765\n",
            " 0.52344733 0.49994627 0.5678645  0.58052236 0.5153365  0.49309167\n",
            " 0.56142515 0.56890315 0.55862814 0.54086477 0.47389668 0.50357085\n",
            " 0.55728287 0.52047384 0.55750346 0.5093623  0.5299407  0.57799137\n",
            " 0.45760393 0.5016693  0.54940045 0.5008278  0.5122836  0.49401787\n",
            " 0.5014177  0.5792194  0.51939195 0.51820356 0.5602082  0.46719298\n",
            " 0.48797107 0.56045455 0.53754354 0.5261636  0.5072985  0.5664973\n",
            " 0.5452152  0.557203   0.5086175  0.5524607  0.49481153 0.5534842\n",
            " 0.5733602  0.53873956 0.48650476 0.5424764  0.48122504 0.5578172\n",
            " 0.49902347 0.5361934  0.5174855  0.4772349  0.5348361  0.56917447\n",
            " 0.4928533  0.5063055  0.52180773 0.5825971  0.51114124 0.48271266\n",
            " 0.57271945 0.51251453 0.52721816 0.52221674 0.47163534 0.56140536\n",
            " 0.51736885 0.49895796 0.48982897 0.5530734  0.49540368 0.5528897\n",
            " 0.5088294  0.5147004  0.5781608  0.5676422  0.48552793 0.540349\n",
            " 0.5041328  0.48488048 0.50262606 0.5069716  0.530205   0.49562448\n",
            " 0.4706923  0.5442716  0.5415893  0.56698143 0.5244207  0.44956496\n",
            " 0.5427952  0.5067294  0.46743903 0.5455462  0.513456   0.49379146\n",
            " 0.55469096 0.57190824 0.53974533 0.5239469  0.5392371  0.5151133\n",
            " 0.487402   0.49514237 0.48738667 0.5612133  0.48274744 0.4997284\n",
            " 0.5490965  0.55084413 0.5577154  0.5424088  0.5450958  0.5389419\n",
            " 0.5603184  0.5162194  0.5350299  0.53292364 0.54308283 0.53701496\n",
            " 0.5021029  0.55593646 0.5152999  0.51975656 0.56315684 0.52147394\n",
            " 0.55832696 0.47234362 0.50534254 0.487419   0.5401331  0.5102362\n",
            " 0.48225847 0.54034483 0.5578124  0.5308682  0.5414653  0.5249413\n",
            " 0.54733324 0.49772274 0.5043859  0.5313019  0.57230693 0.49393433\n",
            " 0.57419586 0.5723681  0.53766274 0.5370534  0.51354414 0.47072077\n",
            " 0.49697798 0.50417703 0.47753042 0.5401884  0.547963   0.49401283\n",
            " 0.5242414  0.48058724 0.56138885 0.57051057 0.50326186 0.47174135\n",
            " 0.44963768 0.53420794 0.547699   0.542942   0.5636646  0.53266\n",
            " 0.49593252 0.53399026 0.5191909  0.57387257 0.5647026  0.50830793\n",
            " 0.51217955 0.53960025 0.5506929  0.49324298 0.47366285 0.54485905\n",
            " 0.5611691  0.5014504  0.5337411  0.5391882  0.5324223  0.5698612\n",
            " 0.54178387 0.49408168 0.5556336  0.5138708  0.54007655 0.51715356\n",
            " 0.49232075 0.5150931  0.5000135  0.55873793 0.49165395 0.5599731\n",
            " 0.5030914  0.5776536  0.5420951  0.500791   0.54459584 0.49460042\n",
            " 0.50163394 0.5008814  0.55290294 0.5075393  0.5102783  0.51632684\n",
            " 0.524492   0.5257363  0.5324508  0.5066372  0.4988176  0.54607105\n",
            " 0.50198144 0.48987132 0.5446239  0.5157378  0.532259   0.5431334\n",
            " 0.490499   0.50506324 0.5281435  0.56951046 0.44939008 0.5503389\n",
            " 0.49425495 0.57764614 0.57373077 0.5097698  0.54423535 0.52429855\n",
            " 0.5371276  0.5273435  0.53251934 0.51466435]\n",
            "[0.46339363 0.32213609 0.71490095 0.48406546 0.45305771 0.49267873\n",
            " 0.41257537 0.58914729 0.57881137 0.46856158 0.4788975  0.56589147\n",
            " 0.43066322 0.43410853 0.51507321 0.51593454 0.47803618 0.44358312\n",
            " 0.53574505 0.50990525 0.50990525 0.48148148 0.59000861 0.42204996\n",
            " 0.46339363 0.52024117 0.60120586 0.44702842 0.39276486 0.44875108\n",
            " 0.43066322 0.49095607 0.52799311 0.43927649 0.51937984 0.6709733\n",
            " 0.49784668 0.63049096 0.49440138 0.57536606 0.55641688 0.51162791\n",
            " 0.53229974 0.40999139 0.40223945 0.41774332 0.4461671  0.57019811\n",
            " 0.45650301 0.416882   0.5667528  0.35486649 0.49956934 0.40137812\n",
            " 0.54263566 0.56847545 0.62187769 0.38156761 0.57105943 0.39276486\n",
            " 0.52971576 0.45908699 0.41860465 0.58139535 0.4952627  0.45219638\n",
            " 0.72782084 0.53143842 0.53402239 0.45047373 0.70542636 0.56072351\n",
            " 0.49698536 0.47459087 0.62446167 0.63738157 0.50301464 0.32988803\n",
            " 0.6089578  0.39793282 0.43496985 0.65288544 0.40826873 0.60723514\n",
            " 0.65116279 0.49009475 0.49784668 0.54694229 0.42291128 0.62187769\n",
            " 0.49784668 0.5538329  0.57536606 0.48751077 0.59259259 0.44272179\n",
            " 0.5796727  0.44702842 0.44702842 0.54435831 0.47631352 0.40999139\n",
            " 0.4918174  0.60981912 0.60206718 0.57105943 0.34797588 0.54435831\n",
            " 0.66063738 0.56244617 0.46511628 0.38501292 0.58225668 0.51851852\n",
            " 0.54694229 0.47631352 0.50043066 0.50990525 0.62618432 0.54349699\n",
            " 1.         0.44444444 0.54521964 0.58570198 0.55211025 0.34453058\n",
            " 0.44875108 0.5503876  0.57795004 0.38845823 0.51937984 0.55727821\n",
            " 0.48492679 0.60292851 0.33850129 0.57278208 0.54952627 0.45822567\n",
            " 0.42118863 0.65891473 0.416882   0.59173127 0.63393626 0.4461671\n",
            " 0.5211025  0.69336779 0.56503015 0.49870801 0.52971576 0.52971576\n",
            " 0.55900086 0.55986219 0.46339363 0.44702842 0.45047373 0.47114556\n",
            " 0.60034453 0.43755383 0.47975883 0.48923342 0.36692506 0.44702842\n",
            " 0.4918174  0.35486649 0.36864772 0.46511628 0.5960379  0.44530577\n",
            " 0.43755383 0.41343669 0.52196382 0.50732127 0.50559862 0.54091301\n",
            " 0.60292851 0.59345392 0.32127476 0.54091301 0.60465116 0.58656331\n",
            " 0.31093885 0.53919035 0.4625323  0.48492679 0.57881137 0.47286822\n",
            " 0.44530577 0.43841516 0.60637382 0.55986219 0.56330749 0.60378984\n",
            " 0.56158484 0.38070629 0.41515935 0.45478036 0.61154177 0.50904393\n",
            " 0.37898363 0.43496985 0.49612403 0.5245478  0.44702842 0.51162791\n",
            " 0.48406546 0.52885444 0.52885444 0.57364341 0.49784668 0.31955211\n",
            " 0.56072351 0.67011197 0.64427218 0.5081826  0.60809647 0.51679587\n",
            " 0.56847545 0.49009475 0.34625323 0.4918174  0.53832903 0.52024117\n",
            " 0.66063738 0.4332472  0.45391904 0.61068045 0.39362618 0.46683893\n",
            " 0.63479759 0.53316107 0.5538329  0.57881137 0.54349699 0.47631352\n",
            " 0.66925065 0.56589147 0.51421189 0.56330749 0.47803618 0.53574505\n",
            " 0.44186047 0.63049096 0.62962963 0.63910422 0.62273902 0.6124031\n",
            " 0.52540913 0.46942291 0.5796727  0.46597761 0.37812231 0.53057709\n",
            " 0.40482343 0.45564169 0.50215332 0.374677   0.374677   0.53057709\n",
            " 0.56933678 0.49095607 0.55641688 0.38845823 0.5047373  0.36606374\n",
            " 0.45305771 0.52885444 0.66408269 0.44788975 0.40568475 0.53660637\n",
            " 0.52799311 0.52282515 0.58570198 0.66149871 0.5047373  0.60292851\n",
            " 0.4918174  0.63824289 0.48406546 0.5667528  0.46339363 0.39362618\n",
            " 0.46770026 0.54780362 0.57364341 0.39276486 0.39879414 0.60206718\n",
            " 0.39534884 0.45305771 0.4918174  0.42204996 0.5081826  0.49956934\n",
            " 0.63996555 0.62446167 0.43496985 0.49870801 0.54694229 0.40654608\n",
            " 0.42807924 0.59259259 0.66494401 0.53316107 0.48751077 0.53660637\n",
            " 0.68217054 0.63135228 0.5245478  0.56761413 0.44358312 0.52799311\n",
            " 0.36089578 0.65202412 0.39793282 0.55727821 0.50301464 0.53229974\n",
            " 0.53143842 0.57364341 0.59862188 0.46770026 0.59086994 0.49009475\n",
            " 0.51507321 0.41257537 0.49440138 0.53143842 0.46511628 0.42635659\n",
            " 0.53229974 0.45047373 0.58656331 0.60034453 0.43669251 0.51507321\n",
            " 0.61068045 0.49698536 0.42635659 0.47717485 0.64857881 0.55641688\n",
            " 0.60292851 0.34022394 0.62618432 0.62187769 0.77174849 0.61154177\n",
            " 0.44358312 0.47028424 0.46339363 0.33936262 0.47459087 0.47717485\n",
            " 0.44530577 0.53488372 0.51507321 0.44444444 0.53143842 0.32730405\n",
            " 0.46339363 0.4203273  0.26614987 0.56072351 0.56933678 0.53574505\n",
            " 0.46683893 0.45908699 0.5796727  0.62015504 0.48062016 0.57536606\n",
            " 0.49784668 0.44099914 0.52024117 0.52627046 0.50732127 0.51937984\n",
            " 0.62704565 0.57450474 0.5667528  0.52368648 0.50301464 0.44875108\n",
            " 0.54952627 0.4754522  0.56072351 0.51076658 0.45736434 0.53574505\n",
            " 0.55900086 0.61498708 0.46942291 0.58656331 0.59173127 0.50990525\n",
            " 0.58656331 0.48923342 0.59000861 0.5374677  0.69853575 0.51421189\n",
            " 0.48751077 0.7329888  0.47459087 0.59086994 0.46942291 0.54177433\n",
            " 0.55124892 0.49095607 0.43669251 0.56761413 0.51765719 0.47975883\n",
            " 0.55555556 0.55211025 0.5667528  0.50129199 0.50387597 0.38673557\n",
            " 0.7002584  0.4754522  0.416882   0.52971576 0.53229974 0.42980189\n",
            " 0.55555556 0.34366925 0.57450474 0.46597761 0.40654608 0.41774332\n",
            " 0.4005168  0.45736434 0.52024117 0.59259259 0.51162791 0.5667528\n",
            " 0.40223945 0.54952627 0.4952627  0.33763997 0.59689922 0.50559862\n",
            " 0.50215332 0.54005168 0.61843239 0.41515935 0.4332472  0.55813953\n",
            " 0.49440138 0.83893196 0.62618432 0.71231697 0.65977606 0.56589147\n",
            " 0.48664944 0.49354005 0.53402239 0.59259259 0.47114556 0.34366925\n",
            " 0.53057709 0.56330749 0.5047373  0.56072351 0.4788975  0.54866494\n",
            " 0.43583118 0.49870801 0.64082687 0.3910422  0.60981912 0.52799311\n",
            " 0.30060293 0.64341085 0.69681309 0.51421189 0.57708872 0.6873385\n",
            " 0.56847545 0.45047373 0.5503876  0.59086994 0.56158484 0.33936262\n",
            " 0.51076658 0.55900086 0.5211025  0.53832903 0.59173127 0.59086994\n",
            " 0.51851852 0.55124892 0.74677003 0.58570198 0.36864772 0.22739018\n",
            " 0.44099914 0.5667528  0.5960379  0.51162791 0.71403962 0.4952627\n",
            " 0.58828596 0.46856158 0.55641688 0.57536606]\n",
            "The trained model has an aproximate error rate of -10.907761934411338 which equates to -2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two main tests have been applied to the model. The Route Mean Squared Error (RMSE) and a comparison between the target values in the testing dataset and the predicted values using the predictors in the testing dataset.\n",
        "\n",
        "Due to the nature of machine learning, different results are achieved with each run of the model.\n",
        "\n",
        "Predominantly the RMSE of the model is lower than that of the average. This indicates that the linear regression model makes more accurate predictions when compared to using just the average. The error rate of the model is aprocimately -2% when comparing the predicited and actual results."
      ],
      "metadata": {
        "id": "kKckcLmPVIMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "RB0Zq1024UmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f872a6d2-4659-4cd8-ec0b-a024cf9e3fe4",
        "id": "d2NB6odM5G9I"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] != 2012]\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] < 2020]\n",
        "cols = df_dewp['NUM_COLLISIONS']\n",
        "df_dewp = df_dewp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp[:6])\n",
        "df_dewp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "25c70713-6477-4ab5-ebc5-71ca06528f89",
        "id": "WwtmLQ6a5rHs"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  24.4             681\n",
            "51    5  2014   1  17  35.8             589\n",
            "54    1  2016   1  25  21.2             658\n",
            "55    5  2016   1  29  36.8             645\n",
            "58    5  2017   1  20  32.5             605\n",
            "59    7  2013   1  13  44.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         dewp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.998434  2015.999217     6.524070    15.723679    44.163170   \n",
              "std       2.000391     2.000000     3.449676     8.801271    16.995303   \n",
              "min       1.000000  2013.000000     1.000000     1.000000    -6.700000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    32.150000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    45.300000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    58.500000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    74.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.109980  \n",
              "std        100.277185  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c544340-f0a6-44f1-a5ce-a1dd3afaad71\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998434</td>\n",
              "      <td>2015.999217</td>\n",
              "      <td>6.524070</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000391</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.449676</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c544340-f0a6-44f1-a5ce-a1dd3afaad71')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c544340-f0a6-44f1-a5ce-a1dd3afaad71 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c544340-f0a6-44f1-a5ce-a1dd3afaad71');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp.iloc[np.random.permutation(len(df_dewp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53532af1-e947-4d0c-a028-d23080ba7b63",
        "id": "KXbAqzNN694C"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  dewp\n",
            "3546    2  2013  12   3  38.2\n",
            "1594    1  2014   6  23  50.3\n",
            "3652    4  2017  12  14  20.4\n",
            "2858    1  2014  10   6  42.6\n",
            "3616    4  2014  12   4  28.0\n",
            "1997    1  2019   7  29  69.7\n",
            "      day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "3546    2  2013  12   3  38.2             553\n",
            "1594    1  2014   6  23  50.3             644\n",
            "3652    4  2017  12  14  20.4             883\n",
            "2858    1  2014  10   6  42.6             639\n",
            "3616    4  2014  12   4  28.0             606\n",
            "1997    1  2019   7  29  69.7             649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26dd2147-f32b-45f8-b755-19d23768d38b",
        "id": "iGbT5sAJ7KTK"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3546    553\n",
            "1594    644\n",
            "3652    883\n",
            "2858    639\n",
            "3616    606\n",
            "1997    649\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488c59c8-89f2-41ca-a555-11fbe8fcca11",
        "id": "vpHbBnml7PZw"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81de6672-0bc0-44e6-990d-0cda86075232",
        "id": "j4GKf5BL7WI3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99cb169d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.2736016, step = 1\n",
            "INFO:tensorflow:global_step/sec: 480.979\n",
            "INFO:tensorflow:loss = 0.005087829, step = 101 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.826\n",
            "INFO:tensorflow:loss = 0.0064100022, step = 201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 758.07\n",
            "INFO:tensorflow:loss = 0.007078087, step = 301 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 742.854\n",
            "INFO:tensorflow:loss = 0.00606805, step = 401 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.659\n",
            "INFO:tensorflow:loss = 0.008043197, step = 501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.545\n",
            "INFO:tensorflow:loss = 0.00697574, step = 601 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 793.297\n",
            "INFO:tensorflow:loss = 0.0063088676, step = 701 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 815.418\n",
            "INFO:tensorflow:loss = 0.009155184, step = 801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.132\n",
            "INFO:tensorflow:loss = 0.0071434746, step = 901 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.874\n",
            "INFO:tensorflow:loss = 0.007919987, step = 1001 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.294\n",
            "INFO:tensorflow:loss = 0.0053975056, step = 1101 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 489.486\n",
            "INFO:tensorflow:loss = 0.004674265, step = 1201 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.164\n",
            "INFO:tensorflow:loss = 0.0076546017, step = 1301 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.48\n",
            "INFO:tensorflow:loss = 0.0059539457, step = 1401 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 758.893\n",
            "INFO:tensorflow:loss = 0.0071929097, step = 1501 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 759.4\n",
            "INFO:tensorflow:loss = 0.00580078, step = 1601 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 613.319\n",
            "INFO:tensorflow:loss = 0.0054620705, step = 1701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 820.916\n",
            "INFO:tensorflow:loss = 0.0071346113, step = 1801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.774\n",
            "INFO:tensorflow:loss = 0.007758608, step = 1901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 807.74\n",
            "INFO:tensorflow:loss = 0.0056170705, step = 2001 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.693\n",
            "INFO:tensorflow:loss = 0.0058448017, step = 2101 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 749.047\n",
            "INFO:tensorflow:loss = 0.00653834, step = 2201 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 313.693\n",
            "INFO:tensorflow:loss = 0.005358521, step = 2301 (0.329 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.899\n",
            "INFO:tensorflow:loss = 0.00654505, step = 2401 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.641\n",
            "INFO:tensorflow:loss = 0.0055179494, step = 2501 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.004\n",
            "INFO:tensorflow:loss = 0.007147734, step = 2601 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.096\n",
            "INFO:tensorflow:loss = 0.0068321982, step = 2701 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 430.51\n",
            "INFO:tensorflow:loss = 0.0060728667, step = 2801 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.237\n",
            "INFO:tensorflow:loss = 0.0067901136, step = 2901 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.407\n",
            "INFO:tensorflow:loss = 0.008544505, step = 3001 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.732\n",
            "INFO:tensorflow:loss = 0.008246124, step = 3101 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.031\n",
            "INFO:tensorflow:loss = 0.008024402, step = 3201 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 375.939\n",
            "INFO:tensorflow:loss = 0.007893236, step = 3301 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.232\n",
            "INFO:tensorflow:loss = 0.006510595, step = 3401 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.744\n",
            "INFO:tensorflow:loss = 0.0070016617, step = 3501 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.92\n",
            "INFO:tensorflow:loss = 0.0067357323, step = 3601 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 421.789\n",
            "INFO:tensorflow:loss = 0.0062591126, step = 3701 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 462.781\n",
            "INFO:tensorflow:loss = 0.0077893157, step = 3801 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.051\n",
            "INFO:tensorflow:loss = 0.0052656936, step = 3901 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 436.057\n",
            "INFO:tensorflow:loss = 0.0053131226, step = 4001 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 409.554\n",
            "INFO:tensorflow:loss = 0.0064454107, step = 4101 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.376\n",
            "INFO:tensorflow:loss = 0.0066600796, step = 4201 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.033\n",
            "INFO:tensorflow:loss = 0.006348986, step = 4301 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.691\n",
            "INFO:tensorflow:loss = 0.00679792, step = 4401 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 320.605\n",
            "INFO:tensorflow:loss = 0.008224811, step = 4501 (0.314 sec)\n",
            "INFO:tensorflow:global_step/sec: 401.471\n",
            "INFO:tensorflow:loss = 0.006286734, step = 4601 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.985\n",
            "INFO:tensorflow:loss = 0.009695031, step = 4701 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.33\n",
            "INFO:tensorflow:loss = 0.005587185, step = 4801 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 323.565\n",
            "INFO:tensorflow:loss = 0.0063366676, step = 4901 (0.315 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.72\n",
            "INFO:tensorflow:loss = 0.008133548, step = 5001 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.8\n",
            "INFO:tensorflow:loss = 0.00748716, step = 5101 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 247.245\n",
            "INFO:tensorflow:loss = 0.0053053517, step = 5201 (0.403 sec)\n",
            "INFO:tensorflow:global_step/sec: 471.93\n",
            "INFO:tensorflow:loss = 0.005156892, step = 5301 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 313.314\n",
            "INFO:tensorflow:loss = 0.0063964585, step = 5401 (0.324 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.01\n",
            "INFO:tensorflow:loss = 0.006094491, step = 5501 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 389.336\n",
            "INFO:tensorflow:loss = 0.0081793815, step = 5601 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 449.371\n",
            "INFO:tensorflow:loss = 0.0065777204, step = 5701 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 433.942\n",
            "INFO:tensorflow:loss = 0.0073241917, step = 5801 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.359\n",
            "INFO:tensorflow:loss = 0.0072042793, step = 5901 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 443.308\n",
            "INFO:tensorflow:loss = 0.0061076554, step = 6001 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.956\n",
            "INFO:tensorflow:loss = 0.0062903734, step = 6101 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 305.548\n",
            "INFO:tensorflow:loss = 0.009837669, step = 6201 (0.348 sec)\n",
            "INFO:tensorflow:global_step/sec: 294.556\n",
            "INFO:tensorflow:loss = 0.0061866175, step = 6301 (0.323 sec)\n",
            "INFO:tensorflow:global_step/sec: 247.901\n",
            "INFO:tensorflow:loss = 0.005275552, step = 6401 (0.404 sec)\n",
            "INFO:tensorflow:global_step/sec: 407.578\n",
            "INFO:tensorflow:loss = 0.00724966, step = 6501 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.058\n",
            "INFO:tensorflow:loss = 0.004348804, step = 6601 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 556.407\n",
            "INFO:tensorflow:loss = 0.008428085, step = 6701 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 441.121\n",
            "INFO:tensorflow:loss = 0.0065321485, step = 6801 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 403.772\n",
            "INFO:tensorflow:loss = 0.007907276, step = 6901 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 328.129\n",
            "INFO:tensorflow:loss = 0.007691744, step = 7001 (0.306 sec)\n",
            "INFO:tensorflow:global_step/sec: 350.746\n",
            "INFO:tensorflow:loss = 0.005522655, step = 7101 (0.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.385\n",
            "INFO:tensorflow:loss = 0.005111266, step = 7201 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 450.257\n",
            "INFO:tensorflow:loss = 0.0060097775, step = 7301 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 439.042\n",
            "INFO:tensorflow:loss = 0.0055114767, step = 7401 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.709\n",
            "INFO:tensorflow:loss = 0.0052906573, step = 7501 (0.250 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.528\n",
            "INFO:tensorflow:loss = 0.009634009, step = 7601 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.275\n",
            "INFO:tensorflow:loss = 0.0050665294, step = 7701 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.187\n",
            "INFO:tensorflow:loss = 0.006355604, step = 7801 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 367.413\n",
            "INFO:tensorflow:loss = 0.007673181, step = 7901 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 280.84\n",
            "INFO:tensorflow:loss = 0.0076629976, step = 8001 (0.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.164\n",
            "INFO:tensorflow:loss = 0.0061652483, step = 8101 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 350.923\n",
            "INFO:tensorflow:loss = 0.0057623778, step = 8201 (0.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 401.521\n",
            "INFO:tensorflow:loss = 0.0067048124, step = 8301 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.093\n",
            "INFO:tensorflow:loss = 0.0059260237, step = 8401 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 423.407\n",
            "INFO:tensorflow:loss = 0.0074485503, step = 8501 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 432.954\n",
            "INFO:tensorflow:loss = 0.0075836135, step = 8601 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 444.693\n",
            "INFO:tensorflow:loss = 0.0068555977, step = 8701 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 575.887\n",
            "INFO:tensorflow:loss = 0.0063181375, step = 8801 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.144\n",
            "INFO:tensorflow:loss = 0.005736267, step = 8901 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.832\n",
            "INFO:tensorflow:loss = 0.0070505766, step = 9001 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 795.729\n",
            "INFO:tensorflow:loss = 0.006812678, step = 9101 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 726.996\n",
            "INFO:tensorflow:loss = 0.008618783, step = 9201 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 776.178\n",
            "INFO:tensorflow:loss = 0.005741417, step = 9301 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 789.005\n",
            "INFO:tensorflow:loss = 0.009029241, step = 9401 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.245\n",
            "INFO:tensorflow:loss = 0.0068218345, step = 9501 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 762.952\n",
            "INFO:tensorflow:loss = 0.005864296, step = 9601 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 797.836\n",
            "INFO:tensorflow:loss = 0.006996244, step = 9701 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.473\n",
            "INFO:tensorflow:loss = 0.008117206, step = 9801 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 782.797\n",
            "INFO:tensorflow:loss = 0.005642876, step = 9901 (0.132 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.007666693.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 98.15872019343414\n",
            "Just using average = 598.5503913894324 has RMSE of 101.44774288608059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5e5758-6218-4064-ce95-528f936ce0cc",
        "id": "CmqqgLT09593"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fd21c90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.5071501  0.5173477  0.53807575 0.5380053  0.4993347  0.545433\n",
            " 0.55663407 0.53290254 0.4356096  0.45301634 0.4517532  0.4918821\n",
            " 0.5560469  0.5714761  0.50050056 0.5597803  0.48728693 0.45440856\n",
            " 0.4730551  0.49790177 0.5191456  0.48749077 0.5572373  0.52836627\n",
            " 0.44886997 0.49089465 0.46411264 0.4454686  0.46870852 0.50530314\n",
            " 0.5014508  0.5300986  0.5470422  0.52702373 0.47059786 0.53770053\n",
            " 0.50189173 0.4894814  0.44880244 0.5121031  0.4660586  0.5004295\n",
            " 0.45065013 0.52709985 0.49543202 0.50966686 0.5489253  0.5285865\n",
            " 0.50540906 0.47518182 0.5196869  0.46776497 0.4537289  0.50808275\n",
            " 0.45812696 0.48357895 0.513404   0.47737828 0.52357376 0.47163028\n",
            " 0.46608952 0.5098348  0.44514132 0.4849006  0.4394685  0.48629153\n",
            " 0.46686152 0.52792996 0.54482913 0.51529807 0.50025    0.5120777\n",
            " 0.48517406 0.46968082 0.4996432  0.5485673  0.4478021  0.44951943\n",
            " 0.4677514  0.5065068  0.479767   0.49127606 0.52099854 0.45093644\n",
            " 0.4841497  0.48392794 0.51630193 0.47214982 0.49802795 0.47817442\n",
            " 0.52136576 0.48736393 0.5224874  0.4440347  0.5456067  0.5391986\n",
            " 0.47866336 0.50657916 0.44885823 0.4603647  0.5368136  0.4696395\n",
            " 0.4897112  0.54731786 0.4899882  0.48340926 0.45375642 0.54870385\n",
            " 0.507044   0.48766586 0.48664063 0.52759963 0.5066511  0.5572073\n",
            " 0.48799044 0.4467561  0.5125509  0.49374193 0.50584966 0.51216793\n",
            " 0.54819226 0.49139735 0.42711216 0.5266367  0.5379549  0.50870734\n",
            " 0.5102013  0.5164123  0.49243516 0.5161264  0.46452877 0.46595326\n",
            " 0.51169634 0.5170839  0.53984404 0.41567296 0.5229349  0.46260786\n",
            " 0.4434114  0.54300135 0.44030136 0.5679346  0.52675444 0.5037296\n",
            " 0.5486655  0.48650014 0.48053798 0.44759488 0.5064528  0.48749357\n",
            " 0.5134029  0.4810645  0.5278984  0.46942896 0.50932515 0.45732123\n",
            " 0.49324736 0.44477168 0.46760133 0.5232946  0.5113355  0.45236728\n",
            " 0.5254242  0.48215818 0.5363044  0.5219957  0.4896763  0.51674736\n",
            " 0.5469347  0.48569912 0.5334226  0.47772115 0.53727424 0.5113984\n",
            " 0.5403359  0.5407455  0.5096958  0.52047044 0.48020414 0.47895363\n",
            " 0.5226107  0.52503896 0.5567798  0.51233053 0.5235515  0.56194466\n",
            " 0.51278895 0.47866023 0.55744004 0.4922107  0.5013722  0.4793266\n",
            " 0.4692385  0.55854714 0.48162144 0.47737795 0.49529913 0.5351105\n",
            " 0.46631074 0.5573211  0.4409001  0.55091625 0.47834665 0.5126602\n",
            " 0.50671226 0.5371553  0.48403743 0.52194124 0.50577295 0.52107215\n",
            " 0.5023234  0.51487845 0.52747387 0.51093596 0.5359919  0.42946854\n",
            " 0.4647696  0.5558794  0.47618127 0.535859   0.48615307 0.48540175\n",
            " 0.4720793  0.45388618 0.43628532 0.5108854  0.5552452  0.5333434\n",
            " 0.49055496 0.46665153 0.5008144  0.49792406 0.52524316 0.47151124\n",
            " 0.49206212 0.43766367 0.46461284 0.49190304 0.5339898  0.54123247\n",
            " 0.55382705 0.49665764 0.47794333 0.4395213  0.48885787 0.4981353\n",
            " 0.55856615 0.46336776 0.501558   0.5208772  0.4655592  0.50141704\n",
            " 0.49781325 0.5195242  0.5581961  0.54596484 0.4698183  0.50724113\n",
            " 0.50034744 0.5460183  0.5264163  0.5035899  0.4950726  0.54402655\n",
            " 0.5142236  0.55909026 0.4838634  0.50980514 0.5147299  0.48642832\n",
            " 0.5211526  0.5556504  0.52452433 0.50433534 0.50259346 0.44500402\n",
            " 0.4777738  0.46269658 0.52130145 0.47368243 0.4311398  0.47298026\n",
            " 0.5112512  0.5277612  0.47113296 0.480388   0.55058205 0.521504\n",
            " 0.48029664 0.5137126  0.5468986  0.51250285 0.5472396  0.5126181\n",
            " 0.5328267  0.46287152 0.5197386  0.54690194 0.5251129  0.44074696\n",
            " 0.482752   0.53374946 0.5060944  0.47063622 0.5585015  0.52216375\n",
            " 0.5316703  0.5551956  0.48675257 0.47451752 0.505791   0.5213914\n",
            " 0.4919657  0.5084233  0.55469596 0.49585587 0.5039442  0.5443658\n",
            " 0.49381742 0.48480892 0.4824271  0.50618577 0.50254947 0.5509097\n",
            " 0.42817754 0.48266417 0.4734483  0.51144165 0.5043869  0.48982692\n",
            " 0.44250256 0.55088    0.52402943 0.49190956 0.46901378 0.4683571\n",
            " 0.44734716 0.5384001  0.46777397 0.47892648 0.50824684 0.4840876\n",
            " 0.56083024 0.45039758 0.47437963 0.5172446  0.5165146  0.4704605\n",
            " 0.49641785 0.47684327 0.4996893  0.49608442 0.5092989  0.4536271\n",
            " 0.48350295 0.4977117  0.52570295 0.51014143 0.47998372 0.5314868\n",
            " 0.5252815  0.56445384 0.51236904 0.534971   0.54082924 0.5015352\n",
            " 0.53689057 0.5232563  0.43684998 0.42842633 0.47909474 0.5421761\n",
            " 0.56128013 0.5434915  0.48375258 0.5444229  0.46903747 0.48906952\n",
            " 0.5063165  0.42164254 0.5011009  0.45969597 0.4502605  0.46010503\n",
            " 0.5498452  0.5126269  0.48735723 0.54327965 0.44354555 0.4999123\n",
            " 0.53052825 0.44852325 0.5102645  0.46138617 0.49850824 0.5537\n",
            " 0.42913148 0.50469744 0.44044596 0.46734878 0.4763735  0.48870677\n",
            " 0.47480154 0.4651119  0.46282136 0.45934108 0.5534103  0.4917349\n",
            " 0.55046743 0.47676316 0.5316548  0.49009612 0.46510747 0.51278394\n",
            " 0.48914823 0.51962566 0.51237947 0.49833643 0.47853768 0.43018925\n",
            " 0.44209066 0.561438   0.5316045  0.5202321  0.4733445  0.4882834\n",
            " 0.50355    0.4877748  0.4791783  0.46612725 0.5111494  0.5178585\n",
            " 0.43453306 0.49918443 0.47079182 0.49897143 0.5466367  0.51942253\n",
            " 0.4704367  0.4948897  0.5265925  0.48241124 0.53214294 0.5563562\n",
            " 0.5612509  0.5395439  0.49225622 0.48424873 0.5585027  0.5495238\n",
            " 0.4498977  0.45189747 0.49200797 0.5087464  0.5041781  0.53608173\n",
            " 0.54334974 0.44275624 0.5299215  0.4560498  0.4914817  0.5091436\n",
            " 0.53501135 0.44113764 0.4868442  0.5703193  0.50981396 0.5255232\n",
            " 0.49532986 0.4374611  0.5269298  0.44957665 0.49821386 0.53199947\n",
            " 0.48967975 0.5411157  0.5329498  0.55509263 0.52050567 0.49890897\n",
            " 0.498738   0.50909096 0.46667376 0.46580365 0.46691096 0.4740676\n",
            " 0.4893476  0.54499537 0.48513404 0.44087726 0.4660814  0.49156237\n",
            " 0.44411784 0.4808975  0.51696193 0.4446333  0.47673312 0.48424634\n",
            " 0.50255316 0.4727954  0.5171844  0.4943782  0.5211311  0.524586\n",
            " 0.49140206 0.53538734 0.48800892 0.49091768 0.48397174 0.56397635\n",
            " 0.44348112]\n",
            "[0.48492679 0.38673557 0.5374677  0.48751077 0.49440138 0.56933678\n",
            " 0.625323   0.54608096 0.583118   0.56158484 0.48837209 0.57105943\n",
            " 0.53402239 0.33763997 0.63910422 0.67700258 0.62360034 0.31438415\n",
            " 0.44444444 0.46683893 0.58484065 0.41429802 0.55211025 0.66494401\n",
            " 0.34711456 0.54866494 0.56847545 0.38587425 0.46511628 0.64685616\n",
            " 0.44444444 0.55641688 0.47286822 0.53402239 0.86046512 0.49354005\n",
            " 0.54349699 0.53143842 0.5245478  0.37898363 0.47459087 0.55211025\n",
            " 0.47459087 0.47717485 0.60292851 0.62962963 0.5374677  0.59431525\n",
            " 0.57536606 0.56158484 0.54177433 0.49784668 0.50559862 0.31007752\n",
            " 0.49009475 0.54694229 0.60378984 0.43496985 0.57450474 0.36434109\n",
            " 0.49784668 0.49354005 0.33419466 0.50129199 0.47717485 0.45564169\n",
            " 0.59000861 0.55727821 0.56330749 0.56761413 0.56589147 0.53660637\n",
            " 0.38845823 0.58139535 0.54177433 0.56761413 0.41257537 0.36864772\n",
            " 0.42894057 0.5245478  0.53143842 0.52540913 0.63652024 0.6416882\n",
            " 0.5047373  0.59259259 0.55555556 0.38845823 0.54263566 0.59431525\n",
            " 0.56072351 0.48062016 0.50301464 0.43238587 0.45047373 0.4625323\n",
            " 0.4918174  0.41429802 0.53229974 0.51421189 0.62101637 0.45822567\n",
            " 0.4788975  0.50129199 0.54349699 0.4788975  0.47803618 0.58570198\n",
            " 0.60378984 0.3910422  0.60809647 0.38156761 0.4918174  0.52885444\n",
            " 0.4754522  0.35400517 0.57450474 0.53488372 0.68044789 0.51937984\n",
            " 0.63135228 0.52282515 0.36864772 0.37726098 0.53402239 0.63221361\n",
            " 0.57536606 0.4918174  0.56072351 0.6580534  0.43583118 0.39190353\n",
            " 0.58053402 0.4625323  0.7002584  0.44444444 0.44702842 0.45305771\n",
            " 0.34022394 0.34969854 0.56072351 0.60723514 0.5538329  0.48751077\n",
            " 0.50559862 0.47459087 0.40826873 0.48923342 0.60809647 0.55813953\n",
            " 0.41257537 0.55555556 0.58656331 0.43583118 0.52540913 0.32472007\n",
            " 0.59431525 0.32385874 0.3712317  0.52971576 0.46167097 0.51593454\n",
            " 0.64427218 0.38501292 0.59000861 0.63910422 0.51076658 0.51937984\n",
            " 0.52282515 0.5047373  0.57278208 0.47200689 0.44444444 0.63652024\n",
            " 0.46770026 0.56158484 0.72782084 0.53316107 0.46511628 0.55813953\n",
            " 0.47803618 0.44702842 0.63652024 0.36864772 0.51679587 0.50215332\n",
            " 0.56244617 0.54177433 0.47459087 0.57881137 0.63221361 0.38845823\n",
            " 0.58828596 0.43152455 0.38415159 0.4203273  0.52971576 0.65202412\n",
            " 0.45305771 0.50387597 0.4203273  0.55813953 0.48234281 0.65202412\n",
            " 0.50990525 0.4918174  0.51507321 0.64513351 0.5796727  0.45908699\n",
            " 0.51507321 0.60034453 0.55555556 0.51507321 0.4496124  0.46167097\n",
            " 0.47114556 0.56158484 0.40137812 0.60551249 0.56330749 0.39965547\n",
            " 0.41774332 0.40913006 0.41171404 0.6546081  0.49870801 0.59259259\n",
            " 0.41343669 0.42463394 0.27648579 0.48062016 0.47372954 0.48148148\n",
            " 0.54521964 0.38845823 0.63049096 0.44875108 0.53229974 0.49956934\n",
            " 0.50990525 0.52799311 0.37984496 0.38587425 0.40913006 0.59689922\n",
            " 0.57622739 0.45305771 0.61154177 0.6287683  0.53574505 0.40568475\n",
            " 0.49095607 0.50990525 0.54005168 0.63996555 0.56933678 0.64857881\n",
            " 0.68303187 0.59000861 0.54694229 0.63049096 0.62704565 0.52368648\n",
            " 0.57364341 0.5503876  0.42291128 0.5667528  0.59431525 0.46080965\n",
            " 0.47114556 0.52024117 0.42463394 0.5211025  0.52799311 0.4005168\n",
            " 0.50129199 0.4005168  0.48664944 0.47975883 0.45908699 0.7329888\n",
            " 0.64599483 0.41515935 0.40999139 0.40223945 0.51765719 0.55641688\n",
            " 0.50732127 0.57795004 0.36520241 0.48578811 0.60206718 0.44186047\n",
            " 0.64427218 0.42635659 0.52196382 0.52196382 0.55986219 0.33850129\n",
            " 0.45219638 0.60637382 0.54349699 0.53143842 0.60292851 0.44875108\n",
            " 0.58139535 0.51507321 0.51765719 0.43496985 0.55297158 0.60120586\n",
            " 0.56330749 0.58225668 0.53919035 0.59086994 0.46167097 0.37898363\n",
            " 0.49956934 0.58656331 0.43755383 0.44186047 0.49784668 0.58570198\n",
            " 0.43496985 0.50387597 0.46942291 0.42894057 0.78466839 0.42980189\n",
            " 0.38673557 0.53057709 0.67011197 0.47631352 0.47975883 0.5503876\n",
            " 0.37639966 0.5667528  0.51851852 0.5960379  0.54952627 0.63910422\n",
            " 0.51851852 0.47372954 0.55986219 0.51421189 0.51679587 0.51421189\n",
            " 0.52799311 0.46942291 0.61326443 0.65202412 0.52713178 0.57364341\n",
            " 0.56847545 0.47459087 0.55641688 0.68561585 0.45047373 0.59862188\n",
            " 0.65374677 0.53143842 0.63910422 0.56416882 0.49009475 0.38845823\n",
            " 0.50043066 0.57019811 0.36950904 0.36089578 0.47028424 0.53316107\n",
            " 0.36089578 0.47975883 0.48492679 0.56416882 0.49956934 0.56072351\n",
            " 0.47631352 0.38329027 0.51248923 0.52024117 0.51765719 0.5047373\n",
            " 0.57105943 0.43669251 0.45994832 0.40568475 0.56158484 0.5374677\n",
            " 0.4461671  0.47631352 0.51593454 0.33936262 0.63910422 0.51248923\n",
            " 0.3875969  0.52885444 0.54091301 0.52024117 0.52799311 0.58656331\n",
            " 0.48148148 0.53143842 0.49354005 0.45564169 0.53229974 0.5503876\n",
            " 0.54952627 0.65202412 0.5667528  0.54521964 0.48751077 0.5211025\n",
            " 0.66063738 0.54263566 0.70198105 0.63996555 0.48492679 0.38329027\n",
            " 0.40654608 0.57450474 0.57278208 0.55986219 0.47459087 0.35486649\n",
            " 0.58656331 0.58053402 0.59431525 0.52713178 0.64857881 0.48320413\n",
            " 0.41343669 0.48923342 0.45822567 0.59259259 0.5081826  0.51507321\n",
            " 0.55900086 0.63479759 0.56072351 0.73815676 0.5994832  0.68475452\n",
            " 0.55211025 0.56589147 0.59259259 0.42291128 0.57536606 0.55986219\n",
            " 0.43066322 0.43669251 0.54435831 0.54952627 0.55641688 0.63996555\n",
            " 0.59345392 0.49784668 0.57881137 0.47200689 0.53402239 0.46511628\n",
            " 0.62187769 0.39965547 0.6546081  0.3712317  0.6873385  0.69336779\n",
            " 0.57622739 0.82773471 0.50559862 0.50732127 0.64857881 0.52024117\n",
            " 0.5081826  0.56589147 0.61068045 0.6089578  0.50387597 0.49612403\n",
            " 0.50387597 0.43927649 0.49612403 0.41946598 0.54005168 0.49354005\n",
            " 0.60809647 0.56503015 0.5796727  0.38673557 0.52024117 0.58656331\n",
            " 0.33850129 0.34797588 0.62101637 0.38156761 0.42635659 0.49440138\n",
            " 0.49870801 0.60465116 0.62790698 0.51076658 0.54694229 0.54091301\n",
            " 0.74677003 0.46511628 0.54521964 0.56244617 0.44358312 0.53143842\n",
            " 0.27562446]\n",
            "The trained model has an aproximate error rate of 20.965304419019215 which equates to 3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure (SLP)\n"
      ],
      "metadata": {
        "id": "O60cqN0x90SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ce2939-ec19-4a4f-cbbe-0f81d109bcb8",
        "id": "u-aXoGB4_v4s"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp = df.drop(columns=['collision_date', 'temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] != 2012]\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] < 2020]\n",
        "cols = df_slp['NUM_COLLISIONS']\n",
        "df_slp = df_slp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp[:6])\n",
        "df_slp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "ef97ff44-ecd9-4d18-dd53-cfb729f780aa",
        "id": "xsAYkW7-ATbk"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da     slp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  1016.1             681\n",
            "51    5  2014   1  17  1014.8             589\n",
            "54    1  2016   1  25  1021.4             658\n",
            "55    5  2016   1  29   999.4             645\n",
            "58    5  2017   1  20  1015.5             605\n",
            "59    7  2013   1  13  1020.7             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da          slp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.999217  2016.000391     6.522114    15.719765  1016.777221   \n",
              "std       2.000783     2.000294     3.447986     8.796698     7.628429   \n",
              "min       1.000000  2013.000000     1.000000     1.000000   989.500000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000  1012.200000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000  1016.700000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000  1021.700000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000  1044.200000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.147162  \n",
              "std        100.268048  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b28a1278-7fc9-4659-8c02-5356db494e5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999217</td>\n",
              "      <td>2016.000391</td>\n",
              "      <td>6.522114</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000783</td>\n",
              "      <td>2.000294</td>\n",
              "      <td>3.447986</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b28a1278-7fc9-4659-8c02-5356db494e5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b28a1278-7fc9-4659-8c02-5356db494e5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b28a1278-7fc9-4659-8c02-5356db494e5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp.iloc[np.random.permutation(len(df_slp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a767804-7649-4670-e562-df509547cac4",
        "id": "I-drEIq1ATbu"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da     slp\n",
            "2235    7  2019   8  18  1016.4\n",
            "1700    4  2013   6  13  1008.2\n",
            "2292    5  2013   8  16  1021.2\n",
            "801     6  2014   3  22  1013.6\n",
            "2278    2  2018   8  14  1013.0\n",
            "827     3  2017   3  22  1011.4\n",
            "      day  year  mo  da     slp  NUM_COLLISIONS\n",
            "2235    7  2019   8  18  1016.4             481\n",
            "1700    4  2013   6  13  1008.2             534\n",
            "2292    5  2013   8  16  1021.2             678\n",
            "801     6  2014   3  22  1013.6             493\n",
            "2278    2  2018   8  14  1013.0             701\n",
            "827     3  2017   3  22  1011.4             617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cd7626-4b7b-4aa1-8f4f-bfa1a7c88867",
        "id": "pq6BF1zLATbv"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2235    481\n",
            "1700    534\n",
            "2292    678\n",
            "801     493\n",
            "2278    701\n",
            "827     617\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a845e5c-084d-4ae2-b2b8-02ef653cddfd",
        "id": "pVdp0YKeATbv"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1e796d-7b86-4304-f665-23f13aa1f1a3",
        "id": "twTCPAjdATbv"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c4b79d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.2676384, step = 1\n",
            "INFO:tensorflow:global_step/sec: 698.505\n",
            "INFO:tensorflow:loss = 0.0069545344, step = 101 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 698.601\n",
            "INFO:tensorflow:loss = 0.007760483, step = 201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.28\n",
            "INFO:tensorflow:loss = 0.009986185, step = 301 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 857.933\n",
            "INFO:tensorflow:loss = 0.0059080934, step = 401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 808.137\n",
            "INFO:tensorflow:loss = 0.0066342466, step = 501 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.085\n",
            "INFO:tensorflow:loss = 0.007184749, step = 601 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 768.6\n",
            "INFO:tensorflow:loss = 0.0057589365, step = 701 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 774.883\n",
            "INFO:tensorflow:loss = 0.0067162234, step = 801 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.713\n",
            "INFO:tensorflow:loss = 0.0070949104, step = 901 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 704.463\n",
            "INFO:tensorflow:loss = 0.008486304, step = 1001 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 703.066\n",
            "INFO:tensorflow:loss = 0.007325507, step = 1101 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.838\n",
            "INFO:tensorflow:loss = 0.0062066265, step = 1201 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.645\n",
            "INFO:tensorflow:loss = 0.006583854, step = 1301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 763.168\n",
            "INFO:tensorflow:loss = 0.0067755487, step = 1401 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 796.625\n",
            "INFO:tensorflow:loss = 0.0061122547, step = 1501 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.304\n",
            "INFO:tensorflow:loss = 0.008870736, step = 1601 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 772.869\n",
            "INFO:tensorflow:loss = 0.006757782, step = 1701 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.305\n",
            "INFO:tensorflow:loss = 0.006061622, step = 1801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 831.212\n",
            "INFO:tensorflow:loss = 0.0073459395, step = 1901 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 785.889\n",
            "INFO:tensorflow:loss = 0.007186766, step = 2001 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.114\n",
            "INFO:tensorflow:loss = 0.007654814, step = 2101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 737.965\n",
            "INFO:tensorflow:loss = 0.0057297815, step = 2201 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 806.504\n",
            "INFO:tensorflow:loss = 0.00832861, step = 2301 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.262\n",
            "INFO:tensorflow:loss = 0.008177428, step = 2401 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 744.275\n",
            "INFO:tensorflow:loss = 0.006561907, step = 2501 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.337\n",
            "INFO:tensorflow:loss = 0.008098301, step = 2601 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 749.321\n",
            "INFO:tensorflow:loss = 0.009485558, step = 2701 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.596\n",
            "INFO:tensorflow:loss = 0.0067947186, step = 2801 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.467\n",
            "INFO:tensorflow:loss = 0.007374755, step = 2901 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.643\n",
            "INFO:tensorflow:loss = 0.010180501, step = 3001 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 710.087\n",
            "INFO:tensorflow:loss = 0.0075309323, step = 3101 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.221\n",
            "INFO:tensorflow:loss = 0.008361889, step = 3201 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 672.645\n",
            "INFO:tensorflow:loss = 0.006301623, step = 3301 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 734.778\n",
            "INFO:tensorflow:loss = 0.00602644, step = 3401 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 475.918\n",
            "INFO:tensorflow:loss = 0.008834489, step = 3501 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 461.859\n",
            "INFO:tensorflow:loss = 0.009031831, step = 3601 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 585.93\n",
            "INFO:tensorflow:loss = 0.005981399, step = 3701 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.567\n",
            "INFO:tensorflow:loss = 0.009307461, step = 3801 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 602.047\n",
            "INFO:tensorflow:loss = 0.006018442, step = 3901 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.125\n",
            "INFO:tensorflow:loss = 0.0064446633, step = 4001 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.335\n",
            "INFO:tensorflow:loss = 0.0059163254, step = 4101 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.476\n",
            "INFO:tensorflow:loss = 0.009497104, step = 4201 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 423.395\n",
            "INFO:tensorflow:loss = 0.0068505798, step = 4301 (0.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.822\n",
            "INFO:tensorflow:loss = 0.005259446, step = 4401 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.656\n",
            "INFO:tensorflow:loss = 0.007644631, step = 4501 (0.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.341\n",
            "INFO:tensorflow:loss = 0.0061799027, step = 4601 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.241\n",
            "INFO:tensorflow:loss = 0.007787194, step = 4701 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 446.436\n",
            "INFO:tensorflow:loss = 0.0072852634, step = 4801 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.028\n",
            "INFO:tensorflow:loss = 0.009766083, step = 4901 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.202\n",
            "INFO:tensorflow:loss = 0.0069651674, step = 5001 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 450.748\n",
            "INFO:tensorflow:loss = 0.0068288706, step = 5101 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.198\n",
            "INFO:tensorflow:loss = 0.007203365, step = 5201 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.567\n",
            "INFO:tensorflow:loss = 0.009311212, step = 5301 (0.213 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.478\n",
            "INFO:tensorflow:loss = 0.006075211, step = 5401 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.194\n",
            "INFO:tensorflow:loss = 0.0052893963, step = 5501 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 523.64\n",
            "INFO:tensorflow:loss = 0.0060518445, step = 5601 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 406.496\n",
            "INFO:tensorflow:loss = 0.005109705, step = 5701 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.622\n",
            "INFO:tensorflow:loss = 0.0065840846, step = 5801 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.153\n",
            "INFO:tensorflow:loss = 0.009097872, step = 5901 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 740.834\n",
            "INFO:tensorflow:loss = 0.006862312, step = 6001 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.097\n",
            "INFO:tensorflow:loss = 0.008241701, step = 6101 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.934\n",
            "INFO:tensorflow:loss = 0.0055763484, step = 6201 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.309\n",
            "INFO:tensorflow:loss = 0.006785227, step = 6301 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 754.139\n",
            "INFO:tensorflow:loss = 0.006146842, step = 6401 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 726.773\n",
            "INFO:tensorflow:loss = 0.008392986, step = 6501 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.574\n",
            "INFO:tensorflow:loss = 0.0077815517, step = 6601 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 795.981\n",
            "INFO:tensorflow:loss = 0.004705821, step = 6701 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.567\n",
            "INFO:tensorflow:loss = 0.0057075755, step = 6801 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 771.891\n",
            "INFO:tensorflow:loss = 0.008100147, step = 6901 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.441\n",
            "INFO:tensorflow:loss = 0.008472009, step = 7001 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.339\n",
            "INFO:tensorflow:loss = 0.0061225034, step = 7101 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.145\n",
            "INFO:tensorflow:loss = 0.005090369, step = 7201 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 796.683\n",
            "INFO:tensorflow:loss = 0.010947056, step = 7301 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 866.645\n",
            "INFO:tensorflow:loss = 0.005548986, step = 7401 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.523\n",
            "INFO:tensorflow:loss = 0.0061801225, step = 7501 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 816.68\n",
            "INFO:tensorflow:loss = 0.0069666565, step = 7601 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 761.141\n",
            "INFO:tensorflow:loss = 0.005230627, step = 7701 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 745.925\n",
            "INFO:tensorflow:loss = 0.0084126275, step = 7801 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 773.318\n",
            "INFO:tensorflow:loss = 0.0067843804, step = 7901 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 813.018\n",
            "INFO:tensorflow:loss = 0.0061374977, step = 8001 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.666\n",
            "INFO:tensorflow:loss = 0.0073082875, step = 8101 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 763.172\n",
            "INFO:tensorflow:loss = 0.006601602, step = 8201 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 748.47\n",
            "INFO:tensorflow:loss = 0.0065407255, step = 8301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 778.771\n",
            "INFO:tensorflow:loss = 0.00782983, step = 8401 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 789.683\n",
            "INFO:tensorflow:loss = 0.008224873, step = 8501 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 678.162\n",
            "INFO:tensorflow:loss = 0.007594944, step = 8601 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 774.2\n",
            "INFO:tensorflow:loss = 0.008577155, step = 8701 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 748.399\n",
            "INFO:tensorflow:loss = 0.0068286858, step = 8801 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 771.7\n",
            "INFO:tensorflow:loss = 0.0059481906, step = 8901 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 710.432\n",
            "INFO:tensorflow:loss = 0.006411841, step = 9001 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 793.806\n",
            "INFO:tensorflow:loss = 0.007098398, step = 9101 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 779.881\n",
            "INFO:tensorflow:loss = 0.0074794595, step = 9201 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.641\n",
            "INFO:tensorflow:loss = 0.004702184, step = 9301 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.177\n",
            "INFO:tensorflow:loss = 0.010276182, step = 9401 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 785.078\n",
            "INFO:tensorflow:loss = 0.006925012, step = 9501 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.986\n",
            "INFO:tensorflow:loss = 0.007411355, step = 9601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 771.612\n",
            "INFO:tensorflow:loss = 0.0075877677, step = 9701 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 777.561\n",
            "INFO:tensorflow:loss = 0.0073188297, step = 9801 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 799.631\n",
            "INFO:tensorflow:loss = 0.0066997306, step = 9901 (0.123 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.007848414.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 92.26679987167856\n",
            "Just using average = 599.5567514677103 has RMSE of 97.76883640443617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba97bed9-d4f1-4292-9d39-cbef25abebaf",
        "id": "RJS2QjMpATbw"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c2923d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47868532 0.5333827  0.47306892 0.5544367  0.5448285  0.4938599\n",
            " 0.48487094 0.5065225  0.59958136 0.5349501  0.5190374  0.5368878\n",
            " 0.557319   0.4817469  0.5364394  0.49532747 0.54422766 0.5153652\n",
            " 0.4636602  0.5233125  0.5400646  0.53247744 0.5256451  0.5130015\n",
            " 0.47018406 0.51014394 0.5302009  0.566365   0.5509676  0.4739107\n",
            " 0.518703   0.48598227 0.4982097  0.5580803  0.50485474 0.5244103\n",
            " 0.49123466 0.5386966  0.54834896 0.4541234  0.48790422 0.48690566\n",
            " 0.52233267 0.5080359  0.51348597 0.5525831  0.5461885  0.5488752\n",
            " 0.49246898 0.549272   0.56279784 0.48030993 0.54479045 0.55760145\n",
            " 0.52341986 0.5001317  0.53902364 0.5405909  0.47027388 0.52854306\n",
            " 0.5455614  0.5151529  0.5095876  0.5034268  0.5468528  0.49195114\n",
            " 0.49722138 0.49761418 0.54901606 0.4716442  0.5477364  0.506483\n",
            " 0.48622844 0.51366645 0.478448   0.504023   0.5294683  0.47456506\n",
            " 0.5344437  0.543488   0.5416919  0.50863916 0.4847067  0.5145234\n",
            " 0.47311282 0.5585964  0.5223745  0.5609128  0.53938264 0.52208424\n",
            " 0.51594096 0.5139722  0.51168835 0.51981425 0.5356688  0.5035669\n",
            " 0.48044235 0.5653193  0.5363283  0.5582579  0.5239163  0.5603389\n",
            " 0.52578074 0.5180176  0.48165816 0.52548    0.49647662 0.5483274\n",
            " 0.4529147  0.5376246  0.6058736  0.5246576  0.5466029  0.5605683\n",
            " 0.49223378 0.54542637 0.5706289  0.4727343  0.54963535 0.4906996\n",
            " 0.53823763 0.5765138  0.5652321  0.49942148 0.5368859  0.52357435\n",
            " 0.50081456 0.52498883 0.57395434 0.51277256 0.4831438  0.57091355\n",
            " 0.48136905 0.50506616 0.473804   0.58498174 0.49605185 0.47876832\n",
            " 0.520918   0.49498174 0.4844505  0.55139786 0.47723326 0.51787287\n",
            " 0.47790968 0.5116453  0.54981947 0.5301476  0.5504021  0.5119016\n",
            " 0.48966742 0.4857113  0.51393723 0.54888725 0.5464748  0.52712405\n",
            " 0.53740346 0.49474978 0.4915472  0.4636127  0.48047003 0.5302827\n",
            " 0.5345352  0.56090945 0.51245177 0.5088288  0.5065906  0.47459492\n",
            " 0.55796933 0.5074721  0.49502558 0.49470133 0.4942168  0.5288932\n",
            " 0.502903   0.46999177 0.52372783 0.50136673 0.56464607 0.54845846\n",
            " 0.57322145 0.4688476  0.4651143  0.49641612 0.49249637 0.5031912\n",
            " 0.5390011  0.5406357  0.5055258  0.53627247 0.52321196 0.5302742\n",
            " 0.55301714 0.48123905 0.5592735  0.546155   0.56879914 0.49817201\n",
            " 0.48461315 0.5249904  0.48663935 0.5503189  0.46163103 0.46486145\n",
            " 0.47682294 0.46586564 0.5540811  0.51910114 0.52780056 0.5181548\n",
            " 0.49935615 0.5427581  0.5473955  0.5200363  0.5684173  0.54724884\n",
            " 0.5396566  0.53693515 0.5239545  0.54666334 0.5572237  0.54550534\n",
            " 0.49085155 0.5203235  0.5388105  0.5293145  0.5383845  0.56190777\n",
            " 0.51992744 0.5300511  0.55355734 0.5563559  0.54092664 0.51432616\n",
            " 0.49185556 0.5629329  0.5676589  0.5456432  0.5129331  0.5319947\n",
            " 0.46773058 0.521711   0.5102786  0.49836355 0.49482012 0.5146043\n",
            " 0.5483508  0.5120814  0.5296001  0.52432567 0.55148476 0.5362181\n",
            " 0.5408431  0.47109792 0.5074568  0.5453166  0.4830846  0.5509356\n",
            " 0.5576186  0.52634066 0.53035134 0.45945317 0.478409   0.5031968\n",
            " 0.53831744 0.52117884 0.50500286 0.47320375 0.550507   0.51538795\n",
            " 0.51146334 0.552501   0.49900678 0.5414284  0.52241766 0.5412591\n",
            " 0.5639968  0.49764985 0.5304705  0.560284   0.45440313 0.50695884\n",
            " 0.5114644  0.46367893 0.4782929  0.5414672  0.49822083 0.48708615\n",
            " 0.5502812  0.52282834 0.4609184  0.4994755  0.47365135 0.51006716\n",
            " 0.46055    0.57279295 0.5450017  0.5333955  0.48243013 0.5138635\n",
            " 0.5217393  0.49268758 0.46139678 0.50855076 0.53258634 0.5278358\n",
            " 0.5458848  0.47238424 0.5006196  0.46247694 0.5160943  0.51759934\n",
            " 0.48929957 0.5618132  0.4724287  0.52700365 0.47834945 0.55507165\n",
            " 0.5369699  0.54138035 0.5657293  0.50170493 0.53953743 0.50822717\n",
            " 0.52078867 0.58107096 0.47883824 0.5561988  0.46948627 0.49425614\n",
            " 0.5015735  0.53226465 0.5282786  0.5621908  0.5820579  0.56436425\n",
            " 0.53334856 0.47589946 0.47523075 0.5169796  0.48141158 0.51460296\n",
            " 0.46063706 0.47067302 0.5528213  0.52637416 0.4522719  0.5280332\n",
            " 0.56002647 0.522356   0.52057296 0.5250593  0.45951855 0.5727197\n",
            " 0.5180277  0.4830453  0.5638276  0.5428424  0.5165826  0.55891037\n",
            " 0.49853286 0.5070194  0.52295715 0.5245987  0.5701899  0.5629602\n",
            " 0.5001706  0.51397157 0.5085231  0.5770585  0.5435302  0.5106138\n",
            " 0.45035294 0.514471   0.50225335 0.5575871  0.51274025 0.4788697\n",
            " 0.49439082 0.54188365 0.4915245  0.56163406 0.52981883 0.5641075\n",
            " 0.51913315 0.53660196 0.53612643 0.5233029  0.45436826 0.49200353\n",
            " 0.5538869  0.5037322  0.44917512 0.5313702  0.5355005  0.5444955\n",
            " 0.53858304 0.46655783 0.502501   0.5653674  0.52323973 0.52265954\n",
            " 0.5038709  0.5080334  0.51053804 0.4709542  0.51741666 0.48919457\n",
            " 0.47332886 0.48979744 0.56961364 0.53304404 0.5716156  0.5183802\n",
            " 0.49923393 0.542585   0.51705617 0.5438718  0.5477711  0.53597426\n",
            " 0.5477571  0.5493542  0.535233   0.5036074  0.49265313 0.5472931\n",
            " 0.48764727 0.48042864 0.499023   0.5274772  0.50511366 0.49539927\n",
            " 0.5230872  0.477065   0.59373343 0.563111   0.5077208  0.5035271\n",
            " 0.4735054  0.53041637 0.51183105 0.5072308  0.51496255 0.5355781\n",
            " 0.5048038  0.4917862  0.5681425  0.46715474 0.56458545 0.47626373\n",
            " 0.4882072  0.5408888  0.49386033 0.5525182  0.4917364  0.48944193\n",
            " 0.5507078  0.5499489  0.48701325 0.58504003 0.5111151  0.4808118\n",
            " 0.4915165  0.55328774 0.49461085 0.53231984 0.5003581  0.5167266\n",
            " 0.5019172  0.56326896 0.4763616  0.5358061  0.5209909  0.48444995\n",
            " 0.52692455 0.45372313 0.49406075 0.5532394  0.5145865  0.46467426\n",
            " 0.5527537  0.5581031  0.53110087 0.47184804 0.556122   0.56077135\n",
            " 0.47063926 0.5174163  0.52248275 0.5212516  0.4777282  0.5353986\n",
            " 0.5270629  0.5611982  0.51860934 0.51143026 0.53806394 0.53843576\n",
            " 0.47968057 0.56554997 0.51206225 0.50779176 0.5222759  0.49921748\n",
            " 0.48378623 0.52387846 0.55936956 0.5489849  0.46602604 0.50248003\n",
            " 0.537962  ]\n",
            "[0.46511628 0.50559862 0.48148148 0.51162791 0.7002584  0.583118\n",
            " 0.45478036 0.64857881 0.60034453 0.54780362 0.63738157 0.58139535\n",
            " 0.54349699 0.41343669 0.41257537 0.52713178 0.51248923 0.52540913\n",
            " 0.36864772 0.61584841 0.55813953 0.59431525 0.50990525 0.50215332\n",
            " 0.44186047 0.52799311 0.53402239 0.49354005 0.52885444 0.42635659\n",
            " 0.52540913 0.43927649 0.47631352 0.60723514 0.63049096 0.44186047\n",
            " 0.38845823 0.6744186  0.58484065 0.40568475 0.50904393 0.44530577\n",
            " 0.46511628 0.64857881 0.60378984 0.76055125 0.58656331 0.45564169\n",
            " 0.73815676 0.47028424 0.625323   0.49095607 0.57881137 0.60723514\n",
            " 0.53229974 0.43496985 0.50559862 0.56761413 0.52024117 0.50559862\n",
            " 0.5538329  0.51765719 0.59259259 0.48923342 0.59689922 0.51679587\n",
            " 0.53660637 0.52282515 0.63652024 0.47631352 0.58914729 0.43496985\n",
            " 0.44358312 0.57278208 0.41085271 0.48406546 0.55727821 0.60723514\n",
            " 0.43755383 0.4918174  0.54866494 0.4005168  0.51421189 0.54091301\n",
            " 0.45564169 0.51593454 0.53660637 0.53919035 0.6124031  0.52713178\n",
            " 0.47286822 0.72437554 0.63221361 0.51765719 0.53488372 0.42894057\n",
            " 0.36347976 0.51851852 0.54349699 0.5503876  0.4952627  0.59259259\n",
            " 0.5667528  0.49612403 0.48148148 0.50301464 0.38501292 0.50990525\n",
            " 0.82773471 0.52971576 0.49956934 0.57881137 0.64599483 0.50990525\n",
            " 0.4918174  0.52024117 0.53229974 0.48234281 0.58397933 0.54866494\n",
            " 0.583118   0.51076658 0.48492679 0.43669251 0.56158484 0.48664944\n",
            " 0.36089578 0.57881137 0.4461671  0.52971576 0.44702842 0.60034453\n",
            " 0.49784668 0.65202412 0.42894057 0.47286822 0.56158484 0.60465116\n",
            " 0.42894057 0.45822567 0.46942291 0.43152455 0.50215332 0.39965547\n",
            " 0.45478036 0.48492679 0.61498708 0.59345392 0.42894057 0.60465116\n",
            " 0.42118863 0.45908699 0.55727821 0.4461671  0.48406546 0.49612403\n",
            " 0.65030146 0.50990525 0.51679587 0.41515935 0.56589147 0.4918174\n",
            " 0.63049096 0.64944014 0.65202412 0.51937984 0.68303187 0.53832903\n",
            " 0.60120586 0.59173127 0.50129199 0.55555556 0.34711456 0.35658915\n",
            " 0.50387597 0.46511628 0.49095607 0.54608096 0.4203273  0.6416882\n",
            " 0.5503876  0.34969854 0.35486649 0.46511628 0.60981912 0.45391904\n",
            " 0.5667528  0.49267873 0.55813953 0.42980189 0.59689922 0.36950904\n",
            " 0.5796727  0.44788975 0.44358312 0.63738157 0.56589147 0.55813953\n",
            " 0.44358312 0.44702842 0.52024117 0.56761413 0.36434109 0.44444444\n",
            " 0.41515935 0.39793282 0.53057709 0.63996555 0.44444444 0.43669251\n",
            " 0.52540913 0.51507321 0.45305771 0.53488372 0.55641688 0.32213609\n",
            " 0.59173127 0.61068045 0.55641688 0.53316107 0.45047373 0.42721792\n",
            " 0.56158484 0.50129199 0.47717485 0.46942291 0.44013781 0.53402239\n",
            " 0.36606374 0.49440138 0.57622739 0.59345392 0.60206718 0.5211025\n",
            " 0.43066322 0.44702842 0.45908699 0.51076658 0.49354005 0.38931955\n",
            " 0.35917313 0.6580534  0.66838932 0.51335056 0.43066322 0.66063738\n",
            " 0.57278208 0.51076658 0.66925065 0.56330749 0.69939707 0.50990525\n",
            " 0.5538329  0.36347976 0.61584841 0.43927649 0.45822567 0.56158484\n",
            " 0.65202412 0.53057709 0.65719208 0.416882   0.37812231 0.42894057\n",
            " 0.56330749 0.66408269 0.52627046 0.45305771 0.52971576 0.31007752\n",
            " 0.53143842 0.56503015 0.45822567 0.55211025 0.51593454 0.55727821\n",
            " 0.54521964 0.6416882  0.54091301 0.37898363 0.33850129 0.38845823\n",
            " 0.47286822 0.4625323  0.44013781 0.51679587 0.43066322 0.38587425\n",
            " 0.51679587 0.63135228 0.33850129 0.53660637 0.53574505 0.52540913\n",
            " 0.37209302 0.60723514 0.53488372 0.52799311 0.43927649 0.43238587\n",
            " 0.50559862 0.44702842 0.39276486 0.51421189 0.52282515 0.50990525\n",
            " 0.52540913 0.37639966 0.50904393 0.53229974 0.53488372 0.5538329\n",
            " 0.63393626 0.53229974 0.4005168  0.59086994 0.50732127 0.38070629\n",
            " 0.57278208 0.59862188 0.47459087 0.5211025  0.44272179 0.62790698\n",
            " 0.42118863 0.55469423 0.50215332 0.62101637 0.46770026 0.44358312\n",
            " 0.58225668 0.59173127 0.48234281 0.56416882 0.28682171 0.63307494\n",
            " 0.51851852 0.41085271 0.40913006 0.56933678 0.48751077 0.39362618\n",
            " 0.46856158 0.41946598 0.56933678 0.48492679 0.43927649 0.48062016\n",
            " 0.28251507 0.44444444 0.63652024 0.6287683  0.48923342 0.53057709\n",
            " 0.6546081  0.49267873 0.54866494 0.4918174  0.49956934 0.63049096\n",
            " 0.55986219 0.52368648 0.60034453 0.5245478  0.60206718 0.66149871\n",
            " 0.40826873 0.49440138 0.5245478  0.43496985 0.59345392 0.60465116\n",
            " 0.39534884 0.55727821 0.51421189 0.59517657 0.52713178 0.49440138\n",
            " 0.59431525 0.51421189 0.48492679 0.54091301 0.49009475 0.6089578\n",
            " 0.62015504 0.43152455 0.56330749 0.48923342 0.32127476 0.44702842\n",
            " 0.56933678 0.35745047 0.39276486 0.48923342 0.46080965 0.48751077\n",
            " 0.54694229 0.36950904 0.59000861 0.55211025 0.59862188 0.40913006\n",
            " 0.46942291 0.56847545 0.56847545 0.33419466 0.53488372 0.40740741\n",
            " 0.36434109 0.53574505 0.5047373  0.42291128 0.62187769 0.56589147\n",
            " 0.45908699 0.69509044 0.53919035 0.55641688 0.31955211 0.59517657\n",
            " 0.63910422 0.51593454 0.55727821 0.41515935 0.45305771 0.64513351\n",
            " 0.47459087 0.51335056 0.52971576 0.49956934 0.48923342 0.48148148\n",
            " 0.40826873 0.47803618 0.59345392 0.49009475 0.59000861 0.56761413\n",
            " 0.42807924 0.51937984 0.64082687 0.64427218 0.59689922 0.69681309\n",
            " 0.51421189 0.47803618 0.5960379  0.50129199 0.52627046 0.39362618\n",
            " 0.54091301 0.54005168 0.46339363 0.47028424 0.47975883 0.5994832\n",
            " 0.4039621  0.65719208 0.55900086 0.63996555 0.53919035 0.51593454\n",
            " 0.62273902 0.43583118 0.51937984 0.54435831 0.47286822 0.4332472\n",
            " 0.54349699 0.46339363 0.45908699 0.60292851 0.59173127 0.4496124\n",
            " 0.49440138 0.45822567 0.60292851 0.56158484 0.6873385  0.44530577\n",
            " 0.57622739 0.52885444 0.58742463 0.50732127 0.59345392 0.59259259\n",
            " 0.38329027 0.41602067 0.48664944 0.5667528  0.5081826  0.44875108\n",
            " 0.50732127 0.37898363 0.45219638 0.70456503 0.57278208 0.47631352\n",
            " 0.36864772 0.53229974 0.50990525 0.56244617 0.49698536 0.52971576\n",
            " 0.40913006 0.50559862 0.50732127 0.35486649 0.32730405 0.47717485\n",
            " 0.69853575]\n",
            "The trained model has an aproximate error rate of -4.697221799839267 which equates to -1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "zwAKPA36B8U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb5b204-ccd7-4d86-d2d9-03ee5446525f",
        "id": "Wphmfh3WB4mu"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "3     5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "11    3  2020   1  15     2020-01-15             508  43.9  38.3  1019.4   \n",
            "12    5  2021   1   1     2021-01-01             257  39.6  29.3  1029.3   \n",
            "14    2  2022   1  25     2022-01-25             235  41.6  31.8  1013.2   \n",
            "18    7  2021   1   3     2021-01-03             186  41.1  32.3  1018.0   \n",
            "19    4  2020   1   2     2020-01-02             413  39.6  28.9  1011.8   \n",
            "\n",
            "    visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "3    10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "11    8.2  ...  51.1  35.1  0.02  999.9    1             0                 0   \n",
            "12   10.0  ...  54.0  33.1  0.13  999.9    0             0                 0   \n",
            "14   10.0  ...  48.9  30.0  0.00  999.9    0             0                 0   \n",
            "18   10.0  ...  53.1  39.0  0.00  999.9    0             0                 0   \n",
            "19   10.0  ...  46.0  33.1  0.01  999.9    0             0                 0   \n",
            "\n",
            "    hail  thunder  tornado_funnel_cloud  \n",
            "3      0        0                     0  \n",
            "11     0        0                 10000  \n",
            "12     0        0                     0  \n",
            "14     0        0                     0  \n",
            "18     0        0                     0  \n",
            "19     0        0                     0  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] != 2012]\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] < 2020]\n",
        "cols = df_gust['NUM_COLLISIONS']\n",
        "df_gust = df_gust.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust[:6])\n",
        "df_gust.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a903c747-79ea-4cea-84cb-45387ff20fda",
        "id": "pzygHg-iB4mv"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  gust  NUM_COLLISIONS\n",
            "74    7  2016   1  17  18.1             451\n",
            "76    4  2014   1   9  20.0             561\n",
            "79    6  2019   1  19  21.0             479\n",
            "80    7  2015   1  11  17.1             341\n",
            "83    4  2015   1  29  20.0             519\n",
            "85    7  2019   1  13  15.9             374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day        year           mo           da         gust  \\\n",
              "count  1629.000000  1629.00000  1629.000000  1629.000000  1629.000000   \n",
              "mean      4.024555  2015.91283     6.278699    15.702885    27.511602   \n",
              "std       1.989070     2.01341     3.747683     8.667634     7.366770   \n",
              "min       1.000000  2013.00000     1.000000     1.000000    14.000000   \n",
              "25%       2.000000  2014.00000     3.000000     8.000000    22.000000   \n",
              "50%       4.000000  2016.00000     6.000000    16.000000    26.000000   \n",
              "75%       6.000000  2018.00000    10.000000    23.000000    31.100000   \n",
              "max       7.000000  2019.00000    12.000000    31.000000    71.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     1629.000000  \n",
              "mean       596.513198  \n",
              "std        104.479660  \n",
              "min        188.000000  \n",
              "25%        526.000000  \n",
              "50%        597.000000  \n",
              "75%        663.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd567149-ad3c-4519-b507-94b507a96e84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.024555</td>\n",
              "      <td>2015.91283</td>\n",
              "      <td>6.278699</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.989070</td>\n",
              "      <td>2.01341</td>\n",
              "      <td>3.747683</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.00000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd567149-ad3c-4519-b507-94b507a96e84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd567149-ad3c-4519-b507-94b507a96e84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd567149-ad3c-4519-b507-94b507a96e84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust.iloc[np.random.permutation(len(df_gust))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382ac1be-c5d2-4ddc-8746-5752a5482f1f",
        "id": "GA5wbn0KB4mw"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  gust\n",
            "1458    2  2017   5  23  24.1\n",
            "771     5  2013   3   1  24.1\n",
            "2105    1  2014   7  28  26.0\n",
            "461     5  2016   2  19  24.1\n",
            "2966    1  2014  10  20  27.0\n",
            "1718    3  2016   6   1  22.0\n",
            "      day  year  mo  da  gust  NUM_COLLISIONS\n",
            "1458    2  2017   5  23  24.1             659\n",
            "771     5  2013   3   1  24.1             603\n",
            "2105    1  2014   7  28  26.0             595\n",
            "461     5  2016   2  19  24.1             532\n",
            "2966    1  2014  10  20  27.0             591\n",
            "1718    3  2016   6   1  22.0             772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e636042-ebb5-415b-85ad-c476700453fb",
        "id": "iJRw3cGvB4mw"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1458    659\n",
            "771     603\n",
            "2105    595\n",
            "461     532\n",
            "2966    591\n",
            "1718    772\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8755a2-1c64-436b-ec6b-ef63e637d915",
        "id": "aZCjpnyzB4mw"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1303\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3375483d-8826-4792-ca56-80d3c1d67cbb",
        "id": "wrbkpT05B4mx"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd9af719110>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27697435, step = 1\n",
            "INFO:tensorflow:global_step/sec: 430.209\n",
            "INFO:tensorflow:loss = 0.008280726, step = 101 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 479.532\n",
            "INFO:tensorflow:loss = 0.007771803, step = 201 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.789\n",
            "INFO:tensorflow:loss = 0.007259279, step = 301 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.134\n",
            "INFO:tensorflow:loss = 0.006926834, step = 401 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 437.827\n",
            "INFO:tensorflow:loss = 0.006699074, step = 501 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 461.495\n",
            "INFO:tensorflow:loss = 0.0057665287, step = 601 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.612\n",
            "INFO:tensorflow:loss = 0.006327304, step = 701 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.674\n",
            "INFO:tensorflow:loss = 0.0063308114, step = 801 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.615\n",
            "INFO:tensorflow:loss = 0.009150356, step = 901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.259\n",
            "INFO:tensorflow:loss = 0.004817141, step = 1001 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.748\n",
            "INFO:tensorflow:loss = 0.007188972, step = 1101 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.013\n",
            "INFO:tensorflow:loss = 0.0058062384, step = 1201 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.927\n",
            "INFO:tensorflow:loss = 0.008181354, step = 1301 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.058\n",
            "INFO:tensorflow:loss = 0.006074873, step = 1401 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.144\n",
            "INFO:tensorflow:loss = 0.006932848, step = 1501 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 450.933\n",
            "INFO:tensorflow:loss = 0.0078158155, step = 1601 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 421.6\n",
            "INFO:tensorflow:loss = 0.0065883817, step = 1701 (0.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.022\n",
            "INFO:tensorflow:loss = 0.0072604613, step = 1801 (0.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.65\n",
            "INFO:tensorflow:loss = 0.007774268, step = 1901 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 789.641\n",
            "INFO:tensorflow:loss = 0.0065398696, step = 2001 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 787.151\n",
            "INFO:tensorflow:loss = 0.0080550285, step = 2101 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.379\n",
            "INFO:tensorflow:loss = 0.007030219, step = 2201 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 767.466\n",
            "INFO:tensorflow:loss = 0.007976908, step = 2301 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 789.115\n",
            "INFO:tensorflow:loss = 0.00803632, step = 2401 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.524\n",
            "INFO:tensorflow:loss = 0.0070948047, step = 2501 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.657\n",
            "INFO:tensorflow:loss = 0.008355545, step = 2601 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 788.827\n",
            "INFO:tensorflow:loss = 0.0072125965, step = 2701 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.48\n",
            "INFO:tensorflow:loss = 0.007835535, step = 2801 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 766.279\n",
            "INFO:tensorflow:loss = 0.0071287635, step = 2901 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 686.969\n",
            "INFO:tensorflow:loss = 0.006663722, step = 3001 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.68\n",
            "INFO:tensorflow:loss = 0.007837959, step = 3101 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 803.607\n",
            "INFO:tensorflow:loss = 0.007556036, step = 3201 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.036\n",
            "INFO:tensorflow:loss = 0.0057898676, step = 3301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 761.132\n",
            "INFO:tensorflow:loss = 0.008242041, step = 3401 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.545\n",
            "INFO:tensorflow:loss = 0.007258126, step = 3501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 796.451\n",
            "INFO:tensorflow:loss = 0.008793417, step = 3601 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.204\n",
            "INFO:tensorflow:loss = 0.0070910165, step = 3701 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 599.177\n",
            "INFO:tensorflow:loss = 0.007123527, step = 3801 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 580.713\n",
            "INFO:tensorflow:loss = 0.008071652, step = 3901 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 705.656\n",
            "INFO:tensorflow:loss = 0.007837791, step = 4001 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 758.571\n",
            "INFO:tensorflow:loss = 0.009576924, step = 4101 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 732.544\n",
            "INFO:tensorflow:loss = 0.008370039, step = 4201 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 820.818\n",
            "INFO:tensorflow:loss = 0.0064391224, step = 4301 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 796.309\n",
            "INFO:tensorflow:loss = 0.0060664243, step = 4401 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.268\n",
            "INFO:tensorflow:loss = 0.0062118922, step = 4501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.921\n",
            "INFO:tensorflow:loss = 0.0064426186, step = 4601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.67\n",
            "INFO:tensorflow:loss = 0.0068406328, step = 4701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 788.019\n",
            "INFO:tensorflow:loss = 0.0067228884, step = 4801 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.237\n",
            "INFO:tensorflow:loss = 0.0073013334, step = 4901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 714.806\n",
            "INFO:tensorflow:loss = 0.006730312, step = 5001 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.496\n",
            "INFO:tensorflow:loss = 0.008087556, step = 5101 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.493\n",
            "INFO:tensorflow:loss = 0.0076189833, step = 5201 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 708.271\n",
            "INFO:tensorflow:loss = 0.0075676, step = 5301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.887\n",
            "INFO:tensorflow:loss = 0.008830828, step = 5401 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 676.372\n",
            "INFO:tensorflow:loss = 0.009490682, step = 5501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 808.075\n",
            "INFO:tensorflow:loss = 0.0063127014, step = 5601 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 754.146\n",
            "INFO:tensorflow:loss = 0.0060460926, step = 5701 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 625.185\n",
            "INFO:tensorflow:loss = 0.007839765, step = 5801 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.032\n",
            "INFO:tensorflow:loss = 0.00862067, step = 5901 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 745.578\n",
            "INFO:tensorflow:loss = 0.008451701, step = 6001 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 760.099\n",
            "INFO:tensorflow:loss = 0.007725089, step = 6101 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 767.607\n",
            "INFO:tensorflow:loss = 0.0073797237, step = 6201 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 779.26\n",
            "INFO:tensorflow:loss = 0.007581992, step = 6301 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 765.306\n",
            "INFO:tensorflow:loss = 0.0075125564, step = 6401 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.099\n",
            "INFO:tensorflow:loss = 0.009237688, step = 6501 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.083\n",
            "INFO:tensorflow:loss = 0.005861747, step = 6601 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.154\n",
            "INFO:tensorflow:loss = 0.006319086, step = 6701 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 799.524\n",
            "INFO:tensorflow:loss = 0.0082795955, step = 6801 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 801.797\n",
            "INFO:tensorflow:loss = 0.007068271, step = 6901 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 744.796\n",
            "INFO:tensorflow:loss = 0.006869954, step = 7001 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 820.331\n",
            "INFO:tensorflow:loss = 0.007618188, step = 7101 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.752\n",
            "INFO:tensorflow:loss = 0.005746916, step = 7201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.653\n",
            "INFO:tensorflow:loss = 0.0071375472, step = 7301 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.731\n",
            "INFO:tensorflow:loss = 0.005877033, step = 7401 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 835.083\n",
            "INFO:tensorflow:loss = 0.007519031, step = 7501 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 793.609\n",
            "INFO:tensorflow:loss = 0.004272503, step = 7601 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 816.449\n",
            "INFO:tensorflow:loss = 0.0075667305, step = 7701 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 790.232\n",
            "INFO:tensorflow:loss = 0.006157063, step = 7801 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.964\n",
            "INFO:tensorflow:loss = 0.009359101, step = 7901 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.709\n",
            "INFO:tensorflow:loss = 0.0075549255, step = 8001 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.316\n",
            "INFO:tensorflow:loss = 0.007646724, step = 8101 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 799.488\n",
            "INFO:tensorflow:loss = 0.008620856, step = 8201 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.322\n",
            "INFO:tensorflow:loss = 0.0061611924, step = 8301 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 783.878\n",
            "INFO:tensorflow:loss = 0.0085048415, step = 8401 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.567\n",
            "INFO:tensorflow:loss = 0.0057794736, step = 8501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 761.164\n",
            "INFO:tensorflow:loss = 0.007557905, step = 8601 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 834.569\n",
            "INFO:tensorflow:loss = 0.013340089, step = 8701 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 749.085\n",
            "INFO:tensorflow:loss = 0.0051316014, step = 8801 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 738.76\n",
            "INFO:tensorflow:loss = 0.005464339, step = 8901 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.432\n",
            "INFO:tensorflow:loss = 0.008191565, step = 9001 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 843.483\n",
            "INFO:tensorflow:loss = 0.0070862975, step = 9101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.992\n",
            "INFO:tensorflow:loss = 0.007155698, step = 9201 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.115\n",
            "INFO:tensorflow:loss = 0.009787617, step = 9301 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 773.962\n",
            "INFO:tensorflow:loss = 0.0061194603, step = 9401 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.791\n",
            "INFO:tensorflow:loss = 0.007498092, step = 9501 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 768.259\n",
            "INFO:tensorflow:loss = 0.007198585, step = 9601 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.659\n",
            "INFO:tensorflow:loss = 0.00681176, step = 9701 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 842.218\n",
            "INFO:tensorflow:loss = 0.010122621, step = 9801 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.931\n",
            "INFO:tensorflow:loss = 0.008206034, step = 9901 (0.123 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0060857413.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 88.83604642247232\n",
            "Just using average = 599.2755180353031 has RMSE of 100.03439310079685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53436428-60c5-48e1-eef5-fd8af80b2eab",
        "id": "ByVWr9PzB4mx"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fe18090>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4808129  0.49754852 0.5343925  0.45430008 0.45905632 0.5268488\n",
            " 0.54438007 0.5532203  0.5508805  0.54666233 0.46852517 0.51174146\n",
            " 0.54744095 0.49077356 0.4979892  0.5045125  0.49858832 0.577427\n",
            " 0.44949535 0.48807988 0.5096381  0.5187636  0.537857   0.5051454\n",
            " 0.5123046  0.4882884  0.56278956 0.5625091  0.52163845 0.5567407\n",
            " 0.44378945 0.46449938 0.4994982  0.5546624  0.46106714 0.4986182\n",
            " 0.46910784 0.50934947 0.55493456 0.54278696 0.494294   0.49253547\n",
            " 0.4881124  0.5327354  0.54614097 0.5154723  0.50101584 0.47131452\n",
            " 0.52982837 0.48684943 0.5194375  0.45543742 0.4643467  0.53313166\n",
            " 0.5352727  0.47446644 0.53988695 0.49811885 0.5139935  0.46114302\n",
            " 0.5776381  0.4690111  0.553347   0.46265244 0.4743408  0.4991158\n",
            " 0.53799284 0.5412227  0.5309977  0.5665094  0.5566231  0.5114611\n",
            " 0.55724216 0.5116754  0.4983055  0.50266916 0.4490461  0.54756314\n",
            " 0.49480188 0.4479491  0.454196   0.49158782 0.5499064  0.46162555\n",
            " 0.55901235 0.4564303  0.54813904 0.4593838  0.5548011  0.46421427\n",
            " 0.5003946  0.51236856 0.45560688 0.52279156 0.5061427  0.50800484\n",
            " 0.50043815 0.4969952  0.54057306 0.53912616 0.46174014 0.4631908\n",
            " 0.4541145  0.4910029  0.5049247  0.52976555 0.48928186 0.45285317\n",
            " 0.5255991  0.4735227  0.510135   0.48260662 0.5407897  0.4388431\n",
            " 0.43960458 0.46774817 0.51646006 0.46988106 0.51744395 0.5230365\n",
            " 0.45111486 0.49304235 0.44568828 0.49637964 0.50172937 0.5088766\n",
            " 0.47930098 0.49339274 0.48461294 0.48367077 0.5659991  0.5180226\n",
            " 0.45275152 0.45806387 0.5009857  0.53567326 0.52993673 0.5490104\n",
            " 0.46985945 0.5096018  0.5405747  0.5302869  0.5128916  0.48285845\n",
            " 0.46144637 0.5097204  0.49749956 0.507199   0.51975965 0.5021811\n",
            " 0.51079345 0.47607642 0.48438644 0.52962786 0.45431527 0.5271203\n",
            " 0.45755804 0.5120359  0.53486943 0.5046405  0.4664076  0.54650754\n",
            " 0.48776358 0.51774967 0.4906894  0.46548447 0.51092154 0.57270604\n",
            " 0.4379     0.56336784 0.50381553 0.5250144  0.5259455  0.5310013\n",
            " 0.4827415  0.5489582  0.51558733 0.46599713 0.45170006 0.5698902\n",
            " 0.54966354 0.51528525 0.4687121  0.47027597 0.54070413 0.4433574\n",
            " 0.5679024  0.56665933 0.5796282  0.5038601  0.52107346 0.54754186\n",
            " 0.5035547  0.54641896 0.5079616  0.5138655  0.4965704  0.5054003\n",
            " 0.5120732  0.51625997 0.47924775 0.50968206 0.52472234 0.44623926\n",
            " 0.5261823  0.47969455 0.5290682  0.55129266 0.50564843 0.53002125\n",
            " 0.5158694  0.5148233  0.4513209  0.50471455 0.5108636  0.50813836\n",
            " 0.5545506  0.4853054  0.5292075  0.49407232 0.47692603 0.5402202\n",
            " 0.5621382  0.4477677  0.5458362  0.51917297 0.4580928  0.46256718\n",
            " 0.4407241  0.54159266 0.4636813  0.51431054 0.53022873 0.5031429\n",
            " 0.44799286 0.51603097 0.57733166 0.533609   0.47546127 0.47869003\n",
            " 0.51778305 0.4955161  0.4949797  0.44967243 0.44245955 0.5303627\n",
            " 0.501039   0.5080909  0.44999543 0.45411208 0.5290824  0.50758404\n",
            " 0.48976278 0.46579665 0.5178915  0.47385788 0.47828776 0.51112205\n",
            " 0.4599878  0.57538676 0.53414774 0.56197643 0.47887236 0.4351205\n",
            " 0.4845891  0.49088025 0.48621008 0.4581946  0.5119704  0.5118506\n",
            " 0.5307097  0.5026627  0.5005737  0.50111526 0.46004307 0.5275461\n",
            " 0.47003022 0.51193124 0.5332661  0.56010616 0.47651017 0.5174159\n",
            " 0.52224076 0.47688493 0.48461992 0.55611444 0.56252325 0.5577871\n",
            " 0.47537154 0.49520624 0.489991   0.5249904  0.51006263 0.54906636\n",
            " 0.45374554 0.4703028  0.43467462 0.5375508  0.5747577  0.52779984\n",
            " 0.5645165  0.4976503  0.48728007 0.4846186  0.46229237 0.4327205\n",
            " 0.54274744 0.5241005  0.45994657 0.45553106 0.53228605 0.5694719\n",
            " 0.46719834 0.55594563 0.50649303 0.44682336 0.5067851  0.5138291\n",
            " 0.4811305  0.4748805  0.5264627  0.46155748 0.5017866  0.47464314\n",
            " 0.49966803 0.51739454]\n",
            "[0.50215332 0.62273902 0.55641688 0.42635659 0.40913006 0.55727821\n",
            " 0.51076658 0.51679587 0.45908699 0.53402239 0.41515935 0.53919035\n",
            " 0.69595177 0.34797588 0.48664944 0.45219638 0.48923342 0.49870801\n",
            " 0.37812231 0.43496985 0.59173127 0.36950904 0.58742463 0.64341085\n",
            " 0.53488372 0.43669251 0.62187769 0.53057709 0.47631352 0.5374677\n",
            " 0.45305771 0.44272179 0.4005168  0.46770026 0.56933678 0.55297158\n",
            " 0.48837209 0.48664944 0.44358312 0.52971576 0.46942291 0.51593454\n",
            " 0.43755383 0.47114556 0.50043066 0.49956934 0.6873385  0.51765719\n",
            " 0.59776055 0.53057709 0.4918174  0.51937984 0.46511628 0.46942291\n",
            " 0.47631352 0.61843239 0.55813953 0.42980189 0.38415159 0.50043066\n",
            " 0.5667528  0.44272179 0.48837209 0.50559862 0.41946598 0.5538329\n",
            " 0.63996555 0.59345392 0.55813953 0.53229974 0.53143842 0.50387597\n",
            " 0.56761413 0.52799311 0.37984496 0.48578811 0.40913006 0.64771748\n",
            " 0.44702842 0.47372954 0.46339363 0.52713178 0.53229974 0.53402239\n",
            " 0.54349699 0.43927649 0.5796727  0.38156761 0.48406546 0.45822567\n",
            " 0.53143842 0.58656331 0.36950904 0.56244617 0.46770026 0.54608096\n",
            " 0.55900086 0.42377261 0.54521964 0.59345392 0.47717485 0.416882\n",
            " 0.39793282 0.48492679 0.47975883 0.43152455 0.54349699 0.41774332\n",
            " 0.41085271 0.53316107 0.55211025 0.52368648 0.52196382 0.43841516\n",
            " 0.35400517 0.44788975 0.45478036 0.50904393 0.52799311 0.48751077\n",
            " 0.41257537 0.47975883 0.40654608 0.43238587 0.53057709 0.67786391\n",
            " 0.64857881 0.40654608 0.39793282 0.3910422  0.64427218 0.71576227\n",
            " 0.38329027 0.45650301 0.46339363 0.68217054 0.43152455 0.54263566\n",
            " 0.64857881 0.56503015 0.63910422 0.51507321 0.52196382 0.57881137\n",
            " 0.45305771 0.46080965 0.5047373  0.43238587 0.30577089 0.52540913\n",
            " 0.53660637 0.45994832 0.49354005 0.46770026 0.46080965 0.36864772\n",
            " 0.4203273  0.54005168 0.39018088 0.46425495 0.49009475 0.52885444\n",
            " 0.49354005 0.58828596 0.62790698 0.42635659 0.51248923 0.43152455\n",
            " 0.32730405 0.57450474 0.52196382 0.65719208 0.6416882  0.48664944\n",
            " 0.68819983 0.74677003 0.4754522  0.47459087 0.39534884 0.58914729\n",
            " 0.59345392 0.53919035 0.44358312 0.53488372 0.51421189 0.39965547\n",
            " 0.63996555 0.51507321 0.62446167 0.43669251 0.41429802 0.53574505\n",
            " 0.48062016 0.52885444 0.54091301 0.67011197 0.63910422 0.54177433\n",
            " 0.4461671  0.583118   0.50387597 0.49784668 0.68044789 0.37209302\n",
            " 0.54866494 0.57364341 0.56416882 0.57364341 0.49440138 0.53574505\n",
            " 0.4918174  0.45822567 0.36692506 0.46167097 0.44530577 0.59431525\n",
            " 0.59862188 0.39448751 0.69853575 0.5047373  0.52282515 0.56589147\n",
            " 0.55727821 0.40913006 0.41257537 0.66063738 0.39362618 0.583118\n",
            " 0.38587425 0.61584841 0.48406546 0.45736434 0.4461671  0.31007752\n",
            " 0.37812231 0.416882   0.5503876  0.40568475 0.45736434 0.52282515\n",
            " 0.58656331 0.59086994 0.51248923 0.32385874 0.3910422  0.57881137\n",
            " 0.44272179 0.50990525 0.43496985 0.44444444 0.59345392 0.71748493\n",
            " 0.5047373  0.36347976 0.57278208 0.51593454 0.52713178 0.60465116\n",
            " 0.42980189 0.60206718 0.36175711 0.49095607 0.42549526 0.34969854\n",
            " 0.56847545 0.66408269 0.56158484 0.34711456 0.63738157 0.47459087\n",
            " 0.45564169 0.52713178 0.48148148 0.49698536 0.4918174  0.42463394\n",
            " 0.49698536 0.54521964 0.60981912 0.56589147 0.38845823 0.5667528\n",
            " 0.55900086 0.44444444 0.5245478  0.57019811 0.60292851 0.60120586\n",
            " 0.40223945 0.51593454 0.45305771 0.54177433 0.59086994 0.48148148\n",
            " 0.41946598 0.44788975 0.36864772 0.49612403 0.42807924 0.50129199\n",
            " 0.58914729 0.49870801 0.56158484 0.4788975  0.46597761 0.36950904\n",
            " 0.49956934 0.51851852 0.77174849 0.46597761 0.6287683  0.53402239\n",
            " 0.39276486 0.58914729 0.38242894 0.40137812 0.49267873 0.53488372\n",
            " 0.34022394 0.65202412 0.58139535 0.46080965 0.64341085 0.50732127\n",
            " 0.34022394 0.56416882]\n",
            "The trained model has an aproximate error rate of -0.053529303406644434 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (Mxpsd)"
      ],
      "metadata": {
        "id": "OxfDYOaGD1Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d096ce-f3b3-4ea2-8f6e-30a26b69589b",
        "id": "9TtzqELvE-aN"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','gust','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] != 2012]\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] < 2020]\n",
        "cols = df_mxpsd['NUM_COLLISIONS']\n",
        "df_mxpsd = df_mxpsd.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd[:6])\n",
        "df_mxpsd.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "59b5f494-76f9-4ec2-eb06-22aa8f615ab4",
        "id": "O_x5VoUvE-aO"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "49    4  2016   1  28    8.9             681\n",
            "51    5  2014   1  17    8.9             589\n",
            "54    1  2016   1  25    8.9             658\n",
            "55    5  2016   1  29    9.9             645\n",
            "58    5  2017   1  20    9.9             605\n",
            "59    7  2013   1  13    9.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da        mxpsd  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      3.999608  2016.001567     6.520564    15.737172    17.240110   \n",
              "std       2.001469     2.000587     3.449204     8.797367     5.858333   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     5.100000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    13.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    15.900000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    20.000000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    49.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2553.000000  \n",
              "mean       599.033686  \n",
              "std        100.284761  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec6ac456-b3a5-407e-a5e4-1490afc31eaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999608</td>\n",
              "      <td>2016.001567</td>\n",
              "      <td>6.520564</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.001469</td>\n",
              "      <td>2.000587</td>\n",
              "      <td>3.449204</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec6ac456-b3a5-407e-a5e4-1490afc31eaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec6ac456-b3a5-407e-a5e4-1490afc31eaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec6ac456-b3a5-407e-a5e4-1490afc31eaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd.iloc[np.random.permutation(len(df_mxpsd))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35174220-b638-4bd1-d810-9391dbd52aa2",
        "id": "oEaMTRvFE-aO"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  mxpsd\n",
            "1631    4  2013   6  20   12.0\n",
            "2824    1  2015  10  12   11.1\n",
            "2274    7  2018   8  26   11.1\n",
            "1077    7  2015   4   5   18.1\n",
            "1778    3  2015   6   3   19.0\n",
            "578     1  2019   2  25   31.1\n",
            "      day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "1631    4  2013   6  20   12.0             681\n",
            "2824    1  2015  10  12   11.1             505\n",
            "2274    7  2018   8  26   11.1             528\n",
            "1077    7  2015   4   5   18.1             422\n",
            "1778    3  2015   6   3   19.0             658\n",
            "578     1  2019   2  25   31.1             700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df401e4f-a57b-4a19-8c26-2f37a8069881",
        "id": "2IjTestaE-aO"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1631    681\n",
            "2824    505\n",
            "2274    528\n",
            "1077    422\n",
            "1778    658\n",
            "578     700\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e74430d-b14c-4eea-941d-40923d64c6f9",
        "id": "ihCguRRBE-aO"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2042\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391cf64a-30ba-4734-bd33-c8ae90f42cba",
        "id": "-hI4RaYNE-aP"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fe2fdd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.26019302, step = 1\n",
            "INFO:tensorflow:global_step/sec: 685.071\n",
            "INFO:tensorflow:loss = 0.00679522, step = 101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 763.159\n",
            "INFO:tensorflow:loss = 0.0066580754, step = 201 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.708\n",
            "INFO:tensorflow:loss = 0.0076028043, step = 301 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 767.354\n",
            "INFO:tensorflow:loss = 0.007029012, step = 401 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 692.29\n",
            "INFO:tensorflow:loss = 0.0071098125, step = 501 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 761.215\n",
            "INFO:tensorflow:loss = 0.0067186514, step = 601 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 738.431\n",
            "INFO:tensorflow:loss = 0.0060497043, step = 701 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.189\n",
            "INFO:tensorflow:loss = 0.006721466, step = 801 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 821.295\n",
            "INFO:tensorflow:loss = 0.006102931, step = 901 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 763.106\n",
            "INFO:tensorflow:loss = 0.0062179514, step = 1001 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.7\n",
            "INFO:tensorflow:loss = 0.006207739, step = 1101 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.632\n",
            "INFO:tensorflow:loss = 0.008103848, step = 1201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.047\n",
            "INFO:tensorflow:loss = 0.006287241, step = 1301 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.335\n",
            "INFO:tensorflow:loss = 0.006070775, step = 1401 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 746.048\n",
            "INFO:tensorflow:loss = 0.0059815, step = 1501 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 812.209\n",
            "INFO:tensorflow:loss = 0.006301297, step = 1601 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.456\n",
            "INFO:tensorflow:loss = 0.007593268, step = 1701 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 807.641\n",
            "INFO:tensorflow:loss = 0.0063871685, step = 1801 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 732.764\n",
            "INFO:tensorflow:loss = 0.0065900567, step = 1901 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.827\n",
            "INFO:tensorflow:loss = 0.009346271, step = 2001 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 729.14\n",
            "INFO:tensorflow:loss = 0.0079907365, step = 2101 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.179\n",
            "INFO:tensorflow:loss = 0.0064518685, step = 2201 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.192\n",
            "INFO:tensorflow:loss = 0.006936039, step = 2301 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.536\n",
            "INFO:tensorflow:loss = 0.0060931584, step = 2401 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 774.603\n",
            "INFO:tensorflow:loss = 0.008409785, step = 2501 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.881\n",
            "INFO:tensorflow:loss = 0.007607714, step = 2601 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.751\n",
            "INFO:tensorflow:loss = 0.006778916, step = 2701 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 688.609\n",
            "INFO:tensorflow:loss = 0.0073934137, step = 2801 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.127\n",
            "INFO:tensorflow:loss = 0.006278823, step = 2901 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.961\n",
            "INFO:tensorflow:loss = 0.006859187, step = 3001 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 765.092\n",
            "INFO:tensorflow:loss = 0.0062295333, step = 3101 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 734.823\n",
            "INFO:tensorflow:loss = 0.006514907, step = 3201 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 738.88\n",
            "INFO:tensorflow:loss = 0.009400882, step = 3301 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.002\n",
            "INFO:tensorflow:loss = 0.00699758, step = 3401 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 680.852\n",
            "INFO:tensorflow:loss = 0.0064976187, step = 3501 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 520.434\n",
            "INFO:tensorflow:loss = 0.0067361756, step = 3601 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.184\n",
            "INFO:tensorflow:loss = 0.0052393624, step = 3701 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 736.935\n",
            "INFO:tensorflow:loss = 0.0057399343, step = 3801 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.752\n",
            "INFO:tensorflow:loss = 0.0065044146, step = 3901 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 847.122\n",
            "INFO:tensorflow:loss = 0.0063355993, step = 4001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 788.824\n",
            "INFO:tensorflow:loss = 0.0059073726, step = 4101 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 776.214\n",
            "INFO:tensorflow:loss = 0.0055305697, step = 4201 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 751.674\n",
            "INFO:tensorflow:loss = 0.007542481, step = 4301 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 802.524\n",
            "INFO:tensorflow:loss = 0.006565943, step = 4401 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.928\n",
            "INFO:tensorflow:loss = 0.0051479572, step = 4501 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 808.642\n",
            "INFO:tensorflow:loss = 0.0063371435, step = 4601 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.244\n",
            "INFO:tensorflow:loss = 0.004754968, step = 4701 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 723.901\n",
            "INFO:tensorflow:loss = 0.008252546, step = 4801 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 702.844\n",
            "INFO:tensorflow:loss = 0.0050983094, step = 4901 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 735.462\n",
            "INFO:tensorflow:loss = 0.005042064, step = 5001 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.199\n",
            "INFO:tensorflow:loss = 0.006114108, step = 5101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 734.527\n",
            "INFO:tensorflow:loss = 0.005083383, step = 5201 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 697.869\n",
            "INFO:tensorflow:loss = 0.006767603, step = 5301 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 809.685\n",
            "INFO:tensorflow:loss = 0.005405742, step = 5401 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 788.532\n",
            "INFO:tensorflow:loss = 0.0059195124, step = 5501 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 750.697\n",
            "INFO:tensorflow:loss = 0.007732361, step = 5601 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 723.922\n",
            "INFO:tensorflow:loss = 0.005725666, step = 5701 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 783.183\n",
            "INFO:tensorflow:loss = 0.007695691, step = 5801 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.806\n",
            "INFO:tensorflow:loss = 0.0050357757, step = 5901 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.974\n",
            "INFO:tensorflow:loss = 0.006944257, step = 6001 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 721.926\n",
            "INFO:tensorflow:loss = 0.00878717, step = 6101 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 804.14\n",
            "INFO:tensorflow:loss = 0.0054761167, step = 6201 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.782\n",
            "INFO:tensorflow:loss = 0.0062287115, step = 6301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.59\n",
            "INFO:tensorflow:loss = 0.006326562, step = 6401 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 738.495\n",
            "INFO:tensorflow:loss = 0.005651747, step = 6501 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 755.148\n",
            "INFO:tensorflow:loss = 0.006404798, step = 6601 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 715.467\n",
            "INFO:tensorflow:loss = 0.0059954133, step = 6701 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.658\n",
            "INFO:tensorflow:loss = 0.0064172074, step = 6801 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 752.286\n",
            "INFO:tensorflow:loss = 0.005417253, step = 6901 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 753.367\n",
            "INFO:tensorflow:loss = 0.008904778, step = 7001 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.008\n",
            "INFO:tensorflow:loss = 0.0069028283, step = 7101 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 685.874\n",
            "INFO:tensorflow:loss = 0.0065670717, step = 7201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 736.305\n",
            "INFO:tensorflow:loss = 0.007024459, step = 7301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.277\n",
            "INFO:tensorflow:loss = 0.008323776, step = 7401 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 780.71\n",
            "INFO:tensorflow:loss = 0.0058166375, step = 7501 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 767.203\n",
            "INFO:tensorflow:loss = 0.0054783556, step = 7601 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 778.223\n",
            "INFO:tensorflow:loss = 0.0092752855, step = 7701 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 769.272\n",
            "INFO:tensorflow:loss = 0.0066548437, step = 7801 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 748.012\n",
            "INFO:tensorflow:loss = 0.006458889, step = 7901 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.254\n",
            "INFO:tensorflow:loss = 0.006584425, step = 8001 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 687.324\n",
            "INFO:tensorflow:loss = 0.006219251, step = 8101 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.416\n",
            "INFO:tensorflow:loss = 0.0071615414, step = 8201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 812.881\n",
            "INFO:tensorflow:loss = 0.0047244066, step = 8301 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.891\n",
            "INFO:tensorflow:loss = 0.005918904, step = 8401 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 785.084\n",
            "INFO:tensorflow:loss = 0.0051508057, step = 8501 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 861.901\n",
            "INFO:tensorflow:loss = 0.007128499, step = 8601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.858\n",
            "INFO:tensorflow:loss = 0.007940366, step = 8701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.046\n",
            "INFO:tensorflow:loss = 0.008458632, step = 8801 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 778.767\n",
            "INFO:tensorflow:loss = 0.0053295745, step = 8901 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 857.114\n",
            "INFO:tensorflow:loss = 0.0059607853, step = 9001 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 860.926\n",
            "INFO:tensorflow:loss = 0.0055090995, step = 9101 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.524\n",
            "INFO:tensorflow:loss = 0.0057784645, step = 9201 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.865\n",
            "INFO:tensorflow:loss = 0.0063145086, step = 9301 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 758.229\n",
            "INFO:tensorflow:loss = 0.00606923, step = 9401 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 754.531\n",
            "INFO:tensorflow:loss = 0.0055173254, step = 9501 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 681.597\n",
            "INFO:tensorflow:loss = 0.006057687, step = 9601 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 748.141\n",
            "INFO:tensorflow:loss = 0.007110549, step = 9701 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 762.058\n",
            "INFO:tensorflow:loss = 0.005520527, step = 9801 (0.133 sec)\n",
            "INFO:tensorflow:global_step/sec: 770.004\n",
            "INFO:tensorflow:loss = 0.005678097, step = 9901 (0.128 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0064712144.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 99.54707597846121\n",
            "Just using average = 599.8344760039178 has RMSE of 106.6443345361231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bfb18a-8f17-416c-8125-6c195a43d35c",
        "id": "5vjLYo0bE-aP"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fce96d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.48685554 0.5017715  0.5195437  0.53615284 0.5454602  0.4770372\n",
            " 0.5478919  0.51928663 0.4630683  0.5554678  0.53757876 0.51046175\n",
            " 0.4606119  0.53223115 0.5676637  0.49264532 0.5143805  0.55177903\n",
            " 0.49720487 0.5292196  0.5016322  0.4959558  0.5637739  0.5187303\n",
            " 0.4729221  0.5310823  0.5245323  0.54790014 0.4961628  0.5474171\n",
            " 0.5230903  0.517643   0.50107825 0.47332734 0.4714707  0.52177274\n",
            " 0.5512216  0.5066799  0.5274798  0.46867326 0.53792155 0.5261571\n",
            " 0.5013206  0.51682    0.46757972 0.5208199  0.5042332  0.51943666\n",
            " 0.5045486  0.5179419  0.53066957 0.5310577  0.49924272 0.4973693\n",
            " 0.47977316 0.5068482  0.49414656 0.5599052  0.5156741  0.48933315\n",
            " 0.52164257 0.525553   0.47889602 0.5219304  0.5284055  0.54281825\n",
            " 0.46601078 0.45353243 0.53122807 0.5408706  0.4562273  0.51327634\n",
            " 0.50791854 0.51089054 0.51573545 0.5449649  0.53421545 0.550504\n",
            " 0.50065804 0.5429421  0.47455156 0.5280237  0.4997188  0.5393757\n",
            " 0.4974781  0.5459225  0.53021854 0.5596765  0.47312063 0.5672009\n",
            " 0.4673077  0.54752845 0.54229134 0.5099148  0.495593   0.5092741\n",
            " 0.53948265 0.5546138  0.52770466 0.4735154  0.5007938  0.519316\n",
            " 0.49349684 0.49898806 0.5178253  0.5431118  0.52258176 0.55730677\n",
            " 0.5450412  0.48819274 0.4781387  0.53090715 0.48264745 0.52474743\n",
            " 0.5376916  0.5446888  0.4945822  0.49862918 0.5370184  0.5259141\n",
            " 0.5589317  0.511402   0.49572992 0.49405676 0.5266539  0.56122327\n",
            " 0.5263234  0.5443087  0.4815888  0.50417674 0.5306322  0.5194509\n",
            " 0.5057259  0.49337825 0.5379311  0.53961444 0.4780355  0.5409513\n",
            " 0.5159482  0.5269133  0.5099086  0.509943   0.47645032 0.47112194\n",
            " 0.5111627  0.55192435 0.45899394 0.5316283  0.5364416  0.4746627\n",
            " 0.50352305 0.4693607  0.46582076 0.4939675  0.49642912 0.46158022\n",
            " 0.51712596 0.51932085 0.542178   0.5035765  0.50724316 0.5359318\n",
            " 0.47609547 0.48649237 0.53874606 0.5028715  0.5064764  0.5313428\n",
            " 0.48725688 0.52922857 0.52546555 0.4622849  0.50001305 0.49107406\n",
            " 0.50256544 0.5658561  0.54263884 0.5488011  0.5263907  0.53721756\n",
            " 0.49791235 0.53154784 0.5047416  0.47419646 0.48793972 0.5174677\n",
            " 0.51816994 0.5379286  0.503488   0.5051627  0.50146246 0.51243764\n",
            " 0.4904783  0.50547767 0.49991265 0.53765094 0.5006447  0.4783819\n",
            " 0.48331013 0.5262028  0.5093145  0.47849506 0.53740674 0.48446897\n",
            " 0.5100596  0.52843225 0.4945734  0.4894895  0.4874934  0.5445079\n",
            " 0.5502267  0.515275   0.5143878  0.5670278  0.5348427  0.479606\n",
            " 0.50119877 0.5365644  0.52360076 0.5145496  0.46888995 0.48572615\n",
            " 0.48134646 0.5042382  0.5237121  0.53073597 0.4902183  0.5005127\n",
            " 0.46331316 0.55187297 0.51985407 0.48156908 0.501078   0.49885568\n",
            " 0.46576095 0.49326128 0.56435484 0.49747556 0.48256207 0.45977372\n",
            " 0.46180552 0.5270362  0.54911023 0.53729326 0.46264276 0.49487734\n",
            " 0.5503344  0.5332989  0.49197206 0.5094972  0.53396827 0.5072817\n",
            " 0.5045238  0.5036817  0.51063806 0.46978992 0.47466838 0.48977733\n",
            " 0.52365386 0.5290835  0.47596952 0.56146026 0.5364375  0.47901666\n",
            " 0.5232943  0.5162686  0.5508102  0.48782435 0.5261868  0.4624441\n",
            " 0.4794042  0.5071171  0.53919476 0.51614976 0.4906166  0.49088615\n",
            " 0.48840526 0.4801976  0.5305448  0.53928345 0.48377863 0.50236\n",
            " 0.52599764 0.4806209  0.48383752 0.49248454 0.5330836  0.53117085\n",
            " 0.54104847 0.49241352 0.4620595  0.5459119  0.51819164 0.5050651\n",
            " 0.52370316 0.48869243 0.51420784 0.47219777 0.5538472  0.48655763\n",
            " 0.45584542 0.47907698 0.53629357 0.48915908 0.50302786 0.5123145\n",
            " 0.5384795  0.52728975 0.49681902 0.4803666  0.5153237  0.47643712\n",
            " 0.55039585 0.5150574  0.50190395 0.4872677  0.48908862 0.523504\n",
            " 0.50354254 0.5219394  0.50331867 0.5011213  0.5248733  0.47085893\n",
            " 0.5285967  0.5605565  0.48842663 0.54400545 0.5344284  0.5204536\n",
            " 0.4538653  0.5191104  0.49626803 0.4544688  0.49466205 0.5117591\n",
            " 0.47794145 0.46465802 0.5314326  0.47504643 0.50573707 0.5252404\n",
            " 0.49307767 0.48354176 0.49108198 0.51387113 0.5019756  0.54330033\n",
            " 0.48235556 0.5496864  0.5208385  0.4757569  0.52065605 0.49259612\n",
            " 0.5148043  0.4909138  0.48307928 0.532557   0.5403539  0.4870017\n",
            " 0.5488951  0.5513349  0.50215113 0.45026612 0.4700168  0.49739036\n",
            " 0.46908382 0.47758335 0.5209645  0.4549843  0.5424231  0.5145909\n",
            " 0.5215699  0.5426644  0.5101945  0.5254881  0.48752278 0.50031227\n",
            " 0.52576715 0.5057072  0.5240888  0.5303316  0.55493194 0.5152836\n",
            " 0.552045   0.50798625 0.464249   0.5241906  0.51107097 0.5147305\n",
            " 0.5163953  0.5467944  0.5072981  0.5377797  0.50776    0.51956356\n",
            " 0.48065782 0.49374965 0.50433797 0.47179997 0.4738736  0.51034814\n",
            " 0.48437303 0.51459086 0.5325481  0.48601687 0.476758   0.47732553\n",
            " 0.5599755  0.5564781  0.5235116  0.53013474 0.5201743  0.5290057\n",
            " 0.50550604 0.4970346  0.48069993 0.5117702  0.46578386 0.5166585\n",
            " 0.53547484 0.48879257 0.4986732  0.5361961  0.473976   0.4927038\n",
            " 0.5469054  0.50582045 0.49550134 0.51771593 0.471248   0.5044456\n",
            " 0.46159893 0.50868785 0.48383757 0.53140426 0.5291086  0.5087871\n",
            " 0.46618062 0.5261164  0.5593194  0.5210621  0.4999149  0.53450686\n",
            " 0.5185841  0.5128168  0.5247683  0.49623376 0.4756947  0.52170664\n",
            " 0.5615461  0.45673808 0.56919163 0.49448165 0.49125728 0.51861984\n",
            " 0.55617964 0.5290115  0.5028912  0.53508735 0.49241027 0.5555204\n",
            " 0.51995087 0.48963735 0.55135626 0.49387947 0.54063356 0.5447562\n",
            " 0.5677064  0.5211267  0.53856766 0.49966016 0.5036277  0.4826619\n",
            " 0.5244595  0.49085516 0.5067349  0.5075524  0.51260954 0.5589267\n",
            " 0.53380704 0.48925838 0.51045763 0.5137617  0.4889487  0.50341004\n",
            " 0.5110124  0.5063879  0.46677184 0.5548449  0.48576906 0.507714\n",
            " 0.5126534  0.5003867  0.5293473  0.5184094  0.51767    0.4790921\n",
            " 0.53610647 0.49169883 0.49940497 0.47016695 0.546803   0.52749634\n",
            " 0.5058208  0.5146395  0.5000936  0.48356792 0.51875806 0.54141426\n",
            " 0.55285   ]\n",
            "[0.40137812 0.51162791 0.44702842 0.60809647 0.54952627 0.40568475\n",
            " 0.45478036 0.53057709 0.40913006 0.60120586 0.42894057 0.16192937\n",
            " 0.34453058 0.5211025  0.5667528  0.43496985 0.38415159 0.54349699\n",
            " 0.42377261 0.65202412 0.49698536 0.60551249 0.43152455 0.57881137\n",
            " 0.34366925 0.56158484 0.47459087 0.63135228 0.66408269 0.5245478\n",
            " 0.5796727  0.4461671  0.51421189 0.44013781 0.50387597 0.64254953\n",
            " 0.53229974 0.46339363 0.47975883 0.49009475 0.51937984 0.54608096\n",
            " 0.45478036 0.51937984 0.38156761 0.61154177 0.49870801 0.54177433\n",
            " 0.46856158 0.66063738 0.4918174  0.4461671  0.59000861 0.26873385\n",
            " 0.48923342 0.46339363 0.44875108 0.63996555 0.57708872 0.57364341\n",
            " 0.50904393 0.48148148 0.42635659 0.57105943 0.45047373 0.5796727\n",
            " 0.38587425 0.32730405 0.37639966 0.55727821 0.37639966 0.49956934\n",
            " 0.56072351 0.58656331 0.56330749 0.53574505 0.39018088 0.56158484\n",
            " 0.46167097 0.51421189 0.48492679 0.5994832  0.5374677  0.57536606\n",
            " 0.49354005 0.53229974 0.55641688 0.47631352 0.50904393 0.60206718\n",
            " 0.48492679 0.52971576 0.63135228 0.38242894 0.60378984 0.37639966\n",
            " 0.37898363 0.55555556 0.69853575 0.47200689 0.4005168  0.71576227\n",
            " 0.46339363 0.60034453 0.56847545 0.54349699 0.55641688 0.57622739\n",
            " 0.55900086 0.5047373  0.58484065 0.51248923 0.52024117 0.65719208\n",
            " 0.54521964 0.54694229 0.3875969  0.49009475 0.52282515 0.42721792\n",
            " 0.49009475 0.63393626 0.43755383 0.51162791 0.54866494 0.35400517\n",
            " 0.52282515 0.53919035 0.60637382 0.51162791 0.57622739 0.55986219\n",
            " 0.56761413 0.30749354 0.5667528  0.53660637 0.49784668 0.61843239\n",
            " 0.56330749 0.57364341 0.65288544 0.43410853 0.45908699 0.33505599\n",
            " 0.61584841 0.28251507 0.3910422  0.43755383 0.51421189 0.48406546\n",
            " 0.63479759 0.47114556 0.39793282 0.50129199 0.45305771 0.41257537\n",
            " 0.54866494 0.52540913 0.43927649 0.52627046 0.72782084 0.82687339\n",
            " 0.39534884 0.58656331 0.63824289 0.47631352 0.59173127 0.5667528\n",
            " 0.43238587 0.58570198 0.48578811 0.45305771 0.39276486 0.62618432\n",
            " 0.49009475 0.62187769 0.62101637 0.52799311 0.45391904 0.37295435\n",
            " 0.62704565 0.44702842 0.57708872 0.43841516 0.48320413 0.46511628\n",
            " 0.66063738 0.63049096 0.44530577 0.47975883 0.6873385  0.48664944\n",
            " 0.47114556 0.57708872 0.70456503 0.52799311 0.52971576 0.41946598\n",
            " 0.52024117 0.42980189 0.58914729 0.50732127 0.66149871 0.48234281\n",
            " 0.43238587 0.48751077 0.45305771 0.4952627  0.53574505 0.70542636\n",
            " 0.57622739 0.55297158 0.64082687 0.62015504 0.49354005 0.40826873\n",
            " 0.54521964 0.60809647 0.46339363 0.65374677 0.4005168  0.49354005\n",
            " 0.47028424 0.50387597 0.53229974 0.34625323 0.45822567 0.46597761\n",
            " 0.40568475 0.57450474 0.50387597 0.55986219 0.5047373  0.72265289\n",
            " 0.41946598 0.45305771 0.57536606 0.42204996 0.44358312 0.26614987\n",
            " 0.41085271 0.54952627 0.6089578  0.56933678 0.43152455 0.47114556\n",
            " 0.54866494 0.57536606 0.4039621  0.4918174  0.48923342 0.56933678\n",
            " 0.43927649 0.65546942 0.54091301 0.416882   0.46511628 0.34797588\n",
            " 0.32213609 0.60551249 0.47459087 0.53574505 0.5667528  0.38070629\n",
            " 0.48664944 0.55900086 0.58914729 0.5796727  0.55727821 0.48062016\n",
            " 0.51335056 0.44272179 0.50215332 0.5960379  0.68819983 0.36950904\n",
            " 0.59431525 0.43583118 0.48234281 0.47717485 0.48492679 0.44013781\n",
            " 0.54435831 0.41429802 0.40999139 0.46167097 0.56072351 0.53229974\n",
            " 0.59345392 0.41860465 0.33936262 0.4754522  0.51421189 0.51937984\n",
            " 0.57450474 0.43755383 0.5245478  0.41343669 0.51851852 0.47459087\n",
            " 0.32127476 0.52024117 0.54263566 0.40137812 0.54091301 0.46339363\n",
            " 0.58914729 0.58225668 0.42807924 0.5245478  0.56589147 0.45219638\n",
            " 0.49440138 0.53143842 0.58656331 0.47717485 0.6089578  0.61498708\n",
            " 0.48148148 0.60378984 0.50215332 0.54177433 0.6124031  0.4203273\n",
            " 0.60292851 0.55211025 0.65891473 0.63738157 0.55727821 0.72437554\n",
            " 0.36003445 0.51765719 0.44702842 0.35400517 0.53574505 0.44358312\n",
            " 0.40137812 0.40568475 0.64513351 0.46511628 0.63221361 0.54694229\n",
            " 0.47803618 0.43496985 0.48062016 0.63824289 0.34969854 0.56933678\n",
            " 0.64857881 0.68044789 0.32816537 0.41085271 0.65719208 0.72782084\n",
            " 0.55211025 0.44875108 0.4625323  0.57622739 0.5374677  0.43066322\n",
            " 0.62360034 0.374677   0.6089578  0.36864772 0.40310078 0.59689922\n",
            " 0.51937984 0.48837209 0.4918174  0.38845823 0.4918174  0.58656331\n",
            " 0.51421189 0.53316107 0.58570198 0.35486649 0.55986219 0.42894057\n",
            " 0.51937984 0.56330749 0.63738157 0.4625323  0.3910422  0.4952627\n",
            " 0.50990525 0.56847545 0.35745047 0.36864772 0.54780362 0.58656331\n",
            " 0.52799311 0.54263566 0.45736434 0.54866494 0.51162791 0.4918174\n",
            " 0.47200689 0.33505599 0.66063738 0.36003445 0.42635659 0.53919035\n",
            " 0.45994832 0.66838932 0.56503015 0.41602067 0.37639966 0.39276486\n",
            " 0.37898363 0.5796727  0.47114556 0.59776055 0.55813953 0.52282515\n",
            " 0.4918174  0.64857881 0.56847545 0.50387597 0.46339363 0.53316107\n",
            " 0.48923342 0.33936262 0.54177433 0.68217054 0.49784668 0.59259259\n",
            " 0.46856158 0.54349699 0.62704565 0.64857881 0.41774332 0.52540913\n",
            " 0.51335056 0.56158484 0.4496124  0.58656331 0.46942291 0.52540913\n",
            " 0.37209302 0.38931955 0.55986219 0.50904393 0.57105943 0.6744186\n",
            " 0.45391904 1.         0.59259259 0.44702842 0.45822567 0.43669251\n",
            " 0.45822567 0.42549526 0.5503876  0.55469423 0.62704565 0.55211025\n",
            " 0.58484065 0.52196382 0.30577089 0.47200689 0.50043066 0.49095607\n",
            " 0.5667528  0.41343669 0.51248923 0.52885444 0.47028424 0.55641688\n",
            " 0.54866494 0.66838932 0.44272179 0.54263566 0.42807924 0.50732127\n",
            " 0.49095607 0.60206718 0.61584841 0.65374677 0.60981912 0.61154177\n",
            " 0.63565891 0.51507321 0.70887166 0.54521964 0.40740741 0.6089578\n",
            " 0.53919035 0.48751077 0.43066322 0.53402239 0.55469423 0.51937984\n",
            " 0.66063738 0.56761413 0.49267873 0.59086994 0.61498708 0.48492679\n",
            " 0.56158484 0.43496985 0.5047373  0.43927649 0.55900086 0.46683893\n",
            " 0.48492679 0.52196382 0.48923342 0.56330749 0.5211025  0.60378984\n",
            " 0.45047373]\n",
            "The trained model has an aproximate error rate of 2.9357345872312615 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others\n"
      ],
      "metadata": {
        "id": "9_3Jr46KHnik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Neural Network (DNN)"
      ],
      "metadata": {
        "id": "xIvme6cXJF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precipition (prcp)"
      ],
      "metadata": {
        "id": "cixnwflQxNZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6bb61e-39cb-409e-fca3-04053d868fe1",
        "id": "F9YrBdhTYhyE"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp_dnn = df.drop(columns=['temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] != 2012]\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] < 2020]\n",
        "cols = df_prcp_dnn['NUM_COLLISIONS']\n",
        "df_prcp_dnn = df_prcp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp_dnn.insert(loc=25, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp_dnn[:6])\n",
        "df_prcp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "jw5elr-LbRBF",
        "outputId": "fc922257-55bd-40dc-d002-1ab25bd4e5e4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  Dec  \\\n",
            "49  2016  28  0.09    0             0                 0     0    0    0    0   \n",
            "51  2014  17  0.00    1             0                 0     0    0    0    0   \n",
            "54  2016  25  0.02    0             0                 0     0    0    0    0   \n",
            "55  2016  29  0.00    0             0                 0     0    0    0    0   \n",
            "58  2017  20  0.00    0             0                 0     0    0    0    0   \n",
            "59  2013  13  0.01    1             0                 0     0    0    0    0   \n",
            "\n",
            "    ...  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49  ...    0    0    0    0    0    0    0    0    1             681  \n",
            "51  ...    0    0    0    0    0    0    1    0    0             589  \n",
            "54  ...    0    0    0    0    0    1    0    0    0             658  \n",
            "55  ...    0    0    0    0    0    0    1    0    0             645  \n",
            "58  ...    0    0    0    0    0    0    1    0    0             605  \n",
            "59  ...    0    0    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         prcp          fog  rain_drizzle  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000   2539.000000   \n",
              "mean   2015.989366    15.745569     0.122588     0.253249      0.375345   \n",
              "std       1.996126     8.803199     0.329143     0.434958      0.484307   \n",
              "min    2013.000000     1.000000     0.000000     0.000000      0.000000   \n",
              "25%    2014.000000     8.000000     0.000000     0.000000      0.000000   \n",
              "50%    2016.000000    16.000000     0.000000     0.000000      0.000000   \n",
              "75%    2018.000000    23.000000     0.060000     1.000000      1.000000   \n",
              "max    2019.000000    31.000000     3.760000     1.000000      1.000000   \n",
              "\n",
              "       snow_ice_pellets         hail          Apr          Aug          Dec  \\\n",
              "count       2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean           0.085467     0.000394     0.082316     0.083497     0.085467   \n",
              "std            0.279630     0.019846     0.274899     0.276687     0.279630   \n",
              "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "       ...          Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  ...  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean   ...     0.082710     0.085467     0.079953     0.142970     0.143364   \n",
              "std    ...     0.275497     0.279630     0.271273     0.350111     0.350512   \n",
              "min    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max    ...     1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000     2539.000000  \n",
              "mean      0.143757     0.142182     0.142576     0.142182      599.135093  \n",
              "std       0.350913     0.349305     0.349709     0.349305      100.299164  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8815e29-692e-40c6-acf1-b2bec7205a58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.989366</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.082316</td>\n",
              "      <td>0.083497</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.079953</td>\n",
              "      <td>0.142970</td>\n",
              "      <td>0.143364</td>\n",
              "      <td>0.143757</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>0.142576</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.996126</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>0.274899</td>\n",
              "      <td>0.276687</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275497</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.271273</td>\n",
              "      <td>0.350111</td>\n",
              "      <td>0.350512</td>\n",
              "      <td>0.350913</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>0.349709</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8815e29-692e-40c6-acf1-b2bec7205a58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8815e29-692e-40c6-acf1-b2bec7205a58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8815e29-692e-40c6-acf1-b2bec7205a58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp_dnn.iloc[np.random.permutation(len(df_prcp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpJi3P_8YcIq",
        "outputId": "28d257bb-7008-4d91-ddf5-7b31efead9a1"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  \\\n",
            "2529  2017  27  0.00    1             0                 0     0    0    0   \n",
            "3057  2014   8  0.07    1             1                 0     0    0    0   \n",
            "751   2019   6  0.00    0             0                 0     0    0    0   \n",
            "2919  2016  30  0.00    0             1                 0     0    0    0   \n",
            "3454  2013   1  0.00    0             1                 0     0    0    0   \n",
            "993   2019  17  0.00    0             0                 0     0    1    0   \n",
            "\n",
            "      Dec  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2529    0  ...    0    0    0    1    0    0    0    0    1    0  \n",
            "3057    0  ...    0    0    1    0    0    0    0    0    1    0  \n",
            "751     0  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "2919    0  ...    0    0    1    0    0    1    0    0    0    0  \n",
            "3454    1  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "993     0  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_A0EFvxZpeA",
        "outputId": "769b75d8-7e34-4538-cd3f-8e3392b865b7"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2529    696\n",
            "3057    639\n",
            "751     589\n",
            "2919    557\n",
            "3454    420\n",
            "993     621\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdO_kvOZZuii",
        "outputId": "f1a59ebd-45bb-4e61-c412-9643b71ecadb"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaTuIaPRfMxx",
        "outputId": "7b60ce7d-f0eb-44fa-c8c0-39deeb97c162"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99cba52d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 5459.094, step = 1\n",
            "INFO:tensorflow:global_step/sec: 402.258\n",
            "INFO:tensorflow:loss = 0.01564254, step = 101 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.193\n",
            "INFO:tensorflow:loss = 0.012573941, step = 201 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.106\n",
            "INFO:tensorflow:loss = 0.014447069, step = 301 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 533.257\n",
            "INFO:tensorflow:loss = 0.010319462, step = 401 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.588\n",
            "INFO:tensorflow:loss = 0.012983334, step = 501 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 472.578\n",
            "INFO:tensorflow:loss = 0.014266018, step = 601 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 427.622\n",
            "INFO:tensorflow:loss = 0.009516504, step = 701 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.357\n",
            "INFO:tensorflow:loss = 0.007387312, step = 801 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.515\n",
            "INFO:tensorflow:loss = 0.017294548, step = 901 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.901\n",
            "INFO:tensorflow:loss = 0.006931669, step = 1001 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.585\n",
            "INFO:tensorflow:loss = 0.0077541173, step = 1101 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 426.136\n",
            "INFO:tensorflow:loss = 0.026445953, step = 1201 (0.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.374\n",
            "INFO:tensorflow:loss = 0.013224203, step = 1301 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 446.578\n",
            "INFO:tensorflow:loss = 0.008016972, step = 1401 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.781\n",
            "INFO:tensorflow:loss = 0.007466113, step = 1501 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 583.835\n",
            "INFO:tensorflow:loss = 0.008482128, step = 1601 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 556.245\n",
            "INFO:tensorflow:loss = 0.019117843, step = 1701 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.427\n",
            "INFO:tensorflow:loss = 0.073699825, step = 1801 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.442\n",
            "INFO:tensorflow:loss = 0.0058398796, step = 1901 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.472\n",
            "INFO:tensorflow:loss = 0.004957799, step = 2001 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.867\n",
            "INFO:tensorflow:loss = 0.010887669, step = 2101 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 544.363\n",
            "INFO:tensorflow:loss = 0.01097423, step = 2201 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.699\n",
            "INFO:tensorflow:loss = 0.0047699567, step = 2301 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.181\n",
            "INFO:tensorflow:loss = 0.005232501, step = 2401 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.276\n",
            "INFO:tensorflow:loss = 0.0074501843, step = 2501 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.325\n",
            "INFO:tensorflow:loss = 0.008735258, step = 2601 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.875\n",
            "INFO:tensorflow:loss = 0.013566669, step = 2701 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.067\n",
            "INFO:tensorflow:loss = 0.0068093417, step = 2801 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.754\n",
            "INFO:tensorflow:loss = 0.010632097, step = 2901 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.187\n",
            "INFO:tensorflow:loss = 0.0058496855, step = 3001 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.525\n",
            "INFO:tensorflow:loss = 0.008248696, step = 3101 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 465.734\n",
            "INFO:tensorflow:loss = 0.011371597, step = 3201 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.274\n",
            "INFO:tensorflow:loss = 0.029223245, step = 3301 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 444.114\n",
            "INFO:tensorflow:loss = 0.028075725, step = 3401 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.451\n",
            "INFO:tensorflow:loss = 0.0071421633, step = 3501 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 496.115\n",
            "INFO:tensorflow:loss = 0.0059181387, step = 3601 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 443.462\n",
            "INFO:tensorflow:loss = 0.010484092, step = 3701 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.627\n",
            "INFO:tensorflow:loss = 0.008637642, step = 3801 (0.281 sec)\n",
            "INFO:tensorflow:global_step/sec: 292.536\n",
            "INFO:tensorflow:loss = 0.006419408, step = 3901 (0.342 sec)\n",
            "INFO:tensorflow:global_step/sec: 318.68\n",
            "INFO:tensorflow:loss = 0.012934178, step = 4001 (0.316 sec)\n",
            "INFO:tensorflow:global_step/sec: 309.427\n",
            "INFO:tensorflow:loss = 0.016474484, step = 4101 (0.321 sec)\n",
            "INFO:tensorflow:global_step/sec: 307.389\n",
            "INFO:tensorflow:loss = 0.007037079, step = 4201 (0.325 sec)\n",
            "INFO:tensorflow:global_step/sec: 346.413\n",
            "INFO:tensorflow:loss = 0.004342857, step = 4301 (0.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.745\n",
            "INFO:tensorflow:loss = 0.0051071974, step = 4401 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 405.458\n",
            "INFO:tensorflow:loss = 0.006508012, step = 4501 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.809\n",
            "INFO:tensorflow:loss = 0.004073868, step = 4601 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.761\n",
            "INFO:tensorflow:loss = 0.006904333, step = 4701 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 363.148\n",
            "INFO:tensorflow:loss = 0.0051414734, step = 4801 (0.276 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.952\n",
            "INFO:tensorflow:loss = 0.004609462, step = 4901 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 387.543\n",
            "INFO:tensorflow:loss = 0.0038909472, step = 5001 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.016\n",
            "INFO:tensorflow:loss = 0.007988296, step = 5101 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 367.822\n",
            "INFO:tensorflow:loss = 0.005925323, step = 5201 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 365.005\n",
            "INFO:tensorflow:loss = 0.0065535028, step = 5301 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 382.964\n",
            "INFO:tensorflow:loss = 0.006452577, step = 5401 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 311.258\n",
            "INFO:tensorflow:loss = 0.0040222923, step = 5501 (0.325 sec)\n",
            "INFO:tensorflow:global_step/sec: 292.966\n",
            "INFO:tensorflow:loss = 0.0057930676, step = 5601 (0.338 sec)\n",
            "INFO:tensorflow:global_step/sec: 277.675\n",
            "INFO:tensorflow:loss = 0.004887349, step = 5701 (0.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 366.709\n",
            "INFO:tensorflow:loss = 0.004457403, step = 5801 (0.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.387\n",
            "INFO:tensorflow:loss = 0.0050751385, step = 5901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.236\n",
            "INFO:tensorflow:loss = 0.004436012, step = 6001 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.131\n",
            "INFO:tensorflow:loss = 0.006021985, step = 6101 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 544.091\n",
            "INFO:tensorflow:loss = 0.011279799, step = 6201 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 447.001\n",
            "INFO:tensorflow:loss = 0.0046060523, step = 6301 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 519.075\n",
            "INFO:tensorflow:loss = 0.0039777085, step = 6401 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.926\n",
            "INFO:tensorflow:loss = 0.005136337, step = 6501 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.505\n",
            "INFO:tensorflow:loss = 0.0065551777, step = 6601 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.153\n",
            "INFO:tensorflow:loss = 0.005246735, step = 6701 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.483\n",
            "INFO:tensorflow:loss = 0.005707517, step = 6801 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.125\n",
            "INFO:tensorflow:loss = 0.0053202156, step = 6901 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.707\n",
            "INFO:tensorflow:loss = 0.0050694766, step = 7001 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.93\n",
            "INFO:tensorflow:loss = 0.0039855307, step = 7101 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 489.663\n",
            "INFO:tensorflow:loss = 0.005265876, step = 7201 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 470.362\n",
            "INFO:tensorflow:loss = 0.007861228, step = 7301 (0.214 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.165\n",
            "INFO:tensorflow:loss = 0.0059312508, step = 7401 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.086\n",
            "INFO:tensorflow:loss = 0.0063011637, step = 7501 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.026\n",
            "INFO:tensorflow:loss = 0.0054975273, step = 7601 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.904\n",
            "INFO:tensorflow:loss = 0.0045534214, step = 7701 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 409.512\n",
            "INFO:tensorflow:loss = 0.007915977, step = 7801 (0.246 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.767\n",
            "INFO:tensorflow:loss = 0.005253304, step = 7901 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.114\n",
            "INFO:tensorflow:loss = 0.0061250655, step = 8001 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.482\n",
            "INFO:tensorflow:loss = 0.008965494, step = 8101 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.162\n",
            "INFO:tensorflow:loss = 0.008651151, step = 8201 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.331\n",
            "INFO:tensorflow:loss = 0.006417107, step = 8301 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.492\n",
            "INFO:tensorflow:loss = 0.007909734, step = 8401 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 575.674\n",
            "INFO:tensorflow:loss = 0.00707433, step = 8501 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 538.689\n",
            "INFO:tensorflow:loss = 0.0070860474, step = 8601 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 561\n",
            "INFO:tensorflow:loss = 0.0074913106, step = 8701 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.569\n",
            "INFO:tensorflow:loss = 0.0059457123, step = 8801 (0.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 432.701\n",
            "INFO:tensorflow:loss = 0.004528818, step = 8901 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 482.017\n",
            "INFO:tensorflow:loss = 0.0068152044, step = 9001 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.363\n",
            "INFO:tensorflow:loss = 0.014148693, step = 9101 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 483.033\n",
            "INFO:tensorflow:loss = 0.007851167, step = 9201 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 454.267\n",
            "INFO:tensorflow:loss = 0.004833995, step = 9301 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.43\n",
            "INFO:tensorflow:loss = 0.008106076, step = 9401 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.163\n",
            "INFO:tensorflow:loss = 0.005465091, step = 9501 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.569\n",
            "INFO:tensorflow:loss = 0.0048744315, step = 9601 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 519.752\n",
            "INFO:tensorflow:loss = 0.0035811365, step = 9701 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.35\n",
            "INFO:tensorflow:loss = 0.0064758174, step = 9801 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 500.22\n",
            "INFO:tensorflow:loss = 0.0046513025, step = 9901 (0.198 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.010137004.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 83.19526445276746\n",
            "Just using average = 599.2383062530773 has RMSE of 101.84652506854763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891a4352-c901-42b7-ec61-4ac4b7563a8f",
        "id": "Ld6baV60hPOs"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c02a650>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5185849  0.5309962  0.5191433  0.51249814 0.54601103 0.55444384\n",
            " 0.540207   0.50155026 0.49606127 0.5438419  0.5246221  0.5095525\n",
            " 0.5004417  0.5425191  0.5442233  0.4401011  0.5079435  0.5368232\n",
            " 0.53625894 0.5224062  0.51932055 0.5204955  0.5187052  0.525364\n",
            " 0.5145357  0.5248027  0.49972147 0.44550538 0.4923159  0.5230683\n",
            " 0.53095466 0.43399203 0.55286604 0.51337296 0.49383473 0.5277997\n",
            " 0.43590927 0.5147517  0.49224252 0.5415322  0.53711516 0.46420795\n",
            " 0.5021202  0.52774054 0.5479791  0.47053337 0.5150261  0.5766055\n",
            " 0.5244117  0.5454013  0.52833426 0.5122015  0.5113655  0.5259783\n",
            " 0.55864173 0.5432283  0.5341896  0.482867   0.49213266 0.43619227\n",
            " 0.43655252 0.48355663 0.4972222  0.51755255 0.48695058 0.52421856\n",
            " 0.511749   0.5399346  0.45272064 0.51803327 0.5284241  0.52026945\n",
            " 0.54353994 0.5543295  0.5280793  0.5172137  0.49829543 0.5373758\n",
            " 0.53211796 0.5152362  0.5297122  0.51960576 0.49036413 0.43316674\n",
            " 0.5031002  0.5366728  0.52114356 0.50552875 0.5348181  0.47720975\n",
            " 0.5352263  0.5270633  0.53310424 0.51459926 0.5340914  0.49289066\n",
            " 0.5405514  0.49728268 0.5655263  0.58365655 0.522612   0.49717498\n",
            " 0.48401546 0.54831994 0.44766092 0.50726455 0.52359295 0.5160958\n",
            " 0.5157457  0.5420325  0.49233735 0.4999327  0.5287487  0.43083358\n",
            " 0.53189594 0.5156814  0.5742683  0.5122511  0.52601695 0.49466097\n",
            " 0.46712065 0.48917776 0.54867566 0.521635   0.48626804 0.53798425\n",
            " 0.5265344  0.50840396 0.50269693 0.52635485 0.5075442  0.4832698\n",
            " 0.5011142  0.55401504 0.5296262  0.54170406 0.5410096  0.54832625\n",
            " 0.5165551  0.49397796 0.51028234 0.53865117 0.51655143 0.5255469\n",
            " 0.49275166 0.5330668  0.494861   0.48470098 0.49496925 0.47837883\n",
            " 0.4478199  0.5270488  0.5085796  0.5069621  0.54915214 0.51581746\n",
            " 0.54102975 0.49312872 0.5263702  0.5123551  0.5329078  0.51441103\n",
            " 0.5212791  0.5261062  0.530135   0.48565686 0.5044578  0.504946\n",
            " 0.46432203 0.55851215 0.535494   0.51240635 0.5424962  0.52819765\n",
            " 0.45251584 0.5313244  0.5226478  0.52670616 0.5466911  0.53127676\n",
            " 0.5206611  0.5175123  0.5342013  0.51793003 0.5285386  0.47086066\n",
            " 0.5277687  0.49464285 0.51489925 0.5131292  0.55526465 0.52252674\n",
            " 0.5466542  0.47760576 0.5079319  0.44138497 0.45517862 0.468633\n",
            " 0.54038674 0.46583468 0.4563427  0.5759195  0.44518    0.49488282\n",
            " 0.495654   0.49331433 0.55410224 0.52203965 0.5503915  0.5443194\n",
            " 0.44866383 0.53098524 0.5316695  0.51355654 0.5203784  0.45616698\n",
            " 0.5336492  0.5341812  0.58114254 0.46415704 0.5138451  0.48548234\n",
            " 0.44944865 0.5363752  0.5318968  0.5196857  0.44864666 0.5242764\n",
            " 0.529875   0.545281   0.56145984 0.5288624  0.53607273 0.5210221\n",
            " 0.4584071  0.53828824 0.512377   0.51694065 0.5306118  0.5218581\n",
            " 0.5022963  0.5260068  0.51579356 0.52507335 0.45818573 0.5446776\n",
            " 0.43431735 0.51206344 0.5210025  0.5536821  0.53192145 0.5108857\n",
            " 0.49558395 0.4605177  0.5318627  0.5423725  0.5588361  0.52986073\n",
            " 0.51908034 0.5177902  0.5200549  0.518273   0.5349133  0.5325589\n",
            " 0.5170737  0.5157376  0.53470194 0.5798361  0.5010361  0.52091455\n",
            " 0.5147331  0.50523573 0.5256544  0.4464913  0.5537051  0.53333074\n",
            " 0.50249267 0.53956574 0.5112287  0.44715154 0.48133647 0.43229753\n",
            " 0.5446086  0.5346165  0.5352562  0.53235483 0.5264056  0.5266559\n",
            " 0.54558533 0.5362789  0.52070886 0.55662    0.50681627 0.5513641\n",
            " 0.5379489  0.54396605 0.5061729  0.53486705 0.5573291  0.52571726\n",
            " 0.55119145 0.5120686  0.46550047 0.5126207  0.52875656 0.4831769\n",
            " 0.52021366 0.47663605 0.50071144 0.52307254 0.44164765 0.5211463\n",
            " 0.5312123  0.51742107 0.49308372 0.5265092  0.54777354 0.53137475\n",
            " 0.58162916 0.5499088  0.5209066  0.5423863  0.5522018  0.51054215\n",
            " 0.46306223 0.5019167  0.50177014 0.5383509  0.50441015 0.5212967\n",
            " 0.55849326 0.5147537  0.5486242  0.5096896  0.5056199  0.47941\n",
            " 0.51993096 0.5314076  0.5373165  0.5635978  0.5386704  0.54366606\n",
            " 0.43303317 0.49849927 0.5245215  0.46739352 0.5427141  0.5117636\n",
            " 0.49134016 0.48987752 0.5150192  0.48065037 0.52651983 0.529017\n",
            " 0.53526384 0.5085766  0.524245   0.500158   0.53036916 0.519884\n",
            " 0.52748555 0.5332829  0.5151289  0.52392477 0.5031244  0.5326508\n",
            " 0.5337897  0.50908524 0.43064886 0.5702415  0.53484076 0.4987725\n",
            " 0.5306219  0.52511954 0.5608273  0.46049988 0.52306247 0.52204293\n",
            " 0.5670756  0.4488061  0.48638946 0.47794622 0.49771476 0.5232909\n",
            " 0.5464999  0.54419386 0.5105516  0.49389827 0.54927045 0.5472136\n",
            " 0.55036473 0.5357817  0.4568085  0.5601479  0.55671453 0.5125695\n",
            " 0.52075243 0.55248713 0.55171955 0.43004912 0.5149978  0.5269854\n",
            " 0.4763775  0.48788315 0.55435497 0.5110025  0.522741   0.5241675\n",
            " 0.5274366  0.58214396 0.53442407 0.5394673  0.5437821  0.49172074\n",
            " 0.47818303 0.51105875 0.50324076 0.53501606 0.46940237 0.55302817\n",
            " 0.50892246 0.5207537  0.4490435  0.4669432  0.55758005 0.43380934\n",
            " 0.53313965 0.52388215 0.43458062 0.5508179  0.54713047 0.5246364\n",
            " 0.53366834 0.45547462 0.5094018  0.559179   0.48497123 0.51825744\n",
            " 0.4839458  0.552084   0.43538278 0.5153535  0.49050963 0.5439715\n",
            " 0.5399903  0.5453301  0.52725714 0.49783826 0.5133063  0.51018286\n",
            " 0.446809   0.449975   0.49280965 0.50326407 0.53458583 0.5141148\n",
            " 0.46544927 0.5134205  0.5270272  0.52363354 0.5303833  0.5392667\n",
            " 0.5551912  0.54090196 0.5770885  0.5157496  0.54085046 0.4936779\n",
            " 0.5331289  0.5335664  0.5229743  0.4912163  0.4948728  0.48432797\n",
            " 0.525262   0.5243348  0.5227879  0.50886124 0.5205702  0.53939414\n",
            " 0.48720378 0.51500237 0.4883961  0.5240847  0.5012929  0.5600861\n",
            " 0.54959714 0.512595   0.4620337  0.5331488  0.46068335 0.50456977\n",
            " 0.51140034 0.5355731  0.52796286 0.5119183  0.54858226 0.46774125\n",
            " 0.53940797 0.48636228 0.5697817  0.5265704  0.51685786 0.52661973\n",
            " 0.5403798  0.4327259  0.4581734  0.52358043]\n",
            "[0.52368648 0.60206718 0.48148148 0.51679587 0.64857881 0.55813953\n",
            " 0.64857881 0.50990525 0.49612403 0.5374677  0.43583118 0.39276486\n",
            " 0.4203273  0.41860465 0.50215332 0.35400517 0.43496985 0.45047373\n",
            " 0.5667528  0.55555556 0.60551249 0.54521964 0.60637382 0.51851852\n",
            " 0.60378984 0.55211025 0.6124031  0.36692506 0.50301464 0.51937984\n",
            " 0.5667528  0.3910422  0.66666667 0.57536606 0.44358312 0.63135228\n",
            " 0.32213609 0.57536606 0.47975883 0.58484065 0.52282515 0.46942291\n",
            " 0.4788975  0.54694229 0.48148148 0.41343669 0.45822567 0.5960379\n",
            " 0.47028424 0.55641688 0.59689922 0.4952627  0.49698536 0.54608096\n",
            " 0.58742463 0.61843239 0.47631352 0.33505599 0.51765719 0.33850129\n",
            " 0.40913006 0.49354005 0.60206718 0.42807924 0.52971576 0.51593454\n",
            " 0.63049096 0.59345392 0.41429802 0.56244617 0.5211025  0.60206718\n",
            " 0.55986219 0.6873385  0.5667528  0.54694229 0.52024117 0.64857881\n",
            " 0.51679587 0.53229974 0.52368648 0.45564169 0.42721792 0.38329027\n",
            " 0.53832903 0.59259259 0.45219638 0.48320413 0.42721792 0.33419466\n",
            " 0.56330749 0.50904393 0.46511628 0.56933678 0.52971576 0.50301464\n",
            " 0.53574505 0.53057709 0.64944014 0.64685616 0.49440138 0.49956934\n",
            " 0.38673557 0.56761413 0.41257537 0.60034453 0.5538329  0.46080965\n",
            " 0.47459087 0.55555556 0.42894057 0.59431525 0.58914729 0.35658915\n",
            " 0.54694229 0.57795004 0.50301464 0.46683893 0.60292851 0.44875108\n",
            " 0.43410853 0.5667528  0.59776055 0.54091301 0.49009475 0.64857881\n",
            " 0.51507321 0.45908699 0.34022394 0.54263566 0.37898363 0.34625323\n",
            " 0.5796727  0.62015504 0.56330749 0.57536606 0.51937984 0.59086994\n",
            " 0.47286822 0.5211025  0.52799311 0.58139535 0.54521964 0.55469423\n",
            " 0.41085271 0.68647717 0.4918174  0.49784668 0.46942291 0.37812231\n",
            " 0.35486649 0.5667528  0.44875108 0.57105943 0.625323   0.43755383\n",
            " 0.63824289 0.38673557 0.45650301 0.54952627 0.31955211 0.44358312\n",
            " 0.59259259 0.57019811 0.52282515 0.54005168 0.56158484 0.51851852\n",
            " 0.40826873 0.55813953 0.59259259 0.54435831 0.5245478  0.54780362\n",
            " 0.42635659 0.5667528  0.55297158 0.4918174  0.58742463 0.47028424\n",
            " 0.53832903 0.47028424 0.5374677  0.47631352 0.5245478  0.50559862\n",
            " 0.64771748 0.48751077 0.50215332 0.49870801 0.63049096 0.54349699\n",
            " 0.56244617 0.36864772 0.55986219 0.35400517 0.34797588 0.40740741\n",
            " 0.73815676 0.44702842 0.416882   0.68130922 0.33850129 0.50387597\n",
            " 0.42635659 0.40137812 0.54349699 0.55813953 0.67011197 0.38415159\n",
            " 0.34366925 0.51593454 0.54177433 0.56072351 0.56158484 0.43496985\n",
            " 0.50301464 0.44530577 0.60981912 0.36089578 0.4203273  0.47286822\n",
            " 0.43238587 0.63738157 0.34453058 0.55900086 0.374677   0.37898363\n",
            " 0.63824289 0.49870801 0.56158484 0.55813953 0.57622739 0.56244617\n",
            " 0.42463394 0.5503876  0.39707149 0.51851852 0.58570198 0.55555556\n",
            " 0.58914729 0.38070629 0.54177433 0.56589147 0.47717485 0.57881137\n",
            " 0.46597761 0.52196382 0.47200689 0.58742463 0.57105943 0.48148148\n",
            " 0.52799311 0.38070629 0.50215332 0.59086994 0.46597761 0.51248923\n",
            " 0.51248923 0.54694229 0.47286822 0.5211025  0.50559862 0.36175711\n",
            " 0.48837209 0.51507321 0.53316107 0.72782084 0.16192937 0.53143842\n",
            " 0.32213609 0.47803618 0.5960379  0.38845823 0.51076658 0.5503876\n",
            " 0.45305771 0.55469423 0.45391904 0.416882   0.49698536 0.3875969\n",
            " 0.57622739 0.63393626 0.80878553 0.68475452 0.43152455 0.5538329\n",
            " 0.6287683  0.59862188 0.42291128 0.57019811 0.44444444 0.63996555\n",
            " 0.55727821 0.6089578  0.47372954 0.42291128 0.6873385  0.50129199\n",
            " 0.61757106 0.40826873 0.48062016 0.56503015 0.4918174  0.60292851\n",
            " 0.4788975  0.48751077 0.49956934 0.54521964 0.33419466 0.57450474\n",
            " 0.91731266 0.44272179 0.47459087 0.46425495 0.60723514 0.53316107\n",
            " 0.64513351 0.65633075 0.52799311 0.59862188 0.56158484 0.5211025\n",
            " 0.41774332 0.47200689 0.4203273  0.70801034 0.68217054 0.59259259\n",
            " 0.53488372 0.44702842 0.48492679 0.49354005 0.50990525 0.44358312\n",
            " 0.51507321 0.53316107 0.62618432 0.55727821 0.52799311 0.58656331\n",
            " 0.37812231 0.54349699 0.60637382 0.44099914 0.5503876  0.5047373\n",
            " 0.43669251 0.46080965 0.5211025  0.46597761 0.50387597 0.51421189\n",
            " 0.63652024 0.49870801 0.61412575 0.54005168 0.51765719 0.56933678\n",
            " 0.5994832  0.49095607 0.4952627  0.53316107 0.45305771 0.5994832\n",
            " 0.61498708 0.49612403 0.36692506 0.55297158 0.48406546 0.45305771\n",
            " 0.47200689 0.66666667 0.66838932 0.46942291 0.51507321 0.56589147\n",
            " 0.63824289 0.34797588 0.34280792 0.46856158 0.59259259 0.53057709\n",
            " 0.63135228 0.59776055 0.44702842 0.52540913 0.56330749 0.63652024\n",
            " 0.57364341 0.53402239 0.47114556 0.69939707 0.55986219 0.44702842\n",
            " 0.47459087 0.64513351 0.36606374 0.4005168  0.48923342 0.51851852\n",
            " 0.48062016 0.40913006 0.58656331 0.4918174  0.44530577 0.40913006\n",
            " 0.53402239 0.65202412 0.6744186  0.4918174  0.58397933 0.50990525\n",
            " 0.43927649 0.48751077 0.44702842 0.55641688 0.45564169 0.56847545\n",
            " 0.47286822 0.51162791 0.44272179 0.43066322 0.54694229 0.38501292\n",
            " 0.52024117 0.56416882 0.36003445 0.68044789 0.6546081  0.60981912\n",
            " 0.49354005 0.44099914 0.48664944 0.6580534  0.48148148 0.50732127\n",
            " 0.50559862 0.66063738 0.29371232 0.59345392 0.48148148 0.57364341\n",
            " 0.54694229 0.54952627 0.38156761 0.58570198 0.55900086 0.42291128\n",
            " 0.38845823 0.42118863 0.53832903 0.47717485 0.54866494 0.53402239\n",
            " 0.40999139 0.57536606 0.6416882  0.50904393 0.4918174  0.51851852\n",
            " 0.58139535 0.55555556 0.61154177 0.48664944 0.60206718 0.4203273\n",
            " 0.32213609 0.67011197 0.60292851 0.4625323  0.55297158 0.5081826\n",
            " 0.53229974 0.56847545 0.51593454 0.62187769 0.5374677  0.48148148\n",
            " 0.41429802 0.6546081  0.54091301 0.52282515 0.46597761 0.71490095\n",
            " 0.56761413 0.54005168 0.40137812 0.4918174  0.46339363 0.56933678\n",
            " 0.38931955 0.65374677 0.47631352 0.5047373  0.6089578  0.44358312\n",
            " 0.48751077 0.44272179 0.56847545 0.53919035 0.48664944 0.45908699\n",
            " 0.5081826  0.36950904 0.38501292 0.48406546]\n",
            "The trained model has an aproximate error rate of 0.7827748668006087 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "yT91LA7Xx77W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac59308-70b3-4381-d3af-b70466d8e7ae",
        "id": "zQAH_kVzyAOD"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp_dnn = df.drop(columns=['temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] != 2012]\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] < 2020]\n",
        "cols = df_dewp_dnn['NUM_COLLISIONS']\n",
        "df_dewp_dnn = df_dewp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp_dnn[:6])\n",
        "df_dewp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "4615c5af-9fea-493f-ab7a-dfd01203f8b8",
        "id": "_qzGppAwyAOE"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  24.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  35.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  21.2    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29  36.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  32.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  44.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         dewp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2015.999217    15.723679    44.163170     0.082192     0.084932   \n",
              "std       2.000000     8.801271    16.995303     0.274710     0.278834   \n",
              "min    2013.000000     1.000000    -6.700000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    32.150000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    45.300000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    58.500000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    74.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084932     0.077104     0.084932     0.084540     0.082192  ...   \n",
              "std       0.278834     0.266808     0.278834     0.278251     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142857      599.109980  \n",
              "std       0.349996     0.349996     0.349996     0.349996      100.277185  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61dff5f9-60e7-4d12-8548-1d516dfd9aa0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.999217</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61dff5f9-60e7-4d12-8548-1d516dfd9aa0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61dff5f9-60e7-4d12-8548-1d516dfd9aa0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61dff5f9-60e7-4d12-8548-1d516dfd9aa0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp_dnn.iloc[np.random.permutation(len(df_dewp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cda119-6be8-4c84-b5d9-c11eacc39a8c",
        "id": "z5-eCTxByAOE"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "3517  2017  17  13.8    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "1448  2018  27  53.4    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "2380  2016   6  67.5    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "2338  2015  30  66.8    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "3273  2019   9  21.8    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "190   2018  16  28.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3517    0    0    1    0    0    0    0  \n",
            "1448    0    0    1    0    0    0    0  \n",
            "2380    0    0    0    0    0    0    0  \n",
            "2338    0    0    1    0    0    0    0  \n",
            "3273    0    0    0    0    0    0    0  \n",
            "190     0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83a4812-487c-4646-bf0f-c6aab0c44adb",
        "id": "BbrOrPfQyAOE"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3517    547\n",
            "1448    487\n",
            "2380    578\n",
            "2338    563\n",
            "3273    502\n",
            "190     617\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e9f8d3-2fc1-4c52-fe62-a34a3c4ef510",
        "id": "T2v7BylMyAOE"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ca8cc4-626f-46cc-8082-4b48a6bbd141",
        "id": "HMGFsHtkyAOE"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fce9090>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 29.350513, step = 1\n",
            "INFO:tensorflow:global_step/sec: 344.184\n",
            "INFO:tensorflow:loss = 0.18778694, step = 101 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.029\n",
            "INFO:tensorflow:loss = 0.1630961, step = 201 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 463.569\n",
            "INFO:tensorflow:loss = 0.116834104, step = 301 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 477.675\n",
            "INFO:tensorflow:loss = 0.093510956, step = 401 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 566.45\n",
            "INFO:tensorflow:loss = 0.0945691, step = 501 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.967\n",
            "INFO:tensorflow:loss = 0.053805728, step = 601 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.036\n",
            "INFO:tensorflow:loss = 0.046030343, step = 701 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 561.291\n",
            "INFO:tensorflow:loss = 0.031384014, step = 801 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.317\n",
            "INFO:tensorflow:loss = 0.023837695, step = 901 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 533.348\n",
            "INFO:tensorflow:loss = 0.013523681, step = 1001 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.029\n",
            "INFO:tensorflow:loss = 0.011359704, step = 1101 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.935\n",
            "INFO:tensorflow:loss = 0.007828165, step = 1201 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.322\n",
            "INFO:tensorflow:loss = 0.007999334, step = 1301 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.773\n",
            "INFO:tensorflow:loss = 0.006736315, step = 1401 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.327\n",
            "INFO:tensorflow:loss = 0.007757576, step = 1501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.549\n",
            "INFO:tensorflow:loss = 0.0051688417, step = 1601 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.294\n",
            "INFO:tensorflow:loss = 0.006536251, step = 1701 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.188\n",
            "INFO:tensorflow:loss = 0.010762904, step = 1801 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.781\n",
            "INFO:tensorflow:loss = 0.00887784, step = 1901 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.916\n",
            "INFO:tensorflow:loss = 0.0058892854, step = 2001 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.225\n",
            "INFO:tensorflow:loss = 0.00570409, step = 2101 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.799\n",
            "INFO:tensorflow:loss = 0.0068884525, step = 2201 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.513\n",
            "INFO:tensorflow:loss = 0.0071930573, step = 2301 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 472.762\n",
            "INFO:tensorflow:loss = 0.0076954067, step = 2401 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 445.962\n",
            "INFO:tensorflow:loss = 0.009076111, step = 2501 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.376\n",
            "INFO:tensorflow:loss = 0.011479251, step = 2601 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.04\n",
            "INFO:tensorflow:loss = 0.009566282, step = 2701 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 559.773\n",
            "INFO:tensorflow:loss = 0.006585739, step = 2801 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.663\n",
            "INFO:tensorflow:loss = 0.012188654, step = 2901 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.752\n",
            "INFO:tensorflow:loss = 0.0063630873, step = 3001 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.641\n",
            "INFO:tensorflow:loss = 0.0053941673, step = 3101 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.025\n",
            "INFO:tensorflow:loss = 0.26400405, step = 3201 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.964\n",
            "INFO:tensorflow:loss = 0.09084116, step = 3301 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 586.559\n",
            "INFO:tensorflow:loss = 0.20885001, step = 3401 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 601.521\n",
            "INFO:tensorflow:loss = 0.06789447, step = 3501 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.259\n",
            "INFO:tensorflow:loss = 0.06161342, step = 3601 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.169\n",
            "INFO:tensorflow:loss = 0.09705219, step = 3701 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 500.938\n",
            "INFO:tensorflow:loss = 0.024701975, step = 3801 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 499.342\n",
            "INFO:tensorflow:loss = 0.12905177, step = 3901 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.048\n",
            "INFO:tensorflow:loss = 0.30419934, step = 4001 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.348\n",
            "INFO:tensorflow:loss = 0.0065064165, step = 4101 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.643\n",
            "INFO:tensorflow:loss = 0.021021318, step = 4201 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 475.403\n",
            "INFO:tensorflow:loss = 0.043324247, step = 4301 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.807\n",
            "INFO:tensorflow:loss = 0.021269666, step = 4401 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 483.976\n",
            "INFO:tensorflow:loss = 0.10665577, step = 4501 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 440.293\n",
            "INFO:tensorflow:loss = 0.28237092, step = 4601 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.193\n",
            "INFO:tensorflow:loss = 0.19239622, step = 4701 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 544.921\n",
            "INFO:tensorflow:loss = 0.041164838, step = 4801 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.342\n",
            "INFO:tensorflow:loss = 0.20589153, step = 4901 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.65\n",
            "INFO:tensorflow:loss = 0.04994441, step = 5001 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.598\n",
            "INFO:tensorflow:loss = 0.0044282377, step = 5101 (0.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.84\n",
            "INFO:tensorflow:loss = 0.07555128, step = 5201 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.511\n",
            "INFO:tensorflow:loss = 0.021565141, step = 5301 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.178\n",
            "INFO:tensorflow:loss = 0.09831436, step = 5401 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.556\n",
            "INFO:tensorflow:loss = 0.005852594, step = 5501 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.205\n",
            "INFO:tensorflow:loss = 0.0064243833, step = 5601 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 464.659\n",
            "INFO:tensorflow:loss = 0.0052964864, step = 5701 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.917\n",
            "INFO:tensorflow:loss = 0.007885405, step = 5801 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.247\n",
            "INFO:tensorflow:loss = 0.064998984, step = 5901 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.245\n",
            "INFO:tensorflow:loss = 0.073701456, step = 6001 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.43\n",
            "INFO:tensorflow:loss = 0.004508093, step = 6101 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.372\n",
            "INFO:tensorflow:loss = 0.03905434, step = 6201 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.073\n",
            "INFO:tensorflow:loss = 0.07799671, step = 6301 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.277\n",
            "INFO:tensorflow:loss = 0.0046293354, step = 6401 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.6\n",
            "INFO:tensorflow:loss = 0.005398999, step = 6501 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.695\n",
            "INFO:tensorflow:loss = 0.041033078, step = 6601 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.433\n",
            "INFO:tensorflow:loss = 0.055107016, step = 6701 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.019\n",
            "INFO:tensorflow:loss = 0.008587794, step = 6801 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 575.68\n",
            "INFO:tensorflow:loss = 0.15053274, step = 6901 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.96\n",
            "INFO:tensorflow:loss = 0.0048348247, step = 7001 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.23\n",
            "INFO:tensorflow:loss = 0.03275532, step = 7101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.452\n",
            "INFO:tensorflow:loss = 0.005869514, step = 7201 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.896\n",
            "INFO:tensorflow:loss = 0.018106136, step = 7301 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.043\n",
            "INFO:tensorflow:loss = 0.00727835, step = 7401 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 492.845\n",
            "INFO:tensorflow:loss = 0.10168059, step = 7501 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.78\n",
            "INFO:tensorflow:loss = 0.01676357, step = 7601 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 424.452\n",
            "INFO:tensorflow:loss = 0.010811932, step = 7701 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 474.331\n",
            "INFO:tensorflow:loss = 0.008300676, step = 7801 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.691\n",
            "INFO:tensorflow:loss = 0.0042462964, step = 7901 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 461.816\n",
            "INFO:tensorflow:loss = 0.003335096, step = 8001 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.031\n",
            "INFO:tensorflow:loss = 0.0072157113, step = 8101 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.39\n",
            "INFO:tensorflow:loss = 0.0046946304, step = 8201 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.754\n",
            "INFO:tensorflow:loss = 0.0100792395, step = 8301 (0.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.727\n",
            "INFO:tensorflow:loss = 0.01278683, step = 8401 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 520.426\n",
            "INFO:tensorflow:loss = 0.005726206, step = 8501 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.397\n",
            "INFO:tensorflow:loss = 0.004543048, step = 8601 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.438\n",
            "INFO:tensorflow:loss = 0.0055453693, step = 8701 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.153\n",
            "INFO:tensorflow:loss = 0.008773924, step = 8801 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.416\n",
            "INFO:tensorflow:loss = 0.01297421, step = 8901 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.64\n",
            "INFO:tensorflow:loss = 0.004720561, step = 9001 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.781\n",
            "INFO:tensorflow:loss = 0.007111546, step = 9101 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.917\n",
            "INFO:tensorflow:loss = 0.025013197, step = 9201 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.846\n",
            "INFO:tensorflow:loss = 0.009510731, step = 9301 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 566.796\n",
            "INFO:tensorflow:loss = 0.017665915, step = 9401 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.446\n",
            "INFO:tensorflow:loss = 0.014352083, step = 9501 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 419.282\n",
            "INFO:tensorflow:loss = 0.01406749, step = 9601 (0.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 426.158\n",
            "INFO:tensorflow:loss = 0.009869041, step = 9701 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 453.038\n",
            "INFO:tensorflow:loss = 0.0077151265, step = 9801 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 426.442\n",
            "INFO:tensorflow:loss = 0.0064104977, step = 9901 (0.234 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0056710667.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 185.74080399524615\n",
            "Just using average = 598.4916829745597 has RMSE of 101.7999841627072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19bc2209-098b-4ce7-e188-4d10afaa8bc3",
        "id": "NCz6izVhyAOF"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c625750>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.40849167 0.44215918 0.37407708 0.4081313  0.3998119  0.3918956\n",
            " 0.3726808  0.36164984 0.35037935 0.3862851  0.4267058  0.39134428\n",
            " 0.30307537 0.3951958  0.38250843 0.38288587 0.38121256 0.42358023\n",
            " 0.39098942 0.32479596 0.32256114 0.38163537 0.31474298 0.4147664\n",
            " 0.41180912 0.27140895 0.43794987 0.34250298 0.30274653 0.38929245\n",
            " 0.30164096 0.3575675  0.3681231  0.34205338 0.33789164 0.40898722\n",
            " 0.29605636 0.28531247 0.32907474 0.3401113  0.3421184  0.37710273\n",
            " 0.39944243 0.3238937  0.38000247 0.40640464 0.31919488 0.39959532\n",
            " 0.3822829  0.42865962 0.40585518 0.41161317 0.41226757 0.36980802\n",
            " 0.3503192  0.32974908 0.39536592 0.40672153 0.3765538  0.37783968\n",
            " 0.35563898 0.396242   0.4327144  0.38217294 0.429427   0.43680263\n",
            " 0.4520945  0.37521145 0.42123803 0.39553756 0.34904563 0.3435706\n",
            " 0.27222505 0.4069332  0.29447147 0.32504317 0.34765205 0.35422838\n",
            " 0.39336738 0.30587947 0.28249878 0.28717044 0.4049695  0.31244603\n",
            " 0.32807747 0.4132818  0.46986586 0.40407854 0.39883104 0.3911832\n",
            " 0.33519182 0.38714495 0.4101517  0.41345173 0.36168924 0.33962518\n",
            " 0.3696019  0.39189625 0.40040165 0.40830046 0.35813016 0.45196337\n",
            " 0.3538174  0.43860018 0.39113176 0.435584   0.28138027 0.4001491\n",
            " 0.30947578 0.32351995 0.38128814 0.35578626 0.32613075 0.33631143\n",
            " 0.38276225 0.30569053 0.40863004 0.413588   0.3694998  0.34068504\n",
            " 0.42720732 0.3335686  0.37818688 0.38129732 0.44666153 0.40789038\n",
            " 0.41555482 0.40565643 0.24651735 0.30506527 0.42231673 0.36863387\n",
            " 0.36938927 0.39412567 0.3741134  0.43992788 0.3827747  0.39292273\n",
            " 0.40196836 0.3425867  0.32525185 0.37427834 0.35049942 0.4337472\n",
            " 0.43182683 0.28989536 0.36267412 0.38148877 0.33314705 0.41758877\n",
            " 0.44320846 0.38315588 0.43087557 0.4392917  0.30201545 0.38347724\n",
            " 0.3898534  0.4141459  0.2735576  0.44699132 0.4375784  0.3760901\n",
            " 0.36896858 0.40717915 0.39697582 0.3028703  0.36837152 0.36383688\n",
            " 0.38894328 0.4054763  0.4046713  0.34146428 0.4234221  0.2981742\n",
            " 0.37819967 0.33075064 0.34229547 0.2629366  0.3785848  0.34853914\n",
            " 0.3777045  0.3201846  0.28325757 0.27614102 0.44680715 0.44170496\n",
            " 0.3620724  0.37931877 0.41024256 0.41153944 0.38658118 0.41927877\n",
            " 0.37835643 0.4014509  0.41282552 0.42101216 0.33941916 0.35140142\n",
            " 0.37677032 0.3558974  0.30510655 0.3513807  0.30900604 0.277169\n",
            " 0.42216325 0.39379838 0.3647985  0.4030034  0.280088   0.4314385\n",
            " 0.39449048 0.34534517 0.35924447 0.39940202 0.43914652 0.3004888\n",
            " 0.33724076 0.33485398 0.35936788 0.3147969  0.39302024 0.4199015\n",
            " 0.36086228 0.37890932 0.3601236  0.34393385 0.3894015  0.35294\n",
            " 0.40400475 0.2675617  0.40309855 0.4168891  0.3185589  0.42702994\n",
            " 0.43026185 0.35079336 0.4243034  0.39218277 0.35551068 0.41796526\n",
            " 0.38593012 0.45158032 0.4407973  0.35200718 0.34390396 0.44041154\n",
            " 0.33759257 0.4101445  0.38317698 0.27341294 0.3371805  0.3685791\n",
            " 0.39237857 0.3961503  0.3996949  0.40614998 0.3275215  0.4044565\n",
            " 0.32559216 0.37739697 0.43099487 0.4345063  0.3976448  0.3488976\n",
            " 0.3684576  0.42517537 0.31147474 0.38219833 0.39383695 0.4055457\n",
            " 0.35921845 0.40169162 0.36879203 0.30858493 0.3294986  0.39196137\n",
            " 0.37655175 0.41673258 0.44705    0.4734208  0.393008   0.41184095\n",
            " 0.3733735  0.24729653 0.3669585  0.3657591  0.27794743 0.40373555\n",
            " 0.3728111  0.3292763  0.3835975  0.38906148 0.3884285  0.408303\n",
            " 0.3954826  0.30570072 0.3923437  0.3973287  0.35886967 0.40397263\n",
            " 0.2557045  0.3805412  0.30735272 0.38246784 0.26498955 0.37896279\n",
            " 0.36874607 0.36551648 0.334418   0.4080392  0.4097354  0.3888842\n",
            " 0.35894072 0.33291197 0.42565167 0.37703285 0.35008156 0.27783233\n",
            " 0.41968697 0.3417205  0.39251292 0.32625353 0.3882415  0.39822\n",
            " 0.4422834  0.34605002 0.3803193  0.38776472 0.27722716 0.4000959\n",
            " 0.34725547 0.31359127 0.40592098 0.36611992 0.37462783 0.36786482\n",
            " 0.4192753  0.39961663 0.38027263 0.4188592  0.40490168 0.3682288\n",
            " 0.4070421  0.3934498  0.4158991  0.43630996 0.39833307 0.35636947\n",
            " 0.41370225 0.34802148 0.4385362  0.36906362 0.27167627 0.3772243\n",
            " 0.4296444  0.34027046 0.3119461  0.4072846  0.36768553 0.38577223\n",
            " 0.3160306  0.2583314  0.39587077 0.34500107 0.41938627 0.3125375\n",
            " 0.3602222  0.44018945 0.42986092 0.41002536 0.35362208 0.4254\n",
            " 0.37928072 0.3530981  0.3713505  0.38450593 0.32201344 0.4370053\n",
            " 0.34301892 0.36083785 0.31251025 0.3486352  0.38964224 0.31719247\n",
            " 0.37929302 0.3810767  0.36162272 0.35818425 0.3665791  0.39646578\n",
            " 0.43151456 0.43868864 0.3906166  0.40662655 0.42299828 0.3047852\n",
            " 0.33183926 0.31512958 0.4200664  0.4091379  0.37450442 0.29746187\n",
            " 0.30613527 0.39570943 0.41092423 0.37058136 0.31045905 0.31529823\n",
            " 0.40246537 0.38307253 0.40855345 0.34211963 0.4125146  0.3721014\n",
            " 0.38032842 0.42226928 0.44444847 0.4310907  0.34803063 0.37494963\n",
            " 0.3838751  0.4061242  0.36856684 0.39718208 0.30844355 0.4146263\n",
            " 0.40210164 0.40445924 0.42755765 0.41925275 0.34028575 0.27549452\n",
            " 0.28867397 0.39035866 0.40801877 0.40962198 0.41691792 0.40214616\n",
            " 0.42830345 0.37806618 0.38898814 0.37945154 0.38190496 0.35442862\n",
            " 0.33513814 0.37012374 0.36971286 0.39234197 0.3899262  0.40906325\n",
            " 0.3917619  0.35589528 0.37237313 0.39563608 0.31151503 0.44224373\n",
            " 0.30806506 0.36617243 0.4289711  0.32760972 0.31108916 0.36377224\n",
            " 0.4218662  0.3312619  0.37246305 0.36745474 0.39906237 0.46632904\n",
            " 0.28652522 0.47796345 0.44344866 0.41629001 0.35329142 0.31759068\n",
            " 0.40456712 0.4063843  0.3824721  0.31715333 0.41913122 0.3198679\n",
            " 0.4320352  0.37611067 0.3747991  0.3885104  0.39519605 0.39024526\n",
            " 0.42422932 0.40299308 0.41351873 0.36539748 0.35129562 0.35984418\n",
            " 0.41904208 0.39234135 0.43031487 0.3993202  0.3839856  0.36392805\n",
            " 0.40701854 0.39470375 0.32204473 0.3605818  0.38310868 0.25128928\n",
            " 0.41117984 0.3406295  0.41704282 0.4472807  0.34455088 0.35559607\n",
            " 0.425686  ]\n",
            "[0.56503015 0.61929371 0.50645995 0.45822567 0.49354005 0.64857881\n",
            " 0.53660637 0.49870801 0.37639966 0.48492679 0.62015504 0.69422911\n",
            " 0.4461671  0.59345392 0.52282515 0.80878553 0.5211025  0.58914729\n",
            " 0.54694229 0.49354005 0.4788975  0.45564169 0.43410853 0.51937984\n",
            " 0.56416882 0.27562446 0.58656331 0.48923342 0.48148148 0.47631352\n",
            " 0.33936262 0.58484065 0.46511628 0.44530577 0.51765719 0.51248923\n",
            " 0.46425495 0.39534884 0.37812231 0.55555556 0.48148148 0.44099914\n",
            " 0.57622739 0.52024117 0.46511628 0.48837209 0.44358312 0.36520241\n",
            " 0.47286822 0.58656331 0.53057709 0.60034453 0.6124031  0.51851852\n",
            " 0.44788975 0.44272179 0.52024117 0.53488372 0.55900086 0.52540913\n",
            " 0.50990525 0.61498708 0.58225668 0.48062016 0.55297158 0.64685616\n",
            " 0.53057709 0.60637382 0.47028424 0.68217054 0.54263566 0.47631352\n",
            " 0.34797588 0.51335056 0.41429802 0.416882   0.4625323  0.72265289\n",
            " 0.51851852 0.42118863 0.37898363 0.41515935 0.6089578  0.41602067\n",
            " 0.41860465 0.54349699 0.63049096 0.52799311 0.52282515 0.60120586\n",
            " 0.45391904 0.53402239 0.59173127 0.5503876  0.48837209 0.51851852\n",
            " 0.39965547 0.59259259 0.59862188 0.60292851 0.55986219 0.66838932\n",
            " 0.47975883 0.66236003 0.56330749 0.66494401 0.2962963  0.64857881\n",
            " 0.36003445 0.34625323 0.4461671  0.4203273  0.47631352 0.46770026\n",
            " 0.52368648 0.41257537 0.61757106 0.52971576 0.5374677  0.47717485\n",
            " 0.63910422 0.47803618 0.45219638 0.4952627  0.62618432 0.47028424\n",
            " 0.42463394 0.60981912 0.42118863 0.42463394 0.64254953 0.46942291\n",
            " 0.58225668 0.36175711 0.51507321 0.6580534  0.38845823 0.5211025\n",
            " 0.59259259 0.45564169 0.45478036 0.51248923 0.51765719 0.67355728\n",
            " 0.66838932 0.47459087 0.4332472  0.49095607 0.60551249 0.51335056\n",
            " 0.58570198 0.5374677  0.54521964 0.48492679 0.44444444 0.54005168\n",
            " 0.64771748 0.6416882  0.4496124  0.67355728 0.62101637 0.76055125\n",
            " 0.32213609 0.50215332 0.60723514 0.40654608 0.52540913 0.64427218\n",
            " 0.5211025  0.66666667 0.52627046 0.45908699 0.60465116 0.43066322\n",
            " 0.46511628 0.56589147 0.58570198 0.38501292 0.60292851 0.54005168\n",
            " 0.43496985 0.40913006 0.34797588 0.37726098 0.71231697 0.58742463\n",
            " 0.35400517 0.62273902 0.55986219 0.55727821 0.50990525 0.56761413\n",
            " 0.57881137 0.54866494 0.56158484 0.5994832  0.61154177 0.49784668\n",
            " 0.53488372 0.44272179 0.39362618 0.49870801 0.45564169 0.43927649\n",
            " 0.63996555 0.63135228 0.45736434 0.65030146 0.35400517 0.63307494\n",
            " 0.44444444 0.45822567 0.49612403 0.55469423 0.65202412 0.44099914\n",
            " 0.48148148 0.41429802 0.4039621  0.43496985 0.71576227 0.58053402\n",
            " 0.35486649 0.56416882 0.45822567 0.5047373  0.44788975 0.54349699\n",
            " 0.47631352 0.36003445 0.5994832  0.38587425 0.49354005 0.65374677\n",
            " 0.52540913 0.43669251 0.63910422 0.54521964 0.5245478  0.67700258\n",
            " 0.59689922 0.60120586 0.53488372 0.57192076 0.58656331 0.64341085\n",
            " 0.43410853 0.51679587 0.60378984 0.36347976 0.49009475 0.57019811\n",
            " 0.48148148 0.53919035 0.54177433 0.52885444 0.57708872 0.55297158\n",
            " 0.66925065 0.71490095 0.65116279 0.61326443 0.62790698 0.60206718\n",
            " 0.45391904 0.57278208 0.42204996 0.47459087 0.63652024 0.4952627\n",
            " 0.45391904 0.58570198 0.41085271 0.43066322 0.68217054 0.62618432\n",
            " 0.51593454 0.50387597 0.56847545 0.38845823 0.5960379  0.5667528\n",
            " 0.48062016 0.32213609 0.43583118 0.62446167 0.41085271 0.56158484\n",
            " 0.5245478  0.43927649 0.64254953 0.45736434 0.59173127 0.5374677\n",
            " 0.48406546 0.41774332 0.54177433 0.47459087 0.5047373  0.44788975\n",
            " 0.45305771 0.56503015 0.36606374 0.51507321 0.59689922 0.52971576\n",
            " 0.55469423 0.43410853 0.50990525 0.5245478  0.66149871 0.51248923\n",
            " 0.41085271 0.4918174  0.57105943 0.52024117 0.49354005 0.33850129\n",
            " 0.64082687 0.38673557 0.5047373  0.50904393 0.41860465 0.53832903\n",
            " 0.56330749 0.40913006 0.46683893 0.54866494 0.46597761 0.56072351\n",
            " 0.49009475 0.35486649 0.36175711 0.54866494 0.56589147 0.48234281\n",
            " 0.68044789 0.54263566 0.51679587 0.54608096 0.46167097 0.43669251\n",
            " 0.26098191 0.46683893 0.63738157 0.55727821 0.49440138 0.59173127\n",
            " 0.65030146 0.44186047 0.54694229 0.5503876  0.38845823 0.52713178\n",
            " 0.39190353 0.53229974 0.41343669 0.62962963 0.48148148 0.49095607\n",
            " 0.40740741 0.37812231 0.54177433 0.4788975  0.65977606 0.49698536\n",
            " 0.45219638 0.57105943 0.52799311 0.51937984 0.56589147 0.70801034\n",
            " 0.6416882  0.47631352 0.65891473 0.51335056 0.34797588 0.50904393\n",
            " 0.44875108 0.5374677  0.42980189 0.43669251 0.55900086 0.43066322\n",
            " 0.57278208 0.61412575 0.56330749 0.52024117 0.55211025 0.53057709\n",
            " 0.49612403 0.57364341 0.6124031  0.44702842 0.66322136 0.44099914\n",
            " 0.39448751 0.44875108 0.53057709 0.3910422  0.58570198 0.3875969\n",
            " 0.38845823 0.36778639 0.47975883 0.59259259 0.43238587 0.44099914\n",
            " 0.58139535 0.54349699 0.52971576 0.50387597 0.55555556 0.47372954\n",
            " 0.52540913 0.58397933 0.50301464 0.61584841 0.44444444 0.39879414\n",
            " 0.46770026 0.58570198 0.46339363 0.49698536 0.44358312 0.37726098\n",
            " 0.56589147 0.64427218 0.60120586 0.50904393 0.53832903 0.33850129\n",
            " 0.39793282 0.54694229 0.51937984 0.5667528  0.57881137 0.55900086\n",
            " 0.57450474 0.44444444 0.51507321 0.60206718 0.55555556 0.51937984\n",
            " 0.45047373 0.41343669 0.43496985 0.51851852 0.60637382 0.56244617\n",
            " 0.68044789 0.49612403 0.4461671  0.51593454 0.47286822 0.7002584\n",
            " 0.4788975  0.60981912 0.4754522  0.41343669 0.45822567 0.51421189\n",
            " 0.48923342 0.58397933 0.61929371 0.47114556 0.60378984 0.62704565\n",
            " 0.43066322 0.72782084 0.60034453 0.58570198 0.58139535 0.47803618\n",
            " 0.53402239 0.50129199 0.49009475 0.40568475 0.47459087 0.44358312\n",
            " 0.70542636 0.54435831 0.42377261 0.54177433 0.625323   0.60120586\n",
            " 0.55986219 0.6287683  0.54694229 0.55813953 0.4918174  0.46770026\n",
            " 0.58742463 0.48837209 0.53488372 0.53316107 0.43755383 0.50129199\n",
            " 0.48923342 0.61584841 0.5047373  0.46770026 0.53229974 0.34969854\n",
            " 0.63824289 0.55124892 0.61843239 0.55727821 0.46511628 0.44702842\n",
            " 0.52885444]\n",
            "The trained model has an aproximate error rate of 166.53006374975942 which equates to 28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure(slp)\n"
      ],
      "metadata": {
        "id": "eJ4eYJryNjGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf61656-a922-4a4b-a378-a8997ce69ea5",
        "id": "mhUamxCZOTrA"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] != 2012]\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] < 2020]\n",
        "cols = df_slp_dnn['NUM_COLLISIONS']\n",
        "df_slp_dnn = df_slp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp_dnn[:6])\n",
        "df_slp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "3446db5e-278f-4ed3-e5b8-bd5818d82160",
        "id": "P-VEj2lxOTrB"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  1016.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  1014.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  1021.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29   999.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  1015.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  1020.7    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da          slp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2016.000391    15.719765  1016.777221     0.082192     0.084932   \n",
              "std       2.000294     8.796698     7.628429     0.274710     0.278834   \n",
              "min    2013.000000     1.000000   989.500000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000  1012.200000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000  1016.700000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000  1021.700000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000  1044.200000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084540     0.077104     0.084932     0.084932     0.082192  ...   \n",
              "std       0.278251     0.266808     0.278834     0.278834     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142466      599.147162  \n",
              "std       0.349996     0.349996     0.349996     0.349596      100.268048  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7cd12701-940e-4517-99d5-d23a0b15844a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.000391</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142466</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000294</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349596</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cd12701-940e-4517-99d5-d23a0b15844a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7cd12701-940e-4517-99d5-d23a0b15844a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7cd12701-940e-4517-99d5-d23a0b15844a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp_dnn.iloc[np.random.permutation(len(df_slp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ec95c5-b641-4610-e28f-8dcc8a7bdd4a",
        "id": "EG2EcMoLOTrC"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "2238  2018   6  1018.6    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "1874  2013  31  1019.3    0    0    0    0    0    1    0  ...    0    0    0   \n",
            "3166  2014  15  1021.7    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "2044  2016  25  1014.0    0    0    0    0    0    1    0  ...    0    0    0   \n",
            "1482  2014  13  1022.0    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "468   2015  23  1020.2    0    0    0    1    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2238    0    0    0    1    0    0    0  \n",
            "1874    0    0    0    0    0    1    0  \n",
            "3166    0    0    0    0    0    0    0  \n",
            "2044    0    0    0    1    0    0    0  \n",
            "1482    0    1    0    0    0    0    0  \n",
            "468     0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ef9b67-dfed-419a-e681-9f353eae6ab6",
        "id": "pd6Uk6a9OTrC"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2238    632\n",
            "1874    553\n",
            "3166    529\n",
            "2044    698\n",
            "1482    560\n",
            "468     574\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db46f3f-8fb9-43ff-f81a-3cfdd53a099e",
        "id": "QX1fWn3jOTrC"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a111b2fb-1ac3-4fac-ecb3-11f9d6ab0df8",
        "id": "C9uL6bF7OTrD"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd9af719890>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 3157.8977, step = 1\n",
            "INFO:tensorflow:global_step/sec: 286.662\n",
            "INFO:tensorflow:loss = 0.81244874, step = 101 (0.351 sec)\n",
            "INFO:tensorflow:global_step/sec: 430.593\n",
            "INFO:tensorflow:loss = 0.55703247, step = 201 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 245.196\n",
            "INFO:tensorflow:loss = 0.3602442, step = 301 (0.405 sec)\n",
            "INFO:tensorflow:global_step/sec: 465.798\n",
            "INFO:tensorflow:loss = 0.21667351, step = 401 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 447.114\n",
            "INFO:tensorflow:loss = 0.09068417, step = 501 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 492.975\n",
            "INFO:tensorflow:loss = 0.047328938, step = 601 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 419.662\n",
            "INFO:tensorflow:loss = 0.02520203, step = 701 (0.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 539.288\n",
            "INFO:tensorflow:loss = 0.017250083, step = 801 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.671\n",
            "INFO:tensorflow:loss = 0.014640178, step = 901 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.148\n",
            "INFO:tensorflow:loss = 0.013320664, step = 1001 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.475\n",
            "INFO:tensorflow:loss = 0.0089137, step = 1101 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.561\n",
            "INFO:tensorflow:loss = 0.008343037, step = 1201 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 530.843\n",
            "INFO:tensorflow:loss = 0.00857817, step = 1301 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.168\n",
            "INFO:tensorflow:loss = 0.006264679, step = 1401 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.15\n",
            "INFO:tensorflow:loss = 0.0066706706, step = 1501 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 500.619\n",
            "INFO:tensorflow:loss = 0.0059800763, step = 1601 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.968\n",
            "INFO:tensorflow:loss = 0.005630414, step = 1701 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 507.413\n",
            "INFO:tensorflow:loss = 0.004263875, step = 1801 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.447\n",
            "INFO:tensorflow:loss = 0.0060875155, step = 1901 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.103\n",
            "INFO:tensorflow:loss = 0.0077080643, step = 2001 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.676\n",
            "INFO:tensorflow:loss = 0.0037314214, step = 2101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.477\n",
            "INFO:tensorflow:loss = 0.0066211596, step = 2201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.235\n",
            "INFO:tensorflow:loss = 0.0056510456, step = 2301 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 563.588\n",
            "INFO:tensorflow:loss = 0.003292684, step = 2401 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.255\n",
            "INFO:tensorflow:loss = 0.0051790914, step = 2501 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.937\n",
            "INFO:tensorflow:loss = 0.004943334, step = 2601 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 556.483\n",
            "INFO:tensorflow:loss = 0.0077087977, step = 2701 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.183\n",
            "INFO:tensorflow:loss = 0.00626809, step = 2801 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 452.459\n",
            "INFO:tensorflow:loss = 0.0071856645, step = 2901 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.916\n",
            "INFO:tensorflow:loss = 0.0062546306, step = 3001 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.092\n",
            "INFO:tensorflow:loss = 0.0031234906, step = 3101 (0.240 sec)\n",
            "INFO:tensorflow:global_step/sec: 437.6\n",
            "INFO:tensorflow:loss = 0.15749462, step = 3201 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.394\n",
            "INFO:tensorflow:loss = 0.08927888, step = 3301 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.385\n",
            "INFO:tensorflow:loss = 0.008773135, step = 3401 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.846\n",
            "INFO:tensorflow:loss = 0.014208781, step = 3501 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.962\n",
            "INFO:tensorflow:loss = 0.18467897, step = 3601 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.27\n",
            "INFO:tensorflow:loss = 0.11317165, step = 3701 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.952\n",
            "INFO:tensorflow:loss = 0.9623004, step = 3801 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 436.261\n",
            "INFO:tensorflow:loss = 0.032123305, step = 3901 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.76\n",
            "INFO:tensorflow:loss = 0.0076618996, step = 4001 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.876\n",
            "INFO:tensorflow:loss = 0.06267601, step = 4101 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.401\n",
            "INFO:tensorflow:loss = 0.005582884, step = 4201 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.962\n",
            "INFO:tensorflow:loss = 0.11036277, step = 4301 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 454.073\n",
            "INFO:tensorflow:loss = 0.039860636, step = 4401 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 498.995\n",
            "INFO:tensorflow:loss = 0.08030832, step = 4501 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.571\n",
            "INFO:tensorflow:loss = 0.10017088, step = 4601 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 520.572\n",
            "INFO:tensorflow:loss = 0.052578032, step = 4701 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.497\n",
            "INFO:tensorflow:loss = 0.008448778, step = 4801 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 472.761\n",
            "INFO:tensorflow:loss = 0.058305845, step = 4901 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.058\n",
            "INFO:tensorflow:loss = 0.0050360905, step = 5001 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.16\n",
            "INFO:tensorflow:loss = 0.029544532, step = 5101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.101\n",
            "INFO:tensorflow:loss = 0.008591238, step = 5201 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.919\n",
            "INFO:tensorflow:loss = 0.017863259, step = 5301 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.45\n",
            "INFO:tensorflow:loss = 0.02640162, step = 5401 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.548\n",
            "INFO:tensorflow:loss = 0.09081859, step = 5501 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.213\n",
            "INFO:tensorflow:loss = 0.060774017, step = 5601 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 360.021\n",
            "INFO:tensorflow:loss = 0.036037527, step = 5701 (0.279 sec)\n",
            "INFO:tensorflow:global_step/sec: 268.075\n",
            "INFO:tensorflow:loss = 0.057117715, step = 5801 (0.376 sec)\n",
            "INFO:tensorflow:global_step/sec: 297.789\n",
            "INFO:tensorflow:loss = 0.007417988, step = 5901 (0.334 sec)\n",
            "INFO:tensorflow:global_step/sec: 315.502\n",
            "INFO:tensorflow:loss = 0.021059379, step = 6001 (0.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 307.035\n",
            "INFO:tensorflow:loss = 0.16111019, step = 6101 (0.322 sec)\n",
            "INFO:tensorflow:global_step/sec: 340.649\n",
            "INFO:tensorflow:loss = 0.007050034, step = 6201 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 356.055\n",
            "INFO:tensorflow:loss = 0.014021248, step = 6301 (0.281 sec)\n",
            "INFO:tensorflow:global_step/sec: 345.646\n",
            "INFO:tensorflow:loss = 0.020767188, step = 6401 (0.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 326.687\n",
            "INFO:tensorflow:loss = 0.008755378, step = 6501 (0.307 sec)\n",
            "INFO:tensorflow:global_step/sec: 291.234\n",
            "INFO:tensorflow:loss = 0.03285618, step = 6601 (0.347 sec)\n",
            "INFO:tensorflow:global_step/sec: 258.788\n",
            "INFO:tensorflow:loss = 0.008639992, step = 6701 (0.383 sec)\n",
            "INFO:tensorflow:global_step/sec: 301.856\n",
            "INFO:tensorflow:loss = 0.011205866, step = 6801 (0.330 sec)\n",
            "INFO:tensorflow:global_step/sec: 341.022\n",
            "INFO:tensorflow:loss = 0.033182908, step = 6901 (0.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 323.621\n",
            "INFO:tensorflow:loss = 0.006409138, step = 7001 (0.306 sec)\n",
            "INFO:tensorflow:global_step/sec: 316.28\n",
            "INFO:tensorflow:loss = 0.0037006168, step = 7101 (0.316 sec)\n",
            "INFO:tensorflow:global_step/sec: 349.719\n",
            "INFO:tensorflow:loss = 0.009478366, step = 7201 (0.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 347.484\n",
            "INFO:tensorflow:loss = 0.007151797, step = 7301 (0.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 345.405\n",
            "INFO:tensorflow:loss = 0.011988606, step = 7401 (0.290 sec)\n",
            "INFO:tensorflow:global_step/sec: 333.226\n",
            "INFO:tensorflow:loss = 0.0144418115, step = 7501 (0.302 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.113\n",
            "INFO:tensorflow:loss = 0.006928518, step = 7601 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 369.453\n",
            "INFO:tensorflow:loss = 0.00680779, step = 7701 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 321.371\n",
            "INFO:tensorflow:loss = 0.0045271292, step = 7801 (0.311 sec)\n",
            "INFO:tensorflow:global_step/sec: 330.116\n",
            "INFO:tensorflow:loss = 0.07777248, step = 7901 (0.311 sec)\n",
            "INFO:tensorflow:global_step/sec: 354.29\n",
            "INFO:tensorflow:loss = 0.032969795, step = 8001 (0.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 378.958\n",
            "INFO:tensorflow:loss = 0.004324868, step = 8101 (0.259 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.7\n",
            "INFO:tensorflow:loss = 0.004596354, step = 8201 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.102\n",
            "INFO:tensorflow:loss = 0.015730703, step = 8301 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.386\n",
            "INFO:tensorflow:loss = 0.005568974, step = 8401 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 530.594\n",
            "INFO:tensorflow:loss = 0.004721772, step = 8501 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.254\n",
            "INFO:tensorflow:loss = 0.0055806832, step = 8601 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.602\n",
            "INFO:tensorflow:loss = 0.025756476, step = 8701 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.963\n",
            "INFO:tensorflow:loss = 0.005589244, step = 8801 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.393\n",
            "INFO:tensorflow:loss = 0.004361365, step = 8901 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.564\n",
            "INFO:tensorflow:loss = 0.0057328725, step = 9001 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.524\n",
            "INFO:tensorflow:loss = 0.005882133, step = 9101 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 439.68\n",
            "INFO:tensorflow:loss = 0.0088145, step = 9201 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.387\n",
            "INFO:tensorflow:loss = 0.003834127, step = 9301 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.621\n",
            "INFO:tensorflow:loss = 0.0048746197, step = 9401 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 547.781\n",
            "INFO:tensorflow:loss = 0.0034240778, step = 9501 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.563\n",
            "INFO:tensorflow:loss = 0.005709751, step = 9601 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.522\n",
            "INFO:tensorflow:loss = 0.006220284, step = 9701 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.558\n",
            "INFO:tensorflow:loss = 0.0049284683, step = 9801 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 504.492\n",
            "INFO:tensorflow:loss = 0.006493333, step = 9901 (0.195 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.004944367.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 77.96618777117176\n",
            "Just using average = 598.7592954990215 has RMSE of 94.24576970337212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15172479-b19e-4f8c-dd3f-a54a1bd28622",
        "id": "mTPv83i1OTrD"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd9a36ff850>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.50069565 0.52468026 0.4782404  0.42154166 0.5360888  0.5616261\n",
            " 0.53668207 0.46657005 0.52105576 0.5324824  0.5085892  0.53905237\n",
            " 0.48806807 0.49131766 0.5450348  0.49998894 0.5401188  0.38731918\n",
            " 0.53999186 0.42192474 0.44779438 0.5156937  0.41259292 0.5670615\n",
            " 0.53810984 0.39906856 0.5110862  0.4731031  0.49670663 0.48035875\n",
            " 0.5104276  0.48828715 0.49793246 0.5445526  0.46972886 0.5415081\n",
            " 0.5368218  0.4681412  0.53912127 0.42641705 0.52203685 0.5365205\n",
            " 0.48546913 0.5189426  0.5420701  0.507533   0.52597064 0.52741885\n",
            " 0.5638306  0.5217331  0.4110508  0.5317384  0.53559625 0.5131709\n",
            " 0.5296     0.53853154 0.5483497  0.51151603 0.49285224 0.51545334\n",
            " 0.5153801  0.51092094 0.52225244 0.5275827  0.54744244 0.5221239\n",
            " 0.551523   0.572836   0.5409065  0.545136   0.52116823 0.49935326\n",
            " 0.4111073  0.53205353 0.416811   0.511338   0.5136957  0.53579295\n",
            " 0.46566176 0.5402014  0.516182   0.5592507  0.5937876  0.46886402\n",
            " 0.57495433 0.41902766 0.4894625  0.4948925  0.524353   0.43682188\n",
            " 0.53040427 0.5340404  0.53553915 0.53424835 0.53424    0.49909315\n",
            " 0.5032908  0.54041797 0.50810367 0.5024911  0.53576404 0.5433265\n",
            " 0.50545514 0.5346739  0.5186476  0.4830349  0.50571537 0.45886108\n",
            " 0.5090837  0.47616613 0.47424483 0.50622356 0.53899586 0.47231016\n",
            " 0.5416446  0.48576888 0.47693422 0.44845062 0.51972854 0.54497784\n",
            " 0.5265066  0.5652427  0.5435937  0.54285395 0.39127597 0.49256042\n",
            " 0.5237665  0.5491636  0.5389444  0.4915472  0.5655936  0.60126793\n",
            " 0.50897706 0.5486521  0.42724654 0.5306029  0.52324915 0.54148406\n",
            " 0.5210332  0.57300127 0.52857107 0.5364374  0.524792   0.57178086\n",
            " 0.49753764 0.5279565  0.53537846 0.44293126 0.413033   0.5259181\n",
            " 0.51254684 0.4514672  0.502483   0.4901649  0.5160978  0.5319647\n",
            " 0.55514455 0.48959965 0.5418146  0.50494635 0.5319505  0.5367537\n",
            " 0.45875502 0.46665186 0.5300764  0.53795725 0.5719902  0.5205459\n",
            " 0.48409203 0.5505607  0.40344664 0.5213814  0.55553883 0.5665565\n",
            " 0.5020776  0.52563864 0.4929956  0.52776814 0.5398468  0.5430111\n",
            " 0.5732875  0.53654146 0.50957805 0.5337194  0.54714775 0.38115188\n",
            " 0.5369987  0.49854627 0.43893614 0.5023605  0.42149305 0.5299946\n",
            " 0.5293469  0.48919377 0.50773644 0.51196396 0.51002747 0.58818424\n",
            " 0.4713464  0.440394   0.5186932  0.5536054  0.4786984  0.5036996\n",
            " 0.5710297  0.42771992 0.40173277 0.42074966 0.5168592  0.43779245\n",
            " 0.42699084 0.54466045 0.40853795 0.52865475 0.5228721  0.5542644\n",
            " 0.49117747 0.5701116  0.52185106 0.5048168  0.5639547  0.5220357\n",
            " 0.47770587 0.3870151  0.5075515  0.5782168  0.4945418  0.43974796\n",
            " 0.518469   0.43780172 0.53595126 0.5485168  0.47329143 0.5346094\n",
            " 0.47394794 0.5416256  0.5301981  0.5016362  0.5347538  0.4955152\n",
            " 0.48469725 0.49919263 0.4847617  0.5057949  0.54709196 0.4951267\n",
            " 0.4815594  0.5230457  0.5084948  0.5772867  0.41934645 0.49804613\n",
            " 0.56412107 0.50884616 0.42832518 0.5532068  0.53176385 0.54307365\n",
            " 0.5585603  0.51211333 0.3928181  0.56802404 0.5038449  0.48985943\n",
            " 0.5318411  0.51409394 0.53484386 0.5254746  0.52183324 0.45867658\n",
            " 0.5456014  0.4831008  0.47094136 0.5382916  0.55479634 0.49035728\n",
            " 0.56078535 0.47734648 0.52765703 0.5377978  0.5322137  0.5322675\n",
            " 0.4954144  0.4284411  0.4362857  0.52199495 0.5268821  0.488995\n",
            " 0.43699405 0.47807464 0.46603948 0.425903   0.5740891  0.5513372\n",
            " 0.5158192  0.54736745 0.51542395 0.4949359  0.5357408  0.38090676\n",
            " 0.45497704 0.4607338  0.4979401  0.57852465 0.4697355  0.5327154\n",
            " 0.42667472 0.5346559  0.5115981  0.4932572  0.49336383 0.5187779\n",
            " 0.56104296 0.5252592  0.42484087 0.52230114 0.5405161  0.5539669\n",
            " 0.43956843 0.47349522 0.4841016  0.5402873  0.49622455 0.47311568\n",
            " 0.508943   0.4851402  0.5214727  0.5087357  0.51148003 0.48873347\n",
            " 0.5163088  0.521335   0.53633815 0.51445764 0.50414556 0.4975359\n",
            " 0.5490966  0.38357434 0.5057495  0.518957   0.40233496 0.4820131\n",
            " 0.5081962  0.56754637 0.45559242 0.5959904  0.51266783 0.5483678\n",
            " 0.491267   0.51902443 0.4890465  0.51183665 0.53968245 0.49603477\n",
            " 0.5354535  0.51187044 0.54841137 0.49377796 0.5005595  0.5384575\n",
            " 0.5542776  0.52892005 0.4936594  0.50771314 0.3939005  0.5133767\n",
            " 0.5464153  0.5097965  0.5401088  0.57030207 0.5482425  0.4966897\n",
            " 0.53597    0.5151047  0.5125136  0.5478174  0.43804017 0.5155318\n",
            " 0.5387968  0.5857615  0.54152673 0.47038746 0.5755766  0.5575328\n",
            " 0.55389583 0.5778642  0.56750154 0.569382   0.54724574 0.431579\n",
            " 0.5628498  0.55184287 0.5175787  0.5819329  0.46439    0.38406977\n",
            " 0.4199163  0.49975    0.5481134  0.52581924 0.4038933  0.4731887\n",
            " 0.5196127  0.5345966  0.47438994 0.4867278  0.5052542  0.5166605\n",
            " 0.4903008  0.49831662 0.4647437  0.49783197 0.48772645 0.5632087\n",
            " 0.50849414 0.5242615  0.5169284  0.51336247 0.40638286 0.57183635\n",
            " 0.55768085 0.5325297  0.56333745 0.5100349  0.51155883 0.5575712\n",
            " 0.5401973  0.5290777  0.50683016 0.54099464 0.4157987  0.58599573\n",
            " 0.510295   0.4522546  0.43756744 0.4805568  0.5375394  0.37617597\n",
            " 0.50855684 0.40115827 0.49117276 0.5056146  0.5476513  0.44684416\n",
            " 0.5460816  0.51134485 0.4795116  0.5180766  0.53091264 0.45658734\n",
            " 0.50340635 0.5953095  0.49214295 0.5528124  0.45154908 0.5225986\n",
            " 0.5328433  0.49046993 0.54625976 0.5519443  0.5437444  0.52314895\n",
            " 0.5761558  0.5427546  0.50535876 0.53366846 0.49112716 0.5762711\n",
            " 0.47671002 0.5558157  0.5277374  0.41582552 0.5242515  0.5227936\n",
            " 0.45232657 0.57333046 0.53457934 0.5227137  0.45509085 0.5769433\n",
            " 0.5296699  0.5022104  0.50528777 0.4811499  0.5394114  0.4911194\n",
            " 0.572144   0.4794684  0.5694557  0.40409705 0.47932616 0.5346924\n",
            " 0.5095791  0.5124879  0.4795892  0.52951807 0.51915085 0.48586118\n",
            " 0.5185834  0.44480956 0.48360005 0.5316942  0.528603   0.472096\n",
            " 0.5012089  0.5252216  0.5132177  0.46794263 0.48517603 0.535472\n",
            " 0.42601624]\n",
            "[0.53057709 0.54866494 0.43410853 0.36175711 0.55813953 0.58139535\n",
            " 0.57622739 0.50301464 0.49009475 0.60465116 0.55986219 0.5081826\n",
            " 0.4952627  0.53919035 0.53919035 0.50043066 0.57019811 0.38587425\n",
            " 0.53402239 0.49354005 0.45564169 0.62187769 0.4332472  0.37639966\n",
            " 0.73815676 0.59689922 0.52799311 0.34711456 0.51421189 0.44358312\n",
            " 0.53057709 0.41343669 0.60034453 0.62790698 0.46683893 0.4952627\n",
            " 0.46511628 0.51076658 0.55813953 0.45822567 0.36175711 0.5503876\n",
            " 0.40826873 0.48751077 0.66149871 0.49009475 0.61929371 0.60292851\n",
            " 0.38242894 0.58570198 0.37639966 0.60034453 0.52196382 0.5047373\n",
            " 0.47631352 0.52024117 0.58397933 0.57450474 0.52713178 0.48837209\n",
            " 0.38845823 0.41515935 0.51851852 0.60809647 0.58656331 0.55986219\n",
            " 0.59776055 0.56158484 0.53919035 0.54091301 0.60120586 0.37037037\n",
            " 0.48492679 0.42291128 0.33936262 0.43238587 0.51851852 0.54694229\n",
            " 0.45219638 0.61326443 0.51765719 0.48148148 0.60120586 0.60465116\n",
            " 0.7002584  0.4005168  0.5047373  0.51937984 0.60034453 0.40826873\n",
            " 0.57105943 0.63393626 0.56416882 0.38070629 0.63135228 0.47372954\n",
            " 0.47975883 0.58484065 0.54694229 0.54349699 0.35486649 0.63307494\n",
            " 0.50559862 0.5667528  0.43152455 0.56847545 0.49956934 0.43496985\n",
            " 0.42118863 0.51421189 0.50559862 0.54952627 0.51507321 0.47372954\n",
            " 0.59431525 0.46339363 0.53057709 0.49784668 0.57536606 0.64857881\n",
            " 0.59689922 0.583118   0.65202412 0.60206718 0.35400517 0.53574505\n",
            " 0.51679587 0.57622739 0.53143842 0.48751077 0.5667528  0.65891473\n",
            " 0.50129199 0.53574505 0.46511628 0.68647717 0.47028424 0.54866494\n",
            " 0.60637382 0.52713178 0.47459087 0.5667528  0.54091301 0.58053402\n",
            " 0.52885444 0.52540913 0.46339363 0.41860465 0.34366925 0.50129199\n",
            " 0.52196382 0.50732127 0.52713178 0.46683893 0.47286822 0.54005168\n",
            " 0.63049096 0.44186047 0.52024117 0.58484065 0.71576227 0.50301464\n",
            " 0.36864772 0.49784668 0.55727821 0.60378984 0.53488372 0.41429802\n",
            " 0.4918174  0.64857881 0.38501292 0.4754522  0.4496124  0.50904393\n",
            " 0.54866494 0.57881137 0.42291128 0.52196382 0.48148148 0.59000861\n",
            " 0.72782084 0.58570198 0.52713178 0.49612403 0.57536606 0.38329027\n",
            " 0.58484065 0.59259259 0.4918174  0.56847545 0.44875108 0.68044789\n",
            " 0.48320413 0.49870801 0.38931955 0.48234281 0.5047373  0.69336779\n",
            " 0.49784668 0.48148148 0.55211025 0.52971576 0.50559862 0.52627046\n",
            " 0.31007752 0.42980189 0.41515935 0.40999139 0.54694229 0.46856158\n",
            " 0.36606374 0.51937984 0.41429802 0.58139535 0.55555556 0.4754522\n",
            " 0.5211025  0.57708872 0.53143842 0.49095607 0.49612403 0.55900086\n",
            " 0.53143842 0.33850129 0.46942291 0.57622739 0.49612403 0.35745047\n",
            " 0.51335056 0.45908699 0.59689922 0.56761413 0.55900086 0.32988803\n",
            " 0.46942291 0.50215332 0.57278208 0.39276486 0.54091301 0.55641688\n",
            " 0.53660637 0.48406546 0.46080965 0.45822567 0.48664944 0.57536606\n",
            " 0.53660637 0.45650301 0.59776055 0.59086994 0.46339363 0.51421189\n",
            " 0.60034453 0.56933678 0.40482343 0.60551249 0.52971576 0.52540913\n",
            " 0.56158484 0.58053402 0.41774332 0.57450474 0.53660637 0.40310078\n",
            " 0.63652024 0.56244617 0.53229974 0.45908699 0.35658915 0.50904393\n",
            " 0.45822567 0.54177433 0.62015504 0.54694229 0.5994832  0.49956934\n",
            " 0.55641688 0.44788975 0.36089578 0.57278208 0.62962963 0.52540913\n",
            " 0.39879414 0.45478036 0.44099914 0.50732127 0.49870801 0.56503015\n",
            " 0.52799311 0.51507321 0.25064599 0.46425495 0.70111972 0.5667528\n",
            " 0.47717485 0.56589147 0.46770026 0.49698536 0.52282515 0.42549526\n",
            " 0.52024117 0.54521964 0.53919035 0.55813953 0.48148148 0.58397933\n",
            " 0.47372954 0.58225668 0.50990525 0.42204996 0.38845823 0.52713178\n",
            " 0.49612403 0.51076658 0.39018088 0.47803618 0.55727821 0.48664944\n",
            " 0.52024117 0.52024117 0.56244617 0.48837209 0.45305771 0.43583118\n",
            " 0.39534884 0.68217054 0.51593454 0.47975883 0.42549526 0.57708872\n",
            " 0.54091301 0.74677003 0.62101637 0.46683893 0.43583118 0.49870801\n",
            " 0.48923342 0.32127476 0.46511628 0.56761413 0.38845823 0.53660637\n",
            " 0.54780362 0.46683893 0.40913006 0.64341085 0.62446167 0.53316107\n",
            " 0.45219638 0.52713178 0.47803618 0.55900086 0.54694229 0.53574505\n",
            " 0.59345392 0.60120586 0.70801034 0.42291128 0.44444444 0.57536606\n",
            " 0.57278208 0.59345392 0.46167097 0.52885444 0.39276486 0.5503876\n",
            " 0.55641688 0.38931955 0.60465116 0.57105943 0.42980189 0.63738157\n",
            " 0.55641688 0.58225668 0.57019811 0.53402239 0.4625323  0.60034453\n",
            " 0.49612403 0.65891473 0.44444444 0.43841516 0.59689922 0.55641688\n",
            " 0.41343669 0.65719208 0.57536606 0.66838932 0.56761413 0.40137812\n",
            " 0.5667528  0.46080965 0.55986219 0.56847545 0.45736434 0.40913006\n",
            " 0.43238587 0.51937984 0.5081826  0.51937984 0.39793282 0.45219638\n",
            " 0.45736434 0.5994832  0.5374677  0.49870801 0.48923342 0.5211025\n",
            " 0.63824289 0.45219638 0.45305771 0.48751077 0.61670973 0.64082687\n",
            " 0.50990525 0.53229974 0.60465116 0.37639966 0.3910422  0.5211025\n",
            " 0.57536606 0.47028424 0.66494401 0.49956934 0.48751077 0.59086994\n",
            " 0.55727821 0.38156761 0.50387597 0.5211025  0.4496124  0.83893196\n",
            " 0.47631352 0.4005168  0.50559862 0.34969854 0.66666667 0.29371232\n",
            " 0.56503015 0.40568475 0.48751077 0.43583118 0.52971576 0.45564169\n",
            " 0.55555556 0.62446167 0.55124892 0.51248923 0.62962963 0.50559862\n",
            " 0.53574505 0.56847545 0.55641688 0.56244617 0.42635659 0.48148148\n",
            " 0.56158484 0.48406546 0.57364341 0.66322136 0.56158484 0.52627046\n",
            " 0.66063738 0.6580534  0.52196382 0.49440138 0.50129199 0.62273902\n",
            " 0.46770026 0.59862188 0.55641688 0.40310078 0.54349699 0.43927649\n",
            " 0.40913006 0.58742463 0.58656331 0.61068045 0.33505599 0.5538329\n",
            " 0.52282515 0.52713178 0.56416882 0.47717485 0.63738157 0.56933678\n",
            " 0.69939707 0.51765719 0.55297158 0.44530577 0.53057709 0.52282515\n",
            " 0.52627046 0.49698536 0.63221361 0.56072351 0.63996555 0.54091301\n",
            " 0.46770026 0.44358312 0.48320413 0.61498708 0.60378984 0.43927649\n",
            " 0.43927649 0.46683893 0.51162791 0.40913006 0.45391904 0.60809647\n",
            " 0.3910422 ]\n",
            "The trained model has an aproximate error rate of 10.238476292727743 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "lHX1HoDlQwJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326fd9ba-a45b-4bd4-dddd-12d913e9b009",
        "id": "Tf_wMIWXQ7zg"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd  gust  \\\n",
            "3   2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0  20.0   \n",
            "11  2020  15             508  43.9  38.3  1019.4    8.2   5.4   14.0  15.0   \n",
            "12  2021   1             257  39.6  29.3  1029.3   10.0   7.6   14.0  20.0   \n",
            "14  2022  25             235  41.6  31.8  1013.2   10.0   9.6   15.0  19.0   \n",
            "18  2021   3             186  41.1  32.3  1018.0   10.0  10.3   19.0  27.0   \n",
            "19  2020   2             413  39.6  28.9  1011.8   10.0  13.0   19.0  26.0   \n",
            "\n",
            "    ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "11  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "12  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "14  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "18  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "19  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] != 2012]\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] < 2020]\n",
        "cols = df_gust_dnn['NUM_COLLISIONS']\n",
        "df_gust_dnn = df_gust_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust_dnn[:6])\n",
        "df_gust_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "2fa98d2e-f787-4404-85d8-4ad83d419222",
        "id": "oTbpzolhQ7zh"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "74  2016  17  18.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "76  2014   9  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "79  2019  19  21.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "80  2015  11  17.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "83  2015  29  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "85  2019  13  15.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "74    0    1    0    0    0    0             451  \n",
            "76    0    0    0    0    0    1             561  \n",
            "79    0    0    0    0    0    0             479  \n",
            "80    0    1    0    0    0    0             341  \n",
            "83    0    0    0    0    0    1             519  \n",
            "85    0    1    0    0    0    0             374  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             year           da         gust          Apr          Aug  \\\n",
              "count  1629.00000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean   2015.91283    15.702885    27.511602     0.095764     0.042357   \n",
              "std       2.01341     8.667634     7.366770     0.294358     0.201465   \n",
              "min    2013.00000     1.000000    14.000000     0.000000     0.000000   \n",
              "25%    2014.00000     8.000000    22.000000     0.000000     0.000000   \n",
              "50%    2016.00000    16.000000    26.000000     0.000000     0.000000   \n",
              "75%    2018.00000    23.000000    31.100000     0.000000     0.000000   \n",
              "max    2019.00000    31.000000    71.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000  ...   \n",
              "mean      0.104359     0.095150     0.108656     0.046041     0.061387  ...   \n",
              "std       0.305819     0.293513     0.311302     0.209637     0.240113  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean      0.096378     0.087784     0.071209     0.139963     0.141191   \n",
              "std       0.295200     0.283067     0.257253     0.347055     0.348325   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000     1629.000000  \n",
              "mean      0.139963     0.151627     0.138122     0.145488      596.513198  \n",
              "std       0.347055     0.358769     0.345133     0.352700      104.479660  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      526.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      597.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      663.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-776d5482-6f9c-4f40-b61a-518773813110\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.91283</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>0.095764</td>\n",
              "      <td>0.042357</td>\n",
              "      <td>0.104359</td>\n",
              "      <td>0.095150</td>\n",
              "      <td>0.108656</td>\n",
              "      <td>0.046041</td>\n",
              "      <td>0.061387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096378</td>\n",
              "      <td>0.087784</td>\n",
              "      <td>0.071209</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.141191</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.151627</td>\n",
              "      <td>0.138122</td>\n",
              "      <td>0.145488</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.01341</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>0.294358</td>\n",
              "      <td>0.201465</td>\n",
              "      <td>0.305819</td>\n",
              "      <td>0.293513</td>\n",
              "      <td>0.311302</td>\n",
              "      <td>0.209637</td>\n",
              "      <td>0.240113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.295200</td>\n",
              "      <td>0.283067</td>\n",
              "      <td>0.257253</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.348325</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.358769</td>\n",
              "      <td>0.345133</td>\n",
              "      <td>0.352700</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.00000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.00000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.00000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.00000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-776d5482-6f9c-4f40-b61a-518773813110')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-776d5482-6f9c-4f40-b61a-518773813110 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-776d5482-6f9c-4f40-b61a-518773813110');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust_dnn.iloc[np.random.permutation(len(df_gust_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517d4373-32d2-47b0-ef1f-1a0dc5da52a4",
        "id": "a_4bidEbQ7zi"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "3316  2013  28  35.9    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "2667  2015  12  22.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "1369  2015  11  21.0    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "3297  2019  24  35.9    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "763   2013  13  26.0    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "3599  2017  23  28.9    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3316    0    0    0    0    0    0    1  \n",
            "2667    1    0    0    0    0    0    0  \n",
            "1369    0    0    0    1    0    0    0  \n",
            "3297    0    0    1    0    0    0    0  \n",
            "763     0    0    0    0    0    1    0  \n",
            "3599    0    0    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee14ece-8688-4829-af62-a96bddd7b876",
        "id": "l4TDr10XQ7zi"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3316    429\n",
            "2667    554\n",
            "1369    645\n",
            "3297    517\n",
            "763     636\n",
            "3599    565\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f089dda6-cee7-4bce-9186-194999f455c8",
        "id": "75AKCrz7Q7zi"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e661dfeb-4608-4275-df01-04522f233bdb",
        "id": "v4av8KXMQ7zj"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c8de790>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 16477.111, step = 1\n",
            "INFO:tensorflow:global_step/sec: 389.548\n",
            "INFO:tensorflow:loss = 0.027402423, step = 101 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.164\n",
            "INFO:tensorflow:loss = 0.01917019, step = 201 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.984\n",
            "INFO:tensorflow:loss = 0.021825876, step = 301 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.633\n",
            "INFO:tensorflow:loss = 0.031008393, step = 401 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.194\n",
            "INFO:tensorflow:loss = 0.027500287, step = 501 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.292\n",
            "INFO:tensorflow:loss = 0.021552578, step = 601 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.379\n",
            "INFO:tensorflow:loss = 0.017725224, step = 701 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.888\n",
            "INFO:tensorflow:loss = 0.022250101, step = 801 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.163\n",
            "INFO:tensorflow:loss = 0.022431027, step = 901 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.885\n",
            "INFO:tensorflow:loss = 0.014622123, step = 1001 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.572\n",
            "INFO:tensorflow:loss = 0.021905284, step = 1101 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.066\n",
            "INFO:tensorflow:loss = 0.02182279, step = 1201 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.228\n",
            "INFO:tensorflow:loss = 0.020697316, step = 1301 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.629\n",
            "INFO:tensorflow:loss = 0.019018492, step = 1401 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.669\n",
            "INFO:tensorflow:loss = 0.017036531, step = 1501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.418\n",
            "INFO:tensorflow:loss = 0.025525501, step = 1601 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.989\n",
            "INFO:tensorflow:loss = 0.01609164, step = 1701 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.33\n",
            "INFO:tensorflow:loss = 0.019877646, step = 1801 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.853\n",
            "INFO:tensorflow:loss = 0.018975941, step = 1901 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.507\n",
            "INFO:tensorflow:loss = 0.019203592, step = 2001 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 567.698\n",
            "INFO:tensorflow:loss = 0.014559481, step = 2101 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.848\n",
            "INFO:tensorflow:loss = 0.02077793, step = 2201 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 415.721\n",
            "INFO:tensorflow:loss = 0.020979773, step = 2301 (0.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 455.912\n",
            "INFO:tensorflow:loss = 0.019252175, step = 2401 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 405.946\n",
            "INFO:tensorflow:loss = 0.02141139, step = 2501 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.788\n",
            "INFO:tensorflow:loss = 0.022208761, step = 2601 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.44\n",
            "INFO:tensorflow:loss = 0.018983595, step = 2701 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 563.975\n",
            "INFO:tensorflow:loss = 0.018203842, step = 2801 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.137\n",
            "INFO:tensorflow:loss = 0.015994323, step = 2901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.841\n",
            "INFO:tensorflow:loss = 0.014808087, step = 3001 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.857\n",
            "INFO:tensorflow:loss = 0.022494376, step = 3101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 492.276\n",
            "INFO:tensorflow:loss = 0.028528135, step = 3201 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 500.236\n",
            "INFO:tensorflow:loss = 0.013054537, step = 3301 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.603\n",
            "INFO:tensorflow:loss = 0.017294612, step = 3401 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 448.457\n",
            "INFO:tensorflow:loss = 0.016652528, step = 3501 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.198\n",
            "INFO:tensorflow:loss = 0.013421204, step = 3601 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 525.571\n",
            "INFO:tensorflow:loss = 0.010709963, step = 3701 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.176\n",
            "INFO:tensorflow:loss = 0.015416703, step = 3801 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 585.036\n",
            "INFO:tensorflow:loss = 0.012936616, step = 3901 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 496.968\n",
            "INFO:tensorflow:loss = 0.015472335, step = 4001 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.425\n",
            "INFO:tensorflow:loss = 0.012018084, step = 4101 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.952\n",
            "INFO:tensorflow:loss = 0.011205002, step = 4201 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 498.563\n",
            "INFO:tensorflow:loss = 0.021805452, step = 4301 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.481\n",
            "INFO:tensorflow:loss = 0.012696568, step = 4401 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.211\n",
            "INFO:tensorflow:loss = 0.010182602, step = 4501 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.283\n",
            "INFO:tensorflow:loss = 0.013044947, step = 4601 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 545.603\n",
            "INFO:tensorflow:loss = 0.012142142, step = 4701 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 567.887\n",
            "INFO:tensorflow:loss = 0.010116653, step = 4801 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.499\n",
            "INFO:tensorflow:loss = 0.011875451, step = 4901 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.458\n",
            "INFO:tensorflow:loss = 0.010371248, step = 5001 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.804\n",
            "INFO:tensorflow:loss = 0.01125083, step = 5101 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 578.743\n",
            "INFO:tensorflow:loss = 0.010896606, step = 5201 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 563.525\n",
            "INFO:tensorflow:loss = 0.009581516, step = 5301 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.937\n",
            "INFO:tensorflow:loss = 0.010462546, step = 5401 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 429.614\n",
            "INFO:tensorflow:loss = 0.006049605, step = 5501 (0.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.843\n",
            "INFO:tensorflow:loss = 0.01089401, step = 5601 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.222\n",
            "INFO:tensorflow:loss = 0.008179759, step = 5701 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.548\n",
            "INFO:tensorflow:loss = 0.009602815, step = 5801 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 489.35\n",
            "INFO:tensorflow:loss = 0.011355452, step = 5901 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.936\n",
            "INFO:tensorflow:loss = 0.007999891, step = 6001 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 520.447\n",
            "INFO:tensorflow:loss = 0.0072508836, step = 6101 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.16\n",
            "INFO:tensorflow:loss = 0.008873237, step = 6201 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.475\n",
            "INFO:tensorflow:loss = 0.009233673, step = 6301 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.155\n",
            "INFO:tensorflow:loss = 0.0073706075, step = 6401 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.693\n",
            "INFO:tensorflow:loss = 0.0042420942, step = 6501 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 441.806\n",
            "INFO:tensorflow:loss = 0.0070302505, step = 6601 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 494.489\n",
            "INFO:tensorflow:loss = 0.0072907666, step = 6701 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 498.045\n",
            "INFO:tensorflow:loss = 0.006759301, step = 6801 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 405.665\n",
            "INFO:tensorflow:loss = 0.0068783443, step = 6901 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 235.728\n",
            "INFO:tensorflow:loss = 0.0056598727, step = 7001 (0.440 sec)\n",
            "INFO:tensorflow:global_step/sec: 260.469\n",
            "INFO:tensorflow:loss = 0.0045440104, step = 7101 (0.377 sec)\n",
            "INFO:tensorflow:global_step/sec: 232.97\n",
            "INFO:tensorflow:loss = 0.0053083897, step = 7201 (0.416 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.994\n",
            "INFO:tensorflow:loss = 0.0045635574, step = 7301 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.584\n",
            "INFO:tensorflow:loss = 0.0067254435, step = 7401 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.493\n",
            "INFO:tensorflow:loss = 0.0039379057, step = 7501 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.099\n",
            "INFO:tensorflow:loss = 0.0029054328, step = 7601 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.734\n",
            "INFO:tensorflow:loss = 0.007046502, step = 7701 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.071\n",
            "INFO:tensorflow:loss = 0.0050187786, step = 7801 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 533.137\n",
            "INFO:tensorflow:loss = 0.007325908, step = 7901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.542\n",
            "INFO:tensorflow:loss = 0.0049058395, step = 8001 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.313\n",
            "INFO:tensorflow:loss = 0.0057515143, step = 8101 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.991\n",
            "INFO:tensorflow:loss = 0.0043444205, step = 8201 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.597\n",
            "INFO:tensorflow:loss = 0.004966668, step = 8301 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.668\n",
            "INFO:tensorflow:loss = 0.005534122, step = 8401 (0.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.772\n",
            "INFO:tensorflow:loss = 0.006825789, step = 8501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 563.439\n",
            "INFO:tensorflow:loss = 0.0067304, step = 8601 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.92\n",
            "INFO:tensorflow:loss = 0.004579592, step = 8701 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 490.29\n",
            "INFO:tensorflow:loss = 0.008246632, step = 8801 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.783\n",
            "INFO:tensorflow:loss = 0.0072659706, step = 8901 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.342\n",
            "INFO:tensorflow:loss = 0.007198486, step = 9001 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.683\n",
            "INFO:tensorflow:loss = 0.0052956715, step = 9101 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.013\n",
            "INFO:tensorflow:loss = 0.005731985, step = 9201 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.13\n",
            "INFO:tensorflow:loss = 0.006441186, step = 9301 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.333\n",
            "INFO:tensorflow:loss = 0.0065022614, step = 9401 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.684\n",
            "INFO:tensorflow:loss = 0.004590307, step = 9501 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 520.959\n",
            "INFO:tensorflow:loss = 0.0058299196, step = 9601 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.113\n",
            "INFO:tensorflow:loss = 0.005089478, step = 9701 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.576\n",
            "INFO:tensorflow:loss = 0.0052371146, step = 9801 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 439.378\n",
            "INFO:tensorflow:loss = 0.004309972, step = 9901 (0.228 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0063550808.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 84.87980838918082\n",
            "Just using average = 594.6139677666922 has RMSE of 97.2516195733968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca89b22b-8a14-4cd9-d96a-45b84f7ac66b",
        "id": "NdhxllohQ7zj"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fe04ad0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5295681  0.50279295 0.5186813  0.5550675  0.50926286 0.51478094\n",
            " 0.34972587 0.5107387  0.48962733 0.48202622 0.5077578  0.45241514\n",
            " 0.5303733  0.5249841  0.4819516  0.42660913 0.3957548  0.50908947\n",
            " 0.5008848  0.5027327  0.47737455 0.5260839  0.37693933 0.42754543\n",
            " 0.42254457 0.5017034  0.48117942 0.48468918 0.5377923  0.40605268\n",
            " 0.49176955 0.5660472  0.5093014  0.51203895 0.5470723  0.53878564\n",
            " 0.57368875 0.35110134 0.5360532  0.5356462  0.5778374  0.5130379\n",
            " 0.5725712  0.5003758  0.47311938 0.5116311  0.47152215 0.5449174\n",
            " 0.38731432 0.5207215  0.5822002  0.45452356 0.48952582 0.50627136\n",
            " 0.49711028 0.42456427 0.5276534  0.4993987  0.5580308  0.5286135\n",
            " 0.530842   0.51208967 0.4694868  0.5561785  0.51324785 0.3787415\n",
            " 0.50695443 0.51553756 0.426965   0.5357587  0.5119739  0.47602448\n",
            " 0.5302774  0.5139575  0.50684    0.4626025  0.5006992  0.50202507\n",
            " 0.45507282 0.5445356  0.5024633  0.46035033 0.48702568 0.54096586\n",
            " 0.49137062 0.47078678 0.46906513 0.53660595 0.49413583 0.56208515\n",
            " 0.3917925  0.58284813 0.6136512  0.54399055 0.38806015 0.54447585\n",
            " 0.50031    0.50059795 0.45328015 0.52153146 0.5190391  0.53173804\n",
            " 0.51755625 0.5065907  0.5386669  0.5308611  0.40301347 0.45141122\n",
            " 0.50258225 0.51202846 0.3975323  0.5374626  0.5623971  0.4951136\n",
            " 0.49897975 0.52115923 0.48865867 0.45493525 0.4631813  0.47915918\n",
            " 0.48364097 0.4598009  0.4844517  0.46840817 0.48918936 0.53601795\n",
            " 0.53590226 0.5063586  0.48816615 0.48990434 0.55404085 0.5765834\n",
            " 0.5076966  0.46981543 0.42900756 0.5136947  0.4721094  0.5858242\n",
            " 0.5733416  0.5124705  0.43201625 0.53485286 0.47858736 0.49760813\n",
            " 0.5462916  0.48102155 0.49214977 0.5119959  0.46158653 0.5138346\n",
            " 0.43823975 0.5030269  0.5457707  0.56450117 0.50187683 0.53611434\n",
            " 0.37537584 0.52164954 0.4779575  0.49072874 0.43379208 0.50616056\n",
            " 0.58251226 0.5832131  0.57786226 0.5055695  0.4584735  0.51391\n",
            " 0.48109597 0.5610681  0.47781217 0.5401305  0.51896083 0.4739053\n",
            " 0.4595921  0.5128704  0.52836585 0.42550224 0.50752926 0.47976568\n",
            " 0.4520603  0.5472777  0.4988885  0.5032157  0.45576122 0.47179857\n",
            " 0.4706963  0.4367553  0.56377894 0.3465704  0.52173215 0.37773663\n",
            " 0.5172009  0.5359326  0.5351858  0.46860585 0.5661537  0.5174699\n",
            " 0.53561497 0.5170666  0.5464727  0.40382513 0.54238915 0.5397718\n",
            " 0.5606007  0.52138364 0.57483166 0.49998048 0.5608205  0.49153847\n",
            " 0.5257375  0.47222456 0.5874198  0.41567373 0.48248148 0.5840844\n",
            " 0.50516397 0.47687578 0.44894293 0.57230026 0.39722925 0.47398588\n",
            " 0.49639028 0.50840735 0.49843317 0.5764044  0.5859183  0.5186235\n",
            " 0.53831947 0.5013333  0.50383174 0.53531843 0.46219394 0.5099394\n",
            " 0.56046927 0.50170124 0.5071354  0.53715056 0.40667433 0.515737\n",
            " 0.4841049  0.53356683 0.46591306 0.5456914  0.6114075  0.37154394\n",
            " 0.526733   0.37533358 0.4025653  0.4750153  0.40510628 0.5073359\n",
            " 0.3874255  0.4182371  0.5384709  0.6121675  0.48203722 0.51441354\n",
            " 0.5306496  0.490161   0.3989355  0.46704903 0.5209192  0.49575636\n",
            " 0.51795554 0.5022264  0.50111234 0.42662793 0.5273202  0.53985673\n",
            " 0.5106865  0.49137068 0.37760207 0.54611665 0.5664726  0.5474696\n",
            " 0.49035722 0.5072388  0.5799802  0.50791    0.55097646 0.5282199\n",
            " 0.5067187  0.50923234 0.5122245  0.47155946 0.46344984 0.5282465\n",
            " 0.48464665 0.51703185 0.48895523 0.52810675 0.5351645  0.469899\n",
            " 0.5077826  0.37603647 0.4809349  0.53094286 0.4509355  0.47048184\n",
            " 0.53903615 0.48074695 0.5097495  0.51572174 0.52610624 0.46867433\n",
            " 0.5297623  0.5251957  0.45996237 0.5273346  0.48809928 0.5305332\n",
            " 0.499281   0.5221395  0.45493722 0.4984856  0.47662976 0.5742906\n",
            " 0.49628043 0.50194955 0.5718119  0.5330871  0.49082196 0.50050163\n",
            " 0.3762033  0.53617674]\n",
            "[0.57019811 0.53832903 0.62962963 0.56330749 0.43927649 0.58570198\n",
            " 0.34969854 0.51937984 0.53402239 0.50990525 0.55986219 0.41429802\n",
            " 0.5994832  0.6287683  0.47372954 0.49784668 0.44099914 0.57364341\n",
            " 0.55555556 0.51593454 0.45564169 0.60378984 0.41429802 0.50301464\n",
            " 0.40223945 0.57278208 0.43669251 0.53919035 0.68044789 0.41171404\n",
            " 0.48751077 0.63738157 0.43238587 0.60637382 0.48234281 0.63910422\n",
            " 0.61757106 0.82773471 0.65030146 0.45564169 0.67355728 0.51851852\n",
            " 0.52799311 0.47975883 0.44358312 0.64513351 0.43066322 0.47028424\n",
            " 0.41257537 0.54263566 0.57536606 0.46511628 0.58053402 0.5503876\n",
            " 0.40999139 0.47459087 0.60723514 0.54694229 0.66063738 0.49698536\n",
            " 0.36864772 0.43496985 0.45650301 0.68217054 0.48751077 0.44272179\n",
            " 0.52540913 0.51421189 0.46770026 0.54435831 0.54349699 0.55986219\n",
            " 0.54263566 0.51593454 0.50559862 0.47717485 0.5047373  0.56244617\n",
            " 0.40137812 0.59259259 0.51679587 0.53660637 0.61154177 0.58484065\n",
            " 0.48406546 0.40913006 0.49009475 0.5245478  0.53402239 0.60809647\n",
            " 0.416882   0.54608096 0.63996555 0.44788975 0.39793282 0.52282515\n",
            " 0.4918174  0.62187769 0.50559862 0.6124031  0.60206718 0.56072351\n",
            " 0.58914729 0.49095607 0.60378984 0.59862188 0.4203273  0.57450474\n",
            " 0.71490095 0.58828596 0.43496985 0.51851852 0.61326443 0.62446167\n",
            " 0.43755383 0.60292851 0.44702842 0.41946598 0.53057709 0.44444444\n",
            " 0.59431525 0.47459087 0.49095607 0.43410853 0.48062016 0.59259259\n",
            " 0.5667528  0.59086994 0.49956934 0.42549526 0.61068045 0.583118\n",
            " 0.47372954 0.45650301 0.34625323 0.47631352 0.44702842 0.60981912\n",
            " 0.36606374 0.57364341 0.45305771 0.59431525 0.44272179 0.52799311\n",
            " 0.56761413 0.5796727  0.56847545 0.54866494 0.51765719 0.51507321\n",
            " 0.39276486 0.53660637 0.51593454 0.60809647 0.48062016 0.53057709\n",
            " 0.34022394 0.65374677 0.56330749 0.28682171 0.51335056 0.32213609\n",
            " 0.64341085 0.67786391 0.55211025 0.52196382 0.54263566 0.7166236\n",
            " 0.51248923 0.5538329  0.4754522  0.53574505 0.28251507 0.55297158\n",
            " 0.58484065 0.51248923 0.67700258 0.4918174  0.63824289 0.5503876\n",
            " 0.30577089 0.54349699 0.54177433 0.4788975  0.46597761 0.47372954\n",
            " 0.46856158 0.42635659 0.57708872 0.44530577 0.42807924 0.42980189\n",
            " 0.53057709 0.49095607 0.47459087 0.49009475 0.66494401 0.65202412\n",
            " 0.5667528  0.49009475 0.48664944 0.35228252 0.51076658 0.61498708\n",
            " 0.75107666 0.5667528  0.37639966 0.53229974 0.59431525 0.61412575\n",
            " 0.54177433 0.3910422  0.56847545 0.36606374 0.54349699 0.66063738\n",
            " 0.51162791 0.47459087 0.46080965 0.58656331 0.4625323  0.61670973\n",
            " 0.50990525 0.48234281 0.57881137 0.52971576 0.68561585 0.63049096\n",
            " 0.64944014 0.52799311 0.56503015 0.42721792 0.54005168 0.4788975\n",
            " 0.53488372 0.5960379  0.44358312 0.59000861 0.4461671  0.53229974\n",
            " 0.54091301 0.54349699 0.54608096 0.55986219 0.56589147 0.38329027\n",
            " 0.46942291 0.38587425 0.50387597 0.56330749 0.40999139 0.52368648\n",
            " 0.39534884 0.36950904 0.63910422 0.63049096 0.56158484 0.54694229\n",
            " 0.53402239 0.63738157 0.42463394 0.47286822 0.60206718 0.53143842\n",
            " 0.54694229 0.4918174  0.57278208 0.34797588 0.56589147 0.57364341\n",
            " 0.54349699 0.55986219 0.41515935 0.50387597 0.59345392 0.45047373\n",
            " 0.56933678 0.44702842 0.64254953 0.44702842 0.61412575 0.51076658\n",
            " 0.54091301 0.55900086 0.51851852 0.43669251 0.51421189 0.47028424\n",
            " 0.50645995 0.4039621  0.54521964 0.53229974 0.5994832  0.43410853\n",
            " 0.47459087 0.40568475 0.44358312 0.53143842 0.45736434 0.4918174\n",
            " 0.51248923 0.56244617 0.5047373  0.51162791 0.48148148 0.42204996\n",
            " 0.54952627 0.40999139 0.47803618 0.54521964 0.50215332 0.50215332\n",
            " 0.42291128 0.60637382 0.36520241 0.48664944 0.56503015 0.71403962\n",
            " 0.53143842 0.52196382 0.60551249 0.60809647 0.51421189 0.46942291\n",
            " 0.33850129 0.52368648]\n",
            "The trained model has an aproximate error rate of 24.105484259366253 which equates to 4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (mxpsd)"
      ],
      "metadata": {
        "id": "tKaVpVT8T55I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d43732-aa09-4f88-dfa2-5ac4532f87d4",
        "id": "_LbHDT1WUbJs"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] != 2012]\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] < 2020]\n",
        "cols = df_mxpsd_dnn['NUM_COLLISIONS']\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd_dnn[:6])\n",
        "df_mxpsd_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "15e79214-b1ea-4dae-8237-d971b44e8b07",
        "id": "OoJSkkKHUbJ3"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da        mxpsd          Apr          Aug  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean   2016.001567    15.737172    17.240110     0.082256     0.084998   \n",
              "std       2.000587     8.797367     5.858333     0.274808     0.278933   \n",
              "min    2013.000000     1.000000     5.100000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    13.000000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    15.900000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    20.000000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    49.000000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000  ...   \n",
              "mean      0.084998     0.077164     0.084998     0.084998     0.082256  ...   \n",
              "std       0.278933     0.266904     0.278933     0.278933     0.274808  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      0.081864     0.084998     0.081473     0.143361     0.142969   \n",
              "std       0.274212     0.278933     0.273613     0.350509     0.350110   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000     2553.000000  \n",
              "mean      0.142969     0.142969     0.142577     0.142186      599.033686  \n",
              "std       0.350110     0.350110     0.349710     0.349309      100.284761  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51d3b0d5-66c8-404e-813d-c5d466f1ede8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.001567</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.077164</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081864</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.081473</td>\n",
              "      <td>0.143361</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142577</td>\n",
              "      <td>0.142186</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000587</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.266904</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274212</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.273613</td>\n",
              "      <td>0.350509</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.349710</td>\n",
              "      <td>0.349309</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51d3b0d5-66c8-404e-813d-c5d466f1ede8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51d3b0d5-66c8-404e-813d-c5d466f1ede8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51d3b0d5-66c8-404e-813d-c5d466f1ede8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd_dnn.iloc[np.random.permutation(len(df_mxpsd_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3841fa99-24a1-41de-f17e-c65f2b5f5cf4",
        "id": "z1Ut5LCiUbJ4"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "2673  2017  29   17.1    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "3007  2014   9   21.0    0    0    0    0    0    0    0  ...    0    0    1   \n",
            "58    2017  20    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "2529  2017  27    9.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "3533  2017  24   18.1    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "870   2018  14   31.1    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2673    1    0    0    0    1    0    0  \n",
            "3007    0    0    0    0    0    0    1  \n",
            "58      0    0    0    0    1    0    0  \n",
            "2529    1    0    0    0    0    1    0  \n",
            "3533    0    0    1    0    0    0    0  \n",
            "870     0    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8aeadc8-3fbf-409c-8ee6-7383e882f9e8",
        "id": "QibQoUleUbJ4"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2673    757\n",
            "3007    499\n",
            "58      605\n",
            "2529    696\n",
            "3533    469\n",
            "870     652\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897c15ea-1d74-4272-8a51-17b36ba209e9",
        "id": "-C_tqkmUUbJ5"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_mxpsd', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27000bc-c14e-4ab1-abc5-6c80abc57d15",
        "id": "C3O7xWeMUbJ5"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99fe18510>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 387.63486, step = 1\n",
            "INFO:tensorflow:global_step/sec: 134.491\n",
            "INFO:tensorflow:loss = 0.2524597, step = 101 (0.761 sec)\n",
            "INFO:tensorflow:global_step/sec: 128.19\n",
            "INFO:tensorflow:loss = 0.2283586, step = 201 (0.768 sec)\n",
            "INFO:tensorflow:global_step/sec: 191.647\n",
            "INFO:tensorflow:loss = 0.21215409, step = 301 (0.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 217.601\n",
            "INFO:tensorflow:loss = 0.17387179, step = 401 (0.459 sec)\n",
            "INFO:tensorflow:global_step/sec: 222.618\n",
            "INFO:tensorflow:loss = 0.16214314, step = 501 (0.454 sec)\n",
            "INFO:tensorflow:global_step/sec: 185.284\n",
            "INFO:tensorflow:loss = 0.13822445, step = 601 (0.537 sec)\n",
            "INFO:tensorflow:global_step/sec: 149.462\n",
            "INFO:tensorflow:loss = 0.11480613, step = 701 (0.683 sec)\n",
            "INFO:tensorflow:global_step/sec: 141.227\n",
            "INFO:tensorflow:loss = 0.0870487, step = 801 (0.700 sec)\n",
            "INFO:tensorflow:global_step/sec: 154.951\n",
            "INFO:tensorflow:loss = 0.080515794, step = 901 (0.632 sec)\n",
            "INFO:tensorflow:global_step/sec: 177.73\n",
            "INFO:tensorflow:loss = 0.0602765, step = 1001 (0.577 sec)\n",
            "INFO:tensorflow:global_step/sec: 198.352\n",
            "INFO:tensorflow:loss = 0.049093798, step = 1101 (0.490 sec)\n",
            "INFO:tensorflow:global_step/sec: 242.043\n",
            "INFO:tensorflow:loss = 0.043484993, step = 1201 (0.419 sec)\n",
            "INFO:tensorflow:global_step/sec: 226.166\n",
            "INFO:tensorflow:loss = 0.03206719, step = 1301 (0.441 sec)\n",
            "INFO:tensorflow:global_step/sec: 212.909\n",
            "INFO:tensorflow:loss = 0.027769353, step = 1401 (0.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 186.659\n",
            "INFO:tensorflow:loss = 0.020404939, step = 1501 (0.529 sec)\n",
            "INFO:tensorflow:global_step/sec: 274.725\n",
            "INFO:tensorflow:loss = 0.016454922, step = 1601 (0.363 sec)\n",
            "INFO:tensorflow:global_step/sec: 188.125\n",
            "INFO:tensorflow:loss = 0.012083519, step = 1701 (0.546 sec)\n",
            "INFO:tensorflow:global_step/sec: 242.479\n",
            "INFO:tensorflow:loss = 0.012395976, step = 1801 (0.404 sec)\n",
            "INFO:tensorflow:global_step/sec: 312.541\n",
            "INFO:tensorflow:loss = 0.011649156, step = 1901 (0.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 259.64\n",
            "INFO:tensorflow:loss = 0.00968192, step = 2001 (0.382 sec)\n",
            "INFO:tensorflow:global_step/sec: 311.796\n",
            "INFO:tensorflow:loss = 0.008346951, step = 2101 (0.321 sec)\n",
            "INFO:tensorflow:global_step/sec: 317.859\n",
            "INFO:tensorflow:loss = 0.008415275, step = 2201 (0.315 sec)\n",
            "INFO:tensorflow:global_step/sec: 304.986\n",
            "INFO:tensorflow:loss = 0.0089824945, step = 2301 (0.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 324.152\n",
            "INFO:tensorflow:loss = 0.0078721065, step = 2401 (0.310 sec)\n",
            "INFO:tensorflow:global_step/sec: 253.423\n",
            "INFO:tensorflow:loss = 0.0069128573, step = 2501 (0.401 sec)\n",
            "INFO:tensorflow:global_step/sec: 223.975\n",
            "INFO:tensorflow:loss = 0.0074342717, step = 2601 (0.447 sec)\n",
            "INFO:tensorflow:global_step/sec: 157.541\n",
            "INFO:tensorflow:loss = 0.008736481, step = 2701 (0.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 262.267\n",
            "INFO:tensorflow:loss = 0.0062962472, step = 2801 (0.392 sec)\n",
            "INFO:tensorflow:global_step/sec: 194.142\n",
            "INFO:tensorflow:loss = 0.008458613, step = 2901 (0.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 191.055\n",
            "INFO:tensorflow:loss = 0.009114728, step = 3001 (0.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 246.794\n",
            "INFO:tensorflow:loss = 0.0063327434, step = 3101 (0.408 sec)\n",
            "INFO:tensorflow:global_step/sec: 252.955\n",
            "INFO:tensorflow:loss = 0.007219297, step = 3201 (0.399 sec)\n",
            "INFO:tensorflow:global_step/sec: 273.457\n",
            "INFO:tensorflow:loss = 0.008174543, step = 3301 (0.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 255.878\n",
            "INFO:tensorflow:loss = 0.006709909, step = 3401 (0.391 sec)\n",
            "INFO:tensorflow:global_step/sec: 290.705\n",
            "INFO:tensorflow:loss = 0.009328935, step = 3501 (0.346 sec)\n",
            "INFO:tensorflow:global_step/sec: 273.714\n",
            "INFO:tensorflow:loss = 0.008351646, step = 3601 (0.365 sec)\n",
            "INFO:tensorflow:global_step/sec: 277.027\n",
            "INFO:tensorflow:loss = 0.006794453, step = 3701 (0.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 273.638\n",
            "INFO:tensorflow:loss = 0.0058503565, step = 3801 (0.361 sec)\n",
            "INFO:tensorflow:global_step/sec: 274.687\n",
            "INFO:tensorflow:loss = 0.0075830426, step = 3901 (0.369 sec)\n",
            "INFO:tensorflow:global_step/sec: 308.029\n",
            "INFO:tensorflow:loss = 0.0070190015, step = 4001 (0.318 sec)\n",
            "INFO:tensorflow:global_step/sec: 387.306\n",
            "INFO:tensorflow:loss = 0.0067536803, step = 4101 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.222\n",
            "INFO:tensorflow:loss = 0.0063421773, step = 4201 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.318\n",
            "INFO:tensorflow:loss = 0.010062398, step = 4301 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.185\n",
            "INFO:tensorflow:loss = 0.007113188, step = 4401 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 500.758\n",
            "INFO:tensorflow:loss = 0.008665113, step = 4501 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 482.644\n",
            "INFO:tensorflow:loss = 0.006336936, step = 4601 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 470.655\n",
            "INFO:tensorflow:loss = 0.007137845, step = 4701 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 433.281\n",
            "INFO:tensorflow:loss = 0.007299641, step = 4801 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 567.91\n",
            "INFO:tensorflow:loss = 0.0068794214, step = 4901 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.099\n",
            "INFO:tensorflow:loss = 0.0064398907, step = 5001 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.802\n",
            "INFO:tensorflow:loss = 0.0071355077, step = 5101 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 590.615\n",
            "INFO:tensorflow:loss = 0.008556318, step = 5201 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 501.027\n",
            "INFO:tensorflow:loss = 0.009101203, step = 5301 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 513.458\n",
            "INFO:tensorflow:loss = 0.0064091026, step = 5401 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.434\n",
            "INFO:tensorflow:loss = 0.0060940646, step = 5501 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.631\n",
            "INFO:tensorflow:loss = 0.006466831, step = 5601 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.463\n",
            "INFO:tensorflow:loss = 0.0061270474, step = 5701 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.326\n",
            "INFO:tensorflow:loss = 0.0071042925, step = 5801 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 470.478\n",
            "INFO:tensorflow:loss = 0.0075332117, step = 5901 (0.213 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.603\n",
            "INFO:tensorflow:loss = 0.006960958, step = 6001 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.613\n",
            "INFO:tensorflow:loss = 0.0074220123, step = 6101 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.902\n",
            "INFO:tensorflow:loss = 0.0060907034, step = 6201 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.806\n",
            "INFO:tensorflow:loss = 0.0065321494, step = 6301 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 471.73\n",
            "INFO:tensorflow:loss = 0.0077069215, step = 6401 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.581\n",
            "INFO:tensorflow:loss = 0.0082532745, step = 6501 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.646\n",
            "INFO:tensorflow:loss = 0.00806609, step = 6601 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.485\n",
            "INFO:tensorflow:loss = 0.007020658, step = 6701 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 594.135\n",
            "INFO:tensorflow:loss = 0.008104332, step = 6801 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.711\n",
            "INFO:tensorflow:loss = 0.0077574686, step = 6901 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.472\n",
            "INFO:tensorflow:loss = 0.0067128018, step = 7001 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.768\n",
            "INFO:tensorflow:loss = 0.0067042178, step = 7101 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 580.82\n",
            "INFO:tensorflow:loss = 0.00644201, step = 7201 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.439\n",
            "INFO:tensorflow:loss = 0.008019918, step = 7301 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.562\n",
            "INFO:tensorflow:loss = 0.007639049, step = 7401 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.986\n",
            "INFO:tensorflow:loss = 0.008332738, step = 7501 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.738\n",
            "INFO:tensorflow:loss = 0.0061526117, step = 7601 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.645\n",
            "INFO:tensorflow:loss = 0.0071228174, step = 7701 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 559.849\n",
            "INFO:tensorflow:loss = 0.007512519, step = 7801 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.845\n",
            "INFO:tensorflow:loss = 0.0094731795, step = 7901 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.802\n",
            "INFO:tensorflow:loss = 0.0058148326, step = 8001 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.61\n",
            "INFO:tensorflow:loss = 0.010036858, step = 8101 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.409\n",
            "INFO:tensorflow:loss = 0.008030435, step = 8201 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.396\n",
            "INFO:tensorflow:loss = 0.007748973, step = 8301 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.336\n",
            "INFO:tensorflow:loss = 0.006621507, step = 8401 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 409.307\n",
            "INFO:tensorflow:loss = 0.008814316, step = 8501 (0.246 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.662\n",
            "INFO:tensorflow:loss = 0.008538632, step = 8601 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.974\n",
            "INFO:tensorflow:loss = 0.0069237575, step = 8701 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 488.722\n",
            "INFO:tensorflow:loss = 0.007688159, step = 8801 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 446.719\n",
            "INFO:tensorflow:loss = 0.007464015, step = 8901 (0.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 463.351\n",
            "INFO:tensorflow:loss = 0.0069954162, step = 9001 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.285\n",
            "INFO:tensorflow:loss = 0.008024704, step = 9101 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.736\n",
            "INFO:tensorflow:loss = 0.007055356, step = 9201 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 556.139\n",
            "INFO:tensorflow:loss = 0.0063067256, step = 9301 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 447.254\n",
            "INFO:tensorflow:loss = 0.0077122217, step = 9401 (0.225 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.559\n",
            "INFO:tensorflow:loss = 0.010223605, step = 9501 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.154\n",
            "INFO:tensorflow:loss = 0.00782764, step = 9601 (0.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.326\n",
            "INFO:tensorflow:loss = 0.007167926, step = 9701 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 483.906\n",
            "INFO:tensorflow:loss = 0.007476995, step = 9801 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.235\n",
            "INFO:tensorflow:loss = 0.009922021, step = 9901 (0.263 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0078783985.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 99.44888454495245\n",
            "Just using average = 598.5901077375122 has RMSE of 99.44707720459567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c76456-8b23-43cb-e466-61a937fe698c",
        "id": "NoIPNaT3UbJ5"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99caa5d90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.42953092 0.58093435 0.547737   0.42803988 0.50829405 0.55152094\n",
            " 0.6126497  0.43592477 0.5476345  0.53471494 0.56686586 0.47182697\n",
            " 0.39176238 0.4693615  0.3729343  0.54688    0.42072406 0.45294148\n",
            " 0.48109403 0.47650725 0.5542305  0.46227285 0.49462044 0.42985213\n",
            " 0.5012755  0.5346714  0.48129952 0.56147915 0.35417247 0.56215984\n",
            " 0.5175171  0.5142127  0.49538976 0.5126357  0.5002532  0.55393535\n",
            " 0.5769882  0.47956684 0.3767095  0.46287817 0.4863152  0.55253226\n",
            " 0.5028946  0.3930714  0.5127156  0.5674427  0.4293223  0.549355\n",
            " 0.5250859  0.4948222  0.5009982  0.4990203  0.54888207 0.50419754\n",
            " 0.5367106  0.53662014 0.5177856  0.5170085  0.5025372  0.49461117\n",
            " 0.5165586  0.47604793 0.48499322 0.49749807 0.5238497  0.5394536\n",
            " 0.48843837 0.5654981  0.47369888 0.5106676  0.5085487  0.5107725\n",
            " 0.6098803  0.61360294 0.4815458  0.38714176 0.54141885 0.4820989\n",
            " 0.48027164 0.4528684  0.5759838  0.39095393 0.52050817 0.5397778\n",
            " 0.5009769  0.47250527 0.50374    0.5533272  0.50827444 0.51172245\n",
            " 0.51803595 0.5286182  0.4563974  0.4212417  0.5212746  0.3548979\n",
            " 0.49216962 0.42863485 0.53834105 0.40059838 0.49932292 0.5130475\n",
            " 0.4982585  0.5107248  0.50180244 0.51745415 0.53892124 0.4896409\n",
            " 0.5152546  0.51402617 0.5400454  0.51129013 0.3781872  0.51198465\n",
            " 0.5531494  0.46094868 0.47981474 0.48330617 0.5118357  0.47467822\n",
            " 0.47021732 0.51314926 0.48298877 0.49790365 0.5385717  0.49431407\n",
            " 0.53674495 0.5273687  0.51225644 0.49625275 0.51463497 0.48664334\n",
            " 0.4075485  0.49211046 0.50558394 0.50919425 0.53833187 0.45278692\n",
            " 0.40156558 0.51464164 0.50653756 0.48144245 0.5283377  0.501762\n",
            " 0.6166728  0.52624327 0.53747076 0.52920383 0.5037012  0.52902347\n",
            " 0.39252776 0.50658363 0.5647134  0.492573   0.5091441  0.5309538\n",
            " 0.47985476 0.5355374  0.42451102 0.5315337  0.4648125  0.5588609\n",
            " 0.5104034  0.5545349  0.50044596 0.38734633 0.41976452 0.3888531\n",
            " 0.5888305  0.51367664 0.42645112 0.5294834  0.55697644 0.45193923\n",
            " 0.48565698 0.3807181  0.578856   0.5003184  0.5098618  0.40394977\n",
            " 0.3509979  0.52282476 0.5293563  0.4604794  0.5069032  0.58969367\n",
            " 0.4909165  0.51396996 0.46276066 0.57955456 0.42576036 0.47385103\n",
            " 0.5102294  0.5324651  0.52790374 0.5270544  0.4897783  0.50916827\n",
            " 0.41783723 0.5050302  0.5082287  0.5114539  0.57873225 0.5126452\n",
            " 0.52104497 0.5017943  0.4522531  0.5376869  0.58336705 0.45518714\n",
            " 0.3699084  0.43547612 0.56831706 0.50879526 0.5378674  0.46643347\n",
            " 0.51712227 0.5076213  0.5378501  0.4971302  0.5262178  0.53443617\n",
            " 0.5125011  0.49042606 0.4932851  0.43739977 0.56470424 0.4549872\n",
            " 0.4560624  0.53289056 0.4909209  0.3778641  0.5412336  0.3993229\n",
            " 0.478805   0.504514   0.49791464 0.5402459  0.53496516 0.38707694\n",
            " 0.4929717  0.5360051  0.51356494 0.47415286 0.5404459  0.5198505\n",
            " 0.49143013 0.4926284  0.5849676  0.4749565  0.47962543 0.5281353\n",
            " 0.51240903 0.5043231  0.500919   0.43834728 0.50021434 0.49513307\n",
            " 0.50290865 0.51122457 0.5371465  0.48872644 0.46688455 0.5123994\n",
            " 0.43587494 0.5146067  0.5854573  0.5089512  0.5046553  0.5187509\n",
            " 0.519861   0.5499838  0.35070953 0.39015475 0.4604973  0.50773644\n",
            " 0.37876192 0.483741   0.4882697  0.48058403 0.4843278  0.47128144\n",
            " 0.45563808 0.45909387 0.5140261  0.39039016 0.4227483  0.48598447\n",
            " 0.46016955 0.5491652  0.5157307  0.5015329  0.5658492  0.493398\n",
            " 0.5028787  0.49750334 0.5879924  0.5654774  0.47439715 0.47335276\n",
            " 0.45515323 0.5027848  0.48243076 0.49815336 0.39352027 0.5544416\n",
            " 0.507771   0.4641294  0.5034512  0.45906517 0.40551347 0.5698377\n",
            " 0.48808318 0.45957452 0.52999645 0.47148535 0.4206375  0.5645469\n",
            " 0.39740643 0.53091013 0.549641   0.5467032  0.5415109  0.5522424\n",
            " 0.53604174 0.49526948 0.5793062  0.5665511  0.47220933 0.43817323\n",
            " 0.5287176  0.5079087  0.52713615 0.46358928 0.52131045 0.46068212\n",
            " 0.53192264 0.48207703 0.5130138  0.49278715 0.45489147 0.5010895\n",
            " 0.5001319  0.547182   0.4804561  0.39944676 0.39772606 0.61356467\n",
            " 0.51286066 0.4295305  0.5367378  0.5551685  0.5514573  0.45447394\n",
            " 0.53145295 0.5096316  0.45264468 0.46605444 0.48249513 0.49113116\n",
            " 0.5302591  0.45783904 0.53943443 0.5126163  0.56465733 0.47459257\n",
            " 0.5669324  0.53633106 0.538688   0.56417555 0.3905152  0.5385812\n",
            " 0.5088369  0.36763623 0.48811975 0.52906096 0.56615245 0.5131252\n",
            " 0.35240775 0.46132776 0.46789047 0.56504166 0.47749445 0.51184016\n",
            " 0.4740069  0.534469   0.49163908 0.5841791  0.5667517  0.55244344\n",
            " 0.4800063  0.51085436 0.48016807 0.39125046 0.5386259  0.5368432\n",
            " 0.5222784  0.5011442  0.5671529  0.3496547  0.48845518 0.45368433\n",
            " 0.533028   0.37081665 0.43418095 0.45344254 0.5260899  0.5002703\n",
            " 0.5092476  0.5278069  0.523388   0.5032665  0.40034774 0.51107097\n",
            " 0.57697046 0.58405375 0.5872035  0.53233063 0.43385485 0.48119697\n",
            " 0.50329274 0.5600231  0.50854087 0.48241442 0.47779965 0.5504843\n",
            " 0.5051634  0.6129597  0.5744411  0.50223047 0.52425486 0.47845963\n",
            " 0.47957477 0.42659745 0.61579436 0.5355243  0.5720882  0.4994714\n",
            " 0.58680767 0.3891302  0.58417016 0.4667926  0.5662783  0.4523896\n",
            " 0.5671499  0.5128186  0.49985358 0.42846698 0.50374436 0.5378378\n",
            " 0.58732456 0.53780675 0.5460137  0.40696907 0.50426847 0.5788903\n",
            " 0.42776918 0.40002474 0.51773083 0.47247943 0.48320895 0.48527202\n",
            " 0.43927142 0.5631464  0.50403154 0.4236318  0.4827088  0.5071741\n",
            " 0.58011943 0.58804566 0.546153   0.49910253 0.53799075 0.3898721\n",
            " 0.51325864 0.5244516  0.4986512  0.4914287  0.5074096  0.54174554\n",
            " 0.48399526 0.53636724 0.45330828 0.5645416  0.56594676 0.5288514\n",
            " 0.48066926 0.52819633 0.5507415  0.48006752 0.43502882 0.4094468\n",
            " 0.51358116 0.52369064 0.5160055  0.39092198 0.48944616 0.51509756\n",
            " 0.5095554  0.5844155  0.3981256  0.4976971  0.45731017 0.5355256\n",
            " 0.51310647 0.5404126  0.526496   0.4817076  0.47607136 0.5101062\n",
            " 0.51712185]\n",
            "[0.50904393 0.66838932 0.53402239 0.50301464 0.47372954 0.53919035\n",
            " 0.60981912 0.38587425 0.48664944 0.59345392 0.50904393 0.57019811\n",
            " 0.48492679 0.42204996 0.42204996 0.54780362 0.45822567 0.41085271\n",
            " 0.4005168  0.48923342 0.31007752 0.42635659 0.53919035 0.44272179\n",
            " 0.55555556 0.43066322 0.51162791 0.75107666 0.29371232 0.55641688\n",
            " 0.46167097 0.69422911 0.49612403 0.56847545 0.49267873 0.6873385\n",
            " 0.62015504 0.47372954 0.40568475 0.50215332 0.60034453 0.57450474\n",
            " 0.54177433 0.39879414 0.63910422 0.57364341 0.44358312 0.4496124\n",
            " 0.54091301 0.53402239 0.50732127 0.42204996 0.59000861 0.56503015\n",
            " 0.53057709 0.59259259 0.55555556 0.55641688 0.54177433 0.60723514\n",
            " 0.53316107 0.49267873 0.44788975 0.52885444 0.56761413 0.57536606\n",
            " 0.45822567 0.59086994 0.56589147 0.5211025  0.43583118 0.54435831\n",
            " 0.56847545 0.66322136 0.60206718 0.47372954 0.46339363 0.4625323\n",
            " 0.49095607 0.50129199 0.63996555 0.4754522  0.61929371 0.55900086\n",
            " 0.51679587 0.5245478  0.53832903 0.58570198 0.47459087 0.56072351\n",
            " 0.49009475 0.65202412 0.50301464 0.44702842 0.46425495 0.37209302\n",
            " 0.48751077 0.47286822 0.59259259 0.37812231 0.50301464 0.55986219\n",
            " 0.54780362 0.62015504 0.50990525 0.6744186  0.51507321 0.52971576\n",
            " 0.58570198 0.50559862 0.55727821 0.55900086 0.41515935 0.45047373\n",
            " 0.63135228 0.47803618 0.41085271 0.48923342 0.54349699 0.55297158\n",
            " 0.34625323 0.57364341 0.48664944 0.57450474 0.67011197 0.54435831\n",
            " 0.65030146 0.49354005 0.45305771 0.62273902 0.60206718 0.50990525\n",
            " 0.4461671  0.61412575 0.55641688 0.33763997 0.48148148 0.49354005\n",
            " 0.39276486 0.51593454 0.61584841 0.40913006 0.49267873 0.52885444\n",
            " 0.64685616 0.49870801 0.63135228 0.5503876  0.53660637 0.54091301\n",
            " 0.40999139 0.54694229 0.63479759 0.52799311 0.48664944 0.5994832\n",
            " 0.44444444 0.5994832  0.51076658 0.53143842 0.51851852 0.60120586\n",
            " 0.45564169 0.52713178 0.4005168  0.40137812 0.43066322 0.35745047\n",
            " 0.60120586 0.50215332 0.40654608 0.55641688 0.58656331 0.43669251\n",
            " 0.53919035 0.34366925 0.62360034 0.52799311 0.45391904 0.49267873\n",
            " 0.32730405 0.60637382 0.56158484 0.55900086 0.58570198 0.56072351\n",
            " 0.47286822 0.5081826  0.43669251 0.74677003 0.4332472  0.82687339\n",
            " 0.58225668 0.91731266 0.37898363 0.58570198 0.48062016 0.40568475\n",
            " 0.3712317  0.50387597 0.5796727  0.55469423 0.55211025 0.35658915\n",
            " 0.70801034 0.49870801 0.46339363 0.51335056 0.57536606 0.42894057\n",
            " 0.37726098 0.48492679 0.5211025  0.48234281 0.60981912 0.39965547\n",
            " 0.55813953 0.43152455 0.44358312 0.48406546 0.54177433 0.58225668\n",
            " 0.53488372 0.4918174  0.45564169 0.40913006 0.50904393 0.54005168\n",
            " 0.51765719 0.59345392 0.60809647 0.40654608 0.61154177 0.44875108\n",
            " 0.4754522  0.47286822 0.46683893 0.61498708 0.62015504 0.36778639\n",
            " 0.48492679 0.52627046 0.61068045 0.44186047 0.65202412 0.28251507\n",
            " 0.52799311 0.5538329  0.66063738 0.45047373 0.51765719 0.61757106\n",
            " 0.49095607 0.47459087 0.47459087 0.47717485 0.57536606 0.54435831\n",
            " 0.4918174  0.49698536 0.51593454 0.52713178 0.49354005 0.50043066\n",
            " 0.45650301 0.51507321 0.46597761 0.47114556 0.57881137 0.66149871\n",
            " 0.68044789 0.3910422  0.34969854 0.44099914 0.58484065 0.54177433\n",
            " 0.38845823 0.43496985 0.47028424 0.44530577 0.49698536 0.46856158\n",
            " 0.4461671  0.56933678 0.46425495 0.42204996 0.41257537 0.48148148\n",
            " 0.55900086 0.51937984 0.64427218 0.51076658 0.5667528  0.46942291\n",
            " 0.45564169 0.48578811 0.69336779 0.59173127 0.54866494 0.48837209\n",
            " 0.44013781 0.52540913 0.47372954 0.28682171 0.47372954 0.6089578\n",
            " 0.53229974 0.51421189 0.53229974 0.50043066 0.45478036 0.58828596\n",
            " 0.5047373  0.50990525 0.47028424 0.41343669 0.46425495 0.51507321\n",
            " 0.38587425 0.47717485 0.49870801 0.55469423 0.53402239 0.42204996\n",
            " 0.45564169 0.50904393 0.61154177 0.58139535 0.43927649 0.51593454\n",
            " 0.60292851 0.50559862 0.68475452 0.46339363 0.54263566 0.46942291\n",
            " 0.50732127 0.48664944 0.59000861 0.50990525 0.45047373 0.62187769\n",
            " 0.53574505 0.66149871 0.41257537 0.43496985 0.4625323  0.38845823\n",
            " 0.43496985 0.4918174  0.5994832  0.625323   0.56416882 0.48148148\n",
            " 0.59862188 0.60292851 0.30577089 0.47803618 0.38501292 0.63738157\n",
            " 0.57019811 0.38673557 0.60378984 0.58828596 0.65891473 0.66925065\n",
            " 0.45994832 0.49095607 0.52799311 0.57708872 0.38070629 0.59517657\n",
            " 0.49698536 0.43841516 0.61154177 0.53057709 0.5538329  0.52196382\n",
            " 0.35400517 0.53660637 0.5994832  0.67183463 0.38845823 0.49784668\n",
            " 0.50043066 0.59431525 0.4918174  0.70887166 0.66494401 0.65288544\n",
            " 0.45219638 0.53660637 0.5374677  0.44875108 0.53402239 0.54435831\n",
            " 0.48234281 0.47975883 0.59345392 0.44530577 0.51765719 0.5667528\n",
            " 0.60809647 0.37812231 0.34625323 0.41085271 0.63738157 0.41257537\n",
            " 0.44272179 0.56589147 0.52799311 0.4918174  0.43927649 0.43583118\n",
            " 0.66838932 0.55813953 0.625323   0.5503876  0.40913006 0.42894057\n",
            " 0.56244617 0.86046512 0.56330749 0.50043066 0.43927649 0.60551249\n",
            " 0.54263566 0.64427218 0.54694229 0.44702842 0.50990525 0.44875108\n",
            " 0.51421189 0.43066322 0.63221361 0.47200689 0.53574505 0.54177433\n",
            " 0.60981912 0.41515935 0.55986219 0.55641688 0.72782084 0.46942291\n",
            " 0.52971576 0.60637382 0.4625323  0.45564169 0.54694229 0.54952627\n",
            " 0.60120586 0.55727821 0.48406546 0.52024117 0.68130922 0.53488372\n",
            " 0.44875108 0.42463394 0.62187769 0.47200689 0.30577089 0.46339363\n",
            " 0.39190353 0.55813953 0.53229974 0.40223945 0.50990525 0.5211025\n",
            " 0.66925065 0.61068045 0.51593454 0.53143842 0.60292851 0.39965547\n",
            " 0.59259259 0.56416882 0.46339363 0.4918174  0.50990525 0.58484065\n",
            " 0.45305771 0.55211025 0.48148148 0.49612403 0.5538329  0.47028424\n",
            " 0.49095607 0.47459087 0.6124031  0.47717485 0.416882   0.45478036\n",
            " 0.54952627 0.64254953 0.46080965 0.43238587 0.52799311 0.4918174\n",
            " 0.3712317  0.55727821 0.4461671  0.57795004 0.43669251 0.63824289\n",
            " 0.57622739 0.42463394 0.54694229 0.5796727  0.44444444 0.51937984\n",
            " 0.63738157]\n",
            "The trained model has an aproximate error rate of 21.30808120872643 which equates to 4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others"
      ],
      "metadata": {
        "id": "Dzn0MDJwVdg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/datadnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b62ef2-058e-469c-fe6b-b2ef0b5f6f5c",
        "id": "r3AhZB1bVybn"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn = df.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "dnn = dnn.loc[dnn[\"year\"] != 2012]\n",
        "dnn = dnn.loc[dnn[\"year\"] < 2020]\n",
        "cols = dnn['NUM_COLLISIONS']\n",
        "dnn = dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "dnn.insert(loc=26, column='NUM_COLLISIONS', value=cols)\n",
        "print(dnn[:6])\n",
        "dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "fc5ec123-9577-48a1-fc4b-2c704e9cc4df",
        "id": "izyExLEIVybo"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  Nov  Oct  \\\n",
            "49  2016  28  35.0   10.0   4.3  46.0  23.0  999.9    0    0  ...    0    0   \n",
            "51  2014  17  38.6    6.7   3.7  44.1  32.0  999.9    0    0  ...    0    0   \n",
            "54  2016  25  33.5   10.0   6.5  37.9  30.0  999.9    0    0  ...    0    0   \n",
            "55  2016  29  41.3   10.0   5.9  45.0  23.0  999.9    0    0  ...    0    0   \n",
            "58  2017  20  39.9   10.0   4.3  45.0  37.0  999.9    0    0  ...    0    0   \n",
            "59  2013  13  45.4    4.3   5.8  46.9  44.1  999.9    0    0  ...    0    0   \n",
            "\n",
            "    Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    0    1             681  \n",
            "51    0    0    0    0    1    0    0             589  \n",
            "54    0    0    0    1    0    0    0             658  \n",
            "55    0    0    0    0    1    0    0             645  \n",
            "58    0    0    0    0    1    0    0             605  \n",
            "59    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 27 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         year           da         temp        visib         wdsp  \\\n",
              "count  2556.0  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean   2016.0    15.725743    51.487715     8.295618    10.682864   \n",
              "std       2.0     8.800168    14.162738     2.207870     4.242687   \n",
              "min    2013.0     1.000000     5.800000     0.200000     2.700000   \n",
              "25%    2014.0     8.000000    40.400000     7.100000     7.700000   \n",
              "50%    2016.0    16.000000    52.000000     9.400000    10.100000   \n",
              "75%    2018.0    23.000000    63.900000    10.000000    13.000000   \n",
              "max    2019.0    31.000000    77.500000    10.000000    39.300000   \n",
              "\n",
              "               max          min          sndp          Apr          Aug  ...  \\\n",
              "count  2556.000000  2556.000000  2.556000e+03  2556.000000  2556.000000  ...   \n",
              "mean     59.564280    43.869757  9.999000e+02     0.082160     0.084898  ...   \n",
              "std      14.279867    14.722751  2.274182e-13     0.274661     0.278785  ...   \n",
              "min      18.000000    -2.000000  9.999000e+02     0.000000     0.000000  ...   \n",
              "25%      48.000000    33.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "50%      60.100000    44.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "75%      72.000000    55.900000  9.999000e+02     0.000000     0.000000  ...   \n",
              "max      90.000000    71.600000  9.999000e+02     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean      0.082160     0.084898     0.082160     0.143192     0.142801   \n",
              "std       0.274661     0.278785     0.274661     0.350338     0.349939   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000     2556.000000  \n",
              "mean      0.142801     0.142801     0.142801     0.142801      599.118936  \n",
              "std       0.349939     0.349939     0.349939     0.349939      100.258581  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ee4732b-de1e-4896-8d0c-b709f9d827ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2556.0</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2.556000e+03</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>15.725743</td>\n",
              "      <td>51.487715</td>\n",
              "      <td>8.295618</td>\n",
              "      <td>10.682864</td>\n",
              "      <td>59.564280</td>\n",
              "      <td>43.869757</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.143192</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>599.118936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.0</td>\n",
              "      <td>8.800168</td>\n",
              "      <td>14.162738</td>\n",
              "      <td>2.207870</td>\n",
              "      <td>4.242687</td>\n",
              "      <td>14.279867</td>\n",
              "      <td>14.722751</td>\n",
              "      <td>2.274182e-13</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.350338</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>100.258581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>40.400000</td>\n",
              "      <td>7.100000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>33.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>9.400000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>60.100000</td>\n",
              "      <td>44.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.0</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>63.900000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>55.900000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.0</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>39.300000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>71.600000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ee4732b-de1e-4896-8d0c-b709f9d827ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ee4732b-de1e-4896-8d0c-b709f9d827ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ee4732b-de1e-4896-8d0c-b709f9d827ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = dnn.iloc[np.random.permutation(len(dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59eeefb3-5432-4e07-ebdd-2a8c72018d15",
        "id": "pOJhsz3dVybo"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  May  Nov  \\\n",
            "233   2016   3  44.4   10.0  15.5  52.0  34.0  999.9    0    0  ...    0    0   \n",
            "407   2013  23  36.3    9.7   7.0  41.0  30.9  999.9    0    0  ...    0    0   \n",
            "1803  2016   9  59.9   10.0  16.7  68.0  54.0  999.9    0    0  ...    0    0   \n",
            "2367  2015   6  73.5   10.0   9.3  81.0  66.9  999.9    0    1  ...    0    0   \n",
            "2226  2018  13  69.1    7.7   6.2  75.9  64.9  999.9    0    1  ...    0    0   \n",
            "1757  2016  14  62.2   10.0  12.9  72.0  54.0  999.9    0    0  ...    0    0   \n",
            "\n",
            "      Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "233     0    0    0    1    0    0    0    0  \n",
            "407     0    0    0    0    0    0    0    0  \n",
            "1803    0    0    0    0    0    0    0    1  \n",
            "2367    0    0    0    0    0    0    0    1  \n",
            "2226    0    0    0    0    1    0    0    0  \n",
            "1757    0    0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3148de4-b19b-4410-ab85-7e21e3c66f25",
        "id": "iOF3gP_XVybp"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "233     432\n",
            "407     521\n",
            "1803    686\n",
            "2367    630\n",
            "2226    617\n",
            "1757    717\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8807070f-c593-4bc6-d467-6404e811cb7a",
        "id": "7XAEry6uVybp"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867b526a-e0e8-403c-c213-59835d3213c4",
        "id": "sb-ehJ2tVybq"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99c5e6510>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 41.121037, step = 1\n",
            "INFO:tensorflow:global_step/sec: 350.821\n",
            "INFO:tensorflow:loss = 0.815585, step = 101 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 447.403\n",
            "INFO:tensorflow:loss = 0.29131812, step = 201 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.276\n",
            "INFO:tensorflow:loss = 0.09008282, step = 301 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.264\n",
            "INFO:tensorflow:loss = 0.052201133, step = 401 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.865\n",
            "INFO:tensorflow:loss = 0.030631417, step = 501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 501.748\n",
            "INFO:tensorflow:loss = 0.027305681, step = 601 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 586.305\n",
            "INFO:tensorflow:loss = 0.030179385, step = 701 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 513.791\n",
            "INFO:tensorflow:loss = 0.024460632, step = 801 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 466.332\n",
            "INFO:tensorflow:loss = 0.030052539, step = 901 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.883\n",
            "INFO:tensorflow:loss = 0.02157535, step = 1001 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 482.731\n",
            "INFO:tensorflow:loss = 0.023479909, step = 1101 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.895\n",
            "INFO:tensorflow:loss = 0.0161607, step = 1201 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 434.072\n",
            "INFO:tensorflow:loss = 0.021214208, step = 1301 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 422.322\n",
            "INFO:tensorflow:loss = 0.015709002, step = 1401 (0.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 374.845\n",
            "INFO:tensorflow:loss = 0.016686205, step = 1501 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.709\n",
            "INFO:tensorflow:loss = 0.014997038, step = 1601 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 448.334\n",
            "INFO:tensorflow:loss = 0.012139967, step = 1701 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 475.295\n",
            "INFO:tensorflow:loss = 0.015390252, step = 1801 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.405\n",
            "INFO:tensorflow:loss = 0.009081246, step = 1901 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 483.88\n",
            "INFO:tensorflow:loss = 0.013321485, step = 2001 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.935\n",
            "INFO:tensorflow:loss = 0.076612055, step = 2101 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 445.87\n",
            "INFO:tensorflow:loss = 0.009294031, step = 2201 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 332.02\n",
            "INFO:tensorflow:loss = 0.012181543, step = 2301 (0.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 331.614\n",
            "INFO:tensorflow:loss = 0.03967172, step = 2401 (0.304 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.255\n",
            "INFO:tensorflow:loss = 0.024578251, step = 2501 (0.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.748\n",
            "INFO:tensorflow:loss = 0.0061509684, step = 2601 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 426.145\n",
            "INFO:tensorflow:loss = 0.01900825, step = 2701 (0.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.102\n",
            "INFO:tensorflow:loss = 0.005693339, step = 2801 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.791\n",
            "INFO:tensorflow:loss = 0.027750913, step = 2901 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.112\n",
            "INFO:tensorflow:loss = 0.01101132, step = 3001 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 472.714\n",
            "INFO:tensorflow:loss = 0.008422564, step = 3101 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.251\n",
            "INFO:tensorflow:loss = 0.0095056575, step = 3201 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 539.019\n",
            "INFO:tensorflow:loss = 0.0050596474, step = 3301 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.123\n",
            "INFO:tensorflow:loss = 0.009722086, step = 3401 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.258\n",
            "INFO:tensorflow:loss = 0.019538067, step = 3501 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.853\n",
            "INFO:tensorflow:loss = 0.0045359684, step = 3601 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.23\n",
            "INFO:tensorflow:loss = 0.03219262, step = 3701 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.857\n",
            "INFO:tensorflow:loss = 0.0082541, step = 3801 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.754\n",
            "INFO:tensorflow:loss = 0.02779917, step = 3901 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 547.879\n",
            "INFO:tensorflow:loss = 0.013227006, step = 4001 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.32\n",
            "INFO:tensorflow:loss = 0.010321554, step = 4101 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.754\n",
            "INFO:tensorflow:loss = 0.0050165774, step = 4201 (0.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 452.878\n",
            "INFO:tensorflow:loss = 0.023380857, step = 4301 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 425.277\n",
            "INFO:tensorflow:loss = 0.016363684, step = 4401 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 404.117\n",
            "INFO:tensorflow:loss = 0.013125347, step = 4501 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 451.402\n",
            "INFO:tensorflow:loss = 0.043913662, step = 4601 (0.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.508\n",
            "INFO:tensorflow:loss = 0.0042059207, step = 4701 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.468\n",
            "INFO:tensorflow:loss = 0.0047325785, step = 4801 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 482.357\n",
            "INFO:tensorflow:loss = 0.09973879, step = 4901 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.683\n",
            "INFO:tensorflow:loss = 0.011007085, step = 5001 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 407.721\n",
            "INFO:tensorflow:loss = 0.013871314, step = 5101 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.142\n",
            "INFO:tensorflow:loss = 0.008320674, step = 5201 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 446.709\n",
            "INFO:tensorflow:loss = 0.006095698, step = 5301 (0.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 474.857\n",
            "INFO:tensorflow:loss = 0.0069011017, step = 5401 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.388\n",
            "INFO:tensorflow:loss = 0.003969486, step = 5501 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 461.949\n",
            "INFO:tensorflow:loss = 0.0052419836, step = 5601 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 456.597\n",
            "INFO:tensorflow:loss = 0.08472985, step = 5701 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 477.852\n",
            "INFO:tensorflow:loss = 0.011999918, step = 5801 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 522.42\n",
            "INFO:tensorflow:loss = 0.006288909, step = 5901 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.027\n",
            "INFO:tensorflow:loss = 0.0059521603, step = 6001 (0.214 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.156\n",
            "INFO:tensorflow:loss = 0.01883695, step = 6101 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 547.054\n",
            "INFO:tensorflow:loss = 0.006284953, step = 6201 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.594\n",
            "INFO:tensorflow:loss = 0.0046311747, step = 6301 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.995\n",
            "INFO:tensorflow:loss = 0.06069475, step = 6401 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.696\n",
            "INFO:tensorflow:loss = 0.039637923, step = 6501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 535.597\n",
            "INFO:tensorflow:loss = 0.007111039, step = 6601 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 538.634\n",
            "INFO:tensorflow:loss = 0.009366106, step = 6701 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.577\n",
            "INFO:tensorflow:loss = 0.004775678, step = 6801 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 514.548\n",
            "INFO:tensorflow:loss = 0.008088751, step = 6901 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 512.433\n",
            "INFO:tensorflow:loss = 0.0048426623, step = 7001 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 465.87\n",
            "INFO:tensorflow:loss = 0.014019792, step = 7101 (0.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 399.953\n",
            "INFO:tensorflow:loss = 0.01047597, step = 7201 (0.253 sec)\n",
            "INFO:tensorflow:global_step/sec: 396.7\n",
            "INFO:tensorflow:loss = 0.0038592778, step = 7301 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 407.092\n",
            "INFO:tensorflow:loss = 0.004220397, step = 7401 (0.240 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.711\n",
            "INFO:tensorflow:loss = 0.00533479, step = 7501 (0.276 sec)\n",
            "INFO:tensorflow:global_step/sec: 371.037\n",
            "INFO:tensorflow:loss = 0.017862938, step = 7601 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 411.372\n",
            "INFO:tensorflow:loss = 0.0127247665, step = 7701 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.237\n",
            "INFO:tensorflow:loss = 0.008917585, step = 7801 (0.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 373.084\n",
            "INFO:tensorflow:loss = 0.004681875, step = 7901 (0.268 sec)\n",
            "INFO:tensorflow:global_step/sec: 435.491\n",
            "INFO:tensorflow:loss = 0.0062672608, step = 8001 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 438.327\n",
            "INFO:tensorflow:loss = 0.008297937, step = 8101 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 328.744\n",
            "INFO:tensorflow:loss = 0.011101236, step = 8201 (0.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.021\n",
            "INFO:tensorflow:loss = 0.0037902743, step = 8301 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.419\n",
            "INFO:tensorflow:loss = 0.008938277, step = 8401 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.01\n",
            "INFO:tensorflow:loss = 0.008897681, step = 8501 (0.212 sec)\n",
            "INFO:tensorflow:global_step/sec: 443.583\n",
            "INFO:tensorflow:loss = 0.008832649, step = 8601 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 439.099\n",
            "INFO:tensorflow:loss = 0.0054526636, step = 8701 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 440.006\n",
            "INFO:tensorflow:loss = 0.004501671, step = 8801 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.032\n",
            "INFO:tensorflow:loss = 0.028410655, step = 8901 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.378\n",
            "INFO:tensorflow:loss = 0.011007977, step = 9001 (0.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 360.029\n",
            "INFO:tensorflow:loss = 0.0072302436, step = 9101 (0.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 477.702\n",
            "INFO:tensorflow:loss = 0.0036410606, step = 9201 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 545.093\n",
            "INFO:tensorflow:loss = 0.004839125, step = 9301 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.235\n",
            "INFO:tensorflow:loss = 0.011042772, step = 9401 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.835\n",
            "INFO:tensorflow:loss = 0.004070849, step = 9501 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 458.286\n",
            "INFO:tensorflow:loss = 0.0081976205, step = 9601 (0.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 486.529\n",
            "INFO:tensorflow:loss = 0.0049935468, step = 9701 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 430.792\n",
            "INFO:tensorflow:loss = 0.0070951465, step = 9801 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.541\n",
            "INFO:tensorflow:loss = 0.011438251, step = 9901 (0.244 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0060012704.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 78.24717634102431\n",
            "Just using average = 599.5909980430529 has RMSE of 99.55323398451917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d157bc-0d83-4c19-f547-9e723d383ca8",
        "id": "ssTSefyuVybq"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99bbfa950>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.018e+03 2.500e+01 4.530e+01 ... 0.000e+00 0.000e+00 1.000e+00]\n",
            " [2.013e+03 1.300e+01 2.810e+01 ... 1.000e+00 0.000e+00 0.000e+00]\n",
            " [2.014e+03 2.300e+01 5.950e+01 ... 0.000e+00 0.000e+00 1.000e+00]\n",
            " ...\n",
            " [2.018e+03 2.100e+01 4.850e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
            " [2.014e+03 1.500e+01 6.490e+01 ... 0.000e+00 1.000e+00 0.000e+00]\n",
            " [2.016e+03 1.700e+01 4.150e+01 ... 0.000e+00 1.000e+00 0.000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5568788  0.5393471  0.5443441  0.43283385 0.41556218 0.573106\n",
            " 0.5909341  0.5033253  0.5413491  0.5049177  0.47777614 0.54672474\n",
            " 0.5639878  0.57162917 0.5412146  0.53128785 0.5016192  0.39523876\n",
            " 0.46182284 0.5725149  0.5145943  0.531485   0.4341458  0.5636309\n",
            " 0.56185985 0.58443606 0.4041516  0.49701694 0.5192967  0.4863644\n",
            " 0.49219057 0.56869566 0.4372357  0.5065642  0.5068138  0.53947985\n",
            " 0.513665   0.4480784  0.50305    0.46447203 0.43132603 0.52332467\n",
            " 0.51088196 0.43565777 0.54483235 0.45655015 0.38729867 0.48060206\n",
            " 0.5368682  0.5477901  0.5097679  0.5493637  0.54218733 0.42459548\n",
            " 0.5168932  0.5528693  0.5290235  0.52981395 0.56537974 0.4876649\n",
            " 0.5584972  0.43458077 0.4603415  0.58139634 0.52097136 0.52923566\n",
            " 0.5807425  0.5560684  0.5416392  0.43494692 0.52106565 0.5496432\n",
            " 0.5339975  0.50240594 0.48058462 0.48125815 0.52696425 0.5093319\n",
            " 0.51559365 0.56482244 0.48939398 0.37166727 0.48819086 0.48747024\n",
            " 0.54286915 0.55485094 0.55647826 0.5767821  0.55075634 0.49695644\n",
            " 0.35678655 0.6073964  0.6028558  0.5493352  0.5703983  0.5489863\n",
            " 0.51289874 0.40037963 0.5475063  0.5300254  0.460835   0.5201348\n",
            " 0.49065837 0.5324412  0.58706605 0.52567613 0.6188333  0.5531651\n",
            " 0.5772875  0.46340844 0.385572   0.57194155 0.56188995 0.5253294\n",
            " 0.538096   0.5378933  0.510479   0.5297499  0.5272855  0.41853252\n",
            " 0.54111236 0.46155062 0.5251344  0.56181693 0.51304585 0.5100695\n",
            " 0.5433807  0.55497175 0.4708131  0.534313   0.53079593 0.53358936\n",
            " 0.47432965 0.48786613 0.5875497  0.4599949  0.5236834  0.4884914\n",
            " 0.5745201  0.51825845 0.5532626  0.53582376 0.5331906  0.5351598\n",
            " 0.5650415  0.60216445 0.38830543 0.50088865 0.47669968 0.50602615\n",
            " 0.5347124  0.49315766 0.54851824 0.39421412 0.49838337 0.4473069\n",
            " 0.40764323 0.52005917 0.51693046 0.5754374  0.53083664 0.54501235\n",
            " 0.5346058  0.5544195  0.47367248 0.46216267 0.4843626  0.5682951\n",
            " 0.4910691  0.545009   0.57004803 0.5102653  0.47624618 0.4790162\n",
            " 0.5409754  0.55696106 0.5398237  0.5489897  0.48505804 0.5159689\n",
            " 0.5126157  0.6004788  0.465593   0.5063476  0.45670635 0.52276295\n",
            " 0.54863167 0.4334028  0.49725142 0.4912561  0.44246015 0.44796672\n",
            " 0.5887557  0.42305002 0.51648307 0.39249143 0.52339655 0.54907244\n",
            " 0.50500864 0.62707454 0.534755   0.6243217  0.5818256  0.41409495\n",
            " 0.45872402 0.5469197  0.5642011  0.56510586 0.5260868  0.4887748\n",
            " 0.53707427 0.597066   0.445443   0.5682204  0.58091116 0.52517223\n",
            " 0.5852169  0.4808992  0.4907275  0.6051415  0.6032489  0.53444403\n",
            " 0.48764136 0.44968387 0.4227201  0.49440596 0.5917662  0.5428741\n",
            " 0.51013976 0.5733095  0.5600526  0.57510537 0.49534455 0.52676296\n",
            " 0.5891308  0.42352858 0.6333791  0.58473814 0.5494923  0.4677608\n",
            " 0.5467266  0.4981118  0.58164126 0.51920974 0.4611187  0.49310234\n",
            " 0.5942918  0.50354743 0.55608714 0.5066557  0.48461276 0.55702615\n",
            " 0.5266832  0.4312621  0.5209134  0.5556253  0.5384466  0.40784156\n",
            " 0.42687485 0.5129507  0.525128   0.42352268 0.44339615 0.5246506\n",
            " 0.37070692 0.36860526 0.4989685  0.43837386 0.5304819  0.38928318\n",
            " 0.52725023 0.37837365 0.52331436 0.48363346 0.47519907 0.48266163\n",
            " 0.52788126 0.41387334 0.46775776 0.54572415 0.5112238  0.48893276\n",
            " 0.5355075  0.5358935  0.5124676  0.5328727  0.56590647 0.55337775\n",
            " 0.4004459  0.5188919  0.5544589  0.4348973  0.43933475 0.5144996\n",
            " 0.49776557 0.55189043 0.5819078  0.39862484 0.5548864  0.43024504\n",
            " 0.5043768  0.51009655 0.49217883 0.5149727  0.5001035  0.5492706\n",
            " 0.5166624  0.4931521  0.56646514 0.5366196  0.46527687 0.49898908\n",
            " 0.574726   0.5883684  0.3950691  0.5374966  0.552587   0.56320864\n",
            " 0.5008078  0.5234751  0.4910166  0.56741506 0.45064262 0.48496634\n",
            " 0.5151946  0.54540545 0.51626307 0.4924858  0.52398777 0.45858452\n",
            " 0.53601617 0.52229774 0.5283469  0.62087303 0.54933393 0.52537775\n",
            " 0.56875837 0.53129244 0.55318385 0.5352832  0.48329479 0.43005532\n",
            " 0.53301346 0.45814583 0.5195212  0.5152594  0.4504889  0.49711207\n",
            " 0.46664795 0.4221561  0.61523414 0.5252914  0.5345161  0.46061948\n",
            " 0.48201975 0.495244   0.5174718  0.530767   0.41774663 0.51905\n",
            " 0.5707006  0.46597418 0.514666   0.47638094 0.4990538  0.46201348\n",
            " 0.46144018 0.48443025 0.5034409  0.42138025 0.49694356 0.4751264\n",
            " 0.5117536  0.5784263  0.5041798  0.55487967 0.4131661  0.5878349\n",
            " 0.55640495 0.47840393 0.4888983  0.5445568  0.42069146 0.52853763\n",
            " 0.5279658  0.4945049  0.59397054 0.4776029  0.5446102  0.48946026\n",
            " 0.536495   0.5502403  0.6040375  0.46484047 0.579855   0.55317384\n",
            " 0.5706611  0.49301782 0.56334966 0.5317426  0.5916549  0.54816264\n",
            " 0.546634   0.58467203 0.507153   0.5503344  0.54990137 0.54262894\n",
            " 0.5681667  0.36639333 0.49971417 0.48511526 0.35269955 0.4659097\n",
            " 0.55133283 0.49136052 0.54580235 0.4139239  0.47583312 0.48706204\n",
            " 0.5274313  0.4601648  0.55662566 0.5562107  0.49720284 0.5104118\n",
            " 0.5069289  0.5044582  0.500978   0.4785161  0.49528095 0.6000421\n",
            " 0.512387   0.57231545 0.52649295 0.5124791  0.48214012 0.53442883\n",
            " 0.541977   0.5746228  0.54182744 0.5675398  0.4212813  0.5422236\n",
            " 0.37732473 0.5592782  0.37255186 0.52720964 0.4822372  0.40592188\n",
            " 0.5286215  0.37900913 0.5399749  0.4980426  0.5602547  0.56832314\n",
            " 0.57236946 0.5053897  0.5431745  0.55501264 0.5398254  0.5402308\n",
            " 0.5788436  0.45803323 0.52425516 0.57324356 0.5107654  0.39857957\n",
            " 0.42973745 0.5836565  0.58528566 0.5054553  0.6388771  0.5478008\n",
            " 0.44561654 0.47230077 0.48362193 0.46259657 0.5634043  0.4151985\n",
            " 0.6174493  0.5539981  0.5224021  0.54882145 0.5184958  0.6119607\n",
            " 0.49555138 0.4093591  0.5571437  0.54964966 0.5878943  0.41389486\n",
            " 0.49375102 0.5289577  0.53728825 0.56888336 0.55325615 0.48072574\n",
            " 0.45824075 0.4509075  0.5128445  0.552841   0.4996318  0.59474796\n",
            " 0.5040945  0.5647741  0.5276064  0.48567238 0.60585874 0.43729818\n",
            " 0.5259113  0.497825  ]\n",
            "[0.65719208 0.59862188 0.5503876  0.44186047 0.40740741 0.65030146\n",
            " 0.60034453 0.49870801 0.68217054 0.56244617 0.45908699 0.43152455\n",
            " 0.50732127 0.51937984 0.55986219 0.46080965 0.47286822 0.48923342\n",
            " 0.51162791 0.27648579 0.53574505 0.49612403 0.46167097 0.51937984\n",
            " 0.64857881 0.70801034 0.43927649 0.374677   0.49440138 0.44444444\n",
            " 0.51076658 0.61412575 0.30577089 0.50904393 0.53143842 0.35486649\n",
            " 0.56761413 0.41171404 0.4788975  0.46339363 0.43066322 0.42463394\n",
            " 0.5047373  0.45391904 0.50215332 0.42721792 0.43066322 0.48578811\n",
            " 0.48664944 0.56244617 0.52196382 0.45133506 0.54694229 0.44358312\n",
            " 0.54177433 0.48062016 0.53402239 0.43496985 0.65116279 0.58570198\n",
            " 0.6089578  0.51335056 0.53832903 0.65288544 0.46770026 0.40568475\n",
            " 0.57708872 0.57278208 0.56416882 0.5047373  0.54521964 0.56847545\n",
            " 0.53919035 0.55297158 0.46080965 0.4918174  0.54694229 0.51248923\n",
            " 0.52627046 0.59776055 0.43755383 0.39965547 0.55986219 0.47631352\n",
            " 0.59862188 0.53143842 0.6089578  0.64341085 0.50215332 0.56933678\n",
            " 0.3875969  0.67355728 0.69164513 0.36089578 0.59776055 0.5994832\n",
            " 0.58914729 0.42118863 0.61929371 0.52799311 0.34711456 0.54177433\n",
            " 0.5960379  0.49354005 0.64082687 0.54435831 0.62273902 0.5960379\n",
            " 0.59431525 0.42894057 0.36434109 0.5211025  0.62618432 0.52024117\n",
            " 0.5994832  0.55727821 0.53919035 0.45736434 0.61929371 0.43755383\n",
            " 0.5374677  0.54780362 0.60809647 0.5374677  0.55986219 0.47975883\n",
            " 0.56158484 0.52196382 0.54005168 0.41085271 0.4203273  0.56589147\n",
            " 0.4918174  0.51937984 0.68044789 0.49009475 0.58828596 0.45391904\n",
            " 0.63652024 0.52196382 0.68475452 0.54349699 0.33850129 0.51593454\n",
            " 0.65202412 0.60551249 0.34969854 0.53832903 0.42807924 0.43152455\n",
            " 0.42463394 0.5211025  0.51335056 0.44530577 0.44788975 0.44358312\n",
            " 0.43238587 0.54694229 0.48664944 0.52971576 0.51937984 0.41860465\n",
            " 0.51765719 0.58570198 0.50129199 0.49784668 0.36864772 0.63738157\n",
            " 0.4952627  0.58914729 0.32988803 0.61154177 0.37984496 0.50387597\n",
            " 0.46167097 0.57364341 0.47631352 0.53919035 0.36606374 0.49870801\n",
            " 0.57795004 0.59173127 0.42635659 0.48751077 0.44272179 0.52282515\n",
            " 0.26098191 0.47803618 0.43755383 0.45047373 0.40913006 0.50990525\n",
            " 0.62273902 0.48751077 0.54694229 0.44444444 0.36175711 0.53574505\n",
            " 0.4788975  0.63479759 0.4952627  0.67355728 0.59689922 0.44358312\n",
            " 0.46856158 0.63565891 0.5211025  0.50129199 0.55641688 0.48148148\n",
            " 0.40999139 0.59689922 0.50559862 0.63996555 0.60120586 0.5211025\n",
            " 0.67011197 0.47975883 0.58484065 0.54521964 0.52971576 0.50387597\n",
            " 0.56933678 0.43927649 0.40137812 0.41515935 0.59345392 0.61929371\n",
            " 0.46339363 0.59776055 0.47631352 0.53402239 0.54091301 0.55813953\n",
            " 0.6416882  0.39362618 0.52196382 0.70542636 0.47028424 0.41946598\n",
            " 0.56847545 0.51248923 0.64685616 0.49956934 0.57622739 0.55555556\n",
            " 0.6873385  0.51851852 0.55297158 0.56158484 0.41946598 0.53488372\n",
            " 0.53057709 0.4461671  0.52885444 0.52971576 0.62101637 0.34022394\n",
            " 0.41085271 0.51248923 0.52627046 0.36520241 0.36347976 0.16192937\n",
            " 0.38845823 0.27562446 0.54435831 0.48492679 0.46942291 0.35400517\n",
            " 0.55641688 0.38501292 0.62187769 0.43927649 0.48320413 0.43755383\n",
            " 0.47200689 0.34797588 0.46683893 0.59259259 0.49009475 0.4918174\n",
            " 0.62015504 0.49440138 0.49095607 0.54263566 0.64857881 0.55555556\n",
            " 0.44272179 0.4918174  0.55211025 0.45822567 0.4918174  0.52885444\n",
            " 0.48406546 0.57536606 0.57450474 0.38329027 0.52540913 0.44530577\n",
            " 0.4918174  0.51507321 0.51937984 0.49440138 0.44444444 0.57364341\n",
            " 0.46770026 0.49612403 0.51593454 0.34022394 0.44099914 0.49698536\n",
            " 0.55641688 0.56158484 0.3910422  0.37726098 0.6124031  0.5667528\n",
            " 0.50990525 0.59776055 0.39190353 0.50559862 0.42807924 0.58828596\n",
            " 0.53660637 0.53488372 0.69509044 0.51765719 0.65891473 0.47717485\n",
            " 0.42291128 0.47717485 0.52799311 0.69336779 0.583118   0.56158484\n",
            " 0.64082687 0.583118   0.50129199 0.54349699 0.49784668 0.41429802\n",
            " 0.61326443 0.55900086 0.43669251 0.5047373  0.49354005 0.49009475\n",
            " 0.45650301 0.43496985 0.52885444 0.53229974 0.51593454 0.55124892\n",
            " 0.53832903 0.57364341 0.48664944 0.50990525 0.45478036 0.60206718\n",
            " 0.55986219 0.47114556 0.49698536 0.4788975  0.48062016 0.46683893\n",
            " 0.52713178 0.39707149 0.51593454 0.3453919  0.50732127 0.4754522\n",
            " 0.48664944 0.49956934 0.54091301 0.45908699 0.34366925 0.66838932\n",
            " 0.49956934 0.4918174  0.47803618 0.52368648 0.36089578 0.52713178\n",
            " 0.56244617 0.43841516 0.66408269 0.56072351 0.44186047 0.47114556\n",
            " 0.51162791 0.58570198 0.56416882 0.51937984 0.55727821 0.56589147\n",
            " 0.56072351 0.52713178 0.61584841 0.58570198 0.74677003 0.46339363\n",
            " 0.6416882  0.583118   0.57450474 0.53402239 0.49440138 0.6709733\n",
            " 0.66149871 0.43238587 0.56503015 0.58570198 0.4005168  0.49354005\n",
            " 0.51851852 0.49095607 0.60809647 0.36003445 0.4005168  0.47114556\n",
            " 0.49956934 0.5667528  0.5994832  0.46942291 0.47114556 0.5245478\n",
            " 0.39276486 0.34969854 0.44788975 0.42721792 0.54177433 0.62015504\n",
            " 0.60637382 0.55297158 0.44702842 0.49267873 0.45736434 0.60809647\n",
            " 0.48492679 0.5994832  0.57278208 0.53488372 0.39190353 0.52627046\n",
            " 0.36692506 0.59517657 0.35745047 0.52282515 0.54866494 0.46597761\n",
            " 0.52885444 0.40568475 0.60206718 0.4918174  0.52368648 0.53229974\n",
            " 0.62015504 0.51937984 0.51076658 0.56330749 0.65374677 0.57278208\n",
            " 0.59086994 0.48751077 0.54780362 0.61929371 0.54435831 0.37812231\n",
            " 0.41774332 0.56589147 0.63824289 0.49267873 0.56589147 0.57105943\n",
            " 0.4005168  0.50215332 0.35486649 0.47286822 0.55641688 0.42463394\n",
            " 0.63049096 0.57881137 0.45305771 0.63910422 0.56503015 0.68561585\n",
            " 0.56503015 0.4005168  0.57019811 0.56330749 0.36606374 0.40137812\n",
            " 0.48148148 0.63049096 0.61068045 0.56330749 0.36778639 0.51335056\n",
            " 0.49095607 0.43152455 0.47631352 0.7329888  0.48062016 0.67355728\n",
            " 0.61757106 0.38156761 0.80878553 0.44702842 0.69939707 0.51076658\n",
            " 0.4952627  0.45219638]\n",
            "The trained model has an aproximate error rate of 1.6106985103106144 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location\n"
      ],
      "metadata": {
        "id": "kHVsefPtZOYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REF https://www.geeksforgeeks.org/read-a-zipped-file-as-a-pandas-dataframe/ 12/11"
      ],
      "metadata": {
        "id": "rxJZ5Y6xoUBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/locdnn.zip', index_col=0,compression='zip' )\n",
        "print(df_loc[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965ff1ae-01ce-4d4a-b918-270b34a6db69",
        "id": "d4VkDncof2uz"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS   latitude  longitude  temp  dewp     slp  visib  \\\n",
            "1  2018   2               1  40.681750 -73.967480  14.7   2.0  1024.9   10.0   \n",
            "2  2018   2               1  40.645370 -73.945110  14.7   2.0  1024.9   10.0   \n",
            "3  2018   2               1  40.614830 -73.998380  14.7   2.0  1024.9   10.0   \n",
            "4  2018   2               1  40.592190 -74.087395  14.7   2.0  1024.9   10.0   \n",
            "5  2018   2               1  40.769817 -73.782370  14.7   2.0  1024.9   10.0   \n",
            "6  2018   2               1  40.660175 -73.928200  14.7   2.0  1024.9   10.0   \n",
            "\n",
            "   wdsp  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "2  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "4  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "5  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc_dnn = df_loc.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] != 2012]\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] < 2020]\n",
        "cols = df_loc_dnn['NUM_COLLISIONS']\n",
        "df_loc_dnn = df_loc_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_loc_dnn.insert(loc=28, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_loc_dnn[:6])\n",
        "df_loc_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "f1003dc6-4252-455e-e10a-0ccdf118be07",
        "id": "hl2pF-yuf2u0"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da   latitude  longitude  temp  visib  wdsp   max  min   sndp  ...  \\\n",
            "1  2018   2  40.681750 -73.967480  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "2  2018   2  40.645370 -73.945110  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "3  2018   2  40.614830 -73.998380  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "4  2018   2  40.592190 -74.087395  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "5  2018   2  40.769817 -73.782370  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "6  2018   2  40.660175 -73.928200  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "\n",
            "   Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "1    0    0    0    1    0    0    0    0    0               1  \n",
            "2    0    0    0    1    0    0    0    0    0               1  \n",
            "3    0    0    0    1    0    0    0    0    0               1  \n",
            "4    0    0    0    1    0    0    0    0    0               1  \n",
            "5    0    0    0    1    0    0    0    0    0               1  \n",
            "6    0    0    0    1    0    0    0    0    0               1  \n",
            "\n",
            "[6 rows x 29 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               year            da      latitude     longitude          temp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   2.016149e+03  1.566747e+01  4.072402e+01 -7.392063e+01  5.203610e+01   \n",
              "std    1.970562e+00  8.752606e+00  7.845179e-02  8.651997e-02  1.410404e+01   \n",
              "min    2.013000e+03  1.000000e+00  4.049895e+01 -7.425453e+01  5.800000e+00   \n",
              "25%    2.014000e+03  8.000000e+00  4.066895e+01 -7.397644e+01  4.100000e+01   \n",
              "50%    2.016000e+03  1.600000e+01  4.072234e+01 -7.392891e+01  5.300000e+01   \n",
              "75%    2.018000e+03  2.300000e+01  4.076838e+01 -7.386641e+01  6.430000e+01   \n",
              "max    2.019000e+03  3.100000e+01  4.091288e+01 -7.366301e+01  7.750000e+01   \n",
              "\n",
              "              visib          wdsp           max           min          sndp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   8.262622e+00  1.066076e+01  6.012695e+01  4.442143e+01  9.999000e+02   \n",
              "std    2.220453e+00  4.197681e+00  1.424273e+01  1.468128e+01  2.842172e-12   \n",
              "min    2.000000e-01  2.700000e+00  1.800000e+01 -2.000000e+00  9.999000e+02   \n",
              "25%    7.000000e+00  7.700000e+00  4.890000e+01  3.310000e+01  9.999000e+02   \n",
              "50%    9.300000e+00  1.010000e+01  6.100000e+01  4.500000e+01  9.999000e+02   \n",
              "75%    1.000000e+01  1.290000e+01  7.300000e+01  5.700000e+01  9.999000e+02   \n",
              "max    1.000000e+01  3.930000e+01  9.000000e+01  7.160000e+01  9.999000e+02   \n",
              "\n",
              "       ...           Nov           Oct           Sep           Mon  \\\n",
              "count  ...  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   ...  8.510758e-02  8.877971e-02  8.587448e-02  1.487626e-01   \n",
              "std    ...  2.790418e-01  2.844256e-01  2.801787e-01  3.558544e-01   \n",
              "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "                Sat           Sun           Thu           Tue           Wed  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   1.173738e-01  1.438814e-01  1.597195e-01  1.467698e-01  1.505395e-01   \n",
              "std    3.218653e-01  3.509695e-01  3.663458e-01  3.538765e-01  3.575996e-01   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count    1.311775e+06  \n",
              "mean     1.026582e+00  \n",
              "std      1.784039e-01  \n",
              "min      1.000000e+00  \n",
              "25%      1.000000e+00  \n",
              "50%      1.000000e+00  \n",
              "75%      1.000000e+00  \n",
              "max      1.100000e+01  \n",
              "\n",
              "[8 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9b462fc-4980-4ea4-90a5-dc29a8bc398d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.016149e+03</td>\n",
              "      <td>1.566747e+01</td>\n",
              "      <td>4.072402e+01</td>\n",
              "      <td>-7.392063e+01</td>\n",
              "      <td>5.203610e+01</td>\n",
              "      <td>8.262622e+00</td>\n",
              "      <td>1.066076e+01</td>\n",
              "      <td>6.012695e+01</td>\n",
              "      <td>4.442143e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>8.510758e-02</td>\n",
              "      <td>8.877971e-02</td>\n",
              "      <td>8.587448e-02</td>\n",
              "      <td>1.487626e-01</td>\n",
              "      <td>1.173738e-01</td>\n",
              "      <td>1.438814e-01</td>\n",
              "      <td>1.597195e-01</td>\n",
              "      <td>1.467698e-01</td>\n",
              "      <td>1.505395e-01</td>\n",
              "      <td>1.026582e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.970562e+00</td>\n",
              "      <td>8.752606e+00</td>\n",
              "      <td>7.845179e-02</td>\n",
              "      <td>8.651997e-02</td>\n",
              "      <td>1.410404e+01</td>\n",
              "      <td>2.220453e+00</td>\n",
              "      <td>4.197681e+00</td>\n",
              "      <td>1.424273e+01</td>\n",
              "      <td>1.468128e+01</td>\n",
              "      <td>2.842172e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>2.790418e-01</td>\n",
              "      <td>2.844256e-01</td>\n",
              "      <td>2.801787e-01</td>\n",
              "      <td>3.558544e-01</td>\n",
              "      <td>3.218653e-01</td>\n",
              "      <td>3.509695e-01</td>\n",
              "      <td>3.663458e-01</td>\n",
              "      <td>3.538765e-01</td>\n",
              "      <td>3.575996e-01</td>\n",
              "      <td>1.784039e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.013000e+03</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>4.049895e+01</td>\n",
              "      <td>-7.425453e+01</td>\n",
              "      <td>5.800000e+00</td>\n",
              "      <td>2.000000e-01</td>\n",
              "      <td>2.700000e+00</td>\n",
              "      <td>1.800000e+01</td>\n",
              "      <td>-2.000000e+00</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.014000e+03</td>\n",
              "      <td>8.000000e+00</td>\n",
              "      <td>4.066895e+01</td>\n",
              "      <td>-7.397644e+01</td>\n",
              "      <td>4.100000e+01</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>7.700000e+00</td>\n",
              "      <td>4.890000e+01</td>\n",
              "      <td>3.310000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.016000e+03</td>\n",
              "      <td>1.600000e+01</td>\n",
              "      <td>4.072234e+01</td>\n",
              "      <td>-7.392891e+01</td>\n",
              "      <td>5.300000e+01</td>\n",
              "      <td>9.300000e+00</td>\n",
              "      <td>1.010000e+01</td>\n",
              "      <td>6.100000e+01</td>\n",
              "      <td>4.500000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.018000e+03</td>\n",
              "      <td>2.300000e+01</td>\n",
              "      <td>4.076838e+01</td>\n",
              "      <td>-7.386641e+01</td>\n",
              "      <td>6.430000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.290000e+01</td>\n",
              "      <td>7.300000e+01</td>\n",
              "      <td>5.700000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.019000e+03</td>\n",
              "      <td>3.100000e+01</td>\n",
              "      <td>4.091288e+01</td>\n",
              "      <td>-7.366301e+01</td>\n",
              "      <td>7.750000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>3.930000e+01</td>\n",
              "      <td>9.000000e+01</td>\n",
              "      <td>7.160000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.100000e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9b462fc-4980-4ea4-90a5-dc29a8bc398d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9b462fc-4980-4ea4-90a5-dc29a8bc398d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9b462fc-4980-4ea4-90a5-dc29a8bc398d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_loc_dnn.iloc[np.random.permutation(len(df_loc_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1cc6d3-5aaf-4c1a-b374-ab0b66bd6415",
        "id": "cPhXM4DJf2u1"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         year  da   latitude  longitude  temp  visib  wdsp   max   min   sndp  \\\n",
            "578269   2013  25  40.856180 -73.928714  55.3   10.0   4.5  64.0  44.1  999.9   \n",
            "436968   2017  28  40.748350 -73.967865  69.2   10.0   8.5  77.0  55.9  999.9   \n",
            "1257363  2016  16  40.749725 -73.983622  45.7    7.3  15.7  51.1  39.9  999.9   \n",
            "854729   2016  11  40.773440 -73.920970  54.9   10.0  16.0  63.0  46.9  999.9   \n",
            "1493674  2019  19  40.770510 -73.925470  62.3    6.7   6.8  69.1  57.9  999.9   \n",
            "799817   2017  15  40.826855 -73.922676  43.9   10.0  11.3  48.9  39.0  999.9   \n",
            "\n",
            "         ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "578269   ...    0    0    0    1    0    0    0    0    1    0  \n",
            "436968   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "1257363  ...    0    0    0    0    0    0    0    0    0    0  \n",
            "854729   ...    0    1    0    0    0    0    0    1    0    0  \n",
            "1493674  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "799817   ...    0    1    0    0    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b928a05-b847-4746-cde1-4a50fc5807ca",
        "id": "sol3wzXcf2u2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "578269     1\n",
            "436968     1\n",
            "1257363    1\n",
            "854729     1\n",
            "1493674    1\n",
            "799817     1\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa51d48-2dc5-4d3e-8eae-2951f4add122",
        "id": "_fZ3mPt-f2u2"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_loc', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23a20ed-0c84-47b6-af14-e58251ab926b",
        "id": "FEMInwLGf2u3"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99ff2c410>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:loss = 8.462106e-07, step = 1\n",
            "INFO:tensorflow:global_step/sec: 416.448\n",
            "INFO:tensorflow:loss = 1.2511026e-08, step = 101 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.272\n",
            "INFO:tensorflow:loss = 1.715747e-08, step = 201 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.822\n",
            "INFO:tensorflow:loss = 3.1736526e-08, step = 301 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.828\n",
            "INFO:tensorflow:loss = 2.2673124e-08, step = 401 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 519.208\n",
            "INFO:tensorflow:loss = 9.552128e-09, step = 501 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.79\n",
            "INFO:tensorflow:loss = 2.2665516e-08, step = 601 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.717\n",
            "INFO:tensorflow:loss = 1.7947363e-08, step = 701 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 509.269\n",
            "INFO:tensorflow:loss = 1.553955e-08, step = 801 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.23\n",
            "INFO:tensorflow:loss = 6.38337e-08, step = 901 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.502\n",
            "INFO:tensorflow:loss = 1.8104643e-08, step = 1001 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.846\n",
            "INFO:tensorflow:loss = 8.395579e-09, step = 1101 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 423.856\n",
            "INFO:tensorflow:loss = 1.705168e-08, step = 1201 (0.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 338.376\n",
            "INFO:tensorflow:loss = 1.1801929e-08, step = 1301 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 314.355\n",
            "INFO:tensorflow:loss = 2.8675462e-08, step = 1401 (0.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.2\n",
            "INFO:tensorflow:loss = 9.74299e-08, step = 1501 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 468.505\n",
            "INFO:tensorflow:loss = 1.0133541e-07, step = 1601 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.484\n",
            "INFO:tensorflow:loss = 1.9154584e-08, step = 1701 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.061\n",
            "INFO:tensorflow:loss = 7.518386e-08, step = 1801 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 521.97\n",
            "INFO:tensorflow:loss = 3.7931663e-08, step = 1901 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 496.201\n",
            "INFO:tensorflow:loss = 2.7952154e-07, step = 2001 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.021\n",
            "INFO:tensorflow:loss = 2.523608e-07, step = 2101 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 390.515\n",
            "INFO:tensorflow:loss = 1.687994e-08, step = 2201 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.694\n",
            "INFO:tensorflow:loss = 2.3322265e-08, step = 2301 (0.246 sec)\n",
            "INFO:tensorflow:global_step/sec: 410.83\n",
            "INFO:tensorflow:loss = 4.5874037e-07, step = 2401 (0.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.855\n",
            "INFO:tensorflow:loss = 4.9970104e-08, step = 2501 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.646\n",
            "INFO:tensorflow:loss = 7.375099e-08, step = 2601 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.533\n",
            "INFO:tensorflow:loss = 5.0246754e-08, step = 2701 (0.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 344.564\n",
            "INFO:tensorflow:loss = 2.6922855e-08, step = 2801 (0.281 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.357\n",
            "INFO:tensorflow:loss = 3.4520664e-08, step = 2901 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 342.969\n",
            "INFO:tensorflow:loss = 2.5007441e-08, step = 3001 (0.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 314.183\n",
            "INFO:tensorflow:loss = 1.12317096e-07, step = 3101 (0.320 sec)\n",
            "INFO:tensorflow:global_step/sec: 367.667\n",
            "INFO:tensorflow:loss = 6.165766e-08, step = 3201 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 352.059\n",
            "INFO:tensorflow:loss = 6.703061e-08, step = 3301 (0.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 366.389\n",
            "INFO:tensorflow:loss = 8.27055e-08, step = 3401 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 370.502\n",
            "INFO:tensorflow:loss = 7.191136e-08, step = 3501 (0.273 sec)\n",
            "INFO:tensorflow:global_step/sec: 206.848\n",
            "INFO:tensorflow:loss = 5.7426867e-08, step = 3601 (0.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 213.114\n",
            "INFO:tensorflow:loss = 4.791563e-07, step = 3701 (0.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 264.044\n",
            "INFO:tensorflow:loss = 6.198916e-08, step = 3801 (0.379 sec)\n",
            "INFO:tensorflow:global_step/sec: 250.881\n",
            "INFO:tensorflow:loss = 2.4545866e-08, step = 3901 (0.395 sec)\n",
            "INFO:tensorflow:global_step/sec: 281.859\n",
            "INFO:tensorflow:loss = 2.1874952e-08, step = 4001 (0.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 203.382\n",
            "INFO:tensorflow:loss = 1.8815567e-07, step = 4101 (0.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 228.058\n",
            "INFO:tensorflow:loss = 4.0990827e-08, step = 4201 (0.445 sec)\n",
            "INFO:tensorflow:global_step/sec: 214.797\n",
            "INFO:tensorflow:loss = 4.8828262e-08, step = 4301 (0.464 sec)\n",
            "INFO:tensorflow:global_step/sec: 254.663\n",
            "INFO:tensorflow:loss = 8.106902e-08, step = 4401 (0.392 sec)\n",
            "INFO:tensorflow:global_step/sec: 267.009\n",
            "INFO:tensorflow:loss = 8.965434e-08, step = 4501 (0.373 sec)\n",
            "INFO:tensorflow:global_step/sec: 257.505\n",
            "INFO:tensorflow:loss = 9.595404e-08, step = 4601 (0.385 sec)\n",
            "INFO:tensorflow:global_step/sec: 299.91\n",
            "INFO:tensorflow:loss = 1.2583227e-07, step = 4701 (0.340 sec)\n",
            "INFO:tensorflow:global_step/sec: 274.273\n",
            "INFO:tensorflow:loss = 7.617476e-08, step = 4801 (0.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 283.286\n",
            "INFO:tensorflow:loss = 1.2199276e-08, step = 4901 (0.346 sec)\n",
            "INFO:tensorflow:global_step/sec: 357.254\n",
            "INFO:tensorflow:loss = 4.569901e-08, step = 5001 (0.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.233\n",
            "INFO:tensorflow:loss = 5.2140084e-08, step = 5101 (0.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 280.069\n",
            "INFO:tensorflow:loss = 1.9367244e-08, step = 5201 (0.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 262.767\n",
            "INFO:tensorflow:loss = 4.744681e-08, step = 5301 (0.387 sec)\n",
            "INFO:tensorflow:global_step/sec: 223.638\n",
            "INFO:tensorflow:loss = 4.49718e-08, step = 5401 (0.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 143.171\n",
            "INFO:tensorflow:loss = 1.1936203e-07, step = 5501 (0.706 sec)\n",
            "INFO:tensorflow:global_step/sec: 187.698\n",
            "INFO:tensorflow:loss = 6.453995e-09, step = 5601 (0.522 sec)\n",
            "INFO:tensorflow:global_step/sec: 380.527\n",
            "INFO:tensorflow:loss = 4.6711483e-07, step = 5701 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 188.908\n",
            "INFO:tensorflow:loss = 2.4128082e-07, step = 5801 (0.536 sec)\n",
            "INFO:tensorflow:global_step/sec: 237.755\n",
            "INFO:tensorflow:loss = 4.4248843e-08, step = 5901 (0.417 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.426\n",
            "INFO:tensorflow:loss = 1.5001199e-08, step = 6001 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 193.286\n",
            "INFO:tensorflow:loss = 5.664918e-08, step = 6101 (0.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.622\n",
            "INFO:tensorflow:loss = 7.7537834e-08, step = 6201 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.304\n",
            "INFO:tensorflow:loss = 8.6991136e-08, step = 6301 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 404.051\n",
            "INFO:tensorflow:loss = 9.263686e-08, step = 6401 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 348.334\n",
            "INFO:tensorflow:loss = 1.931599e-07, step = 6501 (0.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.298\n",
            "INFO:tensorflow:loss = 1.9339302e-07, step = 6601 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.171\n",
            "INFO:tensorflow:loss = 1.7844346e-07, step = 6701 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 480.336\n",
            "INFO:tensorflow:loss = 3.2347824e-08, step = 6801 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 575.558\n",
            "INFO:tensorflow:loss = 2.5333392e-07, step = 6901 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 535.976\n",
            "INFO:tensorflow:loss = 2.0480165e-08, step = 7001 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.663\n",
            "INFO:tensorflow:loss = 5.3611803e-08, step = 7101 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.66\n",
            "INFO:tensorflow:loss = 3.9530242e-08, step = 7201 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 567.758\n",
            "INFO:tensorflow:loss = 2.9045676e-08, step = 7301 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.545\n",
            "INFO:tensorflow:loss = 1.2137899e-08, step = 7401 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 590.222\n",
            "INFO:tensorflow:loss = 1.4455976e-08, step = 7501 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.513\n",
            "INFO:tensorflow:loss = 4.0372495e-08, step = 7601 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.005\n",
            "INFO:tensorflow:loss = 2.4669482e-08, step = 7701 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.557\n",
            "INFO:tensorflow:loss = 5.764514e-08, step = 7801 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.902\n",
            "INFO:tensorflow:loss = 1.1410804e-08, step = 7901 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.8\n",
            "INFO:tensorflow:loss = 8.6669615e-08, step = 8001 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.965\n",
            "INFO:tensorflow:loss = 4.0859106e-08, step = 8101 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 385.909\n",
            "INFO:tensorflow:loss = 2.1581646e-07, step = 8201 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.063\n",
            "INFO:tensorflow:loss = 1.7127746e-07, step = 8301 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 442.637\n",
            "INFO:tensorflow:loss = 3.697358e-08, step = 8401 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 483.974\n",
            "INFO:tensorflow:loss = 1.1400647e-07, step = 8501 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 410.241\n",
            "INFO:tensorflow:loss = 1.6783372e-07, step = 8601 (0.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.914\n",
            "INFO:tensorflow:loss = 3.0883637e-08, step = 8701 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.232\n",
            "INFO:tensorflow:loss = 8.112494e-08, step = 8801 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 501.692\n",
            "INFO:tensorflow:loss = 1.357516e-07, step = 8901 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.26\n",
            "INFO:tensorflow:loss = 3.809416e-08, step = 9001 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 485.481\n",
            "INFO:tensorflow:loss = 5.1674583e-08, step = 9101 (0.206 sec)\n",
            "INFO:tensorflow:global_step/sec: 446.209\n",
            "INFO:tensorflow:loss = 1.1518527e-08, step = 9201 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 450.155\n",
            "INFO:tensorflow:loss = 1.1993431e-07, step = 9301 (0.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 405.596\n",
            "INFO:tensorflow:loss = 8.008091e-08, step = 9401 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.014\n",
            "INFO:tensorflow:loss = 6.5363125e-08, step = 9501 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 484.151\n",
            "INFO:tensorflow:loss = 6.457422e-08, step = 9601 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.538\n",
            "INFO:tensorflow:loss = 1.182808e-08, step = 9701 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.34\n",
            "INFO:tensorflow:loss = 1.47202e-07, step = 9801 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.877\n",
            "INFO:tensorflow:loss = 3.8553118e-08, step = 9901 (0.174 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.7184973e-07.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 0.38386723782836285\n",
            "Just using average = 1.0266642526347887 has RMSE of 0.1763082830732086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3213a2-68ac-4231-d8ae-5a6ed5273407",
        "id": "UoS_IOAwf2u5"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd99ca2ff90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.01900000e+03 6.00000000e+00 4.08398500e+01 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [2.01300000e+03 1.60000000e+01 4.07432641e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01300000e+03 2.30000000e+01 4.07539280e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.01900000e+03 4.00000000e+00 4.06128500e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [2.01400000e+03 7.00000000e+00 4.08268488e+01 ... 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01600000e+03 2.10000000e+01 4.07390820e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00117764 0.00117764 0.00117764 ... 0.00117764 0.00117764 0.00117764]\n",
            "[0.00086133 0.00086133 0.00086133 ... 0.00086133 0.00086133 0.00086133]\n",
            "The trained model has an aproximate error rate of -0.34098329754667495 which equates to -33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcyH_em_Vg-s"
      },
      "execution_count": 172,
      "outputs": []
    }
  ]
}