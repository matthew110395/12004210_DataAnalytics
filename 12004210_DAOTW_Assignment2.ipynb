{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew110395/12004210_DataAnalytics/blob/main/12004210_DAOTW_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "This assignment builds on the New York taxi problem identified in assignment 1, where the City of New York is looking for a way to accurately predict the number of collisions on a particular day of the week. Within this document two different types of machine learning models will be utilised to predict the number of collisions. The linear relationships identified in assignment 1 will be used to create a linear regression model. A Deep Learning Neural Network (DNN) will be used to predict the number of collisions where the relationship is not linear. "
      ],
      "metadata": {
        "id": "-MIIDzFYc6Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n",
        "To prepare data for machine learning the pandas package has been used. The numpy package has been used to aid with mathematical functions.\n",
        "\n",
        "Within this document, Tensorflow has been used for machine learning, with both a linear regression model and a Deep Neural Network model. Tensorflow version 1 is unsupported within Google Colab, therefore must be installed using the pip package manager.\n",
        "\n",
        "Shutil has also been imported to allow for easy file management, in particular the removal of saved models."
      ],
      "metadata": {
        "id": "PP_OBRRWE3tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install tensorflow==1.15.2\n",
        "import tensorflow as tf\n",
        "# needed for high-level file management\n",
        "import shutil  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Vp_IJAE49d",
        "outputId": "3b5f8fb6-1d99-493a-8234-dbf4e00ba53e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regressor\n"
      ],
      "metadata": {
        "id": "SxU2LFGDjN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Precipition"
      ],
      "metadata": {
        "id": "VtJ7HqhA3bIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0ljmfBkDwE",
        "outputId": "34d5a9e7-c7fa-4e98-f353-dd2fbbbbf0bb"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = df.drop(columns=['collision_date', 'temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] != 2012]\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] < 2020]\n",
        "cols = df_prcp['NUM_COLLISIONS']\n",
        "df_prcp = df_prcp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp.insert(loc=9, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp[:6])\n",
        "df_prcp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "nRcEly727YQb",
        "outputId": "72af9120-94f4-4d59-f6cb-3ddad08b0aca"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "49    4  2016   1  28  0.09    0             0                 0     0   \n",
            "51    5  2014   1  17  0.00    1             0                 0     0   \n",
            "54    1  2016   1  25  0.02    0             0                 0     0   \n",
            "55    5  2016   1  29  0.00    0             0                 0     0   \n",
            "58    5  2017   1  20  0.00    0             0                 0     0   \n",
            "59    7  2013   1  13  0.01    1             0                 0     0   \n",
            "\n",
            "    NUM_COLLISIONS  \n",
            "49             681  \n",
            "51             589  \n",
            "54             658  \n",
            "55             645  \n",
            "58             605  \n",
            "59             373  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         prcp  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean      3.998425  2015.989366     6.518708    15.745569     0.122588   \n",
              "std       2.003542     1.996126     3.455211     8.803199     0.329143   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     0.000000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000     0.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000     0.000000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000     0.060000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000     3.760000   \n",
              "\n",
              "               fog  rain_drizzle  snow_ice_pellets         hail  \\\n",
              "count  2539.000000   2539.000000       2539.000000  2539.000000   \n",
              "mean      0.253249      0.375345          0.085467     0.000394   \n",
              "std       0.434958      0.484307          0.279630     0.019846   \n",
              "min       0.000000      0.000000          0.000000     0.000000   \n",
              "25%       0.000000      0.000000          0.000000     0.000000   \n",
              "50%       0.000000      0.000000          0.000000     0.000000   \n",
              "75%       1.000000      1.000000          0.000000     0.000000   \n",
              "max       1.000000      1.000000          1.000000     1.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2539.000000  \n",
              "mean       599.135093  \n",
              "std        100.299164  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0c0d27a-0863-4de2-b531-0c9455901c55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998425</td>\n",
              "      <td>2015.989366</td>\n",
              "      <td>6.518708</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.003542</td>\n",
              "      <td>1.996126</td>\n",
              "      <td>3.455211</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0c0d27a-0863-4de2-b531-0c9455901c55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0c0d27a-0863-4de2-b531-0c9455901c55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0c0d27a-0863-4de2-b531-0c9455901c55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp.iloc[np.random.permutation(len(df_prcp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xT4g-UZFi01",
        "outputId": "48cdc7f2-e1e8-4b93-84b7-507bc940192d"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail\n",
            "1580    1  2013   6  10  0.00    0             1                 0     0\n",
            "668     3  2018   3  28  0.00    0             0                 0     0\n",
            "1067    2  2016   4  12  0.00    0             1                 0     0\n",
            "3359    6  2018  11  10  1.32    0             1                 0     0\n",
            "390     4  2015   2  12  0.00    0             0                 1     0\n",
            "3547    6  2016  12  10  0.00    0             0                 0     0\n",
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "1580    1  2013   6  10  0.00    0             1                 0     0   \n",
            "668     3  2018   3  28  0.00    0             0                 0     0   \n",
            "1067    2  2016   4  12  0.00    0             1                 0     0   \n",
            "3359    6  2018  11  10  1.32    0             1                 0     0   \n",
            "390     4  2015   2  12  0.00    0             0                 1     0   \n",
            "3547    6  2016  12  10  0.00    0             0                 0     0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "1580             624  \n",
            "668              629  \n",
            "1067             638  \n",
            "3359             606  \n",
            "390              526  \n",
            "3547             555  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xkzkVBQjL_",
        "outputId": "03d43512-1c29-42c3-b335-0f07ce7302fa"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1580    624\n",
            "668     629\n",
            "1067    638\n",
            "3359    606\n",
            "390     526\n",
            "3547    555\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_COLLISIONS=1161"
      ],
      "metadata": {
        "id": "k-aEMQX9EuJJ"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 9\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "id": "NwSG_P_nRgPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9534f73-aa14-45ca-e4e5-40f4f66d233b"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2031\n",
            "508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUKI-paISKeh",
        "outputId": "32097b35-21f2-4857-922d-03ee00aa8667"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d784e590>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.2741817, step = 1\n",
            "INFO:tensorflow:global_step/sec: 531.573\n",
            "INFO:tensorflow:loss = 0.008613801, step = 101 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.764\n",
            "INFO:tensorflow:loss = 0.007745385, step = 201 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 751.437\n",
            "INFO:tensorflow:loss = 0.0065500243, step = 301 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 595.845\n",
            "INFO:tensorflow:loss = 0.0060137557, step = 401 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 585.097\n",
            "INFO:tensorflow:loss = 0.00856616, step = 501 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.276\n",
            "INFO:tensorflow:loss = 0.007597821, step = 601 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.888\n",
            "INFO:tensorflow:loss = 0.006424806, step = 701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.392\n",
            "INFO:tensorflow:loss = 0.0090303775, step = 801 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.134\n",
            "INFO:tensorflow:loss = 0.007331684, step = 901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.189\n",
            "INFO:tensorflow:loss = 0.0062037213, step = 1001 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.301\n",
            "INFO:tensorflow:loss = 0.013258274, step = 1101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.928\n",
            "INFO:tensorflow:loss = 0.0060333638, step = 1201 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.775\n",
            "INFO:tensorflow:loss = 0.0061855214, step = 1301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.131\n",
            "INFO:tensorflow:loss = 0.007421081, step = 1401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.738\n",
            "INFO:tensorflow:loss = 0.010159359, step = 1501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 900.993\n",
            "INFO:tensorflow:loss = 0.011326503, step = 1601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.659\n",
            "INFO:tensorflow:loss = 0.01123179, step = 1701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 815.938\n",
            "INFO:tensorflow:loss = 0.009290953, step = 1801 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.455\n",
            "INFO:tensorflow:loss = 0.006192107, step = 1901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.704\n",
            "INFO:tensorflow:loss = 0.021461133, step = 2001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.66\n",
            "INFO:tensorflow:loss = 0.012155593, step = 2101 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.782\n",
            "INFO:tensorflow:loss = 0.008866031, step = 2201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.066\n",
            "INFO:tensorflow:loss = 0.006469746, step = 2301 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.701\n",
            "INFO:tensorflow:loss = 0.010700154, step = 2401 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 729.423\n",
            "INFO:tensorflow:loss = 0.006597315, step = 2501 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.364\n",
            "INFO:tensorflow:loss = 0.008785134, step = 2601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 913.143\n",
            "INFO:tensorflow:loss = 0.011306616, step = 2701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.178\n",
            "INFO:tensorflow:loss = 0.011923233, step = 2801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 939.671\n",
            "INFO:tensorflow:loss = 0.015335108, step = 2901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.617\n",
            "INFO:tensorflow:loss = 0.008063387, step = 3001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.967\n",
            "INFO:tensorflow:loss = 0.0066355127, step = 3101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.371\n",
            "INFO:tensorflow:loss = 0.012393139, step = 3201 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.947\n",
            "INFO:tensorflow:loss = 0.0061449213, step = 3301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.836\n",
            "INFO:tensorflow:loss = 0.0072782543, step = 3401 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.068\n",
            "INFO:tensorflow:loss = 0.011348075, step = 3501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.187\n",
            "INFO:tensorflow:loss = 0.009772728, step = 3601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.365\n",
            "INFO:tensorflow:loss = 0.0071186246, step = 3701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.633\n",
            "INFO:tensorflow:loss = 0.007550086, step = 3801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.1\n",
            "INFO:tensorflow:loss = 0.008412381, step = 3901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 806.211\n",
            "INFO:tensorflow:loss = 0.0117414575, step = 4001 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.9\n",
            "INFO:tensorflow:loss = 0.007925822, step = 4101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 816.957\n",
            "INFO:tensorflow:loss = 0.009860385, step = 4201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.177\n",
            "INFO:tensorflow:loss = 0.007305213, step = 4301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.718\n",
            "INFO:tensorflow:loss = 0.0053311214, step = 4401 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 505.326\n",
            "INFO:tensorflow:loss = 0.007034432, step = 4501 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.949\n",
            "INFO:tensorflow:loss = 0.010103332, step = 4601 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.132\n",
            "INFO:tensorflow:loss = 0.008230352, step = 4701 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.674\n",
            "INFO:tensorflow:loss = 0.006994121, step = 4801 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 614.508\n",
            "INFO:tensorflow:loss = 0.00792187, step = 4901 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.543\n",
            "INFO:tensorflow:loss = 0.0089334, step = 5001 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 667.116\n",
            "INFO:tensorflow:loss = 0.009214077, step = 5101 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 574.799\n",
            "INFO:tensorflow:loss = 0.005928116, step = 5201 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.064\n",
            "INFO:tensorflow:loss = 0.0075787194, step = 5301 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 415.579\n",
            "INFO:tensorflow:loss = 0.018738624, step = 5401 (0.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 445.088\n",
            "INFO:tensorflow:loss = 0.006235265, step = 5501 (0.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.49\n",
            "INFO:tensorflow:loss = 0.015612312, step = 5601 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.168\n",
            "INFO:tensorflow:loss = 0.0115038315, step = 5701 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 542.458\n",
            "INFO:tensorflow:loss = 0.011389489, step = 5801 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 474.527\n",
            "INFO:tensorflow:loss = 0.0070517166, step = 5901 (0.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.186\n",
            "INFO:tensorflow:loss = 0.007262081, step = 6001 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 471.584\n",
            "INFO:tensorflow:loss = 0.008888979, step = 6101 (0.213 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.853\n",
            "INFO:tensorflow:loss = 0.0077891764, step = 6201 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 592.968\n",
            "INFO:tensorflow:loss = 0.013268225, step = 6301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.136\n",
            "INFO:tensorflow:loss = 0.0062768664, step = 6401 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 510.733\n",
            "INFO:tensorflow:loss = 0.005677895, step = 6501 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 390.063\n",
            "INFO:tensorflow:loss = 0.010480151, step = 6601 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 416.116\n",
            "INFO:tensorflow:loss = 0.005934992, step = 6701 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 558.81\n",
            "INFO:tensorflow:loss = 0.010540883, step = 6801 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 287.77\n",
            "INFO:tensorflow:loss = 0.007293096, step = 6901 (0.347 sec)\n",
            "INFO:tensorflow:global_step/sec: 601.338\n",
            "INFO:tensorflow:loss = 0.014674303, step = 7001 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 569.794\n",
            "INFO:tensorflow:loss = 0.0065317517, step = 7101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 494.071\n",
            "INFO:tensorflow:loss = 0.008946255, step = 7201 (0.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.992\n",
            "INFO:tensorflow:loss = 0.011701834, step = 7301 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.882\n",
            "INFO:tensorflow:loss = 0.013037052, step = 7401 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 482.728\n",
            "INFO:tensorflow:loss = 0.008512784, step = 7501 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.138\n",
            "INFO:tensorflow:loss = 0.008829916, step = 7601 (0.214 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.704\n",
            "INFO:tensorflow:loss = 0.008527199, step = 7701 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 400.734\n",
            "INFO:tensorflow:loss = 0.015273876, step = 7801 (0.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.959\n",
            "INFO:tensorflow:loss = 0.00588678, step = 7901 (0.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 323.357\n",
            "INFO:tensorflow:loss = 0.008881614, step = 8001 (0.315 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.954\n",
            "INFO:tensorflow:loss = 0.0078067537, step = 8101 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 435.514\n",
            "INFO:tensorflow:loss = 0.009613015, step = 8201 (0.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 559.791\n",
            "INFO:tensorflow:loss = 0.0046782964, step = 8301 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 377.348\n",
            "INFO:tensorflow:loss = 0.008236204, step = 8401 (0.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 486.05\n",
            "INFO:tensorflow:loss = 0.00963415, step = 8501 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.942\n",
            "INFO:tensorflow:loss = 0.008667046, step = 8601 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 342.363\n",
            "INFO:tensorflow:loss = 0.0152202565, step = 8701 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.194\n",
            "INFO:tensorflow:loss = 0.0066268947, step = 8801 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.053\n",
            "INFO:tensorflow:loss = 0.0073208595, step = 8901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 563.852\n",
            "INFO:tensorflow:loss = 0.02491944, step = 9001 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 502.872\n",
            "INFO:tensorflow:loss = 0.022497075, step = 9101 (0.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 432.284\n",
            "INFO:tensorflow:loss = 0.006834117, step = 9201 (0.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 417.176\n",
            "INFO:tensorflow:loss = 0.0137788635, step = 9301 (0.241 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.037\n",
            "INFO:tensorflow:loss = 0.0084150545, step = 9401 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.611\n",
            "INFO:tensorflow:loss = 0.032704078, step = 9501 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.844\n",
            "INFO:tensorflow:loss = 0.00725258, step = 9601 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 493.509\n",
            "INFO:tensorflow:loss = 0.0072125043, step = 9701 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 741.295\n",
            "INFO:tensorflow:loss = 0.008430457, step = 9801 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.236\n",
            "INFO:tensorflow:loss = 0.0049892454, step = 9901 (0.104 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0102551095.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 133.62876330148148\n",
            "Just using average = 600.0236336779911 has RMSE of 99.74965071762696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fifEsTD98hy",
        "outputId": "7cfeb65a-10ea-4941-8a08-7274092245e9"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d3020890>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n",
            "[0.42987642 0.39094666 0.44640788 0.45076153 0.4303333  0.4814665\n",
            " 0.4429572  0.39470223 0.42254275 0.47868174 0.376789   0.38981938\n",
            " 0.40248543 0.43961838 0.45967454 0.4122563  0.4241293  0.42008883\n",
            " 0.37994906 0.46601748 0.4690539  0.42026335 0.48523942 0.4616028\n",
            " 0.44085035 0.4030289  0.38974577 0.41590118 0.3750105  0.38319585\n",
            " 0.41171265 0.43872818 0.45940155 0.4285984  0.40527162 0.41341102\n",
            " 0.46074495 0.48642397 0.41700634 0.4639533  0.43161786 0.39841288\n",
            " 0.44786492 0.4059397  0.37056044 0.40497023 0.43759826 0.4126105\n",
            " 0.44341254 0.4724253  0.44972336 0.48057538 0.46801826 0.38432828\n",
            " 0.3864209  0.42413628 0.42837426 0.44011092 0.44224393 0.41302282\n",
            " 0.40347815 0.4218455  0.4394321  0.47718993 0.40395692 0.41018307\n",
            " 0.42806706 0.40087438 0.42380098 0.38031417 0.44316158 0.4023234\n",
            " 0.4158103  0.3762061  0.3977744  0.3847511  0.43397504 0.40719318\n",
            " 0.39132127 0.4186721  0.44128954 0.48102093 0.39237988 0.48179236\n",
            " 0.40436926 0.45717964 0.46818137 0.45707345 0.41133514 0.41845042\n",
            " 0.44019073 0.44773987 0.39257568 0.45861757 0.4856438  0.4496723\n",
            " 0.4271171  0.47619393 0.41837892 0.42914814 0.38081333 0.46856105\n",
            " 0.3725807  0.4107771  0.42916322 0.42774332 0.487022   0.42009604\n",
            " 0.41277105 0.42858282 0.42251143 0.42953023 0.48816174 0.42684588\n",
            " 0.40594557 0.46816745 0.46329948 0.41265705 0.46732095 0.4190048\n",
            " 0.44461653 0.47428682 0.38364148 0.3934639  0.4799668  0.39317265\n",
            " 0.43817118 0.4502082  0.42580253 0.3984062  0.3819055  0.4274601\n",
            " 0.41692433 0.47796175 0.38232547 0.39287555 0.4562925  0.413476\n",
            " 0.42561835 0.49527133 0.39794624 0.39679617 0.48431835 0.41343242\n",
            " 0.42523167 0.39982724 0.47956595 0.45892245 0.43565    0.475961\n",
            " 0.47782442 0.42203075 0.44572765 0.4074985  0.47347954 0.43185526\n",
            " 0.41788304 0.4223148  0.4003178  0.40269417 0.48806134 0.40484715\n",
            " 0.42584768 0.454196   0.4019818  0.43586972 0.43418315 0.43558988\n",
            " 0.43968207 0.37789863 0.427608   0.41865268 0.3936603  0.45766693\n",
            " 0.46522498 0.4167951  0.41046965 0.42296723 0.4240816  0.44360608\n",
            " 0.4038454  0.44923192 0.42943126 0.42007536 0.42286995 0.40797532\n",
            " 0.40840217 0.46214172 0.43903577 0.48391813 0.3944514  0.45366007\n",
            " 0.41185355 0.4196485  0.4165696  0.4370606  0.44831517 0.4538319\n",
            " 0.39904857 0.4813075  0.43694165 0.4583241  0.43538642 0.37189975\n",
            " 0.40857503 0.41212478 0.45206282 0.4429236  0.43870255 0.45454893\n",
            " 0.4622413  0.45244056 0.41323712 0.39623183 0.45498237 0.43097\n",
            " 0.44061068 0.44886148 0.39134687 0.40862113 0.46424204 0.3872627\n",
            " 0.37560108 0.4157422  0.43800458 0.5021231  0.3960441  0.3975797\n",
            " 0.43360353 0.46903926 0.40359676 0.41197205 0.4159522  0.48700294\n",
            " 0.47045016 0.4542855  0.43310437 0.42302147 0.438936   0.39937928\n",
            " 0.4273997  0.42094222 0.41467166 0.44961983 0.46864644 0.44203\n",
            " 0.40238202 0.4267491  0.42449588 0.4261823  0.41533974 0.41220406\n",
            " 0.4557325  0.42943966 0.40225077 0.44523674 0.4257149  0.4331373\n",
            " 0.44677347 0.47930965 0.48281437 0.47160518 0.3830142  0.46826804\n",
            " 0.47054696 0.36820132 0.48125917 0.39824376 0.41433883 0.3892469\n",
            " 0.43400145 0.43948856 0.4297948  0.40374205 0.46736538 0.43507427\n",
            " 0.44048584 0.40937495 0.4194828  0.46181467 0.44153726 0.44968417\n",
            " 0.44554594 0.3691059  0.46737978 0.47539645 0.44719693 0.3816416\n",
            " 0.37168962 0.4135288  0.43202856 0.40197194 0.45263648 0.45431066\n",
            " 0.4353488  0.41937563 0.45159727 0.41853362 0.46458685 0.44262266\n",
            " 0.42310202 0.44157574 0.43466192 0.46898758 0.3843006  0.46770972\n",
            " 0.45678344 0.40269434 0.47359538 0.4818269  0.44259703 0.44088113\n",
            " 0.38016236 0.41864887 0.36910158 0.46535212 0.4184778  0.45628724\n",
            " 0.42779553 0.4823815  0.45615244 0.41009313 0.4709558  0.39910927\n",
            " 0.48262578 0.44935912 0.44599235 0.42006937 0.4791145  0.4194171\n",
            " 0.41390792 0.45625952 0.39310673 0.3832627  0.42807457 0.4083308\n",
            " 0.40637004 0.42160493 0.45307356 0.41313016 0.4446273  0.45913768\n",
            " 0.4624351  0.40485555 0.444643   0.44071203 0.41978952 0.42562076\n",
            " 0.434463   0.41835332 0.4342948  0.4422976  0.39913487 0.40442628\n",
            " 0.39490125 0.38940936 0.421039   0.46031561 0.40572375 0.4129905\n",
            " 0.40056646 0.3852491  0.43653813 0.41770577 0.4581907  0.44186294\n",
            " 0.4583281  0.48415536 0.41903785 0.43699828 0.47179538 0.42847714\n",
            " 0.40595677 0.43662947 0.4536273  0.48325992 0.48530224 0.41494715\n",
            " 0.4066057  0.44786432 0.43933287 0.472356   0.42727414 0.46320337\n",
            " 0.3970502  0.47688204 0.43226445 0.4679594  0.46843058 0.43177772\n",
            " 0.5062167  0.4100134  0.46693093 0.36167276 0.44659317 0.4059433\n",
            " 0.48469207 0.4378546  0.46537083 0.47679645 0.4384227  0.38752326\n",
            " 0.46239653 0.3765422  0.43558192 0.4638297  0.39576718 0.47749057\n",
            " 0.42648876 0.46029267 0.44826558 0.39209384 0.4824293  0.38192603\n",
            " 0.41151193 0.40727237 0.41774294 0.41688508 0.4319288  0.45067823\n",
            " 0.434704   0.3666553  0.4526623  0.45380285 0.44264287 0.41445628\n",
            " 0.48955843 0.41143528 0.44446918 0.38779053 0.45172247 0.4577389\n",
            " 0.4154786  0.4411133  0.40241975 0.3952597  0.4109331  0.46007323\n",
            " 0.47675094 0.43825942 0.46141425 0.42751586 0.44083682 0.4452695\n",
            " 0.39267778 0.39138013 0.4306691  0.42702    0.4334369  0.39289618\n",
            " 0.44814324 0.44837832 0.43266705 0.4137023  0.44594264 0.39508408\n",
            " 0.42402562 0.3672161  0.44157392 0.41000506 0.43729252 0.424303\n",
            " 0.41392234 0.4332281  0.42804134 0.41150087 0.49248573 0.4279375\n",
            " 0.46972644 0.46301305 0.43264142 0.4659012  0.4636036  0.42649668\n",
            " 0.4293939  0.40688995 0.36478087 0.4320634  0.43601474 0.4124176\n",
            " 0.384196   0.37715256 0.47993693 0.43094453 0.38958973 0.4653337\n",
            " 0.37617615 0.42173126 0.44261816 0.40284258 0.44195858 0.4030658\n",
            " 0.47693032 0.4286588  0.40371105 0.4713479  0.4539615  0.45161578\n",
            " 0.42141342 0.42781585 0.4611119  0.43096924 0.43939114 0.43424198\n",
            " 0.4186731  0.46629247 0.41726595 0.44308692]\n",
            "[0.49870801 0.47717485 0.59776055 0.58656331 0.45908699 0.50215332\n",
            " 0.2213609  0.62187769 0.55297158 0.43927649 0.35486649 0.41257537\n",
            " 0.43496985 0.61412575 0.4952627  0.49612403 0.42291128 0.6089578\n",
            " 0.40826873 0.50043066 0.53660637 0.4918174  0.68130922 0.49870801\n",
            " 0.54435831 0.46511628 0.32816537 0.5047373  0.7329888  0.48148148\n",
            " 0.49698536 0.56330749 0.58225668 0.47286822 0.36606374 0.56158484\n",
            " 0.55469423 0.5081826  0.4788975  0.59862188 0.60120586 0.44702842\n",
            " 0.60206718 0.53660637 0.42549526 0.47372954 0.52540913 0.54694229\n",
            " 0.61757106 0.52196382 0.56503015 0.42204996 0.57450474 0.35400517\n",
            " 0.49095607 0.44530577 0.51248923 0.47459087 0.54263566 0.64427218\n",
            " 0.43238587 0.62015504 0.54952627 0.61584841 0.48320413 0.45822567\n",
            " 0.48406546 0.45736434 0.43669251 0.47372954 0.48578811 0.41602067\n",
            " 0.56589147 0.43496985 0.416882   0.51593454 0.56761413 0.41774332\n",
            " 0.52024117 0.51507321 0.5503876  0.63652024 0.37812231 0.49870801\n",
            " 0.48148148 0.56072351 0.54177433 0.61843239 0.47459087 0.59776055\n",
            " 0.65977606 0.59345392 0.45391904 0.64082687 0.60292851 0.61154177\n",
            " 0.5211025  0.51937984 0.47631352 0.57795004 0.43669251 0.5994832\n",
            " 0.37209302 0.41257537 0.63652024 0.57019811 0.57450474 0.45391904\n",
            " 0.47717485 0.60981912 0.57622739 0.583118   0.58484065 0.57795004\n",
            " 0.6124031  0.57019811 0.48406546 0.5047373  0.54349699 0.54521964\n",
            " 0.57622739 0.52196382 0.47717485 0.40913006 0.60206718 0.60465116\n",
            " 0.60723514 0.57622739 0.65030146 0.40740741 0.48751077 0.50129199\n",
            " 0.43669251 0.55211025 0.40654608 0.68217054 0.50129199 0.50129199\n",
            " 0.58225668 0.57881137 0.44099914 0.55900086 0.52799311 0.40568475\n",
            " 0.52196382 0.48492679 0.64427218 0.49009475 0.46856158 0.53057709\n",
            " 0.56416882 0.53919035 0.57622739 0.49870801 0.4496124  0.66408269\n",
            " 0.48234281 0.53919035 0.41429802 0.41515935 0.55469423 0.42721792\n",
            " 0.41860465 0.61068045 0.45994832 0.45219638 0.47286822 0.6287683\n",
            " 0.50904393 0.374677   0.53660637 0.43238587 0.50387597 0.49440138\n",
            " 0.46683893 0.50129199 0.45564169 0.44788975 0.49440138 0.55555556\n",
            " 0.3910422  0.59086994 0.56244617 0.40913006 0.54349699 0.43410853\n",
            " 0.5994832  0.59259259 0.62618432 0.52713178 0.46942291 0.52971576\n",
            " 0.48923342 0.43496985 0.5960379  0.59862188 0.55813953 0.46683893\n",
            " 0.51076658 0.52196382 0.65202412 0.47459087 0.38845823 0.45650301\n",
            " 0.56933678 0.68130922 0.53057709 0.45047373 0.47286822 0.61757106\n",
            " 0.54177433 0.55727821 0.5047373  0.51851852 0.57019811 0.58225668\n",
            " 0.58828596 0.67011197 0.40310078 0.42894057 0.6089578  0.4203273\n",
            " 0.4332472  0.42377261 0.48923342 0.6089578  0.51851852 0.45822567\n",
            " 0.59689922 0.69939707 0.58656331 0.52799311 0.46511628 0.56072351\n",
            " 0.5538329  0.5994832  0.62704565 0.52368648 0.42980189 0.40223945\n",
            " 0.63824289 0.56244617 0.39276486 0.45736434 0.48406546 0.48664944\n",
            " 0.53660637 0.44530577 0.67183463 0.66063738 0.45822567 0.50215332\n",
            " 0.50990525 0.58656331 0.47803618 0.61326443 0.70111972 0.51421189\n",
            " 0.43755383 0.54694229 0.42807924 0.374677   0.46339363 0.63049096\n",
            " 0.48751077 0.39534884 0.53229974 0.58570198 0.51162791 0.48148148\n",
            " 0.44444444 0.44702842 0.51937984 0.32472007 0.44875108 0.54263566\n",
            " 0.58828596 0.52024117 0.56503015 0.58656331 0.59173127 0.57364341\n",
            " 0.46511628 0.35658915 0.54866494 0.60809647 0.60981912 0.38156761\n",
            " 0.27562446 0.49440138 0.52713178 0.5994832  0.5667528  0.54091301\n",
            " 0.42980189 0.50559862 0.32988803 0.54780362 0.51507321 0.36864772\n",
            " 0.5667528  0.46683893 0.56416882 0.63996555 0.46770026 0.52885444\n",
            " 0.36175711 0.62704565 0.66149871 0.39276486 0.54091301 0.54005168\n",
            " 0.43927649 0.54866494 0.36003445 0.6287683  0.49956934 0.51851852\n",
            " 0.54349699 0.50904393 0.55986219 0.39276486 0.57622739 0.54091301\n",
            " 0.60809647 0.49956934 0.61068045 0.56847545 0.46425495 0.54177433\n",
            " 0.38070629 0.54866494 0.37553833 0.50732127 0.45305771 0.36089578\n",
            " 0.374677   0.51162791 0.63738157 0.51507321 0.57536606 0.57881137\n",
            " 0.44788975 0.47286822 0.38673557 0.48148148 0.66322136 0.45219638\n",
            " 0.41774332 0.53660637 0.51335056 0.39707149 0.53057709 0.47717485\n",
            " 0.42118863 0.80878553 0.48148148 0.54952627 0.51248923 0.4005168\n",
            " 0.59345392 0.44702842 0.39879414 0.6873385  0.60809647 0.5667528\n",
            " 0.49612403 0.62446167 0.45564169 0.45736434 0.50301464 0.37639966\n",
            " 0.63135228 0.62187769 0.45564169 0.41429802 0.45478036 0.46942291\n",
            " 0.51076658 0.61412575 0.59431525 0.47631352 0.43238587 0.5503876\n",
            " 0.45478036 0.62187769 0.62015504 0.53057709 0.59173127 0.47028424\n",
            " 0.47286822 0.55900086 0.63824289 0.38845823 0.51765719 0.45391904\n",
            " 0.57622739 0.52799311 0.63135228 0.55727821 0.45391904 0.3875969\n",
            " 0.47975883 0.31093885 0.4918174  0.69250646 0.40999139 0.37898363\n",
            " 0.51765719 0.64513351 0.48664944 0.54005168 0.51248923 0.43927649\n",
            " 0.62704565 0.40482343 0.41429802 0.51765719 0.67355728 0.48148148\n",
            " 0.53919035 0.36950904 0.37726098 0.51248923 0.31007752 0.59000861\n",
            " 0.49009475 0.47286822 0.63221361 0.38501292 0.54263566 0.51851852\n",
            " 0.57278208 0.44013781 0.45478036 0.44358312 0.64685616 0.4918174\n",
            " 0.51162791 0.51593454 0.52282515 0.60292851 0.5245478  0.5667528\n",
            " 0.41946598 0.48837209 0.53057709 0.5245478  0.68303187 0.44875108\n",
            " 0.56330749 0.48062016 0.54091301 0.53316107 0.58656331 0.52282515\n",
            " 0.49784668 0.40654608 0.41946598 0.44358312 0.47372954 0.59259259\n",
            " 0.37984496 0.57536606 0.60465116 0.46339363 0.53143842 0.4754522\n",
            " 0.37898363 0.56416882 0.45822567 0.51076658 0.5211025  0.53919035\n",
            " 0.63996555 0.47372954 0.48062016 0.50301464 0.59259259 0.55469423\n",
            " 0.37639966 0.38501292 0.43152455 0.53488372 0.38587425 0.5538329\n",
            " 0.40137812 0.86046512 0.38931955 0.36434109 0.32988803 0.47459087\n",
            " 0.4754522  0.53488372 0.40740741 0.65288544 0.31955211 0.5667528\n",
            " 0.43238587 0.60292851 0.7166236  0.54005168 0.32213609 0.64857881\n",
            " 0.57881137 0.59862188 0.56503015 0.46942291]\n",
            "The trained model has an aproximate error rate of 94.91648787594451 which equates to 16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "RB0Zq1024UmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0f2b83-b9ab-407d-a402-111779683be5",
        "id": "d2NB6odM5G9I"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] != 2012]\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] < 2020]\n",
        "cols = df_dewp['NUM_COLLISIONS']\n",
        "df_dewp = df_dewp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp[:6])\n",
        "df_dewp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "8190ded3-f2cd-4537-b6eb-41edf97e980b",
        "id": "WwtmLQ6a5rHs"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  24.4             681\n",
            "51    5  2014   1  17  35.8             589\n",
            "54    1  2016   1  25  21.2             658\n",
            "55    5  2016   1  29  36.8             645\n",
            "58    5  2017   1  20  32.5             605\n",
            "59    7  2013   1  13  44.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         dewp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.998434  2015.999217     6.524070    15.723679    44.163170   \n",
              "std       2.000391     2.000000     3.449676     8.801271    16.995303   \n",
              "min       1.000000  2013.000000     1.000000     1.000000    -6.700000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    32.150000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    45.300000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    58.500000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    74.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.109980  \n",
              "std        100.277185  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-889b922c-1166-4d1a-b3b1-5ae13131fc99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998434</td>\n",
              "      <td>2015.999217</td>\n",
              "      <td>6.524070</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000391</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.449676</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-889b922c-1166-4d1a-b3b1-5ae13131fc99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-889b922c-1166-4d1a-b3b1-5ae13131fc99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-889b922c-1166-4d1a-b3b1-5ae13131fc99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp.iloc[np.random.permutation(len(df_dewp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05180162-30c6-4dd8-d6a7-3743c52a8ab4",
        "id": "KXbAqzNN694C"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  dewp\n",
            "3205    2  2018  11   6  56.2\n",
            "1032    4  2014   4  10  28.9\n",
            "1244    7  2014   5  25  44.4\n",
            "2048    3  2015   7   8  67.8\n",
            "3541    2  2013  12  17  15.9\n",
            "2533    3  2018   9  12  69.6\n",
            "      day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "3205    2  2018  11   6  56.2             679\n",
            "1032    4  2014   4  10  28.9             645\n",
            "1244    7  2014   5  25  44.4             472\n",
            "2048    3  2015   7   8  67.8             620\n",
            "3541    2  2013  12  17  15.9             703\n",
            "2533    3  2018   9  12  69.6             783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13dbdd8a-9265-4086-a4bc-e54e1ece4f64",
        "id": "iGbT5sAJ7KTK"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3205    679\n",
            "1032    645\n",
            "1244    472\n",
            "2048    620\n",
            "3541    703\n",
            "2533    783\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327eeb9b-6e24-4ce5-9ebd-e1e75bcbc115",
        "id": "vpHbBnml7PZw"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2662a5-da53-47d4-adf1-b602dd127ec6",
        "id": "j4GKf5BL7WI3"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d2e5a310>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27211958, step = 1\n",
            "INFO:tensorflow:global_step/sec: 745.714\n",
            "INFO:tensorflow:loss = 0.007127203, step = 101 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.097\n",
            "INFO:tensorflow:loss = 0.006825781, step = 201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 843.359\n",
            "INFO:tensorflow:loss = 0.005263289, step = 301 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 915.963\n",
            "INFO:tensorflow:loss = 0.0076429676, step = 401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.938\n",
            "INFO:tensorflow:loss = 0.007173229, step = 501 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.845\n",
            "INFO:tensorflow:loss = 0.0051976102, step = 601 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 775.507\n",
            "INFO:tensorflow:loss = 0.0059868535, step = 701 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 762.013\n",
            "INFO:tensorflow:loss = 0.0065685455, step = 801 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.939\n",
            "INFO:tensorflow:loss = 0.006587959, step = 901 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 793.671\n",
            "INFO:tensorflow:loss = 0.0054977546, step = 1001 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.179\n",
            "INFO:tensorflow:loss = 0.00836446, step = 1101 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 608.549\n",
            "INFO:tensorflow:loss = 0.005426192, step = 1201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.819\n",
            "INFO:tensorflow:loss = 0.006802492, step = 1301 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 849.579\n",
            "INFO:tensorflow:loss = 0.0069302483, step = 1401 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 808.417\n",
            "INFO:tensorflow:loss = 0.0062098266, step = 1501 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 838.458\n",
            "INFO:tensorflow:loss = 0.0055860826, step = 1601 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.19\n",
            "INFO:tensorflow:loss = 0.007730303, step = 1701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.934\n",
            "INFO:tensorflow:loss = 0.0052552363, step = 1801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.472\n",
            "INFO:tensorflow:loss = 0.008410051, step = 1901 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.928\n",
            "INFO:tensorflow:loss = 0.0050855633, step = 2001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.956\n",
            "INFO:tensorflow:loss = 0.0062454217, step = 2101 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.728\n",
            "INFO:tensorflow:loss = 0.006683697, step = 2201 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.218\n",
            "INFO:tensorflow:loss = 0.0061797965, step = 2301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 839.48\n",
            "INFO:tensorflow:loss = 0.006515326, step = 2401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.941\n",
            "INFO:tensorflow:loss = 0.0064350055, step = 2501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 802.549\n",
            "INFO:tensorflow:loss = 0.0061805816, step = 2601 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.849\n",
            "INFO:tensorflow:loss = 0.006868569, step = 2701 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 866.789\n",
            "INFO:tensorflow:loss = 0.0077640433, step = 2801 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 439.133\n",
            "INFO:tensorflow:loss = 0.0067445664, step = 2901 (0.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 497.753\n",
            "INFO:tensorflow:loss = 0.005417371, step = 3001 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.024\n",
            "INFO:tensorflow:loss = 0.0067253904, step = 3101 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 463.262\n",
            "INFO:tensorflow:loss = 0.005991839, step = 3201 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 508.66\n",
            "INFO:tensorflow:loss = 0.0072756545, step = 3301 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.813\n",
            "INFO:tensorflow:loss = 0.005784538, step = 3401 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 426.458\n",
            "INFO:tensorflow:loss = 0.007026841, step = 3501 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 603.553\n",
            "INFO:tensorflow:loss = 0.0051971413, step = 3601 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.659\n",
            "INFO:tensorflow:loss = 0.005537437, step = 3701 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.647\n",
            "INFO:tensorflow:loss = 0.007925601, step = 3801 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.17\n",
            "INFO:tensorflow:loss = 0.0060522794, step = 3901 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.499\n",
            "INFO:tensorflow:loss = 0.0072833644, step = 4001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 498.547\n",
            "INFO:tensorflow:loss = 0.0058174427, step = 4101 (0.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.3\n",
            "INFO:tensorflow:loss = 0.009080067, step = 4201 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 559.019\n",
            "INFO:tensorflow:loss = 0.006424537, step = 4301 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.731\n",
            "INFO:tensorflow:loss = 0.006083331, step = 4401 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 539.712\n",
            "INFO:tensorflow:loss = 0.005869478, step = 4501 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.702\n",
            "INFO:tensorflow:loss = 0.007329061, step = 4601 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 506.17\n",
            "INFO:tensorflow:loss = 0.008003237, step = 4701 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.731\n",
            "INFO:tensorflow:loss = 0.0059958417, step = 4801 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 562.634\n",
            "INFO:tensorflow:loss = 0.0067839497, step = 4901 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 595.141\n",
            "INFO:tensorflow:loss = 0.006898949, step = 5001 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.772\n",
            "INFO:tensorflow:loss = 0.005809324, step = 5101 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 428.78\n",
            "INFO:tensorflow:loss = 0.0068591665, step = 5201 (0.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 499.704\n",
            "INFO:tensorflow:loss = 0.0075873802, step = 5301 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.949\n",
            "INFO:tensorflow:loss = 0.0058071176, step = 5401 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 603.666\n",
            "INFO:tensorflow:loss = 0.0054281484, step = 5501 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 501.507\n",
            "INFO:tensorflow:loss = 0.007684704, step = 5601 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 509.549\n",
            "INFO:tensorflow:loss = 0.0056064995, step = 5701 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 518.237\n",
            "INFO:tensorflow:loss = 0.005023506, step = 5801 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 366.995\n",
            "INFO:tensorflow:loss = 0.008856753, step = 5901 (0.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 460.114\n",
            "INFO:tensorflow:loss = 0.005963916, step = 6001 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 378.457\n",
            "INFO:tensorflow:loss = 0.006338149, step = 6101 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 503.575\n",
            "INFO:tensorflow:loss = 0.005690366, step = 6201 (0.196 sec)\n",
            "INFO:tensorflow:global_step/sec: 491.259\n",
            "INFO:tensorflow:loss = 0.008279557, step = 6301 (0.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 332.942\n",
            "INFO:tensorflow:loss = 0.004987841, step = 6401 (0.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 308.453\n",
            "INFO:tensorflow:loss = 0.005882931, step = 6501 (0.324 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.579\n",
            "INFO:tensorflow:loss = 0.0050015263, step = 6601 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 400.874\n",
            "INFO:tensorflow:loss = 0.0052708713, step = 6701 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.677\n",
            "INFO:tensorflow:loss = 0.0058935424, step = 6801 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 433.537\n",
            "INFO:tensorflow:loss = 0.005446907, step = 6901 (0.233 sec)\n",
            "INFO:tensorflow:global_step/sec: 323.768\n",
            "INFO:tensorflow:loss = 0.009135814, step = 7001 (0.305 sec)\n",
            "INFO:tensorflow:global_step/sec: 487.676\n",
            "INFO:tensorflow:loss = 0.0073697004, step = 7101 (0.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.36\n",
            "INFO:tensorflow:loss = 0.00715513, step = 7201 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.459\n",
            "INFO:tensorflow:loss = 0.0059301658, step = 7301 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 467.901\n",
            "INFO:tensorflow:loss = 0.007502681, step = 7401 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 343.576\n",
            "INFO:tensorflow:loss = 0.008552732, step = 7501 (0.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 392.716\n",
            "INFO:tensorflow:loss = 0.0072897645, step = 7601 (0.246 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.156\n",
            "INFO:tensorflow:loss = 0.0075334995, step = 7701 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.009\n",
            "INFO:tensorflow:loss = 0.005250557, step = 7801 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.846\n",
            "INFO:tensorflow:loss = 0.006751149, step = 7901 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 526.434\n",
            "INFO:tensorflow:loss = 0.005943286, step = 8001 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.313\n",
            "INFO:tensorflow:loss = 0.006007133, step = 8101 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 652.444\n",
            "INFO:tensorflow:loss = 0.007624808, step = 8201 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.349\n",
            "INFO:tensorflow:loss = 0.0070502385, step = 8301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.699\n",
            "INFO:tensorflow:loss = 0.0072495127, step = 8401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 752.481\n",
            "INFO:tensorflow:loss = 0.00573631, step = 8501 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.281\n",
            "INFO:tensorflow:loss = 0.005317822, step = 8601 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.644\n",
            "INFO:tensorflow:loss = 0.006878946, step = 8701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.445\n",
            "INFO:tensorflow:loss = 0.0067621116, step = 8801 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.732\n",
            "INFO:tensorflow:loss = 0.008842893, step = 8901 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.502\n",
            "INFO:tensorflow:loss = 0.007032058, step = 9001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 806.626\n",
            "INFO:tensorflow:loss = 0.0076017296, step = 9101 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 852.347\n",
            "INFO:tensorflow:loss = 0.0057365433, step = 9201 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.564\n",
            "INFO:tensorflow:loss = 0.0067734546, step = 9301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.04\n",
            "INFO:tensorflow:loss = 0.006594778, step = 9401 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.081\n",
            "INFO:tensorflow:loss = 0.0065092756, step = 9501 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.917\n",
            "INFO:tensorflow:loss = 0.0069947536, step = 9601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 765.684\n",
            "INFO:tensorflow:loss = 0.0054510003, step = 9701 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.667\n",
            "INFO:tensorflow:loss = 0.0066423207, step = 9801 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.685\n",
            "INFO:tensorflow:loss = 0.005977111, step = 9901 (0.119 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00573849.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 101.39611547216715\n",
            "Just using average = 599.1081213307241 has RMSE of 107.18799439834025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7698a79-7706-4626-cd70-bc13e875b90e",
        "id": "CmqqgLT09593"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77ceeac090>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.49113408 0.4881626  0.5006525  0.50036126 0.5045505  0.50081605\n",
            " 0.5428108  0.47471237 0.5205801  0.49932688 0.5012491  0.54813516\n",
            " 0.53094643 0.5179871  0.52326894 0.47981945 0.5285742  0.48713955\n",
            " 0.49731293 0.4614843  0.47878057 0.5031496  0.50030917 0.5063216\n",
            " 0.47203416 0.4492744  0.52781445 0.46927723 0.4637366  0.50760883\n",
            " 0.4437548  0.5266648  0.49465743 0.472608   0.48636907 0.542532\n",
            " 0.5282972  0.55163616 0.52802694 0.52605313 0.5206262  0.51341325\n",
            " 0.52215505 0.46828103 0.5454357  0.47908694 0.47307602 0.45605692\n",
            " 0.47889918 0.53389543 0.5324805  0.47342047 0.52959937 0.519376\n",
            " 0.5062903  0.49052638 0.4529709  0.48924968 0.511345   0.49358004\n",
            " 0.43346342 0.48982152 0.500176   0.48429143 0.48101178 0.44719625\n",
            " 0.46459624 0.5128059  0.4620526  0.53577125 0.5417075  0.41498902\n",
            " 0.4960257  0.5297887  0.5505703  0.5121371  0.4342537  0.5345716\n",
            " 0.51635736 0.48388025 0.48215672 0.49695295 0.5048197  0.5514952\n",
            " 0.48294827 0.4798197  0.48168457 0.44222897 0.46128792 0.49072903\n",
            " 0.48781672 0.46011186 0.55136675 0.4554154  0.5251725  0.52374846\n",
            " 0.5283055  0.54333395 0.5269842  0.47769037 0.53685915 0.51219547\n",
            " 0.46720928 0.5234663  0.45483163 0.5399846  0.4339675  0.5240331\n",
            " 0.4979138  0.47663355 0.5392175  0.4765421  0.46187076 0.4736819\n",
            " 0.5007338  0.4977934  0.52816904 0.5153507  0.48947346 0.5189322\n",
            " 0.48567542 0.5109916  0.51153666 0.5288252  0.52435565 0.49624306\n",
            " 0.5429924  0.44737    0.51526105 0.4614779  0.48814785 0.4813435\n",
            " 0.47456047 0.5551523  0.5401869  0.48022646 0.5591572  0.48697877\n",
            " 0.56591153 0.50205725 0.4960054  0.47438535 0.45705876 0.5045986\n",
            " 0.53462285 0.4746725  0.51493895 0.49543035 0.5409549  0.49223122\n",
            " 0.51605123 0.52353156 0.5100732  0.506167   0.4913122  0.51648664\n",
            " 0.51727974 0.54838663 0.5183714  0.54783607 0.5435685  0.47297826\n",
            " 0.48755795 0.523856   0.49259868 0.5031734  0.4404184  0.44992223\n",
            " 0.47929844 0.4719366  0.48754686 0.4505813  0.4883332  0.48607174\n",
            " 0.48276174 0.5237812  0.46560162 0.4535078  0.48173618 0.54007524\n",
            " 0.49467432 0.5088938  0.48998114 0.5012443  0.44493493 0.4755435\n",
            " 0.48231032 0.49882004 0.5099812  0.4812689  0.4964561  0.46376127\n",
            " 0.5066745  0.48045892 0.50306576 0.4492054  0.47920072 0.47921085\n",
            " 0.43710816 0.49538577 0.50219536 0.45763776 0.45659846 0.5333258\n",
            " 0.49387592 0.5237326  0.46812788 0.51436996 0.51664    0.5004453\n",
            " 0.48951933 0.4852712  0.48869762 0.49529237 0.49966013 0.5412771\n",
            " 0.5084307  0.5248814  0.45361567 0.47394735 0.54169333 0.49100772\n",
            " 0.4942019  0.528905   0.48000973 0.45530757 0.5052441  0.52525955\n",
            " 0.500649   0.45346534 0.5528238  0.48953044 0.48307264 0.49930656\n",
            " 0.497641   0.508332   0.5148264  0.47277534 0.4807545  0.50825775\n",
            " 0.56239516 0.45905086 0.50304335 0.5488549  0.4493778  0.5479191\n",
            " 0.47155774 0.4589536  0.5091426  0.4908355  0.50203234 0.4882141\n",
            " 0.51775694 0.47441497 0.5018464  0.45777518 0.4827721  0.48513922\n",
            " 0.5007946  0.52475244 0.48455685 0.53560865 0.5378264  0.44511005\n",
            " 0.56167823 0.5601443  0.48000285 0.55831665 0.45133692 0.51995665\n",
            " 0.47369504 0.48706228 0.5178369  0.47304976 0.5139009  0.47757456\n",
            " 0.5399247  0.5422869  0.5206372  0.54273766 0.5222301  0.5477688\n",
            " 0.52217686 0.5430601  0.54377025 0.4935397  0.46178958 0.48119783\n",
            " 0.52245563 0.50826627 0.5415508  0.5040296  0.4998852  0.4688429\n",
            " 0.48933426 0.47199476 0.48664    0.5324146  0.5439592  0.47502583\n",
            " 0.486689   0.508788   0.5094502  0.48260772 0.52530825 0.49763507\n",
            " 0.45396927 0.45657662 0.55890363 0.55215204 0.51958585 0.42370287\n",
            " 0.5131456  0.55998665 0.5114877  0.53431755 0.53022194 0.48199838\n",
            " 0.5094067  0.44681123 0.47123045 0.55388325 0.4856257  0.51552236\n",
            " 0.4753429  0.47771066 0.509224   0.4678436  0.5355745  0.4911624\n",
            " 0.4723047  0.48445508 0.49274394 0.44200826 0.46120235 0.4833114\n",
            " 0.43383768 0.44770005 0.5436104  0.49878415 0.4726215  0.5623761\n",
            " 0.52765626 0.4357656  0.47684264 0.5067242  0.5444998  0.48121083\n",
            " 0.5101785  0.5069486  0.5249245  0.4757854  0.44780183 0.5169088\n",
            " 0.50767535 0.48444128 0.52351636 0.50662583 0.4845738  0.49702603\n",
            " 0.4354652  0.5417952  0.45173943 0.49128124 0.45714182 0.49037954\n",
            " 0.5089871  0.4880522  0.54529905 0.5348283  0.46278948 0.48097724\n",
            " 0.5391224  0.5152649  0.42783752 0.55687237 0.54698795 0.49980462\n",
            " 0.510717   0.52821577 0.4700238  0.50835204 0.5254848  0.52771676\n",
            " 0.5323176  0.46560803 0.5348449  0.5447107  0.5183484  0.42852303\n",
            " 0.5535751  0.49820483 0.47904584 0.5268417  0.5361565  0.5551321\n",
            " 0.44784227 0.5612276  0.46542338 0.5142502  0.49065316 0.49596226\n",
            " 0.54856396 0.5477827  0.54195064 0.5570836  0.48611954 0.5381972\n",
            " 0.5014346  0.51253504 0.52339846 0.54644483 0.54848844 0.5005328\n",
            " 0.47579902 0.5381846  0.5054865  0.4848339  0.44225246 0.5275717\n",
            " 0.48140496 0.50955755 0.47564235 0.525158   0.52741784 0.47530866\n",
            " 0.519884   0.45924014 0.5276556  0.48126417 0.5156317  0.4830321\n",
            " 0.48723283 0.45527765 0.53452283 0.5441362  0.4959684  0.4497985\n",
            " 0.53554666 0.4810951  0.47262204 0.49864256 0.47620913 0.5022235\n",
            " 0.47301322 0.5060886  0.46047425 0.52469516 0.47063005 0.499973\n",
            " 0.49132988 0.5155396  0.4779142  0.4453058  0.492929   0.49442744\n",
            " 0.5357845  0.45417428 0.46814394 0.5150191  0.5215795  0.4489049\n",
            " 0.5385169  0.53537875 0.51929116 0.5336214  0.51264876 0.48272118\n",
            " 0.5029812  0.46900108 0.4528863  0.535057   0.4820647  0.50331753\n",
            " 0.5277033  0.5092562  0.5039807  0.46208838 0.5205633  0.4924295\n",
            " 0.5007601  0.5064813  0.49537557 0.5429312  0.5693452  0.48781747\n",
            " 0.5072365  0.45893204 0.48493454 0.4775482  0.5294507  0.4774493\n",
            " 0.54612106 0.51851004 0.5308666  0.5372789  0.49936306 0.5003789\n",
            " 0.5224354  0.5371736  0.5304927  0.4535466  0.5361784  0.50895107\n",
            " 0.474781   0.48921752 0.4875293  0.5033655  0.46094736 0.50086516\n",
            " 0.48066804]\n",
            "[0.60120586 0.57105943 0.44358312 0.64082687 0.58656331 0.57278208\n",
            " 0.67011197 0.44530577 0.56158484 0.54263566 0.51421189 0.5374677\n",
            " 0.55900086 0.55900086 0.48320413 0.54866494 0.61757106 0.48837209\n",
            " 0.53488372 0.49440138 0.42291128 0.51507321 0.71576227 0.52282515\n",
            " 0.39448751 0.37639966 0.55555556 0.43583118 0.55555556 0.47803618\n",
            " 0.35658915 0.59431525 0.47286822 0.58828596 0.56158484 0.53229974\n",
            " 0.4788975  0.70542636 0.60034453 0.51248923 0.59086994 0.51593454\n",
            " 0.64685616 0.60551249 0.55555556 0.51248923 0.50559862 0.50043066\n",
            " 0.51421189 0.64341085 0.70801034 0.47114556 0.53574505 0.50990525\n",
            " 0.55211025 0.48923342 0.39362618 0.44444444 0.65374677 0.55986219\n",
            " 0.37209302 0.42291128 0.66838932 0.47717485 0.45047373 0.416882\n",
            " 0.39190353 0.5211025  0.47028424 0.46425495 0.55211025 0.36950904\n",
            " 0.4788975  0.43755383 0.57881137 0.6089578  0.35486649 0.54263566\n",
            " 0.34969854 0.41343669 0.40999139 0.54349699 0.61068045 0.51248923\n",
            " 0.57708872 0.5796727  0.38501292 0.36950904 0.43496985 0.52713178\n",
            " 0.55986219 0.43669251 0.63738157 0.43669251 0.52196382 0.4496124\n",
            " 0.36175711 0.61843239 0.62015504 0.4461671  0.50904393 0.53660637\n",
            " 0.58656331 0.47372954 0.49095607 0.56761413 0.46339363 0.64082687\n",
            " 0.60034453 0.60637382 0.63049096 0.48148148 0.45219638 0.44013781\n",
            " 0.63910422 0.34280792 0.46339363 0.60034453 0.4625323  0.48148148\n",
            " 0.41085271 0.6873385  0.58139535 0.48406546 0.6416882  0.58053402\n",
            " 0.6709733  0.38587425 0.57795004 0.47028424 0.40913006 0.51335056\n",
            " 0.40740741 0.68475452 0.59345392 0.59431525 0.5503876  0.45219638\n",
            " 0.41774332 0.42377261 0.50129199 0.30749354 0.58570198 0.583118\n",
            " 0.59862188 0.61843239 0.49095607 0.45219638 0.53229974 0.65977606\n",
            " 0.60034453 0.53660637 0.625323   0.51765719 0.42204996 0.61068045\n",
            " 0.55727821 0.52971576 0.49095607 0.56933678 0.374677   0.40654608\n",
            " 0.51937984 0.5211025  0.49870801 0.66666667 0.82773471 0.40310078\n",
            " 0.47286822 0.45822567 0.40740741 0.52024117 0.52540913 0.52627046\n",
            " 0.4625323  0.55555556 0.48751077 0.50732127 0.38587425 0.56330749\n",
            " 0.54177433 0.45478036 0.34625323 0.46942291 0.35400517 0.4788975\n",
            " 0.4918174  0.53488372 0.4952627  0.54091301 0.39190353 0.54005168\n",
            " 0.56244617 0.46167097 0.54177433 0.2962963  0.48148148 0.43066322\n",
            " 0.36692506 0.91731266 0.66408269 0.60551249 0.5245478  0.51593454\n",
            " 0.69853575 0.50904393 0.55900086 0.45736434 0.55297158 0.66063738\n",
            " 0.4461671  0.44702842 0.62704565 0.60465116 0.68303187 0.49956934\n",
            " 0.46511628 0.59862188 0.47631352 0.38845823 0.64944014 0.50301464\n",
            " 0.51937984 0.61412575 0.49354005 0.49354005 0.57105943 0.62618432\n",
            " 0.5374677  0.34711456 0.53229974 0.51162791 0.50559862 0.63910422\n",
            " 0.75107666 0.4918174  0.47286822 0.40310078 0.35745047 0.52196382\n",
            " 0.53143842 0.56158484 0.49354005 0.51765719 0.39276486 0.54349699\n",
            " 0.36434109 0.47975883 0.54263566 0.46511628 0.48923342 0.57278208\n",
            " 0.64254953 0.39362618 0.52971576 0.44530577 0.65202412 0.5994832\n",
            " 0.40568475 0.51851852 0.45219638 0.44702842 0.48406546 0.33936262\n",
            " 0.36089578 0.52799311 0.34366925 0.34453058 0.40568475 0.58570198\n",
            " 0.42204996 0.47459087 0.4918174  0.53660637 0.22739018 0.46167097\n",
            " 0.56416882 0.59259259 0.58828596 0.56330749 0.48234281 0.52540913\n",
            " 0.60120586 0.57019811 0.49870801 0.54435831 0.43496985 0.37812231\n",
            " 0.60120586 0.32213609 0.39276486 0.63307494 0.54952627 0.52024117\n",
            " 0.51593454 0.45822567 0.56330749 0.68044789 0.55555556 0.43841516\n",
            " 0.26873385 0.66063738 0.59259259 0.37984496 0.63738157 0.65202412\n",
            " 0.50387597 0.50129199 0.54435831 0.60206718 0.42894057 0.4005168\n",
            " 0.63135228 0.50043066 0.68647717 0.58914729 0.46597761 0.47631352\n",
            " 0.59345392 0.42204996 0.42463394 0.6089578  0.46339363 0.52368648\n",
            " 0.5374677  0.36089578 0.53229974 0.56158484 0.50215332 0.50215332\n",
            " 0.48492679 0.54694229 0.54091301 0.41343669 0.47803618 0.54263566\n",
            " 0.51593454 0.39534884 0.62446167 0.5503876  0.53919035 0.42204996\n",
            " 0.46597761 0.36089578 0.45478036 0.5211025  0.6416882  0.57881137\n",
            " 0.47631352 0.44186047 0.61929371 0.54608096 0.48406546 0.57622739\n",
            " 0.45391904 0.49870801 0.62101637 0.55986219 0.49095607 0.51335056\n",
            " 0.37812231 0.60981912 0.36864772 0.60292851 0.60292851 0.47372954\n",
            " 0.60034453 0.43755383 0.51248923 0.58139535 0.53832903 0.416882\n",
            " 0.50559862 0.48923342 0.29371232 0.54005168 0.51162791 0.44272179\n",
            " 0.47631352 0.5994832  0.51076658 0.63221361 0.45564169 0.61412575\n",
            " 0.55727821 0.55297158 0.35400517 0.54177433 0.61154177 0.32213609\n",
            " 0.50990525 0.49095607 0.45478036 0.52971576 0.5994832  0.52885444\n",
            " 0.37898363 0.53143842 0.50732127 0.60551249 0.47028424 0.53832903\n",
            " 0.50904393 0.5081826  0.59345392 0.5667528  0.40137812 0.52024117\n",
            " 0.51248923 0.59086994 0.44788975 0.5994832  0.53402239 0.45736434\n",
            " 0.50215332 0.59173127 0.64685616 0.43669251 0.44702842 0.54263566\n",
            " 0.38501292 0.60292851 0.5081826  0.37726098 0.41515935 0.4496124\n",
            " 0.59345392 0.6416882  0.52971576 0.59345392 0.61068045 0.51593454\n",
            " 0.56847545 0.45047373 0.44702842 0.61154177 0.60378984 0.38156761\n",
            " 0.59259259 0.50732127 0.54005168 0.42118863 0.44358312 0.64857881\n",
            " 0.46511628 0.55727821 0.47459087 0.59776055 0.56933678 0.56761413\n",
            " 0.69336779 0.65202412 0.52799311 0.40654608 0.59689922 0.35486649\n",
            " 0.53229974 0.44186047 0.47803618 0.57795004 0.58484065 0.43238587\n",
            " 0.55469423 0.53229974 0.60809647 0.53488372 0.54694229 0.49009475\n",
            " 0.54780362 0.33850129 0.35917313 0.4203273  0.47114556 0.56761413\n",
            " 0.4332472  0.55986219 0.52885444 0.34280792 0.64513351 0.46856158\n",
            " 0.53143842 0.58570198 0.63738157 0.28682171 0.54435831 0.43238587\n",
            " 0.5245478  0.48492679 0.50990525 0.38070629 0.5994832  0.45822567\n",
            " 0.62618432 0.56761413 0.55124892 0.56330749 0.54091301 0.52799311\n",
            " 0.55986219 0.42204996 0.69336779 0.53229974 0.63652024 0.50904393\n",
            " 0.40826873 1.         0.52713178 0.51593454 0.50732127 0.63221361\n",
            " 0.52540913]\n",
            "The trained model has an aproximate error rate of 18.480962493648267 which equates to 3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure (SLP)\n"
      ],
      "metadata": {
        "id": "O60cqN0x90SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1245d7d7-4aaf-446b-83c8-e92d3fb94a43",
        "id": "u-aXoGB4_v4s"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp = df.drop(columns=['collision_date', 'temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] != 2012]\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] < 2020]\n",
        "cols = df_slp['NUM_COLLISIONS']\n",
        "df_slp = df_slp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp[:6])\n",
        "df_slp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "60cedb02-e231-437d-a156-8faf61e310a9",
        "id": "xsAYkW7-ATbk"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da     slp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  1016.1             681\n",
            "51    5  2014   1  17  1014.8             589\n",
            "54    1  2016   1  25  1021.4             658\n",
            "55    5  2016   1  29   999.4             645\n",
            "58    5  2017   1  20  1015.5             605\n",
            "59    7  2013   1  13  1020.7             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da          slp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.999217  2016.000391     6.522114    15.719765  1016.777221   \n",
              "std       2.000783     2.000294     3.447986     8.796698     7.628429   \n",
              "min       1.000000  2013.000000     1.000000     1.000000   989.500000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000  1012.200000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000  1016.700000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000  1021.700000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000  1044.200000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.147162  \n",
              "std        100.268048  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-953b72a9-0d08-47f8-8f31-56ef008fb1c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999217</td>\n",
              "      <td>2016.000391</td>\n",
              "      <td>6.522114</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000783</td>\n",
              "      <td>2.000294</td>\n",
              "      <td>3.447986</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-953b72a9-0d08-47f8-8f31-56ef008fb1c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-953b72a9-0d08-47f8-8f31-56ef008fb1c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-953b72a9-0d08-47f8-8f31-56ef008fb1c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp.iloc[np.random.permutation(len(df_slp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6ddf33-c036-41a1-deb3-08e4c772dad0",
        "id": "I-drEIq1ATbu"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da     slp\n",
            "1594    1  2014   6  23  1019.7\n",
            "505     4  2016   2   4  1015.2\n",
            "288     2  2014   1   7  1009.6\n",
            "2347    2  2013   8  27  1011.8\n",
            "469     2  2018   2  13  1039.1\n",
            "1243    4  2018   5  17  1016.7\n",
            "      day  year  mo  da     slp  NUM_COLLISIONS\n",
            "1594    1  2014   6  23  1019.7             644\n",
            "505     4  2016   2   4  1015.2             609\n",
            "288     2  2014   1   7  1009.6             491\n",
            "2347    2  2013   8  27  1011.8             506\n",
            "469     2  2018   2  13  1039.1             613\n",
            "1243    4  2018   5  17  1016.7             732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45208cc-5bb7-4347-a2e1-f75b15bb4186",
        "id": "pq6BF1zLATbv"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594    644\n",
            "505     609\n",
            "288     491\n",
            "2347    506\n",
            "469     613\n",
            "1243    732\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef866ee6-5b51-4615-9778-cbebd5e1e9a0",
        "id": "pVdp0YKeATbv"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8142a28-ca72-40d6-b7c3-1d11bddde210",
        "id": "twTCPAjdATbv"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d2c17f50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.2822011, step = 1\n",
            "INFO:tensorflow:global_step/sec: 664.653\n",
            "INFO:tensorflow:loss = 0.0069908286, step = 101 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.698\n",
            "INFO:tensorflow:loss = 0.0058898525, step = 201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 834.234\n",
            "INFO:tensorflow:loss = 0.008527931, step = 301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.96\n",
            "INFO:tensorflow:loss = 0.007766745, step = 401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.974\n",
            "INFO:tensorflow:loss = 0.005420659, step = 501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.156\n",
            "INFO:tensorflow:loss = 0.008386809, step = 601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.098\n",
            "INFO:tensorflow:loss = 0.008839803, step = 701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.069\n",
            "INFO:tensorflow:loss = 0.0058964137, step = 801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.628\n",
            "INFO:tensorflow:loss = 0.0070814975, step = 901 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.027\n",
            "INFO:tensorflow:loss = 0.0069450676, step = 1001 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.068\n",
            "INFO:tensorflow:loss = 0.005020305, step = 1101 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 695.426\n",
            "INFO:tensorflow:loss = 0.0064247455, step = 1201 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.533\n",
            "INFO:tensorflow:loss = 0.006168426, step = 1301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.136\n",
            "INFO:tensorflow:loss = 0.005391335, step = 1401 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.148\n",
            "INFO:tensorflow:loss = 0.0056912247, step = 1501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.792\n",
            "INFO:tensorflow:loss = 0.006877442, step = 1601 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.974\n",
            "INFO:tensorflow:loss = 0.008473856, step = 1701 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.523\n",
            "INFO:tensorflow:loss = 0.0072173933, step = 1801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.152\n",
            "INFO:tensorflow:loss = 0.006335122, step = 1901 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 806.299\n",
            "INFO:tensorflow:loss = 0.008604264, step = 2001 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.179\n",
            "INFO:tensorflow:loss = 0.008289334, step = 2101 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.309\n",
            "INFO:tensorflow:loss = 0.005977382, step = 2201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.046\n",
            "INFO:tensorflow:loss = 0.007185846, step = 2301 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.8\n",
            "INFO:tensorflow:loss = 0.007136031, step = 2401 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.638\n",
            "INFO:tensorflow:loss = 0.007848068, step = 2501 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.521\n",
            "INFO:tensorflow:loss = 0.008368142, step = 2601 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.104\n",
            "INFO:tensorflow:loss = 0.006127141, step = 2701 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.798\n",
            "INFO:tensorflow:loss = 0.0073781125, step = 2801 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 805.991\n",
            "INFO:tensorflow:loss = 0.0057525653, step = 2901 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.887\n",
            "INFO:tensorflow:loss = 0.007200621, step = 3001 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 913.284\n",
            "INFO:tensorflow:loss = 0.0059934054, step = 3101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 842.284\n",
            "INFO:tensorflow:loss = 0.007901663, step = 3201 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.193\n",
            "INFO:tensorflow:loss = 0.009087006, step = 3301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.064\n",
            "INFO:tensorflow:loss = 0.0059818346, step = 3401 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.702\n",
            "INFO:tensorflow:loss = 0.0054965923, step = 3501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.167\n",
            "INFO:tensorflow:loss = 0.005967924, step = 3601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 777.848\n",
            "INFO:tensorflow:loss = 0.00525345, step = 3701 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.588\n",
            "INFO:tensorflow:loss = 0.0057109706, step = 3801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 854.084\n",
            "INFO:tensorflow:loss = 0.006461434, step = 3901 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.488\n",
            "INFO:tensorflow:loss = 0.007269195, step = 4001 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.759\n",
            "INFO:tensorflow:loss = 0.0053311167, step = 4101 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.477\n",
            "INFO:tensorflow:loss = 0.0065400717, step = 4201 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.153\n",
            "INFO:tensorflow:loss = 0.0058185006, step = 4301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 861.317\n",
            "INFO:tensorflow:loss = 0.005072454, step = 4401 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.657\n",
            "INFO:tensorflow:loss = 0.011123434, step = 4501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.38\n",
            "INFO:tensorflow:loss = 0.008289435, step = 4601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 857.84\n",
            "INFO:tensorflow:loss = 0.006499376, step = 4701 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.8\n",
            "INFO:tensorflow:loss = 0.0068720626, step = 4801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.716\n",
            "INFO:tensorflow:loss = 0.005538076, step = 4901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 682.13\n",
            "INFO:tensorflow:loss = 0.005814001, step = 5001 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 638.825\n",
            "INFO:tensorflow:loss = 0.0055350866, step = 5101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.857\n",
            "INFO:tensorflow:loss = 0.0073945755, step = 5201 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.995\n",
            "INFO:tensorflow:loss = 0.0047092475, step = 5301 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 619.512\n",
            "INFO:tensorflow:loss = 0.0050542634, step = 5401 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.022\n",
            "INFO:tensorflow:loss = 0.0052306703, step = 5501 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 638.268\n",
            "INFO:tensorflow:loss = 0.006775211, step = 5601 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.773\n",
            "INFO:tensorflow:loss = 0.0051847384, step = 5701 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.316\n",
            "INFO:tensorflow:loss = 0.006451917, step = 5801 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.645\n",
            "INFO:tensorflow:loss = 0.00801122, step = 5901 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 589.949\n",
            "INFO:tensorflow:loss = 0.0070013907, step = 6001 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 594.685\n",
            "INFO:tensorflow:loss = 0.008310758, step = 6101 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 627.352\n",
            "INFO:tensorflow:loss = 0.006721818, step = 6201 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.373\n",
            "INFO:tensorflow:loss = 0.0052301567, step = 6301 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.587\n",
            "INFO:tensorflow:loss = 0.008428597, step = 6401 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.412\n",
            "INFO:tensorflow:loss = 0.0073432857, step = 6501 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.271\n",
            "INFO:tensorflow:loss = 0.0053238617, step = 6601 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.782\n",
            "INFO:tensorflow:loss = 0.008109338, step = 6701 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.412\n",
            "INFO:tensorflow:loss = 0.0077765314, step = 6801 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.235\n",
            "INFO:tensorflow:loss = 0.0062574, step = 6901 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.669\n",
            "INFO:tensorflow:loss = 0.006361138, step = 7001 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.261\n",
            "INFO:tensorflow:loss = 0.007817974, step = 7101 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 528.143\n",
            "INFO:tensorflow:loss = 0.00613985, step = 7201 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.069\n",
            "INFO:tensorflow:loss = 0.006439571, step = 7301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 592.754\n",
            "INFO:tensorflow:loss = 0.0055071637, step = 7401 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 566.609\n",
            "INFO:tensorflow:loss = 0.0050597647, step = 7501 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 593.961\n",
            "INFO:tensorflow:loss = 0.005779571, step = 7601 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.854\n",
            "INFO:tensorflow:loss = 0.0052045146, step = 7701 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 590.934\n",
            "INFO:tensorflow:loss = 0.0071325926, step = 7801 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.778\n",
            "INFO:tensorflow:loss = 0.009382186, step = 7901 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 852.556\n",
            "INFO:tensorflow:loss = 0.006646679, step = 8001 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.343\n",
            "INFO:tensorflow:loss = 0.008744756, step = 8101 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.242\n",
            "INFO:tensorflow:loss = 0.0058107143, step = 8201 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.507\n",
            "INFO:tensorflow:loss = 0.0073756743, step = 8301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 915.303\n",
            "INFO:tensorflow:loss = 0.006455164, step = 8401 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.379\n",
            "INFO:tensorflow:loss = 0.008019912, step = 8501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.035\n",
            "INFO:tensorflow:loss = 0.007606049, step = 8601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.452\n",
            "INFO:tensorflow:loss = 0.00588299, step = 8701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.331\n",
            "INFO:tensorflow:loss = 0.0077242665, step = 8801 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.301\n",
            "INFO:tensorflow:loss = 0.006104774, step = 8901 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.769\n",
            "INFO:tensorflow:loss = 0.008236082, step = 9001 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.981\n",
            "INFO:tensorflow:loss = 0.0059011783, step = 9101 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.772\n",
            "INFO:tensorflow:loss = 0.0051155044, step = 9201 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.268\n",
            "INFO:tensorflow:loss = 0.005829352, step = 9301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.36\n",
            "INFO:tensorflow:loss = 0.009099756, step = 9401 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 854.582\n",
            "INFO:tensorflow:loss = 0.007125474, step = 9501 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.678\n",
            "INFO:tensorflow:loss = 0.008628234, step = 9601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.551\n",
            "INFO:tensorflow:loss = 0.0071719894, step = 9701 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.488\n",
            "INFO:tensorflow:loss = 0.006263814, step = 9801 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.948\n",
            "INFO:tensorflow:loss = 0.006080875, step = 9901 (0.112 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.005692242.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 99.57043038931194\n",
            "Just using average = 599.0053816046967 has RMSE of 101.6203275486354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f35213-9618-44f6-c9de-c8fec6cb8111",
        "id": "RJS2QjMpATbw"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d2f1fe10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.52776057 0.55039567 0.56388557 0.56103015 0.5632473  0.5694784\n",
            " 0.56920135 0.54858345 0.49145693 0.60125995 0.58724165 0.58848876\n",
            " 0.57424855 0.5052264  0.5344421  0.59125817 0.5666763  0.51085484\n",
            " 0.5052988  0.5773522  0.60142255 0.5502752  0.55396324 0.55439365\n",
            " 0.5511964  0.5029115  0.5738866  0.53181314 0.5552395  0.5701349\n",
            " 0.5732831  0.5835354  0.53424156 0.5264206  0.57829803 0.5454287\n",
            " 0.53141344 0.5630133  0.5320521  0.57594097 0.5217124  0.58564687\n",
            " 0.5332876  0.51614517 0.5094164  0.5226072  0.5006325  0.5035109\n",
            " 0.5768346  0.5375358  0.5490387  0.54216015 0.5261327  0.5263965\n",
            " 0.57459825 0.5227212  0.5118114  0.555989   0.5264751  0.54808635\n",
            " 0.5487294  0.5327456  0.53229076 0.53256637 0.55798393 0.5194811\n",
            " 0.5191365  0.4913669  0.5168462  0.5344946  0.53740305 0.5645722\n",
            " 0.5820208  0.5451066  0.5416231  0.55108875 0.525167   0.52962786\n",
            " 0.5480791  0.5003842  0.56116617 0.53869003 0.50990415 0.5227779\n",
            " 0.5522162  0.53717    0.52465844 0.5931069  0.57737666 0.55111706\n",
            " 0.565855   0.5455082  0.49707696 0.53703755 0.51765496 0.55268073\n",
            " 0.5957416  0.5586204  0.5574314  0.5361845  0.5754019  0.5484888\n",
            " 0.5129101  0.50366473 0.55388    0.55791485 0.52622384 0.5848885\n",
            " 0.55498034 0.5314277  0.5838229  0.5735009  0.5339762  0.53787243\n",
            " 0.56141883 0.54460233 0.55079746 0.56309927 0.5514839  0.5142375\n",
            " 0.59852934 0.52716583 0.58604306 0.5233187  0.52017486 0.5715539\n",
            " 0.5581273  0.56079984 0.53927636 0.54902303 0.52621967 0.51028\n",
            " 0.53134    0.60126644 0.5462277  0.5448518  0.5407082  0.4878322\n",
            " 0.5826036  0.5613491  0.543678   0.5284747  0.54050577 0.5442969\n",
            " 0.5286217  0.4937292  0.58604866 0.567841   0.5610947  0.5648345\n",
            " 0.5268622  0.5708509  0.4956921  0.5508528  0.5443867  0.5750294\n",
            " 0.5552149  0.56646913 0.56255436 0.534591   0.5584621  0.5719866\n",
            " 0.55044997 0.5158046  0.5425685  0.55690455 0.5413664  0.5729873\n",
            " 0.5215001  0.5783729  0.54114395 0.5401128  0.5324527  0.54636043\n",
            " 0.51022476 0.56332016 0.54568386 0.57475656 0.5118741  0.52652925\n",
            " 0.4858804  0.5442025  0.57479036 0.5361578  0.5466692  0.5722365\n",
            " 0.52103126 0.5259802  0.5887298  0.5554492  0.56922346 0.5096543\n",
            " 0.5818778  0.59178656 0.5389693  0.5439044  0.54774773 0.56825817\n",
            " 0.57727265 0.5441949  0.56178313 0.57403207 0.55699474 0.5548993\n",
            " 0.5930245  0.55873585 0.57113284 0.535054   0.5558984  0.54272133\n",
            " 0.5767246  0.5568578  0.53342783 0.5206511  0.5633599  0.55388725\n",
            " 0.57599485 0.49300545 0.5875438  0.53093725 0.4960194  0.57729536\n",
            " 0.55512834 0.53384113 0.5039468  0.553221   0.5796297  0.60580105\n",
            " 0.5894296  0.5698733  0.5348166  0.5660669  0.5107427  0.54257244\n",
            " 0.5571332  0.54180115 0.56015927 0.5569095  0.58421737 0.5573399\n",
            " 0.53725904 0.56707317 0.580052   0.5286393  0.5890583  0.5543769\n",
            " 0.56418973 0.5508344  0.56462955 0.59449494 0.5808129  0.5888378\n",
            " 0.5284178  0.56559473 0.5775086  0.5574102  0.51484036 0.54205465\n",
            " 0.5773554  0.50025624 0.49769944 0.51837796 0.5265229  0.599597\n",
            " 0.51385874 0.56742555 0.52993846 0.58533245 0.49854922 0.52114165\n",
            " 0.5674702  0.5765931  0.5773374  0.5735271  0.5035868  0.55528134\n",
            " 0.5179051  0.5129039  0.5690603  0.5764002  0.5664291  0.5209826\n",
            " 0.5993743  0.57442397 0.57829356 0.5906714  0.5837692  0.59357506\n",
            " 0.57268864 0.5488633  0.5548393  0.556954   0.55274034 0.5110441\n",
            " 0.5649838  0.5456592  0.56917316 0.5192921  0.5860154  0.5613333\n",
            " 0.49263477 0.5876326  0.51562124 0.5252815  0.56711185 0.5597763\n",
            " 0.5858629  0.50095683 0.50093704 0.56337756 0.5645014  0.5022347\n",
            " 0.57456386 0.5798979  0.557132   0.53061515 0.50638604 0.5266042\n",
            " 0.5424263  0.5661647  0.55842656 0.5330295  0.52555573 0.5889298\n",
            " 0.56945467 0.5304494  0.52991396 0.5200996  0.5386576  0.48865083\n",
            " 0.5868015  0.5161825  0.51778984 0.585089   0.54730654 0.51167107\n",
            " 0.5658995  0.57851565 0.5066852  0.57814354 0.5124322  0.5753515\n",
            " 0.52345586 0.49257576 0.5291919  0.5652636  0.52830094 0.5406008\n",
            " 0.53685564 0.5080358  0.56502503 0.56911814 0.51925385 0.5963948\n",
            " 0.5239827  0.51003873 0.59094054 0.51848716 0.5263118  0.5569488\n",
            " 0.53550225 0.54832906 0.5393207  0.5409373  0.5314465  0.5512262\n",
            " 0.5719426  0.51700735 0.5184302  0.5454659  0.5898134  0.57941073\n",
            " 0.5218475  0.5392763  0.56685317 0.56452435 0.57067245 0.5798797\n",
            " 0.555879   0.5778673  0.49972042 0.59770375 0.5331989  0.56231916\n",
            " 0.5208936  0.6178964  0.5852541  0.56063575 0.57470036 0.4869046\n",
            " 0.5551069  0.5254962  0.5391834  0.5529702  0.569474   0.53655577\n",
            " 0.51889026 0.54377854 0.5209943  0.53220946 0.52874476 0.5542043\n",
            " 0.55402935 0.5207338  0.576604   0.5708245  0.5818436  0.54972786\n",
            " 0.5056218  0.5598957  0.5477596  0.5446747  0.52977306 0.50361043\n",
            " 0.48254174 0.51652944 0.53898084 0.6012721  0.519793   0.5397877\n",
            " 0.51992595 0.54758924 0.51303834 0.51051325 0.559468   0.537867\n",
            " 0.57705736 0.60776275 0.5467148  0.50968707 0.59009516 0.5342443\n",
            " 0.4916292  0.5141569  0.5572242  0.56850445 0.54580986 0.5627546\n",
            " 0.51020455 0.54040134 0.49669546 0.56683004 0.58681273 0.60604125\n",
            " 0.51244414 0.5659066  0.5435599  0.5745307  0.5587338  0.57459307\n",
            " 0.52432203 0.5321561  0.4993303  0.5155202  0.5573837  0.55879027\n",
            " 0.5395753  0.5553307  0.56677336 0.5260481  0.53471375 0.50742435\n",
            " 0.5647952  0.5201233  0.59616584 0.5448135  0.58927864 0.58821183\n",
            " 0.5614715  0.53734374 0.5336702  0.5103249  0.5474688  0.5941513\n",
            " 0.50925297 0.53008974 0.50687927 0.51373905 0.5109498  0.58630073\n",
            " 0.55369353 0.5576807  0.51642    0.60685545 0.6024496  0.57877904\n",
            " 0.55173916 0.5478551  0.6047707  0.5562755  0.5027896  0.5905738\n",
            " 0.5338138  0.5924787  0.5855453  0.56581354 0.5413169  0.5600598\n",
            " 0.51569295 0.57758754 0.57778907 0.48788205 0.56412923 0.5652869\n",
            " 0.54261255 0.5652282  0.523951   0.5401454  0.5502481  0.5584614\n",
            " 0.51121503]\n",
            "[0.39793282 0.41602067 0.48062016 0.63738157 0.60465116 0.5667528\n",
            " 0.4952627  0.60120586 0.40568475 0.54694229 0.60292851 0.48837209\n",
            " 0.54952627 0.4332472  0.5960379  0.52627046 0.47459087 0.47459087\n",
            " 0.47631352 0.4496124  0.59173127 0.61757106 0.48751077 0.47114556\n",
            " 0.37984496 0.44530577 0.45908699 0.64857881 0.57622739 0.65202412\n",
            " 0.54091301 0.57622739 0.50387597 0.5047373  0.61843239 0.58053402\n",
            " 0.45908699 0.32816537 0.47459087 0.65719208 0.41774332 0.45047373\n",
            " 0.42635659 0.43152455 0.34366925 0.43410853 0.44530577 0.38673557\n",
            " 0.58484065 0.48062016 0.38242894 0.49870801 0.51335056 0.44702842\n",
            " 0.55469423 0.53488372 0.41085271 0.50301464 0.43066322 0.5047373\n",
            " 0.54780362 0.54694229 0.53574505 0.34022394 0.48234281 0.56072351\n",
            " 0.51851852 0.37898363 0.40310078 0.65977606 0.60981912 0.51421189\n",
            " 0.47631352 0.67786391 0.56847545 0.66408269 0.4005168  0.50904393\n",
            " 0.54091301 0.36692506 0.53919035 0.64427218 0.42894057 0.4918174\n",
            " 0.49612403 0.50387597 0.3910422  0.48492679 0.58914729 0.76141258\n",
            " 0.53229974 0.72437554 0.43841516 0.43410853 0.47200689 0.59086994\n",
            " 0.50215332 0.49956934 0.48492679 0.47286822 0.63565891 0.52713178\n",
            " 0.49354005 0.50129199 0.70198105 0.65374677 0.50301464 0.58570198\n",
            " 0.46339363 0.56158484 0.59517657 0.63738157 0.7037037  0.61154177\n",
            " 0.64254953 0.5374677  0.6580534  0.5081826  0.55900086 0.54005168\n",
            " 0.60034453 0.5796727  0.42463394 0.44099914 0.44788975 0.4788975\n",
            " 0.58914729 0.48406546 0.42807924 0.43669251 0.3910422  0.41774332\n",
            " 0.45822567 0.49354005 0.39965547 0.4496124  0.44530577 0.38329027\n",
            " 0.53057709 0.38931955 0.54177433 0.4918174  0.61843239 0.6089578\n",
            " 0.55641688 0.34969854 0.62101637 0.4496124  0.4918174  0.5047373\n",
            " 0.42204996 0.56761413 0.48923342 0.49784668 0.50215332 0.52024117\n",
            " 0.45822567 0.61498708 0.51851852 0.42291128 0.59086994 0.43927649\n",
            " 0.38415159 0.52713178 0.44530577 0.45822567 0.54780362 0.55555556\n",
            " 0.47372954 0.55555556 0.4754522  0.5667528  0.59431525 0.53143842\n",
            " 0.49698536 0.56933678 0.6287683  0.64513351 0.5245478  0.48148148\n",
            " 0.35486649 0.51076658 0.62101637 0.56761413 0.61412575 0.46942291\n",
            " 0.42807924 0.40654608 0.3910422  0.62360034 0.5374677  0.38673557\n",
            " 0.43583118 0.56503015 0.48664944 0.53574505 0.49440138 0.5667528\n",
            " 0.52971576 0.69939707 0.4918174  0.56330749 0.54694229 0.63135228\n",
            " 0.53143842 0.70111972 0.53229974 0.55297158 0.50732127 0.44358312\n",
            " 0.57019811 0.57019811 0.65030146 0.63135228 0.53229974 0.52196382\n",
            " 0.52971576 0.38587425 0.54005168 0.54866494 0.39276486 0.49870801\n",
            " 0.49095607 0.41257537 0.34280792 0.56847545 0.49440138 0.64427218\n",
            " 0.6089578  0.62015504 0.48751077 0.55641688 0.60723514 0.63479759\n",
            " 0.61757106 0.44186047 0.53229974 0.56158484 0.56158484 0.49009475\n",
            " 0.37812231 0.49009475 0.56933678 0.51507321 0.60206718 0.63738157\n",
            " 0.57536606 0.60034453 0.52282515 0.63652024 0.5796727  0.56416882\n",
            " 0.41171404 0.55641688 0.54694229 0.45736434 0.56933678 0.68217054\n",
            " 0.63910422 0.38156761 0.41257537 0.43410853 0.36175711 0.48234281\n",
            " 0.45908699 0.41257537 0.51937984 0.46511628 0.41515935 0.54091301\n",
            " 0.5667528  0.58397933 0.47717485 0.45305771 0.43496985 0.56330749\n",
            " 0.49009475 0.51593454 0.16192937 0.50129199 0.54177433 0.51593454\n",
            " 0.46425495 0.50990525 0.4952627  0.54435831 0.57105943 0.54435831\n",
            " 0.69509044 0.71576227 0.49440138 0.44530577 0.50990525 0.48320413\n",
            " 0.46683893 0.48664944 0.52971576 0.47717485 0.62446167 0.65719208\n",
            " 0.33850129 0.52024117 0.49095607 0.46683893 0.54694229 0.56589147\n",
            " 0.45478036 0.40913006 0.47803618 0.64341085 0.61068045 0.44530577\n",
            " 0.44272179 0.44444444 0.66925065 0.51335056 0.42807924 0.33074935\n",
            " 0.55727821 0.53919035 0.51937984 0.56072351 0.51507321 0.57622739\n",
            " 0.53229974 0.60206718 0.39362618 0.40740741 0.34366925 0.42204996\n",
            " 0.53919035 0.44702842 0.40999139 0.63135228 0.53488372 0.4918174\n",
            " 0.58828596 0.52971576 0.43669251 0.51679587 0.47028424 0.58570198\n",
            " 0.59086994 0.26614987 0.33936262 0.6416882  0.47631352 0.48837209\n",
            " 0.49095607 0.4918174  0.50990525 0.55641688 0.46080965 0.53402239\n",
            " 0.44272179 0.48234281 0.55986219 0.80878553 0.51851852 0.62618432\n",
            " 0.51507321 0.44444444 0.66236003 0.40999139 0.5503876  0.68217054\n",
            " 0.46597761 0.46942291 0.50732127 0.66063738 0.61929371 0.53660637\n",
            " 0.56072351 0.59173127 0.56158484 0.54694229 0.60809647 0.62618432\n",
            " 0.65977606 0.37295435 0.31093885 0.5503876  0.50387597 0.55727821\n",
            " 0.43496985 0.53402239 0.54866494 0.59086994 0.57019811 0.42549526\n",
            " 0.5503876  0.48320413 0.58656331 0.66666667 0.6709733  0.5245478\n",
            " 0.64857881 0.52713178 0.44444444 0.26873385 0.44099914 0.44444444\n",
            " 0.68647717 0.40137812 0.51679587 0.44272179 0.51421189 0.49095607\n",
            " 0.34022394 0.54263566 0.54005168 0.59689922 0.61068045 0.41257537\n",
            " 0.36950904 0.44099914 0.5538329  0.57364341 0.4952627  0.42635659\n",
            " 0.42980189 0.58828596 0.60551249 0.41085271 0.53316107 0.56244617\n",
            " 0.58570198 0.62446167 0.57795004 0.46167097 0.48406546 0.58225668\n",
            " 0.33850129 0.60465116 0.62187769 0.58139535 0.44186047 0.66494401\n",
            " 0.38070629 0.46683893 0.3875969  0.47631352 0.55900086 0.55124892\n",
            " 0.40913006 0.58139535 0.48492679 0.4625323  0.70801034 0.61412575\n",
            " 0.51765719 0.58570198 0.37812231 0.45391904 0.54349699 0.43066322\n",
            " 0.62790698 0.56761413 0.52368648 0.34797588 0.5047373  0.45564169\n",
            " 0.57278208 0.4332472  0.52713178 0.39190353 0.60465116 0.64944014\n",
            " 0.54608096 0.46339363 0.47975883 0.38587425 0.54091301 0.54435831\n",
            " 0.49784668 0.57364341 0.33505599 0.49956934 0.43583118 0.53919035\n",
            " 0.54177433 0.49612403 0.40223945 0.52627046 0.55211025 0.53143842\n",
            " 0.43238587 0.62015504 0.52799311 0.61498708 0.36692506 0.48751077\n",
            " 0.62618432 0.52971576 0.57364341 0.59000861 0.54091301 0.59086994\n",
            " 0.48148148 0.45564169 0.5211025  0.39276486 0.57622739 0.61757106\n",
            " 0.5994832  0.5994832  0.50559862 0.57192076 0.68044789 0.64857881\n",
            " 0.38673557]\n",
            "The trained model has an aproximate error rate of -36.51764600605881 which equates to -6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "zwAKPA36B8U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5642daac-ca8a-4e0c-8ecc-66500a3ebe56",
        "id": "Wphmfh3WB4mu"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "3     5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "11    3  2020   1  15     2020-01-15             508  43.9  38.3  1019.4   \n",
            "12    5  2021   1   1     2021-01-01             257  39.6  29.3  1029.3   \n",
            "14    2  2022   1  25     2022-01-25             235  41.6  31.8  1013.2   \n",
            "18    7  2021   1   3     2021-01-03             186  41.1  32.3  1018.0   \n",
            "19    4  2020   1   2     2020-01-02             413  39.6  28.9  1011.8   \n",
            "\n",
            "    visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "3    10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "11    8.2  ...  51.1  35.1  0.02  999.9    1             0                 0   \n",
            "12   10.0  ...  54.0  33.1  0.13  999.9    0             0                 0   \n",
            "14   10.0  ...  48.9  30.0  0.00  999.9    0             0                 0   \n",
            "18   10.0  ...  53.1  39.0  0.00  999.9    0             0                 0   \n",
            "19   10.0  ...  46.0  33.1  0.01  999.9    0             0                 0   \n",
            "\n",
            "    hail  thunder  tornado_funnel_cloud  \n",
            "3      0        0                     0  \n",
            "11     0        0                 10000  \n",
            "12     0        0                     0  \n",
            "14     0        0                     0  \n",
            "18     0        0                     0  \n",
            "19     0        0                     0  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] != 2012]\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] < 2020]\n",
        "cols = df_gust['NUM_COLLISIONS']\n",
        "df_gust = df_gust.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust[:6])\n",
        "df_gust.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "89c0b871-d451-453c-af91-62c4b1f5052f",
        "id": "pzygHg-iB4mv"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  gust  NUM_COLLISIONS\n",
            "74    7  2016   1  17  18.1             451\n",
            "76    4  2014   1   9  20.0             561\n",
            "79    6  2019   1  19  21.0             479\n",
            "80    7  2015   1  11  17.1             341\n",
            "83    4  2015   1  29  20.0             519\n",
            "85    7  2019   1  13  15.9             374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day        year           mo           da         gust  \\\n",
              "count  1629.000000  1629.00000  1629.000000  1629.000000  1629.000000   \n",
              "mean      4.024555  2015.91283     6.278699    15.702885    27.511602   \n",
              "std       1.989070     2.01341     3.747683     8.667634     7.366770   \n",
              "min       1.000000  2013.00000     1.000000     1.000000    14.000000   \n",
              "25%       2.000000  2014.00000     3.000000     8.000000    22.000000   \n",
              "50%       4.000000  2016.00000     6.000000    16.000000    26.000000   \n",
              "75%       6.000000  2018.00000    10.000000    23.000000    31.100000   \n",
              "max       7.000000  2019.00000    12.000000    31.000000    71.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     1629.000000  \n",
              "mean       596.513198  \n",
              "std        104.479660  \n",
              "min        188.000000  \n",
              "25%        526.000000  \n",
              "50%        597.000000  \n",
              "75%        663.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93f6ca18-e104-44b8-b05c-39b9839e92da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.024555</td>\n",
              "      <td>2015.91283</td>\n",
              "      <td>6.278699</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.989070</td>\n",
              "      <td>2.01341</td>\n",
              "      <td>3.747683</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.00000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93f6ca18-e104-44b8-b05c-39b9839e92da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93f6ca18-e104-44b8-b05c-39b9839e92da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93f6ca18-e104-44b8-b05c-39b9839e92da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust.iloc[np.random.permutation(len(df_gust))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445b3f1a-bcc8-4432-fe1c-4a30a02bef1f",
        "id": "GA5wbn0KB4mw"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  gust\n",
            "2133    4  2013   7  25  27.0\n",
            "3256    3  2017  11  29  26.0\n",
            "3544    6  2014  12   6  25.1\n",
            "1116    6  2017   4  29  28.0\n",
            "3169    2  2017  11  28  22.0\n",
            "174     1  2017   1   9  22.9\n",
            "      day  year  mo  da  gust  NUM_COLLISIONS\n",
            "2133    4  2013   7  25  27.0             507\n",
            "3256    3  2017  11  29  26.0             701\n",
            "3544    6  2014  12   6  25.1             628\n",
            "1116    6  2017   4  29  28.0             661\n",
            "3169    2  2017  11  28  22.0             649\n",
            "174     1  2017   1   9  22.9             669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97d42df-1fcb-4827-8985-c519782fc481",
        "id": "iJRw3cGvB4mw"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2133    507\n",
            "3256    701\n",
            "3544    628\n",
            "1116    661\n",
            "3169    649\n",
            "174     669\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4e22eb-75f3-4944-b620-c68a4ac07e7d",
        "id": "aZCjpnyzB4mw"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1303\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b47cab7-a0f7-4d9c-d144-501ee2684628",
        "id": "wrbkpT05B4mx"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d2c4a850>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27536982, step = 1\n",
            "INFO:tensorflow:global_step/sec: 794.082\n",
            "INFO:tensorflow:loss = 0.00824094, step = 101 (0.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.256\n",
            "INFO:tensorflow:loss = 0.0077048824, step = 201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 868.591\n",
            "INFO:tensorflow:loss = 0.008711301, step = 301 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 848.131\n",
            "INFO:tensorflow:loss = 0.007413054, step = 401 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 888.238\n",
            "INFO:tensorflow:loss = 0.00823536, step = 501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.486\n",
            "INFO:tensorflow:loss = 0.0076253586, step = 601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.428\n",
            "INFO:tensorflow:loss = 0.00986084, step = 701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.216\n",
            "INFO:tensorflow:loss = 0.00635693, step = 801 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.175\n",
            "INFO:tensorflow:loss = 0.007383378, step = 901 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.402\n",
            "INFO:tensorflow:loss = 0.0061383317, step = 1001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 733.768\n",
            "INFO:tensorflow:loss = 0.007954989, step = 1101 (0.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 809.235\n",
            "INFO:tensorflow:loss = 0.006000248, step = 1201 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.85\n",
            "INFO:tensorflow:loss = 0.0080748405, step = 1301 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.008\n",
            "INFO:tensorflow:loss = 0.006542862, step = 1401 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.571\n",
            "INFO:tensorflow:loss = 0.010354753, step = 1501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 803.684\n",
            "INFO:tensorflow:loss = 0.0064679654, step = 1601 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.545\n",
            "INFO:tensorflow:loss = 0.006794334, step = 1701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.154\n",
            "INFO:tensorflow:loss = 0.0051289573, step = 1801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.475\n",
            "INFO:tensorflow:loss = 0.004773125, step = 1901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.454\n",
            "INFO:tensorflow:loss = 0.009215032, step = 2001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 807.041\n",
            "INFO:tensorflow:loss = 0.008430799, step = 2101 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.76\n",
            "INFO:tensorflow:loss = 0.007999415, step = 2201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.544\n",
            "INFO:tensorflow:loss = 0.0070231855, step = 2301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 858.625\n",
            "INFO:tensorflow:loss = 0.005631892, step = 2401 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.541\n",
            "INFO:tensorflow:loss = 0.0075410604, step = 2501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.959\n",
            "INFO:tensorflow:loss = 0.007705313, step = 2601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.086\n",
            "INFO:tensorflow:loss = 0.0070558833, step = 2701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 781.358\n",
            "INFO:tensorflow:loss = 0.0062681604, step = 2801 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 868.183\n",
            "INFO:tensorflow:loss = 0.007632448, step = 2901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 831.751\n",
            "INFO:tensorflow:loss = 0.007774994, step = 3001 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.979\n",
            "INFO:tensorflow:loss = 0.0059720576, step = 3101 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.336\n",
            "INFO:tensorflow:loss = 0.010756373, step = 3201 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 888.556\n",
            "INFO:tensorflow:loss = 0.006843753, step = 3301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.502\n",
            "INFO:tensorflow:loss = 0.006126529, step = 3401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.447\n",
            "INFO:tensorflow:loss = 0.007727866, step = 3501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.877\n",
            "INFO:tensorflow:loss = 0.0066441763, step = 3601 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 834.754\n",
            "INFO:tensorflow:loss = 0.004307884, step = 3701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.033\n",
            "INFO:tensorflow:loss = 0.008639078, step = 3801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 818.842\n",
            "INFO:tensorflow:loss = 0.0065467274, step = 3901 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.354\n",
            "INFO:tensorflow:loss = 0.007534192, step = 4001 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 850.844\n",
            "INFO:tensorflow:loss = 0.006077178, step = 4101 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 860.969\n",
            "INFO:tensorflow:loss = 0.010335748, step = 4201 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.773\n",
            "INFO:tensorflow:loss = 0.007377364, step = 4301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.44\n",
            "INFO:tensorflow:loss = 0.0059300065, step = 4401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 866.194\n",
            "INFO:tensorflow:loss = 0.0065574874, step = 4501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.277\n",
            "INFO:tensorflow:loss = 0.0072315824, step = 4601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.3\n",
            "INFO:tensorflow:loss = 0.007579338, step = 4701 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.459\n",
            "INFO:tensorflow:loss = 0.0070494195, step = 4801 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.042\n",
            "INFO:tensorflow:loss = 0.007197775, step = 4901 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 857.38\n",
            "INFO:tensorflow:loss = 0.006996853, step = 5001 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 850.573\n",
            "INFO:tensorflow:loss = 0.006091666, step = 5101 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.757\n",
            "INFO:tensorflow:loss = 0.007961992, step = 5201 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 878.289\n",
            "INFO:tensorflow:loss = 0.006994082, step = 5301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.871\n",
            "INFO:tensorflow:loss = 0.0076220287, step = 5401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 831.617\n",
            "INFO:tensorflow:loss = 0.007371978, step = 5501 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 802.44\n",
            "INFO:tensorflow:loss = 0.008581723, step = 5601 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 852.187\n",
            "INFO:tensorflow:loss = 0.0052754157, step = 5701 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.507\n",
            "INFO:tensorflow:loss = 0.0061345254, step = 5801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.847\n",
            "INFO:tensorflow:loss = 0.005749913, step = 5901 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.055\n",
            "INFO:tensorflow:loss = 0.0079001, step = 6001 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.769\n",
            "INFO:tensorflow:loss = 0.006543657, step = 6101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.694\n",
            "INFO:tensorflow:loss = 0.006470968, step = 6201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 842.885\n",
            "INFO:tensorflow:loss = 0.007933908, step = 6301 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.255\n",
            "INFO:tensorflow:loss = 0.007426682, step = 6401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 834.407\n",
            "INFO:tensorflow:loss = 0.0040119276, step = 6501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.435\n",
            "INFO:tensorflow:loss = 0.007954944, step = 6601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.38\n",
            "INFO:tensorflow:loss = 0.0052020587, step = 6701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 786.853\n",
            "INFO:tensorflow:loss = 0.0053976807, step = 6801 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 859.765\n",
            "INFO:tensorflow:loss = 0.008686034, step = 6901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.536\n",
            "INFO:tensorflow:loss = 0.0076516736, step = 7001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 866.971\n",
            "INFO:tensorflow:loss = 0.0077067167, step = 7101 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.637\n",
            "INFO:tensorflow:loss = 0.006990501, step = 7201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.516\n",
            "INFO:tensorflow:loss = 0.0071723848, step = 7301 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 804.941\n",
            "INFO:tensorflow:loss = 0.0066737914, step = 7401 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.659\n",
            "INFO:tensorflow:loss = 0.0057334537, step = 7501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.676\n",
            "INFO:tensorflow:loss = 0.0053617535, step = 7601 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.784\n",
            "INFO:tensorflow:loss = 0.007008086, step = 7701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.48\n",
            "INFO:tensorflow:loss = 0.0064583356, step = 7801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 891.419\n",
            "INFO:tensorflow:loss = 0.005148039, step = 7901 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 888.078\n",
            "INFO:tensorflow:loss = 0.0070089237, step = 8001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.363\n",
            "INFO:tensorflow:loss = 0.009390553, step = 8101 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.534\n",
            "INFO:tensorflow:loss = 0.007089155, step = 8201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.418\n",
            "INFO:tensorflow:loss = 0.006110387, step = 8301 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.863\n",
            "INFO:tensorflow:loss = 0.0073966593, step = 8401 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.713\n",
            "INFO:tensorflow:loss = 0.009465471, step = 8501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.53\n",
            "INFO:tensorflow:loss = 0.0062021776, step = 8601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.661\n",
            "INFO:tensorflow:loss = 0.009732229, step = 8701 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.379\n",
            "INFO:tensorflow:loss = 0.0048225587, step = 8801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.737\n",
            "INFO:tensorflow:loss = 0.006924618, step = 8901 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.837\n",
            "INFO:tensorflow:loss = 0.00654028, step = 9001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.27\n",
            "INFO:tensorflow:loss = 0.0064427992, step = 9101 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 864.677\n",
            "INFO:tensorflow:loss = 0.0056975596, step = 9201 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.603\n",
            "INFO:tensorflow:loss = 0.0058874832, step = 9301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.177\n",
            "INFO:tensorflow:loss = 0.005219978, step = 9401 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.458\n",
            "INFO:tensorflow:loss = 0.006918035, step = 9501 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.6\n",
            "INFO:tensorflow:loss = 0.007320218, step = 9601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.674\n",
            "INFO:tensorflow:loss = 0.005470425, step = 9701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 897.358\n",
            "INFO:tensorflow:loss = 0.0043981173, step = 9801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.244\n",
            "INFO:tensorflow:loss = 0.009352348, step = 9901 (0.112 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.007259787.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 92.94966471518207\n",
            "Just using average = 596.8211818879508 has RMSE of 103.00548183434161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62293203-a3c3-4b7b-a600-104e8ce0b0d6",
        "id": "ByVWr9PzB4mx"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77cec96cd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5557709  0.46615773 0.55753773 0.46943542 0.50679266 0.512485\n",
            " 0.580244   0.5260611  0.4998476  0.46684396 0.5563261  0.53378344\n",
            " 0.5423882  0.54754436 0.48671144 0.50661725 0.4632599  0.47780636\n",
            " 0.47252205 0.49141082 0.5076127  0.5468842  0.4914178  0.5118759\n",
            " 0.48875952 0.5689169  0.45469388 0.55350274 0.5143688  0.5031826\n",
            " 0.48916844 0.5315987  0.4908127  0.5207861  0.55147976 0.55261445\n",
            " 0.48084858 0.48353618 0.54467714 0.56474954 0.47844648 0.55962193\n",
            " 0.5180613  0.58144546 0.51866114 0.48401842 0.5406124  0.46837023\n",
            " 0.53333557 0.56471014 0.51895875 0.54937166 0.47446012 0.52627575\n",
            " 0.50368583 0.53781855 0.45744017 0.5340738  0.45110118 0.46662527\n",
            " 0.47060412 0.4848266  0.5701507  0.5213772  0.52565205 0.49091944\n",
            " 0.543196   0.4990372  0.5575727  0.4920895  0.4823263  0.50947297\n",
            " 0.5409498  0.54177517 0.4924738  0.454851   0.48959112 0.5066214\n",
            " 0.5067871  0.4976507  0.5220085  0.5598408  0.5522935  0.45103437\n",
            " 0.5076723  0.46354663 0.52495784 0.5282798  0.5630474  0.53658473\n",
            " 0.5263193  0.4871692  0.51822525 0.46284443 0.54207915 0.48139435\n",
            " 0.4751255  0.47348234 0.56045234 0.50244087 0.532107   0.4580984\n",
            " 0.54162335 0.5476901  0.5455657  0.55561644 0.5010574  0.51747286\n",
            " 0.5135572  0.49067685 0.5534209  0.45604047 0.5005195  0.55897176\n",
            " 0.49420282 0.4962484  0.5130618  0.5060535  0.53179187 0.4696472\n",
            " 0.51871455 0.47794452 0.5482109  0.5010161  0.47577047 0.51706195\n",
            " 0.49059916 0.5134812  0.47962844 0.49583533 0.47868848 0.46005553\n",
            " 0.4874813  0.5522986  0.523344   0.54160583 0.5316094  0.47245118\n",
            " 0.5026505  0.5041373  0.47845414 0.4593267  0.54831886 0.516433\n",
            " 0.5025911  0.459149   0.54721093 0.5641774  0.51769465 0.4798437\n",
            " 0.47175333 0.48323396 0.45325196 0.51233184 0.49029467 0.46976832\n",
            " 0.5352907  0.513989   0.4947531  0.5605344  0.5427615  0.56244594\n",
            " 0.53370124 0.50302845 0.50547063 0.5501658  0.46961048 0.5179778\n",
            " 0.4975954  0.52871484 0.5098088  0.485136   0.537658   0.53059334\n",
            " 0.5403644  0.45886838 0.49776942 0.48690838 0.5301396  0.5551706\n",
            " 0.52569604 0.53077483 0.50438386 0.5254705  0.48002338 0.46211368\n",
            " 0.50175345 0.5381581  0.53438014 0.517269   0.57535    0.49191868\n",
            " 0.4880345  0.47146243 0.5032828  0.48844388 0.4655635  0.4577161\n",
            " 0.565976   0.5242688  0.53929865 0.5510636  0.51046276 0.56836754\n",
            " 0.45438066 0.4535754  0.5284229  0.47085547 0.57832944 0.47897255\n",
            " 0.51158124 0.5543229  0.50992227 0.560194   0.52847534 0.5633725\n",
            " 0.5295072  0.53580296 0.44980466 0.5057436  0.5327393  0.5164032\n",
            " 0.5222165  0.45347527 0.4530212  0.4955877  0.5451931  0.49242172\n",
            " 0.4966222  0.52063376 0.48056892 0.5331152  0.52716506 0.5290949\n",
            " 0.48039976 0.48712283 0.52121705 0.53034467 0.49863073 0.46326563\n",
            " 0.46636686 0.52292454 0.55742145 0.5537968  0.5440511  0.52674645\n",
            " 0.49775663 0.5289021  0.5101048  0.45791268 0.5213862  0.4530941\n",
            " 0.5664372  0.5095429  0.4906348  0.48688954 0.46342966 0.4694693\n",
            " 0.5309389  0.47848168 0.4985912  0.45961407 0.46651253 0.55289996\n",
            " 0.5015159  0.5288679  0.52024066 0.48972166 0.5590152  0.56240726\n",
            " 0.5398056  0.4833489  0.5260205  0.4838321  0.50536054 0.47146603\n",
            " 0.48565197 0.52137214 0.54967415 0.55128485 0.56460154 0.47731534\n",
            " 0.52755207 0.54231566 0.5061983  0.5335738  0.51361006 0.49171802\n",
            " 0.49355718 0.49438882 0.5118772  0.43624786 0.56966615 0.52090347\n",
            " 0.5244048  0.5402524  0.5540748  0.51207024 0.51336604 0.5008468\n",
            " 0.53092694 0.5310676  0.50335866 0.52561    0.5069879  0.4896727\n",
            " 0.4783288  0.48505154 0.52033675 0.5390022  0.45574972 0.549468\n",
            " 0.55137867 0.49059725 0.50101036 0.5304542  0.56483585 0.5246998\n",
            " 0.47892115 0.53956914 0.52755606 0.5412718  0.48215866 0.53929985\n",
            " 0.4758981  0.51085347]\n",
            "[0.68044789 0.33505599 0.44358312 0.44875108 0.45564169 0.70887166\n",
            " 0.58742463 0.55900086 0.45564169 0.39190353 0.5211025  0.37639966\n",
            " 0.49354005 0.54005168 0.4461671  0.4496124  0.49009475 0.52024117\n",
            " 0.48492679 0.6416882  0.53488372 0.61068045 0.52713178 0.43238587\n",
            " 0.43496985 0.63996555 0.34022394 0.57364341 0.68217054 0.48923342\n",
            " 0.56072351 0.51507321 0.56158484 0.66063738 0.50043066 0.52282515\n",
            " 0.42118863 0.59000861 0.44702842 0.54521964 0.44099914 0.53143842\n",
            " 0.52799311 0.63479759 0.45391904 0.44702842 0.40568475 0.57364341\n",
            " 0.54349699 0.55986219 0.53143842 0.61498708 0.45219638 0.47114556\n",
            " 0.57881137 0.44702842 0.48062016 0.60809647 0.34453058 0.41774332\n",
            " 0.40999139 0.46339363 0.5960379  0.50990525 0.48664944 0.4461671\n",
            " 0.37295435 0.43238587 0.48837209 0.60465116 0.56330749 0.59086994\n",
            " 0.52885444 0.61584841 0.48406546 0.51335056 0.47803618 0.56503015\n",
            " 0.48751077 0.53660637 0.65202412 0.56416882 0.56072351 0.39276486\n",
            " 0.45219638 0.51593454 0.56244617 0.62187769 0.5503876  0.54263566\n",
            " 0.57536606 0.40137812 0.41085271 0.42980189 0.52282515 0.55469423\n",
            " 0.53488372 0.68217054 0.56933678 0.63910422 0.52971576 0.31093885\n",
            " 0.58656331 0.62962963 0.59259259 0.49009475 0.55297158 0.53488372\n",
            " 0.51765719 0.54608096 0.52885444 0.41774332 0.62273902 0.63652024\n",
            " 0.48320413 0.56847545 0.48923342 0.45736434 0.54866494 0.45564169\n",
            " 0.58656331 0.45736434 0.53832903 0.48664944 0.51593454 0.49784668\n",
            " 0.53919035 0.53919035 0.44099914 0.53574505 0.65202412 0.39965547\n",
            " 0.43238587 0.54694229 0.55813953 0.35228252 0.45047373 0.44702842\n",
            " 0.6873385  0.47114556 0.45822567 0.38329027 0.51248923 0.52540913\n",
            " 0.5538329  0.42635659 0.62101637 0.46425495 0.59259259 0.43496985\n",
            " 0.44358312 0.44530577 0.32385874 0.57019811 0.45908699 0.44013781\n",
            " 0.7002584  0.70111972 0.44702842 0.54349699 0.51421189 0.61154177\n",
            " 0.47286822 0.38931955 0.59259259 0.70542636 0.51076658 0.67355728\n",
            " 0.43410853 0.46683893 0.50215332 0.59431525 0.51248923 0.49440138\n",
            " 0.60206718 0.374677   0.5047373  0.51507321 0.36864772 0.51679587\n",
            " 0.48492679 0.6416882  0.47975883 0.50990525 0.52282515 0.43927649\n",
            " 0.34022394 0.49354005 0.4625323  0.56244617 0.55124892 0.56158484\n",
            " 0.46339363 0.44788975 0.54177433 0.42894057 0.45305771 0.39190353\n",
            " 0.55727821 0.86046512 0.39018088 0.51765719 0.67786391 0.57364341\n",
            " 0.33850129 0.37812231 0.59259259 0.47459087 0.5667528  0.47975883\n",
            " 0.57105943 0.50301464 0.52196382 0.56761413 0.58570198 0.49354005\n",
            " 0.54866494 0.46942291 0.43496985 0.53660637 0.53488372 0.63738157\n",
            " 0.59517657 0.32213609 0.40913006 0.54177433 0.60378984 0.50990525\n",
            " 0.52713178 0.49956934 0.44875108 0.48062016 0.58225668 0.34280792\n",
            " 0.51076658 0.53143842 0.4918174  0.56761413 0.16192937 0.50732127\n",
            " 0.416882   0.59086994 0.54263566 0.59862188 0.52024117 0.54694229\n",
            " 0.50215332 0.5047373  0.45736434 0.34280792 0.50559862 0.29371232\n",
            " 0.57450474 0.34366925 0.59776055 0.64685616 0.41515935 0.50559862\n",
            " 0.5667528  0.64857881 0.59776055 0.36950904 0.38673557 0.69595177\n",
            " 0.39190353 0.34625323 0.48148148 0.43066322 0.55727821 0.5538329\n",
            " 0.60809647 0.4918174  0.51851852 0.49354005 0.35228252 0.48320413\n",
            " 0.63135228 0.60809647 0.55641688 0.51851852 0.52627046 0.64857881\n",
            " 0.49612403 0.63049096 0.61412575 0.76055125 0.44186047 0.51937984\n",
            " 0.43755383 0.53574505 0.40999139 0.44530577 0.47631352 0.55727821\n",
            " 0.53402239 0.53660637 0.63393626 0.49267873 0.59086994 0.43927649\n",
            " 0.49956934 0.55297158 0.48664944 0.58742463 0.57622739 0.4039621\n",
            " 0.42463394 0.43066322 0.61929371 0.48664944 0.38587425 0.55986219\n",
            " 0.59345392 0.36950904 0.40913006 0.53402239 0.62187769 0.53919035\n",
            " 0.58570198 0.47631352 0.56330749 0.52368648 0.51421189 0.66666667\n",
            " 0.50990525 0.45219638]\n",
            "The trained model has an aproximate error rate of 1.523378330597116 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (Mxpsd)"
      ],
      "metadata": {
        "id": "OxfDYOaGD1Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb5bff4-57cb-4c0c-8641-de9b95834e86",
        "id": "9TtzqELvE-aN"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','gust','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] != 2012]\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] < 2020]\n",
        "cols = df_mxpsd['NUM_COLLISIONS']\n",
        "df_mxpsd = df_mxpsd.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd[:6])\n",
        "df_mxpsd.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "45f17a45-ed0f-4cd4-8771-f161a2f9d106",
        "id": "O_x5VoUvE-aO"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "49    4  2016   1  28    8.9             681\n",
            "51    5  2014   1  17    8.9             589\n",
            "54    1  2016   1  25    8.9             658\n",
            "55    5  2016   1  29    9.9             645\n",
            "58    5  2017   1  20    9.9             605\n",
            "59    7  2013   1  13    9.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da        mxpsd  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      3.999608  2016.001567     6.520564    15.737172    17.240110   \n",
              "std       2.001469     2.000587     3.449204     8.797367     5.858333   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     5.100000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    13.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    15.900000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    20.000000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    49.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2553.000000  \n",
              "mean       599.033686  \n",
              "std        100.284761  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c4d4784-372a-4df6-aab2-1fc80f20bd9b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999608</td>\n",
              "      <td>2016.001567</td>\n",
              "      <td>6.520564</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.001469</td>\n",
              "      <td>2.000587</td>\n",
              "      <td>3.449204</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c4d4784-372a-4df6-aab2-1fc80f20bd9b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c4d4784-372a-4df6-aab2-1fc80f20bd9b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c4d4784-372a-4df6-aab2-1fc80f20bd9b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd.iloc[np.random.permutation(len(df_mxpsd))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5314017d-44d2-44e4-91b5-5b39d6be53f2",
        "id": "oEaMTRvFE-aO"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  mxpsd\n",
            "441     7  2017   2  19   17.1\n",
            "2967    1  2017  10   9   18.1\n",
            "1957    1  2017   7  17   11.1\n",
            "1595    6  2016   6   4    9.9\n",
            "2942    7  2014  10   5   17.1\n",
            "1645    1  2014   6  30   12.0\n",
            "      day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "441     7  2017   2  19   17.1             438\n",
            "2967    1  2017  10   9   18.1             546\n",
            "1957    1  2017   7  17   11.1             639\n",
            "1595    6  2016   6   4    9.9             635\n",
            "2942    7  2014  10   5   17.1             505\n",
            "1645    1  2014   6  30   12.0             610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddff9e8-b95c-4571-a9e8-c007be3b8ae9",
        "id": "2IjTestaE-aO"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "441     438\n",
            "2967    546\n",
            "1957    639\n",
            "1595    635\n",
            "2942    505\n",
            "1645    610\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b15ebb-a7ea-4927-ebfd-044f2bd54b3e",
        "id": "ihCguRRBE-aO"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2042\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc6483d-7b99-47c0-f0f5-57554f642366",
        "id": "-hI4RaYNE-aP"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77cee99550>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27496588, step = 1\n",
            "INFO:tensorflow:global_step/sec: 707.813\n",
            "INFO:tensorflow:loss = 0.0067426204, step = 101 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 826.68\n",
            "INFO:tensorflow:loss = 0.006906179, step = 201 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 770.618\n",
            "INFO:tensorflow:loss = 0.008527726, step = 301 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 815.18\n",
            "INFO:tensorflow:loss = 0.0063469727, step = 401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 867.903\n",
            "INFO:tensorflow:loss = 0.008620524, step = 501 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.404\n",
            "INFO:tensorflow:loss = 0.008293173, step = 601 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.981\n",
            "INFO:tensorflow:loss = 0.0061335806, step = 701 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.098\n",
            "INFO:tensorflow:loss = 0.007831785, step = 801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 839.791\n",
            "INFO:tensorflow:loss = 0.007958703, step = 901 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 816.825\n",
            "INFO:tensorflow:loss = 0.008740334, step = 1001 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 859.663\n",
            "INFO:tensorflow:loss = 0.0067693335, step = 1101 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.574\n",
            "INFO:tensorflow:loss = 0.0071274713, step = 1201 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.795\n",
            "INFO:tensorflow:loss = 0.008047054, step = 1301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 905.745\n",
            "INFO:tensorflow:loss = 0.0068103503, step = 1401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 848.251\n",
            "INFO:tensorflow:loss = 0.007901215, step = 1501 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 740.564\n",
            "INFO:tensorflow:loss = 0.0064220065, step = 1601 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.235\n",
            "INFO:tensorflow:loss = 0.0060875234, step = 1701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.206\n",
            "INFO:tensorflow:loss = 0.008278402, step = 1801 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.048\n",
            "INFO:tensorflow:loss = 0.007899201, step = 1901 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 832.294\n",
            "INFO:tensorflow:loss = 0.00694347, step = 2001 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 792.643\n",
            "INFO:tensorflow:loss = 0.006599495, step = 2101 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 859.036\n",
            "INFO:tensorflow:loss = 0.0058768336, step = 2201 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.571\n",
            "INFO:tensorflow:loss = 0.005753144, step = 2301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.456\n",
            "INFO:tensorflow:loss = 0.007636328, step = 2401 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.33\n",
            "INFO:tensorflow:loss = 0.005490286, step = 2501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.77\n",
            "INFO:tensorflow:loss = 0.009516946, step = 2601 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.051\n",
            "INFO:tensorflow:loss = 0.006781727, step = 2701 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.583\n",
            "INFO:tensorflow:loss = 0.006684105, step = 2801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.283\n",
            "INFO:tensorflow:loss = 0.0042636245, step = 2901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 778.968\n",
            "INFO:tensorflow:loss = 0.0062883627, step = 3001 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.163\n",
            "INFO:tensorflow:loss = 0.0067165634, step = 3101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.702\n",
            "INFO:tensorflow:loss = 0.0066270307, step = 3201 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.35\n",
            "INFO:tensorflow:loss = 0.0068640476, step = 3301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.142\n",
            "INFO:tensorflow:loss = 0.007524232, step = 3401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 856.354\n",
            "INFO:tensorflow:loss = 0.0052800355, step = 3501 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.359\n",
            "INFO:tensorflow:loss = 0.005868536, step = 3601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.609\n",
            "INFO:tensorflow:loss = 0.007153947, step = 3701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.461\n",
            "INFO:tensorflow:loss = 0.007991838, step = 3801 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.011\n",
            "INFO:tensorflow:loss = 0.005821717, step = 3901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 820.442\n",
            "INFO:tensorflow:loss = 0.005990307, step = 4001 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.063\n",
            "INFO:tensorflow:loss = 0.005344251, step = 4101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.176\n",
            "INFO:tensorflow:loss = 0.0066171032, step = 4201 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.319\n",
            "INFO:tensorflow:loss = 0.006646371, step = 4301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.661\n",
            "INFO:tensorflow:loss = 0.0061851908, step = 4401 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.163\n",
            "INFO:tensorflow:loss = 0.0069421004, step = 4501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.181\n",
            "INFO:tensorflow:loss = 0.0076426235, step = 4601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.871\n",
            "INFO:tensorflow:loss = 0.005566065, step = 4701 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 784.737\n",
            "INFO:tensorflow:loss = 0.0063281273, step = 4801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.535\n",
            "INFO:tensorflow:loss = 0.006158338, step = 4901 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.636\n",
            "INFO:tensorflow:loss = 0.006184238, step = 5001 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.346\n",
            "INFO:tensorflow:loss = 0.0064299162, step = 5101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.163\n",
            "INFO:tensorflow:loss = 0.0063890335, step = 5201 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 848.472\n",
            "INFO:tensorflow:loss = 0.005911369, step = 5301 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 860.997\n",
            "INFO:tensorflow:loss = 0.005865219, step = 5401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 848.227\n",
            "INFO:tensorflow:loss = 0.007905265, step = 5501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 811.04\n",
            "INFO:tensorflow:loss = 0.006166347, step = 5601 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.988\n",
            "INFO:tensorflow:loss = 0.006330585, step = 5701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 824.112\n",
            "INFO:tensorflow:loss = 0.0064732907, step = 5801 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.644\n",
            "INFO:tensorflow:loss = 0.006576457, step = 5901 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 866.16\n",
            "INFO:tensorflow:loss = 0.006351185, step = 6001 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 843.174\n",
            "INFO:tensorflow:loss = 0.005716703, step = 6101 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 838.006\n",
            "INFO:tensorflow:loss = 0.008212168, step = 6201 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.053\n",
            "INFO:tensorflow:loss = 0.005219523, step = 6301 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.899\n",
            "INFO:tensorflow:loss = 0.0058479765, step = 6401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 809.041\n",
            "INFO:tensorflow:loss = 0.0075202617, step = 6501 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.968\n",
            "INFO:tensorflow:loss = 0.005891024, step = 6601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 905.94\n",
            "INFO:tensorflow:loss = 0.0068406733, step = 6701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.593\n",
            "INFO:tensorflow:loss = 0.0059345523, step = 6801 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.492\n",
            "INFO:tensorflow:loss = 0.007235276, step = 6901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.046\n",
            "INFO:tensorflow:loss = 0.0049801935, step = 7001 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.98\n",
            "INFO:tensorflow:loss = 0.0064501697, step = 7101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.649\n",
            "INFO:tensorflow:loss = 0.0067488765, step = 7201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.343\n",
            "INFO:tensorflow:loss = 0.00632841, step = 7301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 829.622\n",
            "INFO:tensorflow:loss = 0.007326349, step = 7401 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 829.515\n",
            "INFO:tensorflow:loss = 0.0077578817, step = 7501 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.031\n",
            "INFO:tensorflow:loss = 0.0062175295, step = 7601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.508\n",
            "INFO:tensorflow:loss = 0.005936479, step = 7701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 915.745\n",
            "INFO:tensorflow:loss = 0.005715204, step = 7801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.471\n",
            "INFO:tensorflow:loss = 0.006389696, step = 7901 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.349\n",
            "INFO:tensorflow:loss = 0.0062564667, step = 8001 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.445\n",
            "INFO:tensorflow:loss = 0.005322862, step = 8101 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.599\n",
            "INFO:tensorflow:loss = 0.0048830286, step = 8201 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 821.66\n",
            "INFO:tensorflow:loss = 0.006711765, step = 8301 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 846.881\n",
            "INFO:tensorflow:loss = 0.0058353427, step = 8401 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 860.734\n",
            "INFO:tensorflow:loss = 0.0052932976, step = 8501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.83\n",
            "INFO:tensorflow:loss = 0.006431135, step = 8601 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.375\n",
            "INFO:tensorflow:loss = 0.0054634577, step = 8701 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 849.374\n",
            "INFO:tensorflow:loss = 0.007685625, step = 8801 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 885.842\n",
            "INFO:tensorflow:loss = 0.00522078, step = 8901 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.74\n",
            "INFO:tensorflow:loss = 0.006727823, step = 9001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.445\n",
            "INFO:tensorflow:loss = 0.005328943, step = 9101 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 857.091\n",
            "INFO:tensorflow:loss = 0.0059844786, step = 9201 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.106\n",
            "INFO:tensorflow:loss = 0.006180904, step = 9301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.195\n",
            "INFO:tensorflow:loss = 0.006751202, step = 9401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.361\n",
            "INFO:tensorflow:loss = 0.00905379, step = 9501 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 880.45\n",
            "INFO:tensorflow:loss = 0.0059330533, step = 9601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.741\n",
            "INFO:tensorflow:loss = 0.0050622416, step = 9701 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 713.831\n",
            "INFO:tensorflow:loss = 0.006114354, step = 9801 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.944\n",
            "INFO:tensorflow:loss = 0.005315422, step = 9901 (0.111 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0044342517.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 101.52706958809091\n",
            "Just using average = 599.2531831537708 has RMSE of 105.32995403460292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacd270a-259b-4da1-f458-8225017bf439",
        "id": "5vjLYo0bE-aP"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77e789d7d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.5293082  0.45215654 0.5274805  0.5408062  0.5190872  0.5424172\n",
            " 0.51296085 0.5314365  0.5083273  0.45073065 0.47601122 0.47475526\n",
            " 0.55633026 0.4968665  0.5222989  0.50373894 0.48763293 0.49780604\n",
            " 0.47671187 0.4903994  0.53298473 0.5074837  0.53918684 0.5596663\n",
            " 0.486301   0.49028987 0.5306759  0.51404196 0.51402485 0.5284784\n",
            " 0.5108726  0.49801183 0.5377447  0.48471156 0.4934774  0.50311685\n",
            " 0.4780389  0.46411392 0.45845762 0.48775643 0.49416402 0.52164763\n",
            " 0.5230926  0.50950664 0.52390456 0.4723207  0.49155954 0.48216158\n",
            " 0.47403118 0.4793328  0.47140902 0.5235308  0.50898224 0.49873555\n",
            " 0.53903866 0.49509123 0.5201238  0.5538446  0.46266568 0.496634\n",
            " 0.52019906 0.5249832  0.5053259  0.52857107 0.46512175 0.48035\n",
            " 0.52722013 0.4559736  0.47541964 0.53982013 0.48098582 0.49597365\n",
            " 0.53846496 0.48954016 0.45195374 0.4956513  0.47107282 0.52475154\n",
            " 0.55246025 0.52242595 0.5120174  0.5077789  0.48453414 0.5286889\n",
            " 0.5011728  0.44837517 0.52174324 0.52493167 0.5374102  0.4832397\n",
            " 0.46945694 0.49481007 0.5284255  0.4652467  0.54003084 0.4957129\n",
            " 0.51551104 0.5079077  0.523043   0.51744103 0.514051   0.50843066\n",
            " 0.50292635 0.49383426 0.4499518  0.5431698  0.506342   0.5037961\n",
            " 0.52791005 0.51296204 0.46881348 0.46700573 0.5074602  0.54277015\n",
            " 0.53087646 0.4616952  0.49975014 0.46670565 0.51916754 0.4909374\n",
            " 0.5378599  0.4557144  0.4594619  0.49457866 0.49034438 0.55780053\n",
            " 0.4621139  0.49313658 0.47360772 0.5086652  0.46257818 0.50600624\n",
            " 0.46007156 0.462827   0.4825165  0.456229   0.4889359  0.54238933\n",
            " 0.49332508 0.5326726  0.4621922  0.46048343 0.4726659  0.5483893\n",
            " 0.5211848  0.5248277  0.5203536  0.5197848  0.5123951  0.44186285\n",
            " 0.5230551  0.44605103 0.53341323 0.4745971  0.54381984 0.5031247\n",
            " 0.50258505 0.5097521  0.5034127  0.5224311  0.52179134 0.4946259\n",
            " 0.5346712  0.52622163 0.4772459  0.5556669  0.5257478  0.5222988\n",
            " 0.50438535 0.53232765 0.4542408  0.49117088 0.52207595 0.46955952\n",
            " 0.4736475  0.46901646 0.45428836 0.5001014  0.52458227 0.5120558\n",
            " 0.50420856 0.5089218  0.49738815 0.47884667 0.50421363 0.4399515\n",
            " 0.54573923 0.55694103 0.49960044 0.48201048 0.4927461  0.47007546\n",
            " 0.4936977  0.5052163  0.5426711  0.53432626 0.50432134 0.4664144\n",
            " 0.5299384  0.50798994 0.49298254 0.48111746 0.53558785 0.48173788\n",
            " 0.45254317 0.5417538  0.52942604 0.4800482  0.4527491  0.51522917\n",
            " 0.5055897  0.47046643 0.49012583 0.49796084 0.45429224 0.48319668\n",
            " 0.50938696 0.53266865 0.4924461  0.5129002  0.49885347 0.49452212\n",
            " 0.5269505  0.47325268 0.4880407  0.45799145 0.49107528 0.50891644\n",
            " 0.48129797 0.5224439  0.48181495 0.45199525 0.53710365 0.5006877\n",
            " 0.46576625 0.4855093  0.5246754  0.51985747 0.5222409  0.463008\n",
            " 0.49765885 0.4832366  0.46549138 0.49701083 0.49088457 0.49908137\n",
            " 0.47800332 0.5158246  0.48074073 0.46626934 0.48245573 0.51256084\n",
            " 0.44617936 0.5077384  0.5508129  0.524232   0.47035998 0.47516963\n",
            " 0.45043448 0.51803476 0.44715106 0.51180017 0.5136121  0.4852093\n",
            " 0.4666164  0.47806734 0.5302214  0.5143023  0.47868162 0.4817471\n",
            " 0.52868503 0.46044725 0.5330783  0.5260817  0.51765    0.5161038\n",
            " 0.50335985 0.48969314 0.5055328  0.50497115 0.5229709  0.4859981\n",
            " 0.4699025  0.46941906 0.44923913 0.5035726  0.44271117 0.4468606\n",
            " 0.52194405 0.4820316  0.5300263  0.5170216  0.4730678  0.47222516\n",
            " 0.48170877 0.45564768 0.53716207 0.5361475  0.48722398 0.473005\n",
            " 0.49392238 0.47177887 0.5306673  0.53870463 0.49557653 0.51088136\n",
            " 0.48779225 0.5322139  0.5234403  0.53455603 0.45785904 0.4938015\n",
            " 0.5189157  0.4551206  0.4658361  0.47829825 0.47978634 0.47065124\n",
            " 0.53808767 0.5237971  0.45685846 0.4652176  0.47900796 0.48166236\n",
            " 0.44634163 0.5096713  0.51173323 0.4430701  0.48308167 0.46588263\n",
            " 0.5132552  0.4419739  0.49851894 0.53437656 0.536423   0.477859\n",
            " 0.5089098  0.50857055 0.4749175  0.465718   0.5344189  0.5198069\n",
            " 0.49471256 0.52518046 0.46380237 0.4793745  0.47996035 0.5036036\n",
            " 0.52145267 0.45694897 0.4707832  0.49928865 0.52275485 0.45152915\n",
            " 0.46646675 0.5061026  0.5034113  0.44152576 0.54480535 0.5366614\n",
            " 0.4410268  0.4890761  0.5220626  0.43788844 0.4790772  0.5338263\n",
            " 0.52119017 0.49677068 0.51280075 0.5105709  0.49405602 0.50425315\n",
            " 0.48718947 0.52763474 0.4847281  0.5214876  0.49990106 0.47240877\n",
            " 0.5121034  0.48244613 0.48857915 0.4763661  0.48748884 0.4407028\n",
            " 0.50402164 0.4966694  0.5500344  0.51239115 0.5347331  0.53502953\n",
            " 0.48076615 0.506548   0.548017   0.4732909  0.49662864 0.5115634\n",
            " 0.4790917  0.50665975 0.5519939  0.45634353 0.5183018  0.4384193\n",
            " 0.48344356 0.48218247 0.545254   0.52368194 0.49985594 0.5111385\n",
            " 0.54814386 0.44587424 0.52610934 0.46952724 0.47608685 0.4787265\n",
            " 0.4705162  0.45015797 0.5582452  0.54610443 0.510468   0.46263292\n",
            " 0.4950492  0.49093723 0.47935548 0.47904846 0.46455956 0.50975\n",
            " 0.47905064 0.47909802 0.5263269  0.49163124 0.55391705 0.5025919\n",
            " 0.43996397 0.5073158  0.5292204  0.5400382  0.49071702 0.49060324\n",
            " 0.46051592 0.49248394 0.496685   0.4740601  0.5168829  0.49966094\n",
            " 0.49085596 0.52583015 0.53521675 0.5015846  0.525046   0.509906\n",
            " 0.49529654 0.482501   0.4987014  0.49942613 0.4635559  0.5255832\n",
            " 0.51587427 0.5565978  0.48191413 0.48399657 0.51530576 0.52069974\n",
            " 0.4645544  0.53974694 0.5211518  0.5003327  0.51556915 0.50941855\n",
            " 0.544207   0.53165823 0.48761937 0.48246795 0.4946141  0.50548035\n",
            " 0.50262487 0.51164234 0.51595676 0.49007252 0.49838912 0.5052678\n",
            " 0.519803   0.5074939  0.5151787  0.50077516 0.49628735 0.4577855\n",
            " 0.5197518  0.5242269  0.46877265 0.4546811  0.48066187 0.53200924\n",
            " 0.5363786  0.46058264 0.5558493  0.48074362 0.5103373  0.47500956\n",
            " 0.43957773 0.52064323 0.5176964  0.47593904 0.54617655 0.47745174\n",
            " 0.52739805 0.52024883 0.45819813 0.5109016  0.48568884 0.4976617\n",
            " 0.5240242 ]\n",
            "[0.51162791 0.40654608 0.57536606 0.50301464 0.59431525 0.4005168\n",
            " 0.43669251 0.49698536 0.53919035 0.41085271 0.6416882  0.59000861\n",
            " 0.53574505 0.41085271 0.53660637 0.60034453 0.71490095 0.56158484\n",
            " 0.46167097 0.50043066 0.51851852 0.53488372 0.5245478  0.56072351\n",
            " 0.48923342 0.47286822 0.2213609  0.50990525 0.48578811 0.57105943\n",
            " 0.65719208 0.60034453 0.49956934 0.52885444 0.42980189 0.48148148\n",
            " 0.45305771 0.37812231 0.45994832 0.42118863 0.43238587 0.60809647\n",
            " 0.56072351 0.40826873 0.56847545 0.56330749 0.45564169 0.49784668\n",
            " 0.42204996 0.62704565 0.50732127 0.51421189 0.56847545 0.5211025\n",
            " 0.74677003 0.51507321 0.6416882  0.63996555 0.45219638 0.6287683\n",
            " 0.5994832  0.51679587 0.54694229 0.40568475 0.49612403 0.44358312\n",
            " 0.4496124  0.54521964 0.45478036 0.63135228 0.43410853 0.57881137\n",
            " 0.49440138 0.46511628 0.41429802 0.65546942 0.64857881 0.66149871\n",
            " 0.52627046 0.69939707 0.57019811 0.5538329  0.49698536 0.52368648\n",
            " 1.         0.45305771 0.54177433 0.5503876  0.58656331 0.47114556\n",
            " 0.54091301 0.63049096 0.51765719 0.42807924 0.61326443 0.51076658\n",
            " 0.56933678 0.52799311 0.63738157 0.51593454 0.65202412 0.48751077\n",
            " 0.64944014 0.60551249 0.40913006 0.5538329  0.5374677  0.70111972\n",
            " 0.66666667 0.62273902 0.41429802 0.45478036 0.60206718 0.54005168\n",
            " 0.42291128 0.40137812 0.44530577 0.51851852 0.40913006 0.46856158\n",
            " 0.50904393 0.49354005 0.48148148 0.50387597 0.51421189 0.49870801\n",
            " 0.45908699 0.56503015 0.4788975  0.49612403 0.48234281 0.51507321\n",
            " 0.48406546 0.40826873 0.52713178 0.50301464 0.51593454 0.38156761\n",
            " 0.34022394 0.47803618 0.52024117 0.46511628 0.60292851 0.64254953\n",
            " 0.56847545 0.4918174  0.55813953 0.56416882 0.51851852 0.32816537\n",
            " 0.55813953 0.31093885 0.54091301 0.64685616 0.53402239 0.70542636\n",
            " 0.5667528  0.6124031  0.63393626 0.44702842 0.52196382 0.48923342\n",
            " 0.49009475 0.6287683  0.43755383 0.4461671  0.47631352 0.54694229\n",
            " 0.62015504 0.60292851 0.44272179 0.49870801 0.45564169 0.58828596\n",
            " 0.64857881 0.65202412 0.51593454 0.53660637 0.4918174  0.50129199\n",
            " 0.66925065 0.48148148 0.45994832 0.49354005 0.55813953 0.39793282\n",
            " 0.53057709 0.52799311 0.58656331 0.59086994 0.49870801 0.56158484\n",
            " 0.47631352 0.53832903 0.42463394 0.56072351 0.52713178 0.54005168\n",
            " 0.56158484 0.48923342 0.45564169 0.52368648 0.46856158 0.49267873\n",
            " 0.51937984 0.56072351 0.36520241 0.40482343 0.38156761 0.4039621\n",
            " 0.59689922 0.4918174  0.45736434 0.57364341 0.36003445 0.45822567\n",
            " 0.59776055 0.64513351 0.6089578  0.45736434 0.61757106 0.625323\n",
            " 0.60378984 0.40223945 0.62790698 0.44013781 0.43583118 0.60809647\n",
            " 0.47372954 0.59345392 0.26873385 0.38587425 0.52799311 0.46683893\n",
            " 0.51593454 0.47631352 0.51421189 0.64513351 0.57364341 0.55297158\n",
            " 0.59259259 0.60120586 0.36434109 0.54177433 0.46167097 0.57795004\n",
            " 0.63393626 0.37639966 0.60292851 0.56847545 0.42463394 0.59431525\n",
            " 0.37209302 0.51937984 0.37898363 0.61929371 0.45650301 0.56589147\n",
            " 0.43238587 0.48664944 0.42204996 0.5667528  0.30577089 0.53660637\n",
            " 0.60637382 0.44530577 0.49956934 0.55641688 0.40826873 0.60120586\n",
            " 0.61584841 0.50559862 0.61068045 0.50559862 0.46683893 0.42894057\n",
            " 0.52885444 0.43755383 0.40999139 0.49956934 0.58656331 0.70456503\n",
            " 0.59000861 0.49956934 0.34797588 0.46511628 0.3453919  0.47372954\n",
            " 0.53143842 0.7037037  0.44444444 0.51765719 0.52799311 0.51335056\n",
            " 0.47975883 0.46511628 0.59517657 0.51248923 0.5960379  0.44358312\n",
            " 0.55555556 0.44099914 0.61929371 0.62360034 0.50387597 0.61757106\n",
            " 0.50215332 0.52196382 0.60981912 0.49784668 0.50732127 0.52540913\n",
            " 0.58914729 0.39276486 0.37639966 0.3910422  0.34022394 0.51851852\n",
            " 0.52885444 0.49009475 0.47286822 0.48492679 0.53574505 0.3875969\n",
            " 0.42291128 0.52971576 0.54177433 0.35658915 0.43927649 0.54005168\n",
            " 0.42980189 0.3910422  0.63910422 0.49870801 0.51421189 0.57536606\n",
            " 0.86046512 0.61412575 0.47459087 0.41860465 0.62962963 0.58397933\n",
            " 0.59086994 0.48062016 0.44358312 0.40568475 0.40654608 0.63738157\n",
            " 0.82687339 0.36434109 0.50559862 0.56072351 0.63565891 0.43927649\n",
            " 0.47028424 0.54952627 0.53143842 0.36692506 0.50732127 0.59000861\n",
            " 0.32127476 0.56761413 0.49612403 0.45822567 0.45822567 0.57622739\n",
            " 0.6124031  0.52540913 0.56416882 0.61670973 0.45047373 0.52627046\n",
            " 0.40568475 0.6124031  0.63049096 0.61154177 0.64082687 0.41171404\n",
            " 0.54091301 0.53574505 0.63135228 0.59431525 0.56244617 0.36003445\n",
            " 0.56330749 0.55986219 0.48148148 0.63307494 0.6416882  0.51679587\n",
            " 0.46425495 0.48234281 0.41774332 0.40137812 0.47803618 0.70801034\n",
            " 0.59259259 0.46080965 0.60637382 0.47803618 0.60465116 0.32730405\n",
            " 0.72782084 0.48923342 0.374677   0.62101637 0.52196382 0.51421189\n",
            " 0.58914729 0.3875969  0.5667528  0.3910422  0.48320413 0.45822567\n",
            " 0.44788975 0.37898363 0.5503876  0.54263566 0.59431525 0.40310078\n",
            " 0.61326443 0.45478036 0.33936262 0.66838932 0.50559862 0.58742463\n",
            " 0.42291128 0.43238587 0.63824289 0.41946598 0.39276486 0.49956934\n",
            " 0.35400517 0.56244617 0.31955211 0.45047373 0.57105943 0.5081826\n",
            " 0.43669251 0.48148148 0.61584841 0.51076658 0.57536606 0.46425495\n",
            " 0.47286822 0.50129199 0.68475452 0.36606374 0.54263566 0.5796727\n",
            " 0.55813953 0.47286822 0.53919035 0.7329888  0.42807924 0.4952627\n",
            " 0.53402239 0.33936262 0.44875108 0.6546081  0.54952627 0.58570198\n",
            " 0.44875108 0.52885444 0.44702842 0.53488372 0.57881137 0.5211025\n",
            " 0.51851852 0.60378984 0.42204996 0.49009475 0.30577089 0.61154177\n",
            " 0.60292851 0.59086994 0.47286822 0.55297158 0.55727821 0.56761413\n",
            " 0.60292851 0.38587425 0.53402239 0.56330749 0.48837209 0.50904393\n",
            " 0.48406546 0.53057709 0.48492679 0.40826873 0.42635659 0.4039621\n",
            " 0.45478036 0.44702842 0.48234281 0.4461671  0.68044789 0.51765719\n",
            " 0.33419466 0.42894057 0.51162791 0.51851852 0.49095607 0.5245478\n",
            " 0.4203273  0.43152455 0.4203273  0.50904393 0.5047373  0.48751077\n",
            " 0.4496124 ]\n",
            "The trained model has an aproximate error rate of 18.847327857918003 which equates to 3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others\n"
      ],
      "metadata": {
        "id": "9_3Jr46KHnik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Neural Network (DNN)"
      ],
      "metadata": {
        "id": "xIvme6cXJF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precipition (prcp)"
      ],
      "metadata": {
        "id": "cixnwflQxNZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09197eb-38f3-487a-f73c-8876a61ba5ba",
        "id": "F9YrBdhTYhyE"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp_dnn = df.drop(columns=['temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] != 2012]\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] < 2020]\n",
        "cols = df_prcp_dnn['NUM_COLLISIONS']\n",
        "df_prcp_dnn = df_prcp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp_dnn.insert(loc=25, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp_dnn[:6])\n",
        "df_prcp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "jw5elr-LbRBF",
        "outputId": "b6c548b8-8510-40cc-9849-fec7353d0b7e"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  Dec  \\\n",
            "49  2016  28  0.09    0             0                 0     0    0    0    0   \n",
            "51  2014  17  0.00    1             0                 0     0    0    0    0   \n",
            "54  2016  25  0.02    0             0                 0     0    0    0    0   \n",
            "55  2016  29  0.00    0             0                 0     0    0    0    0   \n",
            "58  2017  20  0.00    0             0                 0     0    0    0    0   \n",
            "59  2013  13  0.01    1             0                 0     0    0    0    0   \n",
            "\n",
            "    ...  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49  ...    0    0    0    0    0    0    0    0    1             681  \n",
            "51  ...    0    0    0    0    0    0    1    0    0             589  \n",
            "54  ...    0    0    0    0    0    1    0    0    0             658  \n",
            "55  ...    0    0    0    0    0    0    1    0    0             645  \n",
            "58  ...    0    0    0    0    0    0    1    0    0             605  \n",
            "59  ...    0    0    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         prcp          fog  rain_drizzle  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000   2539.000000   \n",
              "mean   2015.989366    15.745569     0.122588     0.253249      0.375345   \n",
              "std       1.996126     8.803199     0.329143     0.434958      0.484307   \n",
              "min    2013.000000     1.000000     0.000000     0.000000      0.000000   \n",
              "25%    2014.000000     8.000000     0.000000     0.000000      0.000000   \n",
              "50%    2016.000000    16.000000     0.000000     0.000000      0.000000   \n",
              "75%    2018.000000    23.000000     0.060000     1.000000      1.000000   \n",
              "max    2019.000000    31.000000     3.760000     1.000000      1.000000   \n",
              "\n",
              "       snow_ice_pellets         hail          Apr          Aug          Dec  \\\n",
              "count       2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean           0.085467     0.000394     0.082316     0.083497     0.085467   \n",
              "std            0.279630     0.019846     0.274899     0.276687     0.279630   \n",
              "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "       ...          Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  ...  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean   ...     0.082710     0.085467     0.079953     0.142970     0.143364   \n",
              "std    ...     0.275497     0.279630     0.271273     0.350111     0.350512   \n",
              "min    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max    ...     1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000     2539.000000  \n",
              "mean      0.143757     0.142182     0.142576     0.142182      599.135093  \n",
              "std       0.350913     0.349305     0.349709     0.349305      100.299164  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-521b5bcd-43bb-4093-b1f2-ea1352b316cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.989366</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.082316</td>\n",
              "      <td>0.083497</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.079953</td>\n",
              "      <td>0.142970</td>\n",
              "      <td>0.143364</td>\n",
              "      <td>0.143757</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>0.142576</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.996126</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>0.274899</td>\n",
              "      <td>0.276687</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275497</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.271273</td>\n",
              "      <td>0.350111</td>\n",
              "      <td>0.350512</td>\n",
              "      <td>0.350913</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>0.349709</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-521b5bcd-43bb-4093-b1f2-ea1352b316cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-521b5bcd-43bb-4093-b1f2-ea1352b316cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-521b5bcd-43bb-4093-b1f2-ea1352b316cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp_dnn.iloc[np.random.permutation(len(df_prcp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpJi3P_8YcIq",
        "outputId": "cfd87bd0-57bb-4a70-a5e6-ec2ded5654fa"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  \\\n",
            "3239  2016  15  0.00    0             1                 0     0    0    0   \n",
            "3223  2014  14  0.22    0             1                 0     0    0    0   \n",
            "132   2018  25  0.00    0             0                 0     0    0    0   \n",
            "223   2013  25  0.00    0             0                 0     0    0    0   \n",
            "3304  2015   6  0.14    1             1                 0     0    0    0   \n",
            "2858  2014   6  0.00    0             0                 0     0    0    0   \n",
            "\n",
            "      Dec  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3239    0  ...    0    1    0    0    1    0    0    0    0    0  \n",
            "3223    0  ...    0    1    0    0    0    0    0    1    0    0  \n",
            "132     0  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "223     0  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "3304    0  ...    0    1    0    0    0    0    0    1    0    0  \n",
            "2858    0  ...    0    0    1    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_A0EFvxZpeA",
        "outputId": "9cf16f49-9344-4935-8922-e9a02e414957"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3239    741\n",
            "3223    654\n",
            "132     602\n",
            "223     674\n",
            "3304    616\n",
            "2858    639\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdO_kvOZZuii",
        "outputId": "4166db8e-9ad4-4fe2-bfe6-6151c57edc46"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaTuIaPRfMxx",
        "outputId": "2025a658-7c29-4f7f-96ad-4d79d5af9872"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d7a2a590>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 8614.432, step = 1\n",
            "INFO:tensorflow:global_step/sec: 468.388\n",
            "INFO:tensorflow:loss = 0.06773439, step = 101 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.575\n",
            "INFO:tensorflow:loss = 0.046054713, step = 201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.734\n",
            "INFO:tensorflow:loss = 0.030378692, step = 301 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.202\n",
            "INFO:tensorflow:loss = 0.01904181, step = 401 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 619.223\n",
            "INFO:tensorflow:loss = 0.013624711, step = 501 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.021\n",
            "INFO:tensorflow:loss = 0.012396911, step = 601 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.162\n",
            "INFO:tensorflow:loss = 0.008858361, step = 701 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 610.272\n",
            "INFO:tensorflow:loss = 0.008373805, step = 801 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.481\n",
            "INFO:tensorflow:loss = 0.008350795, step = 901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.3\n",
            "INFO:tensorflow:loss = 0.0072910804, step = 1001 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.873\n",
            "INFO:tensorflow:loss = 0.006804031, step = 1101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.546\n",
            "INFO:tensorflow:loss = 0.008118564, step = 1201 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.894\n",
            "INFO:tensorflow:loss = 0.0052299807, step = 1301 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.242\n",
            "INFO:tensorflow:loss = 0.017465422, step = 1401 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 586.263\n",
            "INFO:tensorflow:loss = 0.014993436, step = 1501 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.889\n",
            "INFO:tensorflow:loss = 0.0053247446, step = 1601 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 625.478\n",
            "INFO:tensorflow:loss = 0.0666861, step = 1701 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 608.776\n",
            "INFO:tensorflow:loss = 0.16567336, step = 1801 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.277\n",
            "INFO:tensorflow:loss = 1.180902, step = 1901 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.699\n",
            "INFO:tensorflow:loss = 0.030871041, step = 2001 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.148\n",
            "INFO:tensorflow:loss = 2.6195188, step = 2101 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 619.32\n",
            "INFO:tensorflow:loss = 0.1662564, step = 2201 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.838\n",
            "INFO:tensorflow:loss = 0.08829763, step = 2301 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.073\n",
            "INFO:tensorflow:loss = 12.762354, step = 2401 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 625.499\n",
            "INFO:tensorflow:loss = 0.004661005, step = 2501 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.309\n",
            "INFO:tensorflow:loss = 0.01744774, step = 2601 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.243\n",
            "INFO:tensorflow:loss = 0.730735, step = 2701 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.847\n",
            "INFO:tensorflow:loss = 0.70398766, step = 2801 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.755\n",
            "INFO:tensorflow:loss = 0.23110816, step = 2901 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.797\n",
            "INFO:tensorflow:loss = 0.013484217, step = 3001 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.984\n",
            "INFO:tensorflow:loss = 0.009846692, step = 3101 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 583.21\n",
            "INFO:tensorflow:loss = 0.0316619, step = 3201 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 599.248\n",
            "INFO:tensorflow:loss = 0.057044182, step = 3301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.558\n",
            "INFO:tensorflow:loss = 0.009216825, step = 3401 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.111\n",
            "INFO:tensorflow:loss = 0.025919713, step = 3501 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.831\n",
            "INFO:tensorflow:loss = 0.10919081, step = 3601 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.174\n",
            "INFO:tensorflow:loss = 0.057368897, step = 3701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.51\n",
            "INFO:tensorflow:loss = 0.3974732, step = 3801 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.946\n",
            "INFO:tensorflow:loss = 0.022715569, step = 3901 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.336\n",
            "INFO:tensorflow:loss = 0.09261198, step = 4001 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.855\n",
            "INFO:tensorflow:loss = 12.698189, step = 4101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.191\n",
            "INFO:tensorflow:loss = 0.004850672, step = 4201 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.316\n",
            "INFO:tensorflow:loss = 0.0101005975, step = 4301 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.513\n",
            "INFO:tensorflow:loss = 0.025116451, step = 4401 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.027\n",
            "INFO:tensorflow:loss = 0.019087814, step = 4501 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.645\n",
            "INFO:tensorflow:loss = 0.2324802, step = 4601 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.84\n",
            "INFO:tensorflow:loss = 0.005047852, step = 4701 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.737\n",
            "INFO:tensorflow:loss = 0.020957407, step = 4801 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.053\n",
            "INFO:tensorflow:loss = 0.004870772, step = 4901 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 552.137\n",
            "INFO:tensorflow:loss = 0.0045735827, step = 5001 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.811\n",
            "INFO:tensorflow:loss = 0.031943943, step = 5101 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.828\n",
            "INFO:tensorflow:loss = 0.053114407, step = 5201 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.512\n",
            "INFO:tensorflow:loss = 0.04816205, step = 5301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.414\n",
            "INFO:tensorflow:loss = 0.0056677237, step = 5401 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.465\n",
            "INFO:tensorflow:loss = 0.030814737, step = 5501 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.185\n",
            "INFO:tensorflow:loss = 0.3539663, step = 5601 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.384\n",
            "INFO:tensorflow:loss = 0.009234874, step = 5701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 575.187\n",
            "INFO:tensorflow:loss = 0.014079014, step = 5801 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.36\n",
            "INFO:tensorflow:loss = 0.017690476, step = 5901 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.618\n",
            "INFO:tensorflow:loss = 0.28340966, step = 6001 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.034\n",
            "INFO:tensorflow:loss = 0.013341939, step = 6101 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.277\n",
            "INFO:tensorflow:loss = 0.10504508, step = 6201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.198\n",
            "INFO:tensorflow:loss = 0.062566295, step = 6301 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.086\n",
            "INFO:tensorflow:loss = 0.0056036506, step = 6401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.77\n",
            "INFO:tensorflow:loss = 0.07504946, step = 6501 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.38\n",
            "INFO:tensorflow:loss = 0.005280396, step = 6601 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.848\n",
            "INFO:tensorflow:loss = 0.00981817, step = 6701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.236\n",
            "INFO:tensorflow:loss = 0.003980291, step = 6801 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 608.696\n",
            "INFO:tensorflow:loss = 0.017036892, step = 6901 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.999\n",
            "INFO:tensorflow:loss = 0.018073354, step = 7001 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 591.183\n",
            "INFO:tensorflow:loss = 0.0122930985, step = 7101 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 610.644\n",
            "INFO:tensorflow:loss = 0.055136215, step = 7201 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.819\n",
            "INFO:tensorflow:loss = 0.0060952306, step = 7301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.709\n",
            "INFO:tensorflow:loss = 0.0097297905, step = 7401 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.709\n",
            "INFO:tensorflow:loss = 0.0276623, step = 7501 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.633\n",
            "INFO:tensorflow:loss = 0.004862195, step = 7601 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.192\n",
            "INFO:tensorflow:loss = 0.018453997, step = 7701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.227\n",
            "INFO:tensorflow:loss = 0.0058880793, step = 7801 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.436\n",
            "INFO:tensorflow:loss = 0.011233523, step = 7901 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.077\n",
            "INFO:tensorflow:loss = 0.006110885, step = 8001 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.776\n",
            "INFO:tensorflow:loss = 0.005351289, step = 8101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.86\n",
            "INFO:tensorflow:loss = 0.04063575, step = 8201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.431\n",
            "INFO:tensorflow:loss = 0.013887329, step = 8301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 578.639\n",
            "INFO:tensorflow:loss = 0.0048933555, step = 8401 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.909\n",
            "INFO:tensorflow:loss = 0.039077505, step = 8501 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.589\n",
            "INFO:tensorflow:loss = 0.033490963, step = 8601 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.953\n",
            "INFO:tensorflow:loss = 0.005728643, step = 8701 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.104\n",
            "INFO:tensorflow:loss = 0.025644, step = 8801 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 606.685\n",
            "INFO:tensorflow:loss = 0.0071065677, step = 8901 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 607.513\n",
            "INFO:tensorflow:loss = 0.017472178, step = 9001 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.72\n",
            "INFO:tensorflow:loss = 0.005138441, step = 9101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.475\n",
            "INFO:tensorflow:loss = 0.0059129805, step = 9201 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.556\n",
            "INFO:tensorflow:loss = 0.0055363956, step = 9301 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.623\n",
            "INFO:tensorflow:loss = 0.025670713, step = 9401 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.127\n",
            "INFO:tensorflow:loss = 0.0060767494, step = 9501 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 559.976\n",
            "INFO:tensorflow:loss = 0.022341691, step = 9601 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.58\n",
            "INFO:tensorflow:loss = 0.013737937, step = 9701 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.786\n",
            "INFO:tensorflow:loss = 0.007276731, step = 9801 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.657\n",
            "INFO:tensorflow:loss = 0.0047039413, step = 9901 (0.162 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.006454532.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 80.16418117461276\n",
            "Just using average = 600.313638601674 has RMSE of 98.09899836406518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e61036a-2643-44da-9deb-4f191076a038",
        "id": "Ld6baV60hPOs"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d74c3ad0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.53202736 0.44927192 0.5047906  0.59436613 0.467834   0.47121316\n",
            " 0.48251694 0.5731366  0.5438848  0.53789884 0.5975057  0.5458243\n",
            " 0.510034   0.43512762 0.49221745 0.53598803 0.5023608  0.55658\n",
            " 0.47223178 0.44824737 0.47996423 0.5489555  0.45635927 0.59668696\n",
            " 0.503149   0.52206355 0.5676175  0.40022203 0.554863   0.55872226\n",
            " 0.4613889  0.5138441  0.49740174 0.51578164 0.51450515 0.5533862\n",
            " 0.54902065 0.531412   0.45461357 0.566634   0.4943463  0.46970457\n",
            " 0.5407769  0.56227213 0.5535639  0.5587907  0.5188254  0.5568911\n",
            " 0.53676575 0.5904612  0.5449157  0.5839668  0.54432863 0.5513478\n",
            " 0.50105643 0.4920383  0.4117169  0.5811577  0.52079606 0.5106035\n",
            " 0.54925734 0.5553905  0.5509704  0.63057345 0.54948807 0.5078773\n",
            " 0.5663043  0.5840157  0.51808244 0.48944616 0.46527946 0.50407535\n",
            " 0.5640226  0.5703889  0.55228263 0.5783069  0.45676404 0.501073\n",
            " 0.5144739  0.5235623  0.5108188  0.55908084 0.5510254  0.52196366\n",
            " 0.5790218  0.5383944  0.52452874 0.58626074 0.5585373  0.53522164\n",
            " 0.5163     0.5578246  0.5817858  0.53732264 0.50551385 0.49766886\n",
            " 0.49428177 0.5852184  0.46730334 0.5676146  0.5683825  0.5304043\n",
            " 0.529737   0.5397367  0.55404276 0.43214175 0.5220579  0.59160984\n",
            " 0.5100481  0.491285   0.5088481  0.4035671  0.5612557  0.4217489\n",
            " 0.5756055  0.58140063 0.41985184 0.51526606 0.49738407 0.45022863\n",
            " 0.56872404 0.5469327  0.44959766 0.4899238  0.4433184  0.47862324\n",
            " 0.5519103  0.52941155 0.6055805  0.5949529  0.47021744 0.48083094\n",
            " 0.50056976 0.40758225 0.51109135 0.47730312 0.5368049  0.5867716\n",
            " 0.5050797  0.4602802  0.5360246  0.51626414 0.5029271  0.5033591\n",
            " 0.5340866  0.5509582  0.53954476 0.5653935  0.4953553  0.60484743\n",
            " 0.5441249  0.54655147 0.52734625 0.5469829  0.5132987  0.51766795\n",
            " 0.57732934 0.44100806 0.42327917 0.5316265  0.5243982  0.53490084\n",
            " 0.54846734 0.51683384 0.53140366 0.50039756 0.4440883  0.53898525\n",
            " 0.5436921  0.57502586 0.50682515 0.53353673 0.52741617 0.50066483\n",
            " 0.55858195 0.5901982  0.5430467  0.50767404 0.4174781  0.4912983\n",
            " 0.52638817 0.5232987  0.5460271  0.5588871  0.5331253  0.5015866\n",
            " 0.57641274 0.5116334  0.50100154 0.44767585 0.55979913 0.51182073\n",
            " 0.46238863 0.55296737 0.52909386 0.49434936 0.57935536 0.58869994\n",
            " 0.5552089  0.51434886 0.5067879  0.44929558 0.56656873 0.52983063\n",
            " 0.47922075 0.42128086 0.5688689  0.6233462  0.58939445 0.54348844\n",
            " 0.5252488  0.50503516 0.52340955 0.5313409  0.50149614 0.5438394\n",
            " 0.51408523 0.56601614 0.5186354  0.5888134  0.5646352  0.49076352\n",
            " 0.61717135 0.5851604  0.5617942  0.50987095 0.5465479  0.5385433\n",
            " 0.5443141  0.56663936 0.5489406  0.57152957 0.5144372  0.5447131\n",
            " 0.4505299  0.43235308 0.53528625 0.5223573  0.41649494 0.5922545\n",
            " 0.52709514 0.5168737  0.52824354 0.5519349  0.49431178 0.5080007\n",
            " 0.5227575  0.50095224 0.6130937  0.5265398  0.55085355 0.45006588\n",
            " 0.480337   0.5695312  0.5909596  0.48748907 0.5410354  0.5285859\n",
            " 0.5080293  0.51909536 0.6009113  0.578871   0.5824248  0.5443265\n",
            " 0.5703285  0.46428984 0.5415788  0.4852995  0.54312104 0.49811873\n",
            " 0.5148784  0.58782166 0.4436315  0.5114467  0.5146559  0.5169172\n",
            " 0.58985704 0.5203515  0.49547645 0.45654956 0.53834784 0.5416932\n",
            " 0.589165   0.44645563 0.52837735 0.4716594  0.5158077  0.5407718\n",
            " 0.58751434 0.5100596  0.4657824  0.47278944 0.5209266  0.5397767\n",
            " 0.42709798 0.46791026 0.50451654 0.45726547 0.50531775 0.5170383\n",
            " 0.5467403  0.6165226  0.50372386 0.52078855 0.5626635  0.5558185\n",
            " 0.5461166  0.56670856 0.5418056  0.46792746 0.53217447 0.4711355\n",
            " 0.5146124  0.5216408  0.5867705  0.56612784 0.529477   0.5439985\n",
            " 0.5217315  0.45203426 0.5435985  0.5878206  0.53417486 0.4104229\n",
            " 0.5486552  0.54619473 0.51917624 0.55234337 0.58748245 0.5205375\n",
            " 0.53254426 0.5016256  0.5339378  0.5380628  0.58053863 0.5502069\n",
            " 0.43489292 0.542307   0.46717823 0.548361   0.44473577 0.4444633\n",
            " 0.54277235 0.5928584  0.52783275 0.52607363 0.54189706 0.51925296\n",
            " 0.5296242  0.5910477  0.508836   0.41752836 0.52277136 0.47057772\n",
            " 0.46550882 0.50823444 0.5421168  0.42944348 0.525993   0.51559937\n",
            " 0.5339171  0.5242814  0.5251479  0.5374417  0.57712567 0.5376609\n",
            " 0.597488   0.56848395 0.5540504  0.5522205  0.4426577  0.5778712\n",
            " 0.4888889  0.42446253 0.46354017 0.5403076  0.5041257  0.5071787\n",
            " 0.51202863 0.5726947  0.557277   0.5046815  0.5182916  0.5427348\n",
            " 0.4499716  0.41894624 0.52531433 0.50510275 0.56234044 0.5900654\n",
            " 0.57979864 0.54431456 0.61188745 0.5196907  0.60809344 0.5559143\n",
            " 0.56512535 0.55117255 0.5728546  0.53983814 0.5566852  0.536947\n",
            " 0.6024168  0.48061582 0.40766457 0.5564067  0.43073094 0.525082\n",
            " 0.56094    0.5553248  0.5707145  0.5269794  0.55559283 0.5085522\n",
            " 0.49721816 0.5745772  0.5634121  0.56689906 0.5699117  0.51824003\n",
            " 0.5548388  0.5794028  0.43747792 0.41313416 0.5328451  0.58565813\n",
            " 0.5284502  0.44481984 0.54440814 0.4669933  0.5182626  0.53663075\n",
            " 0.589246   0.55143267 0.44510698 0.4112718  0.53856176 0.51269704\n",
            " 0.54745984 0.54649913 0.49850163 0.52168924 0.58415467 0.5167742\n",
            " 0.5441246  0.48141214 0.53659785 0.527717   0.4964769  0.5445193\n",
            " 0.5725296  0.4901075  0.4300557  0.5319858  0.5707842  0.5509294\n",
            " 0.5013583  0.5389968  0.47443712 0.50599456 0.5359619  0.55417824\n",
            " 0.53735054 0.51581097 0.57085013 0.5586953  0.5099933  0.60650814\n",
            " 0.5552439  0.46436888 0.53463316 0.59058505 0.5488543  0.50971663\n",
            " 0.5002439  0.502301   0.5718924  0.44544992 0.43630308 0.53984493\n",
            " 0.41864848 0.58429587 0.5590349  0.52161086 0.49566498 0.5702692\n",
            " 0.5400592  0.51637995 0.55188864 0.51367015 0.5538681  0.5157996\n",
            " 0.52252376 0.56164205 0.4043485  0.54755265 0.5864965  0.52755886\n",
            " 0.5311626  0.54647    0.50465876 0.5639047  0.52077496 0.5810905\n",
            " 0.50936043 0.54138494 0.50420904 0.55189466]\n",
            "[0.52713178 0.41774332 0.47717485 0.52799311 0.52799311 0.41429802\n",
            " 0.50559862 0.60034453 0.50559862 0.61584841 0.60034453 0.50732127\n",
            " 0.53660637 0.4039621  0.39018088 0.45391904 0.55297158 0.59259259\n",
            " 0.44444444 0.46942291 0.41085271 0.61068045 0.43496985 0.6873385\n",
            " 0.47717485 0.43755383 0.52971576 0.44530577 0.56330749 0.51851852\n",
            " 0.4788975  0.52799311 0.56330749 0.50043066 0.50990525 0.4952627\n",
            " 0.52885444 0.58914729 0.41602067 0.64082687 0.5211025  0.42635659\n",
            " 0.38845823 0.47028424 0.54952627 0.48148148 0.46770026 0.55900086\n",
            " 0.51593454 0.52196382 0.6287683  0.59431525 0.54091301 0.56416882\n",
            " 0.60809647 0.47200689 0.34453058 0.27648579 0.53919035 0.4754522\n",
            " 0.44530577 0.60465116 0.40568475 0.65202412 0.54091301 0.41257537\n",
            " 0.49956934 0.62015504 0.53660637 0.48406546 0.42721792 0.49267873\n",
            " 0.51937984 0.56330749 0.54866494 0.6089578  0.40740741 0.44358312\n",
            " 0.47803618 0.44702842 0.48148148 0.65977606 0.58139535 0.54091301\n",
            " 0.625323   0.49354005 0.6546081  0.52540913 0.49354005 0.58225668\n",
            " 0.53143842 0.53488372 0.57105943 0.4005168  0.54952627 0.56589147\n",
            " 0.49956934 0.57105943 0.4461671  0.63135228 0.59345392 0.46683893\n",
            " 0.72265289 0.51335056 0.5667528  0.42204996 0.54694229 0.58225668\n",
            " 0.54866494 0.47114556 0.41257537 0.41429802 0.59000861 0.35486649\n",
            " 0.5374677  0.56416882 0.48923342 0.49354005 0.44358312 0.40568475\n",
            " 0.55727821 0.55297158 0.4461671  0.40913006 0.4461671  0.4005168\n",
            " 0.51593454 0.41860465 0.5538329  0.45133506 0.47803618 0.49698536\n",
            " 0.58139535 0.37209302 0.55124892 0.50559862 0.63049096 0.65546942\n",
            " 0.42377261 0.43496985 0.2213609  0.30749354 0.53832903 0.48923342\n",
            " 0.60206718 0.46080965 0.55813953 0.63910422 0.42635659 0.55727821\n",
            " 0.43152455 0.59086994 0.49956934 0.56847545 0.54177433 0.44702842\n",
            " 0.6089578  0.36864772 0.42980189 0.59689922 0.52713178 0.54694229\n",
            " 0.60809647 0.56503015 0.374677   0.47717485 0.30060293 0.5667528\n",
            " 0.45047373 0.51076658 0.48320413 0.57795004 0.53143842 0.57019811\n",
            " 0.56503015 0.5538329  0.4754522  0.47200689 0.34969854 0.45736434\n",
            " 0.53229974 0.52196382 0.57881137 0.37726098 0.583118   0.40913006\n",
            " 0.57450474 0.44358312 0.43583118 0.36950904 0.5245478  0.42635659\n",
            " 0.4918174  0.49612403 0.54349699 0.49440138 0.6546081  0.63221361\n",
            " 0.5503876  0.49698536 0.50129199 0.50387597 0.62360034 0.47975883\n",
            " 0.48664944 0.43927649 0.60120586 0.66408269 0.62101637 0.53229974\n",
            " 0.58828596 0.52540913 0.48837209 0.52885444 0.59173127 0.4754522\n",
            " 0.52799311 0.5081826  0.39879414 0.55813953 0.65116279 0.44186047\n",
            " 0.52368648 0.59259259 0.65202412 0.49784668 0.4788975  0.51765719\n",
            " 0.53316107 0.66322136 0.583118   0.38415159 0.57881137 0.54952627\n",
            " 0.39018088 0.47372954 0.47631352 0.48664944 0.38501292 0.54608096\n",
            " 0.63479759 0.5538329  0.57536606 0.53316107 0.48923342 0.4461671\n",
            " 0.43583118 0.43669251 0.69853575 0.47459087 0.58570198 0.44358312\n",
            " 0.43669251 0.64857881 0.49612403 0.36864772 0.51507321 0.40913006\n",
            " 0.53660637 0.5503876  0.67355728 0.41343669 0.5667528  0.51593454\n",
            " 0.56761413 0.53229974 0.52627046 0.4918174  0.69422911 0.45564169\n",
            " 0.5796727  0.55727821 0.39276486 0.56072351 0.54177433 0.37037037\n",
            " 0.58053402 0.4461671  0.3910422  0.4332472  0.46770026 0.60292851\n",
            " 0.55813953 0.36175711 0.68130922 0.44875108 0.51421189 0.54866494\n",
            " 0.60809647 0.44272179 0.43927649 0.416882   0.54780362 0.47028424\n",
            " 0.43066322 0.33419466 0.52799311 0.41085271 0.55813953 0.55124892\n",
            " 0.5796727  0.64427218 0.41343669 0.57622739 0.54349699 0.51765719\n",
            " 0.50215332 0.65030146 0.63049096 0.50732127 0.59431525 0.44788975\n",
            " 0.50990525 0.51421189 0.59086994 0.68044789 0.51248923 0.61068045\n",
            " 0.54435831 0.45994832 0.57450474 0.58225668 0.49784668 0.36434109\n",
            " 0.49440138 0.48234281 0.52713178 0.53402239 0.51593454 0.50387597\n",
            " 0.56761413 0.53919035 0.22739018 0.57450474 0.38242894 0.68647717\n",
            " 0.40999139 0.52282515 0.40913006 0.58914729 0.4005168  0.40913006\n",
            " 0.43496985 0.66494401 0.57881137 0.54694229 0.54263566 0.51162791\n",
            " 0.43152455 0.67355728 0.32816537 0.39276486 0.60551249 0.41860465\n",
            " 0.51335056 0.4625323  0.53660637 0.41515935 0.56933678 0.53057709\n",
            " 0.44530577 0.50129199 0.43927649 0.61757106 0.65633075 0.49095607\n",
            " 0.62618432 0.48234281 0.37898363 0.68217054 0.40137812 0.58656331\n",
            " 0.47803618 0.35745047 0.4203273  0.61757106 0.54866494 0.4496124\n",
            " 0.4918174  0.6124031  0.34022394 0.4005168  0.40826873 0.50732127\n",
            " 0.49267873 0.3910422  0.52368648 0.4952627  0.59086994 0.4754522\n",
            " 0.57708872 0.64685616 0.66666667 0.44702842 0.62273902 0.57622739\n",
            " 0.46339363 0.4918174  0.6089578  0.50387597 0.52799311 0.54521964\n",
            " 0.60292851 0.54005168 0.41774332 0.63049096 0.44099914 0.4625323\n",
            " 0.6089578  0.42894057 0.58656331 0.59259259 0.49870801 0.51507321\n",
            " 0.44272179 0.58053402 0.53574505 0.59000861 0.59517657 0.42118863\n",
            " 0.60809647 0.45047373 0.39793282 0.31093885 0.49440138 0.56847545\n",
            " 0.48664944 0.47459087 0.47286822 0.45650301 0.55900086 0.53057709\n",
            " 0.54694229 0.50990525 0.33074935 0.46597761 0.45650301 0.51162791\n",
            " 0.52024117 0.51679587 0.49870801 0.55211025 0.45994832 0.57536606\n",
            " 0.60292851 0.49784668 0.50043066 0.47028424 0.58656331 0.63135228\n",
            " 0.69595177 0.52024117 0.39362618 0.56244617 0.40999139 0.62101637\n",
            " 0.4625323  0.49956934 0.48492679 0.4788975  0.59259259 0.51248923\n",
            " 0.45736434 0.56244617 0.67700258 0.5245478  0.51765719 0.64944014\n",
            " 0.52368648 0.43669251 0.60120586 0.65719208 0.56761413 0.56503015\n",
            " 0.50904393 0.34969854 0.48664944 0.39190353 0.416882   0.61929371\n",
            " 0.40568475 0.53143842 0.50387597 0.50215332 0.40826873 0.5538329\n",
            " 0.37898363 0.52885444 0.3712317  0.39965547 0.71576227 0.46511628\n",
            " 0.60723514 0.7329888  0.33850129 0.50215332 0.51507321 0.52024117\n",
            " 0.5245478  0.55641688 0.43496985 0.59345392 0.55469423 0.70542636\n",
            " 0.4496124  0.44444444 0.41343669 0.58225668]\n",
            "The trained model has an aproximate error rate of -15.144316069605782 which equates to -3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "yT91LA7Xx77W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30083320-01b5-4970-f43c-80a2c4520fe5",
        "id": "zQAH_kVzyAOD"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp_dnn = df.drop(columns=['temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] != 2012]\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] < 2020]\n",
        "cols = df_dewp_dnn['NUM_COLLISIONS']\n",
        "df_dewp_dnn = df_dewp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp_dnn[:6])\n",
        "df_dewp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "7b0f9c95-60ae-48b0-ebd5-204d51fc9639",
        "id": "_qzGppAwyAOE"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  24.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  35.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  21.2    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29  36.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  32.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  44.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         dewp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2015.999217    15.723679    44.163170     0.082192     0.084932   \n",
              "std       2.000000     8.801271    16.995303     0.274710     0.278834   \n",
              "min    2013.000000     1.000000    -6.700000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    32.150000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    45.300000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    58.500000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    74.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084932     0.077104     0.084932     0.084540     0.082192  ...   \n",
              "std       0.278834     0.266808     0.278834     0.278251     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142857      599.109980  \n",
              "std       0.349996     0.349996     0.349996     0.349996      100.277185  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-699be7ba-a362-420e-824c-26f56b16155b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.999217</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-699be7ba-a362-420e-824c-26f56b16155b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-699be7ba-a362-420e-824c-26f56b16155b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-699be7ba-a362-420e-824c-26f56b16155b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp_dnn.iloc[np.random.permutation(len(df_dewp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557f5b40-0369-4d7d-ced8-47416957dcc9",
        "id": "z5-eCTxByAOE"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "2514  2013  25  45.3    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2622  2019  16  58.5    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "1104  2017  22  44.5    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "3107  2017  25  47.2    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "2589  2016   9  68.3    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2518  2018  30  53.5    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2514    1    0    0    0    0    1    0  \n",
            "2622    1    0    0    1    0    0    0  \n",
            "1104    0    0    0    0    0    0    0  \n",
            "3107    0    0    0    0    0    0    0  \n",
            "2589    1    0    0    0    1    0    0  \n",
            "2518    1    0    1    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0fa543-6aee-4321-d5f1-59393a7f2b4e",
        "id": "BbrOrPfQyAOE"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2514    539\n",
            "2622    620\n",
            "1104    551\n",
            "3107    456\n",
            "2589    715\n",
            "2518    510\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac52824e-d37a-4166-fe69-06a867c50e00",
        "id": "T2v7BylMyAOE"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a041886c-ab8e-429c-d22b-959beb1128f2",
        "id": "HMGFsHtkyAOE"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77d7a19750>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 32075.318, step = 1\n",
            "INFO:tensorflow:global_step/sec: 469.223\n",
            "INFO:tensorflow:loss = 0.040787622, step = 101 (0.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.594\n",
            "INFO:tensorflow:loss = 0.055134676, step = 201 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.53\n",
            "INFO:tensorflow:loss = 0.033296518, step = 301 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.356\n",
            "INFO:tensorflow:loss = 0.036332756, step = 401 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.563\n",
            "INFO:tensorflow:loss = 0.027359705, step = 501 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.818\n",
            "INFO:tensorflow:loss = 0.024996342, step = 601 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 578.209\n",
            "INFO:tensorflow:loss = 0.020741882, step = 701 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 613.984\n",
            "INFO:tensorflow:loss = 0.020266369, step = 801 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.201\n",
            "INFO:tensorflow:loss = 0.014728111, step = 901 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.504\n",
            "INFO:tensorflow:loss = 0.011000979, step = 1001 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.822\n",
            "INFO:tensorflow:loss = 0.010330549, step = 1101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 619.067\n",
            "INFO:tensorflow:loss = 0.011877521, step = 1201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.533\n",
            "INFO:tensorflow:loss = 0.0076593366, step = 1301 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.464\n",
            "INFO:tensorflow:loss = 0.009670612, step = 1401 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.255\n",
            "INFO:tensorflow:loss = 0.005548408, step = 1501 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.572\n",
            "INFO:tensorflow:loss = 0.005733031, step = 1601 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.966\n",
            "INFO:tensorflow:loss = 0.0055962973, step = 1701 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 657.924\n",
            "INFO:tensorflow:loss = 0.007192395, step = 1801 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.379\n",
            "INFO:tensorflow:loss = 0.0066427235, step = 1901 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.801\n",
            "INFO:tensorflow:loss = 0.005484574, step = 2001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.324\n",
            "INFO:tensorflow:loss = 0.005296728, step = 2101 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.182\n",
            "INFO:tensorflow:loss = 0.0070549888, step = 2201 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 599.585\n",
            "INFO:tensorflow:loss = 0.0055323644, step = 2301 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 613.375\n",
            "INFO:tensorflow:loss = 0.0065992507, step = 2401 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.283\n",
            "INFO:tensorflow:loss = 0.0050623813, step = 2501 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.626\n",
            "INFO:tensorflow:loss = 0.008972229, step = 2601 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.282\n",
            "INFO:tensorflow:loss = 0.0062000453, step = 2701 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.292\n",
            "INFO:tensorflow:loss = 0.0063880826, step = 2801 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.261\n",
            "INFO:tensorflow:loss = 0.006927251, step = 2901 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.246\n",
            "INFO:tensorflow:loss = 0.00596582, step = 3001 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.626\n",
            "INFO:tensorflow:loss = 0.0053262324, step = 3101 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.499\n",
            "INFO:tensorflow:loss = 0.005158165, step = 3201 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 624.033\n",
            "INFO:tensorflow:loss = 0.0054272898, step = 3301 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 648.745\n",
            "INFO:tensorflow:loss = 0.009239179, step = 3401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 580.242\n",
            "INFO:tensorflow:loss = 0.0053291162, step = 3501 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.27\n",
            "INFO:tensorflow:loss = 0.0049437317, step = 3601 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.99\n",
            "INFO:tensorflow:loss = 0.006333356, step = 3701 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.914\n",
            "INFO:tensorflow:loss = 0.0068400456, step = 3801 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.029\n",
            "INFO:tensorflow:loss = 0.01000993, step = 3901 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 602.978\n",
            "INFO:tensorflow:loss = 0.0041971854, step = 4001 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.072\n",
            "INFO:tensorflow:loss = 0.0050461707, step = 4101 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.037\n",
            "INFO:tensorflow:loss = 0.00720177, step = 4201 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.809\n",
            "INFO:tensorflow:loss = 0.007989469, step = 4301 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.203\n",
            "INFO:tensorflow:loss = 0.00360944, step = 4401 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.493\n",
            "INFO:tensorflow:loss = 0.018868871, step = 4501 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 625.288\n",
            "INFO:tensorflow:loss = 0.006270036, step = 4601 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 641.648\n",
            "INFO:tensorflow:loss = 0.009016797, step = 4701 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.445\n",
            "INFO:tensorflow:loss = 0.053880423, step = 4801 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.374\n",
            "INFO:tensorflow:loss = 0.08884475, step = 4901 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.99\n",
            "INFO:tensorflow:loss = 0.573097, step = 5001 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.001\n",
            "INFO:tensorflow:loss = 0.005907057, step = 5101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.118\n",
            "INFO:tensorflow:loss = 0.07632114, step = 5201 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 603.692\n",
            "INFO:tensorflow:loss = 0.0035867323, step = 5301 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 610.2\n",
            "INFO:tensorflow:loss = 0.011930517, step = 5401 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.764\n",
            "INFO:tensorflow:loss = 0.3432827, step = 5501 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.012\n",
            "INFO:tensorflow:loss = 0.08032772, step = 5601 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.479\n",
            "INFO:tensorflow:loss = 0.058195524, step = 5701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.14\n",
            "INFO:tensorflow:loss = 0.012242008, step = 5801 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.756\n",
            "INFO:tensorflow:loss = 0.06432434, step = 5901 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 580.548\n",
            "INFO:tensorflow:loss = 0.63912785, step = 6001 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.899\n",
            "INFO:tensorflow:loss = 0.087049186, step = 6101 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.757\n",
            "INFO:tensorflow:loss = 0.016231002, step = 6201 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 602.615\n",
            "INFO:tensorflow:loss = 0.006539918, step = 6301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.065\n",
            "INFO:tensorflow:loss = 0.07568617, step = 6401 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.424\n",
            "INFO:tensorflow:loss = 0.004545114, step = 6501 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.996\n",
            "INFO:tensorflow:loss = 0.029137308, step = 6601 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.849\n",
            "INFO:tensorflow:loss = 0.005344177, step = 6701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 659.493\n",
            "INFO:tensorflow:loss = 0.004327202, step = 6801 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.534\n",
            "INFO:tensorflow:loss = 0.0064536873, step = 6901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.299\n",
            "INFO:tensorflow:loss = 0.01621613, step = 7001 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 654.32\n",
            "INFO:tensorflow:loss = 0.03077675, step = 7101 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.356\n",
            "INFO:tensorflow:loss = 0.028983053, step = 7201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.059\n",
            "INFO:tensorflow:loss = 0.091519274, step = 7301 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 592.28\n",
            "INFO:tensorflow:loss = 0.115641735, step = 7401 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 638.951\n",
            "INFO:tensorflow:loss = 0.0055533918, step = 7501 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 637.733\n",
            "INFO:tensorflow:loss = 0.0936002, step = 7601 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 644.279\n",
            "INFO:tensorflow:loss = 0.0057142517, step = 7701 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.184\n",
            "INFO:tensorflow:loss = 0.0037425803, step = 7801 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.245\n",
            "INFO:tensorflow:loss = 0.005373283, step = 7901 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 587.235\n",
            "INFO:tensorflow:loss = 0.09522489, step = 8001 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 636.924\n",
            "INFO:tensorflow:loss = 0.0060152076, step = 8101 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 622.065\n",
            "INFO:tensorflow:loss = 0.007993734, step = 8201 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.056\n",
            "INFO:tensorflow:loss = 0.025870837, step = 8301 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 663.828\n",
            "INFO:tensorflow:loss = 0.03656706, step = 8401 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.897\n",
            "INFO:tensorflow:loss = 0.008889465, step = 8501 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 610.879\n",
            "INFO:tensorflow:loss = 0.012862734, step = 8601 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.709\n",
            "INFO:tensorflow:loss = 0.02912217, step = 8701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.962\n",
            "INFO:tensorflow:loss = 0.0063540824, step = 8801 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.921\n",
            "INFO:tensorflow:loss = 0.023806615, step = 8901 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.088\n",
            "INFO:tensorflow:loss = 0.04933068, step = 9001 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 607.473\n",
            "INFO:tensorflow:loss = 0.010015899, step = 9101 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 642.795\n",
            "INFO:tensorflow:loss = 0.0074788565, step = 9201 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 530.634\n",
            "INFO:tensorflow:loss = 0.0065590604, step = 9301 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.505\n",
            "INFO:tensorflow:loss = 0.0042537544, step = 9401 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.198\n",
            "INFO:tensorflow:loss = 0.018284632, step = 9501 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.339\n",
            "INFO:tensorflow:loss = 0.010298615, step = 9601 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 626.301\n",
            "INFO:tensorflow:loss = 0.032650333, step = 9701 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.752\n",
            "INFO:tensorflow:loss = 0.010187378, step = 9801 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 606.072\n",
            "INFO:tensorflow:loss = 0.018897496, step = 9901 (0.166 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0054294984.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 81.82060903868319\n",
            "Just using average = 600.6712328767123 has RMSE of 99.44895988564187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbcc9984-29fd-4aaf-dcf5-936d907839ea",
        "id": "NCz6izVhyAOF"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77e6cf6dd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4684564  0.44927323 0.47069657 0.46099484 0.48245823 0.40897667\n",
            " 0.50621045 0.46217644 0.4745742  0.4928733  0.48814213 0.5281583\n",
            " 0.5031606  0.5651351  0.48149502 0.5063163  0.5367156  0.49897206\n",
            " 0.39752305 0.5558244  0.4252435  0.48009408 0.52517235 0.5220634\n",
            " 0.5132257  0.5206796  0.52939045 0.49740136 0.41314328 0.47809517\n",
            " 0.45336258 0.4983865  0.545889   0.5651723  0.5377675  0.5540229\n",
            " 0.5008346  0.45196736 0.5725223  0.49669373 0.51260865 0.49270642\n",
            " 0.4686042  0.4556123  0.42840683 0.53765786 0.5647088  0.4490615\n",
            " 0.47431767 0.4909488  0.53751004 0.4324571  0.53868973 0.4546758\n",
            " 0.41799843 0.5105077  0.36416352 0.39913857 0.5099584  0.45390332\n",
            " 0.51978505 0.4897548  0.4223863  0.49297917 0.5541154  0.40536702\n",
            " 0.46667016 0.45212662 0.47995102 0.45412457 0.5657712  0.53416836\n",
            " 0.37718785 0.55272305 0.36525452 0.3874904  0.4446317  0.4991504\n",
            " 0.5447093  0.51048577 0.53260624 0.51708233 0.4323951  0.4336568\n",
            " 0.52571213 0.44959462 0.41206563 0.5014945  0.54465973 0.538087\n",
            " 0.44560158 0.4558593  0.46367943 0.46074212 0.468835   0.51813424\n",
            " 0.517522   0.53308594 0.4385482  0.39976323 0.48738587 0.5045959\n",
            " 0.47884667 0.48438656 0.47326577 0.49341786 0.39842522 0.548483\n",
            " 0.5159217  0.54444516 0.5287467  0.47974312 0.5203153  0.51205266\n",
            " 0.50413144 0.45474637 0.47173798 0.39157784 0.3968593  0.5084783\n",
            " 0.5324422  0.4288522  0.5479833  0.47308457 0.51616585 0.53228104\n",
            " 0.44293702 0.5149261  0.57842743 0.48029435 0.5225812  0.5736753\n",
            " 0.5217248  0.53168976 0.48595917 0.42898667 0.4794885  0.4092542\n",
            " 0.47665417 0.510007   0.53064835 0.52060425 0.37878525 0.51928437\n",
            " 0.52059376 0.5189544  0.57774746 0.5174161  0.4112903  0.5071746\n",
            " 0.40994275 0.49395287 0.5219966  0.5095378  0.37592518 0.493909\n",
            " 0.4509269  0.5182296  0.4770099  0.3552171  0.4163686  0.55347836\n",
            " 0.47360814 0.41078866 0.5419332  0.4945203  0.45284474 0.57557595\n",
            " 0.5287391  0.4901048  0.36957276 0.44936    0.46531212 0.3832122\n",
            " 0.5095264  0.4188739  0.4988948  0.5744935  0.50273716 0.49483216\n",
            " 0.5376903  0.5321399  0.49895394 0.5164987  0.55415833 0.4558164\n",
            " 0.38034356 0.5416595  0.47548497 0.4526683  0.482072   0.3934375\n",
            " 0.33969033 0.46938336 0.49071324 0.5070106  0.5526067  0.39609253\n",
            " 0.5019914  0.49196827 0.44014275 0.5291177  0.46761334 0.53159153\n",
            " 0.44423878 0.5475503  0.5325805  0.47674763 0.5069858  0.5597478\n",
            " 0.4509765  0.4174558  0.44191945 0.46525395 0.5018741  0.47690403\n",
            " 0.5635015  0.48837674 0.51880276 0.5106145  0.474455   0.5133792\n",
            " 0.51033604 0.51369774 0.41784775 0.4160396  0.43530476 0.522689\n",
            " 0.40061104 0.50462735 0.52410424 0.5200912  0.40975296 0.47208226\n",
            " 0.4483472  0.56665623 0.57409394 0.4790107  0.49247944 0.47112668\n",
            " 0.5423604  0.37452233 0.41188633 0.4670068  0.47629464 0.40859997\n",
            " 0.45988858 0.5404483  0.46417916 0.42130387 0.53045094 0.49924958\n",
            " 0.44293702 0.36873543 0.50599205 0.5872899  0.42781174 0.52687275\n",
            " 0.4676     0.5105859  0.5135499  0.4587375  0.4562255  0.53647625\n",
            " 0.41238225 0.48884213 0.47285283 0.47678292 0.54181015 0.5085641\n",
            " 0.5659715  0.45188534 0.4704038  0.5278865  0.50680363 0.55152714\n",
            " 0.480075   0.50119793 0.54886544 0.41249    0.53130066 0.43859208\n",
            " 0.45475018 0.5037118  0.51428235 0.456051   0.5924636  0.5486165\n",
            " 0.39271557 0.5432712  0.54110444 0.534484   0.38011944 0.54062665\n",
            " 0.5562545  0.46957314 0.53304493 0.45813382 0.5299636  0.5142089\n",
            " 0.4844705  0.55268013 0.5074464  0.40859902 0.48702538 0.53052723\n",
            " 0.47650254 0.5381824  0.4820205  0.47983086 0.52579415 0.50965226\n",
            " 0.54026616 0.39794838 0.47345364 0.5589982  0.36028683 0.37583077\n",
            " 0.46261132 0.5243503  0.5397159  0.5391847  0.50742066 0.4760362\n",
            " 0.49527752 0.488227   0.47802556 0.5797416  0.5063039  0.46488392\n",
            " 0.5063039  0.5260688  0.49924004 0.48398125 0.46620762 0.43947804\n",
            " 0.4072925  0.54479706 0.5009576  0.5081998  0.55465424 0.4914323\n",
            " 0.50201523 0.47904027 0.48666012 0.54563916 0.5059968  0.48794472\n",
            " 0.53819954 0.50239384 0.55895054 0.51454747 0.42208493 0.44520676\n",
            " 0.407632   0.45371068 0.51791584 0.5529882  0.5376121  0.48844063\n",
            " 0.45112908 0.5227624  0.51947796 0.46171868 0.4985658  0.44926274\n",
            " 0.4701196  0.51714146 0.5962783  0.53436005 0.4899274  0.43596375\n",
            " 0.5094892  0.44914162 0.53345215 0.5490247  0.5029489  0.51132023\n",
            " 0.49149144 0.4523517  0.5419656  0.47786915 0.5676471  0.51419556\n",
            " 0.48162568 0.4065801  0.51886    0.5126306  0.48615086 0.47810566\n",
            " 0.53709424 0.5293838  0.4991752  0.54289067 0.3763038  0.42392933\n",
            " 0.4791137  0.5325767  0.4759084  0.4829818  0.48107922 0.3709594\n",
            " 0.5555717  0.4310304  0.51486504 0.47289097 0.50551426 0.5451566\n",
            " 0.38420212 0.59184754 0.5459901  0.5013906  0.5363666  0.4802295\n",
            " 0.43051636 0.36979496 0.39440072 0.40427792 0.50574696 0.5169555\n",
            " 0.5259515  0.48822224 0.43473923 0.47174752 0.5249444  0.5014049\n",
            " 0.5637504  0.46781933 0.57743657 0.52936375 0.54882824 0.5331203\n",
            " 0.4673282  0.48559391 0.55115044 0.5216199  0.48602116 0.51179993\n",
            " 0.5276309  0.3738767  0.51261914 0.4468547  0.5044414  0.5534879\n",
            " 0.46704304 0.4828502  0.51019204 0.4579631  0.45284283 0.45026886\n",
            " 0.5115119  0.50523484 0.5529891  0.5202199  0.43460858 0.50728047\n",
            " 0.500044   0.33752358 0.41412842 0.42440426 0.50378335 0.44261277\n",
            " 0.4226514  0.50209534 0.4905101  0.517686   0.47165787 0.5586301\n",
            " 0.4224168  0.573187   0.38204873 0.5075942  0.48098576 0.48912728\n",
            " 0.5554162  0.5010215  0.41528428 0.50354874 0.3698312  0.46635735\n",
            " 0.53893864 0.48072827 0.4065801  0.55832684 0.5283166  0.49119198\n",
            " 0.5315629  0.48957932 0.4694749  0.50916874 0.5074235  0.54833424\n",
            " 0.5011779  0.44803822 0.41190922 0.47969067 0.41630375 0.4820491\n",
            " 0.47205937 0.54170716 0.4071952  0.35670197 0.49291146 0.49855435\n",
            " 0.53767216 0.36002362 0.5461036  0.5208341  0.53854764 0.43385422\n",
            " 0.51433957]\n",
            "[0.4461671  0.40826873 0.43927649 0.46511628 0.46942291 0.42635659\n",
            " 0.52540913 0.34969854 0.5503876  0.47631352 0.54435831 0.51937984\n",
            " 0.52885444 0.57708872 0.54091301 0.60206718 0.59862188 0.44530577\n",
            " 0.45994832 0.59086994 0.41343669 0.56244617 0.58225668 0.53057709\n",
            " 0.61929371 0.60120586 0.59431525 0.5374677  0.44099914 0.49698536\n",
            " 0.51765719 0.48062016 0.53919035 0.58139535 0.57622739 0.5538329\n",
            " 0.36864772 0.55900086 0.59086994 0.5081826  0.56158484 0.55211025\n",
            " 0.48406546 0.5374677  0.47803618 0.59173127 0.55813953 0.52024117\n",
            " 0.4625323  0.48320413 0.54608096 0.34711456 0.58742463 0.45478036\n",
            " 0.43066322 0.61498708 0.45822567 0.40482343 0.52799311 0.46511628\n",
            " 0.50215332 0.53919035 0.36520241 0.50990525 0.57622739 0.36175711\n",
            " 0.4203273  0.56933678 0.50904393 0.58656331 0.7329888  0.54091301\n",
            " 0.43066322 0.74677003 0.39965547 0.43238587 0.49956934 0.45650301\n",
            " 0.50387597 0.60637382 0.51765719 0.50559862 0.45736434 0.54521964\n",
            " 0.59000861 0.54005168 0.47286822 0.74677003 0.64082687 0.60120586\n",
            " 0.60551249 0.54866494 0.50043066 0.44702842 0.44444444 0.55641688\n",
            " 0.51162791 0.59776055 0.47803618 0.51593454 0.5503876  0.54263566\n",
            " 0.46511628 0.53660637 0.55986219 0.54091301 0.44099914 0.60551249\n",
            " 0.52196382 0.54435831 0.52368648 0.4005168  0.62962963 0.55727821\n",
            " 0.55211025 0.57364341 0.4754522  0.38070629 0.42635659 0.61412575\n",
            " 0.60292851 0.50732127 0.56330749 0.54349699 0.34022394 0.51593454\n",
            " 0.48148148 0.62015504 0.61068045 0.5081826  0.44358312 0.55900086\n",
            " 0.50043066 0.59086994 0.48234281 0.44358312 0.40568475 0.41774332\n",
            " 0.47631352 0.51593454 0.58225668 0.42291128 0.36692506 0.60723514\n",
            " 0.55297158 0.62790698 0.56072351 0.58570198 0.45478036 0.59345392\n",
            " 0.36003445 0.58225668 0.48751077 0.71576227 0.32127476 0.64513351\n",
            " 0.44358312 0.54177433 0.54177433 0.42549526 0.50387597 0.60551249\n",
            " 0.43927649 0.40826873 0.6546081  0.50215332 0.53660637 0.61154177\n",
            " 0.53660637 0.68130922 0.35486649 0.41085271 0.55211025 0.42291128\n",
            " 0.49009475 0.4625323  0.51765719 0.56847545 0.48234281 0.53660637\n",
            " 0.6124031  0.56330749 0.60034453 0.31955211 0.56416882 0.51162791\n",
            " 0.40568475 0.5994832  0.47028424 0.57450474 0.56416882 0.39362618\n",
            " 0.26614987 0.57536606 0.58225668 0.51335056 0.61584841 0.36950904\n",
            " 0.5503876  0.52799311 0.34280792 0.6089578  0.50387597 0.54866494\n",
            " 0.48837209 0.49870801 0.53574505 0.54177433 0.63910422 0.57708872\n",
            " 0.60465116 0.52799311 0.42291128 0.40913006 0.47631352 0.51937984\n",
            " 0.63479759 0.46080965 0.583118   0.52196382 0.53143842 0.54866494\n",
            " 0.54263566 0.42204996 0.43496985 0.43410853 0.43927649 0.50732127\n",
            " 0.42377261 0.38156761 0.47028424 0.44702842 0.44099914 0.49698536\n",
            " 0.52713178 0.67355728 0.67355728 0.48664944 0.54263566 0.55900086\n",
            " 0.64254953 0.41085271 0.3712317  0.49870801 0.56072351 0.4461671\n",
            " 0.46942291 0.52885444 0.48837209 0.34797588 0.53057709 0.46683893\n",
            " 0.52024117 0.27562446 0.54521964 0.59689922 0.416882   0.5667528\n",
            " 0.51851852 0.53402239 0.56330749 0.45391904 0.41946598 0.5081826\n",
            " 0.46942291 0.56072351 0.46597761 0.47286822 0.72437554 0.33850129\n",
            " 0.65891473 0.44444444 0.52799311 0.56933678 0.62187769 0.55641688\n",
            " 0.54952627 0.52799311 0.57105943 0.43152455 0.4952627  0.42635659\n",
            " 0.49009475 0.47631352 0.54263566 0.41343669 0.63049096 0.59086994\n",
            " 0.41429802 0.59259259 0.62187769 0.56244617 0.38587425 0.59431525\n",
            " 0.69939707 0.44186047 0.50215332 0.51421189 0.63738157 0.50732127\n",
            " 0.54694229 0.47286822 0.49440138 0.49354005 0.62962963 0.64341085\n",
            " 0.41085271 0.51851852 0.5503876  0.51248923 0.53402239 0.61584841\n",
            " 0.5796727  0.40999139 0.56072351 0.70887166 0.44444444 0.48923342\n",
            " 0.50129199 0.6089578  0.59086994 0.52799311 0.59173127 0.58053402\n",
            " 0.46770026 0.48406546 0.42291128 0.60292851 0.45305771 0.54177433\n",
            " 0.64254953 0.56589147 0.52971576 0.5994832  0.34022394 0.39190353\n",
            " 0.43238587 0.58570198 0.47286822 0.58656331 0.57450474 0.52282515\n",
            " 0.53229974 0.4039621  0.45736434 0.55297158 0.63393626 0.51679587\n",
            " 0.63996555 0.44788975 0.50904393 0.51248923 0.46080965 0.66925065\n",
            " 0.36606374 0.58397933 0.55727821 0.46597761 0.49095607 0.56933678\n",
            " 0.5374677  0.61068045 0.53488372 0.58828596 0.4918174  0.43669251\n",
            " 0.47286822 0.48406546 0.68303187 0.42463394 0.58828596 0.44530577\n",
            " 0.49095607 0.49956934 0.65030146 0.53143842 0.4203273  0.5503876\n",
            " 0.60637382 0.45822567 0.60292851 0.52368648 0.68561585 0.47631352\n",
            " 0.51248923 0.45908699 0.46597761 0.53316107 0.45736434 0.49095607\n",
            " 0.49956934 0.50129199 0.54349699 0.69250646 0.36778639 0.26873385\n",
            " 0.47372954 0.55211025 0.48406546 0.53660637 0.54177433 0.41774332\n",
            " 0.41343669 0.46942291 0.50215332 0.45305771 0.58139535 0.55986219\n",
            " 0.43066322 0.66322136 0.63996555 0.51248923 0.62704565 0.42118863\n",
            " 0.50990525 0.32385874 0.43066322 0.4332472  0.52368648 0.53660637\n",
            " 0.4952627  0.44530577 0.48492679 0.44444444 0.49612403 0.49956934\n",
            " 0.60034453 0.53919035 0.63393626 0.47372954 0.62360034 0.49612403\n",
            " 0.50732127 0.53832903 0.45133506 0.50215332 0.4461671  0.43152455\n",
            " 0.50732127 0.36950904 0.36175711 0.45047373 0.41429802 0.61326443\n",
            " 0.49354005 0.39793282 0.47631352 0.16192937 0.68217054 0.34366925\n",
            " 0.54694229 0.35658915 0.50904393 0.6416882  0.42807924 0.53229974\n",
            " 0.53574505 0.40913006 0.36089578 0.45650301 0.4754522  0.53057709\n",
            " 0.46167097 0.4496124  0.58053402 0.44358312 0.46425495 0.59776055\n",
            " 0.41257537 0.56847545 0.39534884 0.5503876  0.52627046 0.48751077\n",
            " 0.57105943 0.52627046 0.42807924 0.51162791 0.33850129 0.63221361\n",
            " 0.5796727  0.45305771 0.38587425 0.69939707 0.40568475 0.43669251\n",
            " 0.53488372 0.5047373  0.42894057 0.50732127 0.51937984 0.65288544\n",
            " 0.52368648 0.49612403 0.44530577 0.52540913 0.40568475 0.56072351\n",
            " 0.46339363 0.45478036 0.47717485 0.38329027 0.22739018 0.63049096\n",
            " 0.68217054 0.31093885 0.58139535 0.37295435 0.56330749 0.47717485\n",
            " 0.52540913]\n",
            "The trained model has an aproximate error rate of 27.15427662038523 which equates to 5%\n"
          ]
        }
      ]
    }
  ]
}