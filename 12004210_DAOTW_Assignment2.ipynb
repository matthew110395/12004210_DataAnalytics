{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew110395/12004210_DataAnalytics/blob/main/12004210_DAOTW_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "This assignment builds on the New York taxi problem identified in assignment 1, where the City of New York is looking for a way to accurately predict the number of collisions on a particular day of the week. Within this document two different types of machine learning models will be utilised to predict the number of collisions. The linear relationships identified in assignment 1 will be used to create a linear regression model. A Deep Learning Neural Network (DNN) will be used to predict the number of collisions where the relationship is not linear. "
      ],
      "metadata": {
        "id": "-MIIDzFYc6Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n",
        "To prepare data for machine learning the pandas package has been used. The numpy package has been used to aid with mathematical functions.\n",
        "\n",
        "As within part 1 of this assignment the data file containing location data exceeds the size limit for hosting within github. To overcome this the file was zipped. In order to extract the data from the zip file the zipfile package has been used.\n",
        "\n",
        "Within this document, Tensorflow has been used for machine learning, with both a linear regression model and a Deep Neural Network model. Tensorflow version 1 is unsupported within Google Colab, therefore must be installed using the pip package manager.\n",
        "\n",
        "Shutil has also been imported to allow for easy file management, in particular the removal of saved models."
      ],
      "metadata": {
        "id": "PP_OBRRWE3tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "\n",
        "!pip install tensorflow==1.15.2\n",
        "import tensorflow as tf\n",
        "# needed for high-level file management\n",
        "import shutil  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Vp_IJAE49d",
        "outputId": "9294bc8a-e4f5-42b1-eaf4-330fb60529df"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.38.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regressor\n",
        "Throughout assignment 1 a number of linear relationships were uncovered within the dataset. These relationsips form the basis of the linear regression models below.\n",
        "\n"
      ],
      "metadata": {
        "id": "SxU2LFGDjN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Precipition"
      ],
      "metadata": {
        "id": "VtJ7HqhA3bIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0ljmfBkDwE",
        "outputId": "223993c8-066d-4db0-9afd-37394c63ff32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp = df.drop(columns=['collision_date', 'temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] != 2012]\n",
        "df_prcp = df_prcp.loc[df_prcp[\"year\"] < 2020]\n",
        "cols = df_prcp['NUM_COLLISIONS']\n",
        "df_prcp = df_prcp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp.insert(loc=9, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp[:6])\n",
        "df_prcp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "nRcEly727YQb",
        "outputId": "0375f4fc-a0ea-4036-cccc-11c89d3d842d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "49    4  2016   1  28  0.09    0             0                 0     0   \n",
            "51    5  2014   1  17  0.00    1             0                 0     0   \n",
            "54    1  2016   1  25  0.02    0             0                 0     0   \n",
            "55    5  2016   1  29  0.00    0             0                 0     0   \n",
            "58    5  2017   1  20  0.00    0             0                 0     0   \n",
            "59    7  2013   1  13  0.01    1             0                 0     0   \n",
            "\n",
            "    NUM_COLLISIONS  \n",
            "49             681  \n",
            "51             589  \n",
            "54             658  \n",
            "55             645  \n",
            "58             605  \n",
            "59             373  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         prcp  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean      3.998425  2015.989366     6.518708    15.745569     0.122588   \n",
              "std       2.003542     1.996126     3.455211     8.803199     0.329143   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     0.000000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000     0.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000     0.000000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000     0.060000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000     3.760000   \n",
              "\n",
              "               fog  rain_drizzle  snow_ice_pellets         hail  \\\n",
              "count  2539.000000   2539.000000       2539.000000  2539.000000   \n",
              "mean      0.253249      0.375345          0.085467     0.000394   \n",
              "std       0.434958      0.484307          0.279630     0.019846   \n",
              "min       0.000000      0.000000          0.000000     0.000000   \n",
              "25%       0.000000      0.000000          0.000000     0.000000   \n",
              "50%       0.000000      0.000000          0.000000     0.000000   \n",
              "75%       1.000000      1.000000          0.000000     0.000000   \n",
              "max       1.000000      1.000000          1.000000     1.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2539.000000  \n",
              "mean       599.135093  \n",
              "std        100.299164  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d656173-f12c-456f-a80f-6e857b7b9fc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998425</td>\n",
              "      <td>2015.989366</td>\n",
              "      <td>6.518708</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.003542</td>\n",
              "      <td>1.996126</td>\n",
              "      <td>3.455211</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d656173-f12c-456f-a80f-6e857b7b9fc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d656173-f12c-456f-a80f-6e857b7b9fc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d656173-f12c-456f-a80f-6e857b7b9fc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp.iloc[np.random.permutation(len(df_prcp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xT4g-UZFi01",
        "outputId": "a59b67ce-2d56-402d-84a7-b362709828b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail\n",
            "213     3  2014   1   1  0.00    0             0                 0     0\n",
            "2023    6  2015   7  11  0.00    0             0                 0     0\n",
            "1473    5  2017   5  26  0.10    1             1                 0     0\n",
            "2996    1  2018  10  29  0.00    0             1                 0     0\n",
            "1151    4  2015   4   9  0.18    0             1                 0     0\n",
            "240     2  2015   1  13  0.82    0             1                 1     0\n",
            "      day  year  mo  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  \\\n",
            "213     3  2014   1   1  0.00    0             0                 0     0   \n",
            "2023    6  2015   7  11  0.00    0             0                 0     0   \n",
            "1473    5  2017   5  26  0.10    1             1                 0     0   \n",
            "2996    1  2018  10  29  0.00    0             1                 0     0   \n",
            "1151    4  2015   4   9  0.18    0             1                 0     0   \n",
            "240     2  2015   1  13  0.82    0             1                 1     0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "213              399  \n",
            "2023             597  \n",
            "1473             776  \n",
            "2996             696  \n",
            "1151             452  \n",
            "240              613  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xkzkVBQjL_",
        "outputId": "4a1f2f8a-4070-48cb-910a-221fdd14db70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213     399\n",
            "2023    597\n",
            "1473    776\n",
            "2996    696\n",
            "1151    452\n",
            "240     613\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_COLLISIONS=1161"
      ],
      "metadata": {
        "id": "k-aEMQX9EuJJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 9\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "id": "NwSG_P_nRgPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdbc01c-aeac-458a-b05b-3ef52447a012"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2031\n",
            "508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUKI-paISKeh",
        "outputId": "85cb97ad-6349-4949-9caa-ec5eca5cca19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-033c6ea13921>:7: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please convert numpy dtypes explicitly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:183: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/linear.py:740: regression_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.contrib.estimator.*_head.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0666d3490>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:From <ipython-input-8-033c6ea13921>:7: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to the Estimator interface.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.2718349, step = 1\n",
            "INFO:tensorflow:global_step/sec: 1274.24\n",
            "INFO:tensorflow:loss = 0.0070736543, step = 101 (0.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 1480.03\n",
            "INFO:tensorflow:loss = 0.0050733453, step = 201 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1310.89\n",
            "INFO:tensorflow:loss = 0.007885993, step = 301 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1457.39\n",
            "INFO:tensorflow:loss = 0.006817926, step = 401 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1369.62\n",
            "INFO:tensorflow:loss = 0.0076487446, step = 501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1308.28\n",
            "INFO:tensorflow:loss = 0.0065248143, step = 601 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1264.22\n",
            "INFO:tensorflow:loss = 0.0066063367, step = 701 (0.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 1442.38\n",
            "INFO:tensorflow:loss = 0.0072688274, step = 801 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1348.88\n",
            "INFO:tensorflow:loss = 0.008207085, step = 901 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1385.56\n",
            "INFO:tensorflow:loss = 0.006667953, step = 1001 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1400.25\n",
            "INFO:tensorflow:loss = 0.005445567, step = 1101 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1385.94\n",
            "INFO:tensorflow:loss = 0.009968674, step = 1201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1429.21\n",
            "INFO:tensorflow:loss = 0.012904378, step = 1301 (0.071 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1301 vs previous value: 1301. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 1186.26\n",
            "INFO:tensorflow:loss = 0.0070228707, step = 1401 (0.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.181\n",
            "INFO:tensorflow:loss = 0.009551362, step = 1501 (0.105 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1501 vs previous value: 1501. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 890.618\n",
            "INFO:tensorflow:loss = 0.011085218, step = 1601 (0.118 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1601 vs previous value: 1601. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 879.857\n",
            "INFO:tensorflow:loss = 0.016906071, step = 1701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1484.98\n",
            "INFO:tensorflow:loss = 0.012755055, step = 1801 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1373.31\n",
            "INFO:tensorflow:loss = 0.0071692085, step = 1901 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1303.09\n",
            "INFO:tensorflow:loss = 0.008569207, step = 2001 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1397.96\n",
            "INFO:tensorflow:loss = 0.0066128857, step = 2101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1470.35\n",
            "INFO:tensorflow:loss = 0.011221998, step = 2201 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1460.34\n",
            "INFO:tensorflow:loss = 0.0060713906, step = 2301 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1450.76\n",
            "INFO:tensorflow:loss = 0.030102804, step = 2401 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1309.28\n",
            "INFO:tensorflow:loss = 0.007391344, step = 2501 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1364.64\n",
            "INFO:tensorflow:loss = 0.009946657, step = 2601 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1445.7\n",
            "INFO:tensorflow:loss = 0.021228723, step = 2701 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1483.54\n",
            "INFO:tensorflow:loss = 0.010151393, step = 2801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1454.5\n",
            "INFO:tensorflow:loss = 0.011364004, step = 2901 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1483.23\n",
            "INFO:tensorflow:loss = 0.0062720682, step = 3001 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1469.52\n",
            "INFO:tensorflow:loss = 0.010624712, step = 3101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1427.44\n",
            "INFO:tensorflow:loss = 0.0072345063, step = 3201 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1336.48\n",
            "INFO:tensorflow:loss = 0.007091263, step = 3301 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1138.77\n",
            "INFO:tensorflow:loss = 0.00672022, step = 3401 (0.088 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 3401 vs previous value: 3401. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 1421.49\n",
            "INFO:tensorflow:loss = 0.0074883816, step = 3501 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1485.22\n",
            "INFO:tensorflow:loss = 0.0075860154, step = 3601 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1412.06\n",
            "INFO:tensorflow:loss = 0.014829785, step = 3701 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1418.52\n",
            "INFO:tensorflow:loss = 0.027627993, step = 3801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1333.25\n",
            "INFO:tensorflow:loss = 0.006953127, step = 3901 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1257.79\n",
            "INFO:tensorflow:loss = 0.008190133, step = 4001 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1457.92\n",
            "INFO:tensorflow:loss = 0.011951455, step = 4101 (0.068 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 4101 vs previous value: 4101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 1261.49\n",
            "INFO:tensorflow:loss = 0.006687616, step = 4201 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1353.39\n",
            "INFO:tensorflow:loss = 0.0081680175, step = 4301 (0.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 1057.81\n",
            "INFO:tensorflow:loss = 0.007942941, step = 4401 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1469.89\n",
            "INFO:tensorflow:loss = 0.0072313957, step = 4501 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1509.8\n",
            "INFO:tensorflow:loss = 0.026253976, step = 4601 (0.064 sec)\n",
            "INFO:tensorflow:global_step/sec: 1479.75\n",
            "INFO:tensorflow:loss = 0.0041737505, step = 4701 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1215.52\n",
            "INFO:tensorflow:loss = 0.007971079, step = 4801 (0.082 sec)\n",
            "INFO:tensorflow:global_step/sec: 1467.68\n",
            "INFO:tensorflow:loss = 0.007942576, step = 4901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1343.29\n",
            "INFO:tensorflow:loss = 0.016315563, step = 5001 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1424.44\n",
            "INFO:tensorflow:loss = 0.0065865014, step = 5101 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1400.16\n",
            "INFO:tensorflow:loss = 0.0178153, step = 5201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1503.59\n",
            "INFO:tensorflow:loss = 0.0053663617, step = 5301 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1456.85\n",
            "INFO:tensorflow:loss = 0.005795181, step = 5401 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1451.36\n",
            "INFO:tensorflow:loss = 0.00546529, step = 5501 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1392.33\n",
            "INFO:tensorflow:loss = 0.018053293, step = 5601 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1351.61\n",
            "INFO:tensorflow:loss = 0.011595665, step = 5701 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1399.89\n",
            "INFO:tensorflow:loss = 0.013245174, step = 5801 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1346.05\n",
            "INFO:tensorflow:loss = 0.009815721, step = 5901 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1458.49\n",
            "INFO:tensorflow:loss = 0.016276818, step = 6001 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.05\n",
            "INFO:tensorflow:loss = 0.0061569223, step = 6101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1379.86\n",
            "INFO:tensorflow:loss = 0.0072251563, step = 6201 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1438.54\n",
            "INFO:tensorflow:loss = 0.007300961, step = 6301 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1217.95\n",
            "INFO:tensorflow:loss = 0.023444299, step = 6401 (0.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 796.56\n",
            "INFO:tensorflow:loss = 0.0077261557, step = 6501 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 661.86\n",
            "INFO:tensorflow:loss = 0.007974387, step = 6601 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 898.371\n",
            "INFO:tensorflow:loss = 0.019290505, step = 6701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 831.883\n",
            "INFO:tensorflow:loss = 0.008936329, step = 6801 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.086\n",
            "INFO:tensorflow:loss = 0.0073035695, step = 6901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.194\n",
            "INFO:tensorflow:loss = 0.012165356, step = 7001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 853.898\n",
            "INFO:tensorflow:loss = 0.010669392, step = 7101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.05\n",
            "INFO:tensorflow:loss = 0.0060689077, step = 7201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 725.23\n",
            "INFO:tensorflow:loss = 0.014929969, step = 7301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.983\n",
            "INFO:tensorflow:loss = 0.022300344, step = 7401 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 740.746\n",
            "INFO:tensorflow:loss = 0.006987621, step = 7501 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 717.579\n",
            "INFO:tensorflow:loss = 0.022309398, step = 7601 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.013\n",
            "INFO:tensorflow:loss = 0.011415602, step = 7701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.148\n",
            "INFO:tensorflow:loss = 0.008973112, step = 7801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.619\n",
            "INFO:tensorflow:loss = 0.01047124, step = 7901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.2\n",
            "INFO:tensorflow:loss = 0.009409616, step = 8001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.484\n",
            "INFO:tensorflow:loss = 0.016898643, step = 8101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.022\n",
            "INFO:tensorflow:loss = 0.010882098, step = 8201 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.513\n",
            "INFO:tensorflow:loss = 0.006331944, step = 8301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.88\n",
            "INFO:tensorflow:loss = 0.006735133, step = 8401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.156\n",
            "INFO:tensorflow:loss = 0.0073713334, step = 8501 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.454\n",
            "INFO:tensorflow:loss = 0.0064207893, step = 8601 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.925\n",
            "INFO:tensorflow:loss = 0.0061744326, step = 8701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.844\n",
            "INFO:tensorflow:loss = 0.0078034457, step = 8801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 794.871\n",
            "INFO:tensorflow:loss = 0.0072975354, step = 8901 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 469.384\n",
            "INFO:tensorflow:loss = 0.005865884, step = 9001 (0.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 787.667\n",
            "INFO:tensorflow:loss = 0.008964146, step = 9101 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.774\n",
            "INFO:tensorflow:loss = 0.0068482175, step = 9201 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.637\n",
            "INFO:tensorflow:loss = 0.008239493, step = 9301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 730.814\n",
            "INFO:tensorflow:loss = 0.00520122, step = 9401 (0.137 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.214\n",
            "INFO:tensorflow:loss = 0.007828534, step = 9501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.089\n",
            "INFO:tensorflow:loss = 0.0077145654, step = 9601 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.56\n",
            "INFO:tensorflow:loss = 0.011838891, step = 9701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 535.468\n",
            "INFO:tensorflow:loss = 0.006755948, step = 9801 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.469\n",
            "INFO:tensorflow:loss = 0.006689152, step = 9901 (0.106 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.008769373.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 120.27965286688338\n",
            "Just using average = 598.9635647464303 has RMSE of 99.45388340325029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_prcp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fifEsTD98hy",
        "outputId": "a509934a-fa4c-40be-993e-d1ac42c39560"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff06543cf50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.46280783 0.39800078 0.4636122  0.42282948 0.46114007 0.43913487\n",
            " 0.449243   0.47614846 0.4382128  0.45474368 0.44615972 0.41813916\n",
            " 0.4794206  0.47818336 0.49988747 0.45169005 0.4742704  0.42404553\n",
            " 0.46931753 0.49836254 0.43093872 0.51345295 0.46592873 0.4660528\n",
            " 0.486162   0.4725375  0.49260208 0.5088301  0.4666033  0.4598983\n",
            " 0.4463999  0.50934154 0.4714387  0.44519517 0.42109233 0.47672996\n",
            " 0.43556514 0.4130507  0.46865654 0.43353376 0.45110175 0.4701876\n",
            " 0.4564987  0.41614628 0.41391096 0.42304745 0.46023485 0.4267523\n",
            " 0.42839003 0.43011484 0.43222958 0.49868333 0.49761024 0.48804387\n",
            " 0.44515806 0.5009714  0.42252412 0.3916985  0.4185493  0.4585322\n",
            " 0.47623903 0.4829306  0.4543146  0.47516623 0.46681905 0.4869174\n",
            " 0.42103022 0.43544436 0.43491256 0.50192577 0.4223419  0.44819778\n",
            " 0.42044258 0.45645896 0.47989038 0.46869466 0.4944869  0.47781485\n",
            " 0.4751654  0.4240815  0.45045096 0.38560256 0.44755152 0.40396866\n",
            " 0.38276157 0.5017927  0.44771236 0.47452453 0.45047826 0.48305547\n",
            " 0.48661417 0.4633224  0.46301714 0.43626    0.44311634 0.49024668\n",
            " 0.43913862 0.43134302 0.43022323 0.49473643 0.44880825 0.4748926\n",
            " 0.44411793 0.49539262 0.41699225 0.4442099  0.4760774  0.4440946\n",
            " 0.43687278 0.44636124 0.49052867 0.41467005 0.48272356 0.4208912\n",
            " 0.4336151  0.46791232 0.44319314 0.45613617 0.49219698 0.4079311\n",
            " 0.39761895 0.4625698  0.48630062 0.4211535  0.4173069  0.45837417\n",
            " 0.46875295 0.45022768 0.39704642 0.48045695 0.47907692 0.44958326\n",
            " 0.47647938 0.43849796 0.48167676 0.40367487 0.5053229  0.48275587\n",
            " 0.4173469  0.4707698  0.47760397 0.4743182  0.41302848 0.42393538\n",
            " 0.4992559  0.41327047 0.49222103 0.49334952 0.48271394 0.39746824\n",
            " 0.47612077 0.46915084 0.43170288 0.47330582 0.4282003  0.40858227\n",
            " 0.43239528 0.4244821  0.42981982 0.42591074 0.45965326 0.42883393\n",
            " 0.40189815 0.51078326 0.44689772 0.4398388  0.4752933  0.47012436\n",
            " 0.4787747  0.42679864 0.4926619  0.4678985  0.43324262 0.4769354\n",
            " 0.49987465 0.4917734  0.50254166 0.47824666 0.4615646  0.45715764\n",
            " 0.44086766 0.44855767 0.46724924 0.50308865 0.48777348 0.40160495\n",
            " 0.41735482 0.42058665 0.45640898 0.48627654 0.48052117 0.49356863\n",
            " 0.4710534  0.4057431  0.41442055 0.51533794 0.4431323  0.38931093\n",
            " 0.45031124 0.4519257  0.42538285 0.4399036  0.44531617 0.4453635\n",
            " 0.4401764  0.41893053 0.47248074 0.45886967 0.46469578 0.43309525\n",
            " 0.45587182 0.4214968  0.47354144 0.50745803 0.43239802 0.4789842\n",
            " 0.46933666 0.47651652 0.43027973 0.48393422 0.47642583 0.43312213\n",
            " 0.45588765 0.49476057 0.45328707 0.40283132 0.42622972 0.4465123\n",
            " 0.507252   0.41301805 0.45026025 0.41065404 0.4654213  0.50656533\n",
            " 0.48825735 0.41133082 0.44733614 0.4298097  0.4172638  0.48849538\n",
            " 0.41897118 0.42755002 0.46757954 0.4167823  0.47413275 0.4807482\n",
            " 0.48742878 0.48303893 0.39456448 0.45512256 0.46146595 0.47870317\n",
            " 0.49894238 0.42137745 0.4344824  0.5039792  0.5079525  0.4495917\n",
            " 0.43676284 0.42229307 0.41711417 0.43821487 0.47201806 0.40019557\n",
            " 0.4742637  0.43098128 0.5134744  0.45951256 0.48886532 0.48403397\n",
            " 0.49760514 0.49503797 0.5067596  0.42649332 0.45221335 0.47064218\n",
            " 0.4302945  0.45758712 0.4374954  0.45663548 0.48510516 0.44768006\n",
            " 0.5214173  0.45964175 0.4750922  0.51349026 0.4323236  0.49501115\n",
            " 0.4486007  0.43227935 0.4200379  0.49175218 0.40931827 0.4271689\n",
            " 0.45588583 0.43368703 0.4505108  0.44355866 0.40595117 0.49168468\n",
            " 0.42191473 0.46551165 0.47763622 0.49679273 0.44382176 0.47589266\n",
            " 0.4628694  0.4024322  0.44418564 0.46453136 0.41584584 0.46303508\n",
            " 0.46302778 0.5032217  0.45994097 0.4367954  0.4393257  0.4423386\n",
            " 0.500016   0.4405306  0.44628027 0.48916748 0.42901537 0.4192708\n",
            " 0.40956172 0.4437977  0.4444618  0.46021736 0.4512697  0.4453962\n",
            " 0.46211597 0.41491812 0.4019237  0.4719712  0.41095603 0.44088858\n",
            " 0.4589741  0.4412175  0.45474714 0.5006602  0.42722622 0.4735065\n",
            " 0.39587402 0.45504767 0.46188745 0.44282404 0.41670248 0.42844182\n",
            " 0.47157443 0.47935605 0.40385124 0.42581308 0.4137287  0.45770097\n",
            " 0.4637029  0.39647695 0.40273163 0.42694625 0.41725126 0.46707916\n",
            " 0.47004578 0.5053974  0.44902763 0.50094956 0.43635067 0.45354232\n",
            " 0.4892636  0.47131348 0.47874603 0.4663125  0.3956574  0.4140387\n",
            " 0.44600978 0.47206414 0.48425415 0.45871282 0.43125328 0.43987486\n",
            " 0.47907126 0.4655947  0.42366382 0.48141918 0.4551986  0.45177126\n",
            " 0.4319463  0.48509777 0.42483732 0.39768904 0.49741843 0.38734812\n",
            " 0.48461825 0.46760717 0.48440877 0.46225628 0.4488458  0.4292695\n",
            " 0.46865147 0.4611178  0.43578658 0.44452196 0.51113385 0.4293203\n",
            " 0.40441886 0.4292047  0.4020482  0.4353122  0.4630739  0.49050468\n",
            " 0.47720823 0.41412437 0.40569508 0.4829603  0.49625772 0.4668193\n",
            " 0.45686823 0.48362786 0.4486789  0.4689073  0.43453696 0.40404898\n",
            " 0.5151772  0.43124276 0.46900722 0.468662   0.48983616 0.46565953\n",
            " 0.45301884 0.45314693 0.43907022 0.44224134 0.4367713  0.45649606\n",
            " 0.38970238 0.42652905 0.43348286 0.4581403  0.43843314 0.4527856\n",
            " 0.47733116 0.44462833 0.42416567 0.43807983 0.44455934 0.42535484\n",
            " 0.46176538 0.48048922 0.41475326 0.43964693 0.4847257  0.45932594\n",
            " 0.42711657 0.46837723 0.49641997 0.45156008 0.46314406 0.4953385\n",
            " 0.4966757  0.47022966 0.45056584 0.42186487 0.42481238 0.4351548\n",
            " 0.40126926 0.4781011  0.43402466 0.44321078 0.5025289  0.46687382\n",
            " 0.46566975 0.4796301  0.5000808  0.43832874 0.42139593 0.44159585\n",
            " 0.47218725 0.454861   0.44611067 0.44506338 0.40016875 0.39950728\n",
            " 0.4485252  0.4640512  0.44348416 0.43645653 0.4354064  0.4097104\n",
            " 0.5074161  0.4548887  0.429893   0.40247935 0.43962613 0.45055836\n",
            " 0.4957826  0.4387748  0.44598573 0.44464135 0.4502925  0.42921314\n",
            " 0.46480337 0.4599706  0.45162517 0.39735994 0.4888862  0.48764312\n",
            " 0.45808744 0.48519406 0.4396583  0.4815604 ]\n",
            "[0.56244617 0.45650301 0.52540913 0.41774332 0.59259259 0.40310078\n",
            " 0.51507321 0.60809647 0.46942291 0.42980189 0.51507321 0.58570198\n",
            " 0.47028424 0.51248923 0.59345392 0.56589147 0.66838932 0.58656331\n",
            " 0.4625323  0.58914729 0.43669251 0.53143842 0.53402239 0.49009475\n",
            " 0.5667528  0.63910422 0.56330749 0.60206718 0.50301464 0.50559862\n",
            " 0.52540913 0.5667528  0.50559862 0.72782084 0.50215332 0.37295435\n",
            " 0.54866494 0.44099914 0.58828596 0.41515935 0.4918174  0.55813953\n",
            " 0.64857881 0.37553833 0.51593454 0.48320413 0.6124031  0.44358312\n",
            " 0.67355728 0.48406546 0.64857881 0.60723514 0.49009475 0.55641688\n",
            " 0.52196382 0.58570198 0.64857881 0.39534884 0.54521964 0.47631352\n",
            " 0.61757106 0.51679587 0.53832903 0.51593454 0.65030146 0.59259259\n",
            " 0.44272179 0.5960379  0.61154177 0.46856158 0.40137812 0.58484065\n",
            " 0.55986219 0.58742463 0.49095607 0.53488372 0.63824289 0.49612403\n",
            " 0.63479759 0.47803618 0.49095607 0.33850129 0.64857881 0.39793282\n",
            " 0.45822567 0.55211025 0.48492679 0.53229974 0.59086994 0.52196382\n",
            " 0.59000861 0.58914729 0.34969854 0.5374677  0.54952627 0.37898363\n",
            " 0.45564169 0.43927649 0.55900086 0.45650301 0.41343669 0.71403962\n",
            " 0.53919035 0.44702842 0.51851852 0.43496985 0.45305771 0.49784668\n",
            " 0.5047373  0.54177433 0.46770026 0.47975883 0.64513351 0.51076658\n",
            " 0.4918174  0.60809647 0.54091301 0.45822567 0.42549526 0.44875108\n",
            " 0.48923342 0.54866494 0.46511628 0.48320413 0.47028424 0.64944014\n",
            " 0.57536606 0.63652024 0.34797588 0.44272179 0.55555556 0.57536606\n",
            " 0.31955211 0.52713178 0.59862188 0.51765719 0.35400517 0.50904393\n",
            " 0.44099914 0.44186047 0.52368648 0.58828596 0.42807924 0.53660637\n",
            " 0.53402239 0.43583118 0.58914729 0.53402239 0.43152455 0.44530577\n",
            " 0.64082687 0.48148148 0.44702842 0.39018088 0.51937984 0.60465116\n",
            " 0.4005168  0.47717485 0.65202412 0.43066322 0.62790698 0.51679587\n",
            " 0.42635659 0.53660637 0.59345392 0.63910422 0.55297158 0.43755383\n",
            " 0.51421189 0.4461671  0.4496124  0.44702842 0.60981912 0.60809647\n",
            " 0.6124031  0.52799311 0.61154177 0.57536606 0.5503876  0.50043066\n",
            " 0.42721792 0.63996555 0.59431525 0.40999139 0.5245478  0.50301464\n",
            " 0.51765719 0.46511628 0.47286822 0.56503015 0.5994832  0.51679587\n",
            " 0.27648579 0.44358312 0.60551249 0.57881137 0.47975883 0.32213609\n",
            " 0.56330749 0.62015504 0.4039621  0.58053402 0.52885444 0.45908699\n",
            " 0.6089578  0.50301464 0.61498708 0.49612403 0.54608096 0.43755383\n",
            " 0.54866494 0.39793282 0.58656331 0.65288544 0.49956934 0.36175711\n",
            " 0.60551249 0.4203273  0.56589147 0.52971576 0.6124031  0.50559862\n",
            " 0.69164513 0.58570198 0.58225668 0.39965547 0.51248923 0.38501292\n",
            " 0.46425495 0.40913006 0.43238587 0.50904393 0.53143842 0.42204996\n",
            " 0.48837209 0.60723514 0.71490095 0.47975883 0.42204996 0.57622739\n",
            " 0.42980189 0.51679587 0.53057709 0.41515935 0.49784668 0.49440138\n",
            " 0.61498708 0.57881137 0.41257537 0.51248923 0.63221361 0.54349699\n",
            " 0.49870801 0.39018088 0.55124892 0.4461671  0.42807924 0.51335056\n",
            " 0.56761413 0.49354005 0.33850129 0.6089578  0.47717485 0.45650301\n",
            " 0.59259259 0.36175711 0.49956934 0.5796727  0.60120586 0.51679587\n",
            " 0.60637382 0.5538329  0.33936262 0.6416882  0.53143842 0.57881137\n",
            " 0.53316107 0.48492679 0.5796727  0.54952627 0.46683893 0.37639966\n",
            " 0.6089578  0.56158484 0.54263566 0.57019811 0.52713178 0.49095607\n",
            " 0.54521964 0.42463394 0.5081826  0.50559862 0.50990525 0.55641688\n",
            " 0.53574505 0.53660637 0.583118   0.45736434 0.47200689 0.65202412\n",
            " 0.33936262 0.71748493 0.56933678 0.59689922 0.625323   0.58828596\n",
            " 0.42894057 0.43669251 0.53057709 0.54952627 0.54866494 0.46856158\n",
            " 0.54608096 0.42204996 0.41946598 0.44099914 0.42635659 0.53574505\n",
            " 0.53057709 0.66063738 0.42635659 0.59862188 0.47803618 0.53402239\n",
            " 0.57450474 0.55297158 0.68819983 0.56416882 0.60120586 0.53919035\n",
            " 0.46425495 0.39362618 0.34969854 0.62015504 0.50559862 0.63479759\n",
            " 0.51851852 0.52971576 0.52713178 0.51507321 0.4918174  0.48664944\n",
            " 0.39276486 0.55641688 0.61584841 0.47459087 0.53488372 0.42377261\n",
            " 0.39707149 0.53919035 0.33505599 0.45822567 0.41429802 0.91731266\n",
            " 0.48148148 0.51335056 0.41946598 0.53574505 0.45822567 0.37639966\n",
            " 0.55813953 0.64427218 0.53660637 0.51248923 0.48406546 0.55211025\n",
            " 0.50990525 0.374677   0.61929371 0.54177433 0.36089578 0.40913006\n",
            " 0.52971576 0.47200689 0.55727821 0.32816537 0.55641688 0.30577089\n",
            " 0.54866494 0.59345392 0.45736434 0.52627046 0.54521964 0.38845823\n",
            " 0.39276486 0.60034453 0.35228252 0.43927649 0.47028424 0.33850129\n",
            " 0.51507321 0.5667528  0.5994832  0.45391904 0.46080965 0.53574505\n",
            " 0.47631352 0.50301464 0.42894057 0.40999139 0.42463394 0.42894057\n",
            " 0.34366925 0.60292851 0.4625323  0.5047373  0.56933678 0.5796727\n",
            " 0.57364341 0.4754522  0.4203273  0.4918174  0.50215332 0.49267873\n",
            " 0.58742463 0.52196382 0.54005168 0.53574505 0.5374677  0.5081826\n",
            " 0.57450474 0.59431525 0.53488372 0.60465116 0.7002584  0.57536606\n",
            " 0.38415159 0.48923342 0.4788975  0.43669251 0.53660637 0.66925065\n",
            " 0.45305771 0.53057709 0.48837209 0.5211025  0.40913006 0.50732127\n",
            " 0.44875108 0.59259259 0.56847545 0.57278208 0.37984496 0.56158484\n",
            " 0.62618432 0.56933678 0.38587425 0.63996555 0.4005168  0.30577089\n",
            " 0.31438415 0.4918174  0.57105943 0.47631352 0.52971576 0.59173127\n",
            " 0.5374677  0.53057709 0.70111972 0.55900086 0.55900086 0.55900086\n",
            " 0.33419466 0.5211025  0.5960379  0.48837209 0.50732127 0.66666667\n",
            " 0.5081826  0.66494401 0.54435831 0.63824289 0.55469423 0.4625323\n",
            " 0.67011197 0.6873385  0.56158484 0.55986219 0.44530577 0.40654608\n",
            " 0.5211025  0.51679587 0.56330749 0.46339363 0.4005168  0.45305771\n",
            " 0.65374677 0.58053402 0.5994832  0.36347976 0.52799311 0.4754522\n",
            " 0.51765719 0.56933678 0.48406546 0.43583118 0.59689922 0.64341085\n",
            " 0.48664944 0.71231697 0.56761413 0.4005168  0.59517657 0.69939707\n",
            " 0.60206718 0.4461671  0.56847545 0.41515935]\n",
            "The trained model has an aproximate error rate of 73.66366484057247 which equates to 12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "RB0Zq1024UmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930993f0-956d-43d1-edae-773da0798afd",
        "id": "d2NB6odM5G9I"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] != 2012]\n",
        "df_dewp = df_dewp.loc[df_dewp[\"year\"] < 2020]\n",
        "cols = df_dewp['NUM_COLLISIONS']\n",
        "df_dewp = df_dewp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp[:6])\n",
        "df_dewp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "ea69a2fc-88cb-421d-a32f-2370e3070ab7",
        "id": "WwtmLQ6a5rHs"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  24.4             681\n",
            "51    5  2014   1  17  35.8             589\n",
            "54    1  2016   1  25  21.2             658\n",
            "55    5  2016   1  29  36.8             645\n",
            "58    5  2017   1  20  32.5             605\n",
            "59    7  2013   1  13  44.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da         dewp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.998434  2015.999217     6.524070    15.723679    44.163170   \n",
              "std       2.000391     2.000000     3.449676     8.801271    16.995303   \n",
              "min       1.000000  2013.000000     1.000000     1.000000    -6.700000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    32.150000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    45.300000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    58.500000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    74.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.109980  \n",
              "std        100.277185  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c7f1588-f716-47a2-ab14-eb4c15ec0ec0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.998434</td>\n",
              "      <td>2015.999217</td>\n",
              "      <td>6.524070</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000391</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.449676</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c7f1588-f716-47a2-ab14-eb4c15ec0ec0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c7f1588-f716-47a2-ab14-eb4c15ec0ec0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c7f1588-f716-47a2-ab14-eb4c15ec0ec0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp.iloc[np.random.permutation(len(df_dewp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a671fbae-abc6-46ba-df61-43935a07dd70",
        "id": "KXbAqzNN694C"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  dewp\n",
            "453     3  2018   2  28  38.2\n",
            "983     6  2014   4  12  40.8\n",
            "2120    4  2016   7  14  65.9\n",
            "744     1  2016   3  28  41.4\n",
            "2705    5  2014   9  19  42.7\n",
            "3489    7  2019  12  22  34.9\n",
            "      day  year  mo  da  dewp  NUM_COLLISIONS\n",
            "453     3  2018   2  28  38.2             654\n",
            "983     6  2014   4  12  40.8             578\n",
            "2120    4  2016   7  14  65.9             741\n",
            "744     1  2016   3  28  41.4             658\n",
            "2705    5  2014   9  19  42.7             648\n",
            "3489    7  2019  12  22  34.9             510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db5beec-23db-49c6-e47e-8476dbae9d3f",
        "id": "iGbT5sAJ7KTK"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "453     654\n",
            "983     578\n",
            "2120    741\n",
            "744     658\n",
            "2705    648\n",
            "3489    510\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42824ce3-98c2-43dd-85cb-ea3e4f4f70e6",
        "id": "vpHbBnml7PZw"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120d0a52-8d38-4491-a6dc-55bd60860bee",
        "id": "j4GKf5BL7WI3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff05e6b1a90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.28520566, step = 1\n",
            "INFO:tensorflow:global_step/sec: 925.282\n",
            "INFO:tensorflow:loss = 0.008250323, step = 101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 1472.21\n",
            "INFO:tensorflow:loss = 0.0055447062, step = 201 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1456.22\n",
            "INFO:tensorflow:loss = 0.0073354435, step = 301 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1422.52\n",
            "INFO:tensorflow:loss = 0.0069964584, step = 401 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1376.37\n",
            "INFO:tensorflow:loss = 0.006039256, step = 501 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1275.28\n",
            "INFO:tensorflow:loss = 0.005632888, step = 601 (0.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 1446.33\n",
            "INFO:tensorflow:loss = 0.006950683, step = 701 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1422.32\n",
            "INFO:tensorflow:loss = 0.0058646705, step = 801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1432.52\n",
            "INFO:tensorflow:loss = 0.00566072, step = 901 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1467.87\n",
            "INFO:tensorflow:loss = 0.006112217, step = 1001 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1467.31\n",
            "INFO:tensorflow:loss = 0.0074108997, step = 1101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1445.71\n",
            "INFO:tensorflow:loss = 0.006350604, step = 1201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.07\n",
            "INFO:tensorflow:loss = 0.005248653, step = 1301 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1412.77\n",
            "INFO:tensorflow:loss = 0.0076057576, step = 1401 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1494.68\n",
            "INFO:tensorflow:loss = 0.0063967546, step = 1501 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1255.86\n",
            "INFO:tensorflow:loss = 0.007299154, step = 1601 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1382.68\n",
            "INFO:tensorflow:loss = 0.008170528, step = 1701 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1416.09\n",
            "INFO:tensorflow:loss = 0.006598955, step = 1801 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1417.02\n",
            "INFO:tensorflow:loss = 0.0059437677, step = 1901 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1440.29\n",
            "INFO:tensorflow:loss = 0.0048040748, step = 2001 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1412.43\n",
            "INFO:tensorflow:loss = 0.0070154117, step = 2101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1417.84\n",
            "INFO:tensorflow:loss = 0.0077989763, step = 2201 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1470.53\n",
            "INFO:tensorflow:loss = 0.008075452, step = 2301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1168.76\n",
            "INFO:tensorflow:loss = 0.006434778, step = 2401 (0.084 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.707\n",
            "INFO:tensorflow:loss = 0.006276261, step = 2501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.985\n",
            "INFO:tensorflow:loss = 0.0059621762, step = 2601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1079.93\n",
            "INFO:tensorflow:loss = 0.0070158127, step = 2701 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1435.98\n",
            "INFO:tensorflow:loss = 0.0072201397, step = 2801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1352.07\n",
            "INFO:tensorflow:loss = 0.006599799, step = 2901 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1219.64\n",
            "INFO:tensorflow:loss = 0.004984701, step = 3001 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.15\n",
            "INFO:tensorflow:loss = 0.00572777, step = 3101 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1241.26\n",
            "INFO:tensorflow:loss = 0.006117381, step = 3201 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1491.15\n",
            "INFO:tensorflow:loss = 0.008548643, step = 3301 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1470.21\n",
            "INFO:tensorflow:loss = 0.0055706077, step = 3401 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1441.65\n",
            "INFO:tensorflow:loss = 0.0053222943, step = 3501 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1477.88\n",
            "INFO:tensorflow:loss = 0.005048821, step = 3601 (0.064 sec)\n",
            "INFO:tensorflow:global_step/sec: 1410.28\n",
            "INFO:tensorflow:loss = 0.005849556, step = 3701 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1354.03\n",
            "INFO:tensorflow:loss = 0.0056286426, step = 3801 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1425.11\n",
            "INFO:tensorflow:loss = 0.0058418503, step = 3901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1481.76\n",
            "INFO:tensorflow:loss = 0.0079029165, step = 4001 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1499.09\n",
            "INFO:tensorflow:loss = 0.005021923, step = 4101 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1331.34\n",
            "INFO:tensorflow:loss = 0.00648973, step = 4201 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1361.63\n",
            "INFO:tensorflow:loss = 0.0053625302, step = 4301 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1367.47\n",
            "INFO:tensorflow:loss = 0.0056746295, step = 4401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1445.86\n",
            "INFO:tensorflow:loss = 0.006061141, step = 4501 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1392.12\n",
            "INFO:tensorflow:loss = 0.0066571883, step = 4601 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1443.49\n",
            "INFO:tensorflow:loss = 0.006616327, step = 4701 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1510.85\n",
            "INFO:tensorflow:loss = 0.0074340436, step = 4801 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1511.54\n",
            "INFO:tensorflow:loss = 0.007497117, step = 4901 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1460.75\n",
            "INFO:tensorflow:loss = 0.007993874, step = 5001 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1405.97\n",
            "INFO:tensorflow:loss = 0.00690044, step = 5101 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1320.5\n",
            "INFO:tensorflow:loss = 0.0049455334, step = 5201 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1427.65\n",
            "INFO:tensorflow:loss = 0.006025361, step = 5301 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1332.97\n",
            "INFO:tensorflow:loss = 0.0055167414, step = 5401 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1234.94\n",
            "INFO:tensorflow:loss = 0.00591728, step = 5501 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1394.98\n",
            "INFO:tensorflow:loss = 0.005408938, step = 5601 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1332.16\n",
            "INFO:tensorflow:loss = 0.008885844, step = 5701 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1363.79\n",
            "INFO:tensorflow:loss = 0.0059952745, step = 5801 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1290.79\n",
            "INFO:tensorflow:loss = 0.0091226585, step = 5901 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1399.35\n",
            "INFO:tensorflow:loss = 0.006367703, step = 6001 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1442.14\n",
            "INFO:tensorflow:loss = 0.009482084, step = 6101 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1203.05\n",
            "INFO:tensorflow:loss = 0.006415552, step = 6201 (0.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 1327.87\n",
            "INFO:tensorflow:loss = 0.007843919, step = 6301 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1435.09\n",
            "INFO:tensorflow:loss = 0.008130776, step = 6401 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1320.43\n",
            "INFO:tensorflow:loss = 0.007400469, step = 6501 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1346.11\n",
            "INFO:tensorflow:loss = 0.008382855, step = 6601 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 720.252\n",
            "INFO:tensorflow:loss = 0.006097098, step = 6701 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.377\n",
            "INFO:tensorflow:loss = 0.005926244, step = 6801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 807.799\n",
            "INFO:tensorflow:loss = 0.0071243783, step = 6901 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.523\n",
            "INFO:tensorflow:loss = 0.008939354, step = 7001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.081\n",
            "INFO:tensorflow:loss = 0.0061967866, step = 7101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.802\n",
            "INFO:tensorflow:loss = 0.0049837036, step = 7201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.932\n",
            "INFO:tensorflow:loss = 0.005066463, step = 7301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.156\n",
            "INFO:tensorflow:loss = 0.006438577, step = 7401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.947\n",
            "INFO:tensorflow:loss = 0.004578932, step = 7501 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 755.004\n",
            "INFO:tensorflow:loss = 0.0058274865, step = 7601 (0.134 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.402\n",
            "INFO:tensorflow:loss = 0.0051985495, step = 7701 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 804.717\n",
            "INFO:tensorflow:loss = 0.007262532, step = 7801 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 646.395\n",
            "INFO:tensorflow:loss = 0.0063168216, step = 7901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 738.44\n",
            "INFO:tensorflow:loss = 0.0073803295, step = 8001 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.658\n",
            "INFO:tensorflow:loss = 0.0069994275, step = 8101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 785.993\n",
            "INFO:tensorflow:loss = 0.006606118, step = 8201 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 800.203\n",
            "INFO:tensorflow:loss = 0.007131857, step = 8301 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.701\n",
            "INFO:tensorflow:loss = 0.0052199373, step = 8401 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.077\n",
            "INFO:tensorflow:loss = 0.0064825504, step = 8501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 783.029\n",
            "INFO:tensorflow:loss = 0.008127022, step = 8601 (0.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.701\n",
            "INFO:tensorflow:loss = 0.004806741, step = 8701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.988\n",
            "INFO:tensorflow:loss = 0.00744632, step = 8801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.504\n",
            "INFO:tensorflow:loss = 0.009997434, step = 8901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 854.491\n",
            "INFO:tensorflow:loss = 0.005607001, step = 9001 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 766.815\n",
            "INFO:tensorflow:loss = 0.0056526205, step = 9101 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 707.979\n",
            "INFO:tensorflow:loss = 0.005712293, step = 9201 (0.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.834\n",
            "INFO:tensorflow:loss = 0.008318361, step = 9301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.746\n",
            "INFO:tensorflow:loss = 0.005554439, step = 9401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.448\n",
            "INFO:tensorflow:loss = 0.007335822, step = 9501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.62\n",
            "INFO:tensorflow:loss = 0.0053603277, step = 9601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.96\n",
            "INFO:tensorflow:loss = 0.006968491, step = 9701 (0.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 858.619\n",
            "INFO:tensorflow:loss = 0.008533341, step = 9801 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.068\n",
            "INFO:tensorflow:loss = 0.005887746, step = 9901 (0.101 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.008255527.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 97.94437967770767\n",
            "Just using average = 599.2808219178082 has RMSE of 102.93516150827149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_dewp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b180a40e-c5e3-4a30-dab9-2c1ed7cc7656",
        "id": "CmqqgLT09593"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0633140d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_dewp/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.50059634 0.47600403 0.5100045  0.538167   0.5501877  0.47055468\n",
            " 0.5253374  0.49427247 0.4663128  0.53107166 0.51078576 0.5553396\n",
            " 0.5104437  0.5738878  0.519115   0.47461873 0.44475165 0.52343017\n",
            " 0.5296347  0.4917729  0.48960534 0.45493123 0.4693856  0.48472452\n",
            " 0.5681601  0.48693624 0.47232756 0.4571172  0.56385714 0.54767275\n",
            " 0.5015362  0.5185676  0.50891745 0.5047301  0.49179333 0.5014119\n",
            " 0.53723615 0.5529622  0.498054   0.53834313 0.4897869  0.4926679\n",
            " 0.56658465 0.48149166 0.4754755  0.51804906 0.55515873 0.5401877\n",
            " 0.4655891  0.4660888  0.4817309  0.46763402 0.53577554 0.51789695\n",
            " 0.4939717  0.52303934 0.55928564 0.5311088  0.51918095 0.47887585\n",
            " 0.5129394  0.45448965 0.5673249  0.44762564 0.54415876 0.5492752\n",
            " 0.49934    0.48102468 0.52760416 0.46029854 0.52262765 0.49600267\n",
            " 0.5136431  0.5013631  0.48088166 0.55106974 0.4518191  0.49173287\n",
            " 0.53584874 0.4596553  0.52352434 0.5009683  0.48000833 0.53057534\n",
            " 0.48602706 0.49181232 0.49291882 0.571389   0.4901338  0.5111236\n",
            " 0.5283262  0.56211156 0.51690763 0.50787807 0.46374264 0.45098495\n",
            " 0.48723876 0.47862864 0.48137888 0.50344616 0.5189476  0.4453413\n",
            " 0.55330986 0.54492563 0.54045725 0.5426825  0.491931   0.49043268\n",
            " 0.5425645  0.47131848 0.5422722  0.49103147 0.53587407 0.48003602\n",
            " 0.53020215 0.5641664  0.5026859  0.48506984 0.44221383 0.46598554\n",
            " 0.4894733  0.55488944 0.55581385 0.5424676  0.5614842  0.5208733\n",
            " 0.5304849  0.5383282  0.5683533  0.5591131  0.5098232  0.49859235\n",
            " 0.53203696 0.5279347  0.54651105 0.5019058  0.5112197  0.45561126\n",
            " 0.49508104 0.48014936 0.49128172 0.54670966 0.48040894 0.49570468\n",
            " 0.46166214 0.56988484 0.4904596  0.48820612 0.46142155 0.46866402\n",
            " 0.5229694  0.54730546 0.49530458 0.53935176 0.4618157  0.50663877\n",
            " 0.52758783 0.43705758 0.54429615 0.5225402  0.4879333  0.4997688\n",
            " 0.49088717 0.4668296  0.5269633  0.5408696  0.54825974 0.46420768\n",
            " 0.5227581  0.44642586 0.54756194 0.522898   0.48791358 0.4885458\n",
            " 0.4685625  0.49952388 0.4766534  0.48645416 0.4686644  0.47951916\n",
            " 0.4716302  0.5310742  0.4671319  0.56682754 0.5533192  0.49967325\n",
            " 0.5073444  0.48354208 0.53608423 0.5357773  0.4993167  0.5534055\n",
            " 0.50764704 0.5133061  0.475533   0.504743   0.5293127  0.49873856\n",
            " 0.5160015  0.51895344 0.5433719  0.54194534 0.5027699  0.49831292\n",
            " 0.46486387 0.5141736  0.47704896 0.5609557  0.51343364 0.5366806\n",
            " 0.52507263 0.5655318  0.47408715 0.53608173 0.55151623 0.54354316\n",
            " 0.5536367  0.50085884 0.505407   0.49480683 0.4580595  0.5410546\n",
            " 0.48316488 0.4915798  0.4652336  0.47357708 0.5399538  0.49700746\n",
            " 0.54388493 0.52749056 0.52938455 0.5415969  0.5219305  0.50519294\n",
            " 0.55024856 0.51619345 0.51995414 0.49745926 0.44944683 0.51069003\n",
            " 0.4638445  0.5023     0.5043736  0.46290198 0.49227908 0.4976968\n",
            " 0.47658494 0.52531683 0.53199816 0.51123846 0.5003075  0.55283487\n",
            " 0.5059893  0.52415025 0.5162837  0.51610506 0.48428112 0.4850648\n",
            " 0.44192982 0.48464492 0.502203   0.51615536 0.5061706  0.4921058\n",
            " 0.50963604 0.5305858  0.46667725 0.55124885 0.4791443  0.5184634\n",
            " 0.4552551  0.50631016 0.49632072 0.4391078  0.506488   0.46429247\n",
            " 0.47669983 0.5600645  0.5101575  0.4651051  0.51994884 0.48673263\n",
            " 0.47813994 0.47890913 0.54516906 0.50358266 0.55508894 0.5082282\n",
            " 0.5535748  0.55691    0.4972997  0.50109607 0.5272607  0.4857502\n",
            " 0.53787273 0.47311637 0.5201558  0.46576896 0.5698204  0.5465735\n",
            " 0.56288713 0.44575256 0.5309542  0.52251184 0.4934852  0.5187905\n",
            " 0.5235136  0.5458634  0.50829047 0.50424355 0.5487934  0.5367567\n",
            " 0.5168606  0.47563037 0.43782285 0.5129363  0.4861802  0.46704236\n",
            " 0.49743316 0.52465856 0.56469995 0.48504943 0.50328887 0.45720083\n",
            " 0.48394513 0.48071536 0.4988367  0.5556889  0.4958524  0.5661062\n",
            " 0.48153666 0.54929155 0.567361   0.52950513 0.4999531  0.48121795\n",
            " 0.54948205 0.52536666 0.557775   0.49707934 0.53270406 0.51688147\n",
            " 0.52923155 0.4923012  0.49059767 0.54150575 0.5258506  0.55444866\n",
            " 0.5435539  0.47800222 0.4763167  0.4409356  0.54682404 0.4837922\n",
            " 0.5180625  0.46392918 0.49517915 0.564118   0.54101104 0.47231004\n",
            " 0.49996078 0.4542462  0.5311906  0.5110263  0.5041227  0.47602037\n",
            " 0.5000282  0.51810575 0.5058312  0.47471055 0.49820274 0.5397763\n",
            " 0.47575542 0.56317544 0.5445439  0.47380877 0.52576953 0.51388115\n",
            " 0.54454607 0.53148514 0.49287823 0.56604695 0.4915618  0.51658624\n",
            " 0.5525546  0.48505613 0.539754   0.56399536 0.48345962 0.53173494\n",
            " 0.46604905 0.5145934  0.552344   0.48474836 0.52856755 0.50444514\n",
            " 0.49440438 0.4962604  0.48423755 0.45603675 0.50134176 0.4834439\n",
            " 0.5211829  0.48925644 0.5224182  0.52838147 0.49455878 0.5390051\n",
            " 0.5105192  0.5275695  0.45285943 0.45842627 0.5353289  0.48173928\n",
            " 0.5454959  0.4856222  0.50201905 0.5074513  0.5095399  0.5674996\n",
            " 0.47317433 0.55528694 0.5172383  0.56252205 0.53219295 0.5296133\n",
            " 0.52563757 0.48054793 0.53141683 0.4997247  0.53980666 0.4461435\n",
            " 0.45576072 0.45158014 0.5529405  0.54731494 0.5188038  0.5497484\n",
            " 0.54611886 0.5601068  0.48035267 0.52924573 0.5188554  0.5197567\n",
            " 0.5550495  0.53522694 0.48314765 0.50955695 0.55357283 0.50106543\n",
            " 0.48212558 0.5277179  0.51471454 0.50973153 0.5209693  0.4925732\n",
            " 0.4573995  0.4975994  0.511986   0.5483835  0.4728723  0.48935446\n",
            " 0.5063182  0.4991103  0.4510973  0.56010175 0.48502508 0.5028974\n",
            " 0.46014905 0.56582206 0.51321363 0.52961755 0.49015594 0.49570104\n",
            " 0.5277003  0.49663442 0.52577657 0.4894201  0.5355034  0.4386656\n",
            " 0.49463528 0.45851123 0.5527945  0.5288001  0.52301705 0.47762492\n",
            " 0.52970546 0.5155676  0.5076682  0.5192324  0.53945404 0.48640484\n",
            " 0.4875043  0.47460997 0.45763847 0.4915073  0.47926983 0.5229187\n",
            " 0.50799894 0.51844084 0.46711034 0.44717723 0.45138136 0.5069083\n",
            " 0.48324123 0.556896   0.52974445 0.5559876  0.5256458  0.5209901\n",
            " 0.4872071 ]\n",
            "[0.51937984 0.43238587 0.37639966 0.58914729 0.43583118 0.4005168\n",
            " 0.44788975 0.55555556 0.51851852 0.42894057 0.63221361 0.54091301\n",
            " 0.50301464 0.47028424 0.57364341 0.40310078 0.82773471 0.4754522\n",
            " 0.47975883 0.45305771 0.54694229 0.39362618 0.45564169 0.48492679\n",
            " 0.61929371 0.49009475 0.72265289 0.44272179 0.53402239 0.58742463\n",
            " 0.68130922 0.6416882  0.54521964 0.59173127 0.41860465 0.45908699\n",
            " 0.5374677  0.51248923 0.72437554 0.56589147 0.43669251 0.50990525\n",
            " 0.5796727  0.48148148 0.44358312 0.54952627 0.62360034 0.55297158\n",
            " 0.45736434 0.46511628 0.52282515 0.44444444 0.54952627 0.53919035\n",
            " 0.53057709 0.46425495 0.57364341 0.51851852 0.46167097 0.43841516\n",
            " 0.45391904 0.40482343 0.42204996 0.43152455 0.53574505 0.53660637\n",
            " 0.65202412 0.43496985 0.52799311 0.39793282 0.50559862 0.65891473\n",
            " 0.57881137 0.50387597 0.45391904 0.59259259 0.39276486 0.45219638\n",
            " 0.69336779 0.50732127 0.53832903 0.54091301 0.60120586 0.6089578\n",
            " 0.44099914 0.54263566 0.57278208 0.4952627  0.36606374 0.66494401\n",
            " 0.52971576 0.5047373  0.65546942 0.61326443 0.39190353 0.42807924\n",
            " 0.59431525 0.45822567 0.41515935 0.16192937 0.63135228 0.39276486\n",
            " 0.53402239 0.49870801 0.54608096 0.5503876  0.62273902 0.4754522\n",
            " 0.62790698 0.48148148 0.52885444 0.40913006 0.46339363 0.51937984\n",
            " 0.5538329  0.57536606 0.47372954 0.46683893 0.37553833 0.47028424\n",
            " 0.38242894 0.49698536 0.50301464 0.52024117 0.47459087 0.54349699\n",
            " 0.5994832  0.47028424 0.57881137 0.60378984 0.64513351 0.52971576\n",
            " 0.50559862 0.374677   0.64944014 0.43066322 0.44702842 0.37812231\n",
            " 0.42291128 0.44272179 0.52024117 0.60981912 0.41774332 0.49354005\n",
            " 0.50559862 0.36778639 0.47372954 0.43583118 0.46080965 0.4918174\n",
            " 0.46425495 0.55297158 0.70111972 0.56072351 0.56158484 0.54091301\n",
            " 0.47286822 0.37209302 0.59862188 0.52540913 0.3910422  0.60465116\n",
            " 0.76141258 0.34280792 0.46080965 0.33936262 0.38070629 0.50215332\n",
            " 0.42894057 0.45994832 0.51593454 0.41257537 0.47372954 0.60378984\n",
            " 0.43066322 0.4952627  0.36175711 0.58484065 0.41946598 0.51335056\n",
            " 0.35228252 0.54349699 0.40137812 0.60034453 0.56503015 0.56589147\n",
            " 0.66408269 0.38587425 0.47200689 0.54263566 0.54349699 0.60034453\n",
            " 0.57536606 0.60292851 0.30060293 0.34280792 0.59086994 0.46597761\n",
            " 0.45478036 0.58225668 0.53229974 0.4496124  0.45305771 0.63307494\n",
            " 0.33936262 0.54866494 0.53143842 0.5960379  0.61757106 0.54005168\n",
            " 0.53574505 0.50387597 0.52713178 0.60034453 0.47631352 0.42204996\n",
            " 0.51765719 0.42894057 0.49870801 0.44099914 0.45047373 0.54694229\n",
            " 0.43755383 0.48406546 0.47200689 0.50904393 0.4918174  0.65977606\n",
            " 0.53402239 0.63652024 0.5211025  0.65030146 0.47459087 0.54521964\n",
            " 0.60206718 0.49354005 0.52368648 0.58656331 0.40654608 0.58656331\n",
            " 0.49440138 0.66666667 0.55986219 0.80878553 0.60723514 0.49698536\n",
            " 0.52024117 0.42721792 0.52024117 0.4918174  0.46511628 0.54608096\n",
            " 0.54177433 0.64513351 0.47631352 0.60206718 0.52799311 0.44099914\n",
            " 0.43066322 0.51765719 0.39018088 0.72782084 0.38845823 0.5960379\n",
            " 0.51765719 0.67011197 0.38242894 0.40568475 0.583118   0.22739018\n",
            " 0.48923342 0.63910422 0.55641688 0.39965547 0.56847545 0.44702842\n",
            " 0.41257537 0.66149871 0.52282515 0.43496985 0.60378984 0.48406546\n",
            " 0.47975883 0.48148148 0.48751077 0.53402239 0.45047373 0.48664944\n",
            " 0.57450474 0.63996555 0.53574505 0.54349699 0.5667528  0.44702842\n",
            " 0.61929371 0.49784668 0.61154177 0.38845823 0.53143842 0.56589147\n",
            " 0.55986219 0.53488372 0.74677003 0.5503876  0.45994832 0.71231697\n",
            " 0.6124031  0.53316107 0.48923342 0.51593454 0.53574505 0.63393626\n",
            " 0.52540913 0.52196382 0.34969854 0.61929371 0.50387597 0.4005168\n",
            " 0.5081826  0.61757106 0.34453058 0.45908699 0.65719208 0.48664944\n",
            " 0.49956934 0.4461671  0.60465116 0.59517657 0.49784668 0.62618432\n",
            " 0.52627046 0.53057709 0.36089578 0.59431525 0.47459087 0.41946598\n",
            " 0.50732127 0.55297158 0.58570198 0.55900086 0.52196382 0.45564169\n",
            " 0.5374677  0.51162791 0.56847545 0.54263566 0.4918174  0.5994832\n",
            " 0.5667528  0.49956934 0.42204996 0.41257537 0.48406546 0.34366925\n",
            " 0.59086994 0.48492679 0.52799311 0.63652024 0.31955211 0.41946598\n",
            " 0.54177433 0.43927649 0.56416882 0.68044789 0.60292851 0.53919035\n",
            " 0.63479759 0.63652024 0.51593454 0.44186047 0.6546081  0.5994832\n",
            " 0.44530577 0.5796727  0.49009475 0.45219638 0.58570198 0.63135228\n",
            " 0.60637382 0.48062016 0.47459087 0.43152455 0.54005168 0.71490095\n",
            " 0.60809647 0.43927649 0.64427218 0.52885444 0.51851852 0.62704565\n",
            " 0.40482343 0.54694229 0.49956934 0.50732127 0.58484065 0.64513351\n",
            " 0.66838932 0.41343669 0.48234281 0.47114556 0.76055125 0.34969854\n",
            " 0.49267873 0.42377261 0.49095607 0.48751077 0.50645995 0.58656331\n",
            " 0.27648579 0.5081826  0.37898363 0.55469423 0.59431525 0.40999139\n",
            " 0.53229974 0.47975883 0.35486649 0.3255814  0.5047373  0.54521964\n",
            " 0.40654608 0.59345392 0.48923342 0.48751077 0.44875108 0.54091301\n",
            " 0.65202412 0.50215332 0.64082687 0.51765719 0.47459087 0.45822567\n",
            " 0.44875108 0.48923342 0.52971576 0.61068045 0.53660637 0.5538329\n",
            " 0.54694229 0.62187769 0.46167097 0.54694229 0.51765719 0.53057709\n",
            " 0.49440138 0.54866494 0.47286822 0.64857881 0.38845823 0.47631352\n",
            " 0.46339363 0.33936262 0.44358312 0.54177433 0.55900086 0.48664944\n",
            " 0.48320413 0.62360034 0.51851852 0.62015504 0.45908699 0.583118\n",
            " 0.53488372 0.54866494 0.39534884 0.50990525 0.46511628 0.42635659\n",
            " 0.43669251 0.51507321 0.48492679 0.55986219 0.6546081  0.38931955\n",
            " 0.48234281 0.46511628 0.51507321 0.59776055 0.53229974 0.36089578\n",
            " 0.56072351 0.50990525 0.56503015 0.55986219 0.4918174  0.40913006\n",
            " 0.65719208 0.68647717 0.51421189 0.48148148 0.64427218 0.47631352\n",
            " 0.39793282 0.59000861 0.36864772 0.42721792 0.52799311 0.62015504\n",
            " 0.63910422 0.61068045 0.56158484 0.46597761 0.48406546 0.60551249\n",
            " 0.47975883 0.5374677  0.55727821 0.48234281 0.48148148 0.64857881\n",
            " 0.55727821]\n",
            "The trained model has an aproximate error rate of 8.037786547101874 which equates to 1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure (SLP)\n"
      ],
      "metadata": {
        "id": "O60cqN0x90SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f12f9fb-6404-4537-ac8b-be61dbbcc1e5",
        "id": "u-aXoGB4_v4s"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp = df.drop(columns=['collision_date', 'temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] != 2012]\n",
        "df_slp = df_slp.loc[df_slp[\"year\"] < 2020]\n",
        "cols = df_slp['NUM_COLLISIONS']\n",
        "df_slp = df_slp.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp[:6])\n",
        "df_slp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "cd3b2c7e-e0d8-4b49-b579-86e02dc6bd38",
        "id": "xsAYkW7-ATbk"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da     slp  NUM_COLLISIONS\n",
            "49    4  2016   1  28  1016.1             681\n",
            "51    5  2014   1  17  1014.8             589\n",
            "54    1  2016   1  25  1021.4             658\n",
            "55    5  2016   1  29   999.4             645\n",
            "58    5  2017   1  20  1015.5             605\n",
            "59    7  2013   1  13  1020.7             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da          slp  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      3.999217  2016.000391     6.522114    15.719765  1016.777221   \n",
              "std       2.000783     2.000294     3.447986     8.796698     7.628429   \n",
              "min       1.000000  2013.000000     1.000000     1.000000   989.500000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000  1012.200000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000  1016.700000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000  1021.700000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000  1044.200000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2555.000000  \n",
              "mean       599.147162  \n",
              "std        100.268048  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b55bcb5-bcbd-4013-83fa-27f73f0b95e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999217</td>\n",
              "      <td>2016.000391</td>\n",
              "      <td>6.522114</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000783</td>\n",
              "      <td>2.000294</td>\n",
              "      <td>3.447986</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b55bcb5-bcbd-4013-83fa-27f73f0b95e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b55bcb5-bcbd-4013-83fa-27f73f0b95e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b55bcb5-bcbd-4013-83fa-27f73f0b95e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp.iloc[np.random.permutation(len(df_slp))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377ba53b-5642-4036-b0f0-1670014dd959",
        "id": "I-drEIq1ATbu"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da     slp\n",
            "1758    4  2019   6   6  1007.5\n",
            "831     3  2016   3  16  1007.8\n",
            "174     1  2017   1   9  1035.5\n",
            "2858    1  2014  10   6  1018.6\n",
            "2385    4  2016   8  11  1018.5\n",
            "576     3  2015   2  11  1014.0\n",
            "      day  year  mo  da     slp  NUM_COLLISIONS\n",
            "1758    4  2019   6   6  1007.5             744\n",
            "831     3  2016   3  16  1007.8             629\n",
            "174     1  2017   1   9  1035.5             669\n",
            "2858    1  2014  10   6  1018.6             639\n",
            "2385    4  2016   8  11  1018.5             672\n",
            "576     3  2015   2  11  1014.0             610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31bd353-7f02-4ba8-9285-9461c5f880d9",
        "id": "pq6BF1zLATbv"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1758    744\n",
            "831     629\n",
            "174     669\n",
            "2858    639\n",
            "2385    672\n",
            "576     610\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a60565-f8ed-4a0d-bb31-c8adcb1f9e6e",
        "id": "pVdp0YKeATbv"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2044\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b33f0ca-bc19-4a33-a082-cfa1465f1fb9",
        "id": "twTCPAjdATbv"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff065487b10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.28725958, step = 1\n",
            "INFO:tensorflow:global_step/sec: 493.866\n",
            "INFO:tensorflow:loss = 0.007920365, step = 101 (0.205 sec)\n",
            "INFO:tensorflow:global_step/sec: 684.571\n",
            "INFO:tensorflow:loss = 0.008888739, step = 201 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 875.031\n",
            "INFO:tensorflow:loss = 0.009648033, step = 301 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 614.28\n",
            "INFO:tensorflow:loss = 0.007889533, step = 401 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.63\n",
            "INFO:tensorflow:loss = 0.006255895, step = 501 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.949\n",
            "INFO:tensorflow:loss = 0.006308317, step = 601 (0.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 424.679\n",
            "INFO:tensorflow:loss = 0.0069407383, step = 701 (0.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 582.716\n",
            "INFO:tensorflow:loss = 0.008436618, step = 801 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 823.032\n",
            "INFO:tensorflow:loss = 0.0060538193, step = 901 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 805.559\n",
            "INFO:tensorflow:loss = 0.007408769, step = 1001 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.665\n",
            "INFO:tensorflow:loss = 0.0075543644, step = 1101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 782.302\n",
            "INFO:tensorflow:loss = 0.0069699627, step = 1201 (0.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.419\n",
            "INFO:tensorflow:loss = 0.006813809, step = 1301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1062.52\n",
            "INFO:tensorflow:loss = 0.008869404, step = 1401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 605.763\n",
            "INFO:tensorflow:loss = 0.0074498234, step = 1501 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 845.906\n",
            "INFO:tensorflow:loss = 0.00793185, step = 1601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.865\n",
            "INFO:tensorflow:loss = 0.006346326, step = 1701 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 1039.11\n",
            "INFO:tensorflow:loss = 0.0072919815, step = 1801 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1063.07\n",
            "INFO:tensorflow:loss = 0.007379775, step = 1901 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 651.411\n",
            "INFO:tensorflow:loss = 0.0049579544, step = 2001 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 867.885\n",
            "INFO:tensorflow:loss = 0.007863396, step = 2101 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 674.358\n",
            "INFO:tensorflow:loss = 0.0073533654, step = 2201 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 578.779\n",
            "INFO:tensorflow:loss = 0.0058193663, step = 2301 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.462\n",
            "INFO:tensorflow:loss = 0.007883491, step = 2401 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.101\n",
            "INFO:tensorflow:loss = 0.0061507635, step = 2501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.727\n",
            "INFO:tensorflow:loss = 0.008365985, step = 2601 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 1041.62\n",
            "INFO:tensorflow:loss = 0.0065710293, step = 2701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1114.97\n",
            "INFO:tensorflow:loss = 0.006540839, step = 2801 (0.090 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.893\n",
            "INFO:tensorflow:loss = 0.005874497, step = 2901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 1021.97\n",
            "INFO:tensorflow:loss = 0.008024303, step = 3001 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.307\n",
            "INFO:tensorflow:loss = 0.0065793386, step = 3101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1059.95\n",
            "INFO:tensorflow:loss = 0.010714421, step = 3201 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.49\n",
            "INFO:tensorflow:loss = 0.0064890375, step = 3301 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1084.15\n",
            "INFO:tensorflow:loss = 0.0066968794, step = 3401 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1008.87\n",
            "INFO:tensorflow:loss = 0.007401669, step = 3501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.935\n",
            "INFO:tensorflow:loss = 0.0077625206, step = 3601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.539\n",
            "INFO:tensorflow:loss = 0.009032607, step = 3701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.918\n",
            "INFO:tensorflow:loss = 0.0077644214, step = 3801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.5\n",
            "INFO:tensorflow:loss = 0.008379652, step = 3901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 935.681\n",
            "INFO:tensorflow:loss = 0.00552513, step = 4001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 899.712\n",
            "INFO:tensorflow:loss = 0.0062355967, step = 4101 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 850.953\n",
            "INFO:tensorflow:loss = 0.007838776, step = 4201 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.553\n",
            "INFO:tensorflow:loss = 0.0058135246, step = 4301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.262\n",
            "INFO:tensorflow:loss = 0.006465271, step = 4401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.881\n",
            "INFO:tensorflow:loss = 0.011078891, step = 4501 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.512\n",
            "INFO:tensorflow:loss = 0.0062504797, step = 4601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 890.374\n",
            "INFO:tensorflow:loss = 0.007220138, step = 4701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.329\n",
            "INFO:tensorflow:loss = 0.0061707105, step = 4801 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 938.966\n",
            "INFO:tensorflow:loss = 0.008056626, step = 4901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.394\n",
            "INFO:tensorflow:loss = 0.0068502333, step = 5001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.174\n",
            "INFO:tensorflow:loss = 0.0067769797, step = 5101 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1075.36\n",
            "INFO:tensorflow:loss = 0.0064849206, step = 5201 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1367.14\n",
            "INFO:tensorflow:loss = 0.0058374135, step = 5301 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1355.84\n",
            "INFO:tensorflow:loss = 0.0077444664, step = 5401 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1414.71\n",
            "INFO:tensorflow:loss = 0.006267134, step = 5501 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1457.83\n",
            "INFO:tensorflow:loss = 0.006769072, step = 5601 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1431.82\n",
            "INFO:tensorflow:loss = 0.0059734713, step = 5701 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1324\n",
            "INFO:tensorflow:loss = 0.0055983113, step = 5801 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1413.59\n",
            "INFO:tensorflow:loss = 0.010179228, step = 5901 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1459.64\n",
            "INFO:tensorflow:loss = 0.007645692, step = 6001 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1354.5\n",
            "INFO:tensorflow:loss = 0.0062238816, step = 6101 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1397.61\n",
            "INFO:tensorflow:loss = 0.0070707714, step = 6201 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1359.95\n",
            "INFO:tensorflow:loss = 0.0072211893, step = 6301 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1404.65\n",
            "INFO:tensorflow:loss = 0.006115356, step = 6401 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.33\n",
            "INFO:tensorflow:loss = 0.006930174, step = 6501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1508.27\n",
            "INFO:tensorflow:loss = 0.0060022995, step = 6601 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1441.18\n",
            "INFO:tensorflow:loss = 0.007164014, step = 6701 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1465.53\n",
            "INFO:tensorflow:loss = 0.007078684, step = 6801 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1398.24\n",
            "INFO:tensorflow:loss = 0.0062662363, step = 6901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1443.68\n",
            "INFO:tensorflow:loss = 0.0071398923, step = 7001 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1321.5\n",
            "INFO:tensorflow:loss = 0.0060409205, step = 7101 (0.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 1425.96\n",
            "INFO:tensorflow:loss = 0.005367446, step = 7201 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1357.6\n",
            "INFO:tensorflow:loss = 0.008559659, step = 7301 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.42\n",
            "INFO:tensorflow:loss = 0.006299106, step = 7401 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1345.83\n",
            "INFO:tensorflow:loss = 0.0054250155, step = 7501 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1311.35\n",
            "INFO:tensorflow:loss = 0.0054674754, step = 7601 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1417.05\n",
            "INFO:tensorflow:loss = 0.0069969767, step = 7701 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1435.56\n",
            "INFO:tensorflow:loss = 0.0060548084, step = 7801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1295.66\n",
            "INFO:tensorflow:loss = 0.0077125556, step = 7901 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1474.9\n",
            "INFO:tensorflow:loss = 0.007570415, step = 8001 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1353.13\n",
            "INFO:tensorflow:loss = 0.00698348, step = 8101 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1439.38\n",
            "INFO:tensorflow:loss = 0.006040477, step = 8201 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1426.78\n",
            "INFO:tensorflow:loss = 0.0073334007, step = 8301 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1390.74\n",
            "INFO:tensorflow:loss = 0.012713883, step = 8401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1482.52\n",
            "INFO:tensorflow:loss = 0.0057801288, step = 8501 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1457.82\n",
            "INFO:tensorflow:loss = 0.0063738446, step = 8601 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1341.96\n",
            "INFO:tensorflow:loss = 0.007722075, step = 8701 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1447.18\n",
            "INFO:tensorflow:loss = 0.0077673024, step = 8801 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1422.53\n",
            "INFO:tensorflow:loss = 0.010243682, step = 8901 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1287.45\n",
            "INFO:tensorflow:loss = 0.0058124694, step = 9001 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1471.5\n",
            "INFO:tensorflow:loss = 0.005410348, step = 9101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1411.67\n",
            "INFO:tensorflow:loss = 0.011598472, step = 9201 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1362.99\n",
            "INFO:tensorflow:loss = 0.0067141494, step = 9301 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1380.49\n",
            "INFO:tensorflow:loss = 0.0072992276, step = 9401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1389.59\n",
            "INFO:tensorflow:loss = 0.008825903, step = 9501 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1305.36\n",
            "INFO:tensorflow:loss = 0.0074112946, step = 9601 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1417.41\n",
            "INFO:tensorflow:loss = 0.0057009812, step = 9701 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1367.46\n",
            "INFO:tensorflow:loss = 0.006242795, step = 9801 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1441.04\n",
            "INFO:tensorflow:loss = 0.005983148, step = 9901 (0.072 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.006275372.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 95.13112435157943\n",
            "Just using average = 598.6340508806262 has RMSE of 101.05888584014674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_slp', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d904d615-f615-4883-9ca0-2e2ceac419c0",
        "id": "RJS2QjMpATbw"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0654e6b50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.52682537 0.49981183 0.50644207 0.44715485 0.48286563 0.53737247\n",
            " 0.5141071  0.5589524  0.52432066 0.54054755 0.525875   0.49438748\n",
            " 0.5254183  0.51265687 0.51494324 0.4689978  0.53749907 0.49513215\n",
            " 0.5295475  0.45016575 0.53822476 0.4784605  0.51097596 0.49904215\n",
            " 0.52671385 0.50889647 0.48168465 0.5588118  0.51342297 0.5197076\n",
            " 0.51154536 0.4758883  0.45808664 0.50623    0.5348296  0.52293646\n",
            " 0.4867742  0.4672252  0.49655572 0.49356687 0.5587935  0.5211494\n",
            " 0.5678288  0.48533905 0.5426654  0.5393282  0.51707697 0.4684079\n",
            " 0.52812093 0.5529818  0.5123113  0.47001424 0.47784555 0.53001744\n",
            " 0.48605138 0.48843557 0.5167751  0.5278232  0.52132607 0.50046384\n",
            " 0.5042566  0.51408064 0.4951198  0.5463461  0.4888631  0.51103014\n",
            " 0.53763545 0.54957104 0.53674024 0.5571053  0.5411252  0.48911384\n",
            " 0.48324132 0.51716036 0.5345853  0.51393646 0.48754728 0.55031455\n",
            " 0.49590802 0.46609488 0.5654458  0.5165404  0.4954292  0.54154015\n",
            " 0.5510669  0.48657876 0.52368623 0.51143825 0.56101054 0.49921453\n",
            " 0.50749576 0.46936852 0.5038338  0.5083013  0.47784013 0.5375195\n",
            " 0.5401259  0.52691543 0.48937497 0.5513232  0.544188   0.507846\n",
            " 0.5279746  0.50267    0.507763   0.49340189 0.52140415 0.52372795\n",
            " 0.51966214 0.51801556 0.5304637  0.53432393 0.49743563 0.4746858\n",
            " 0.50470644 0.5137575  0.4961607  0.5201046  0.4971014  0.4860309\n",
            " 0.49747986 0.52335924 0.5245381  0.46649173 0.5293164  0.514379\n",
            " 0.47799265 0.5249556  0.47560754 0.49469152 0.50694877 0.486554\n",
            " 0.5563534  0.49318594 0.5399513  0.5040701  0.54987836 0.54440886\n",
            " 0.49117556 0.5356957  0.459972   0.49326953 0.48589072 0.5245698\n",
            " 0.48728544 0.5259405  0.45582882 0.52274203 0.5426087  0.5295746\n",
            " 0.5205302  0.46786413 0.5694415  0.47682893 0.5133217  0.5162408\n",
            " 0.48250708 0.5530049  0.5423025  0.52432835 0.48274547 0.5043006\n",
            " 0.46927503 0.5519006  0.50727403 0.48771796 0.45723003 0.53142583\n",
            " 0.4898882  0.46208006 0.5347452  0.5004082  0.45058283 0.5575188\n",
            " 0.4606944  0.5630825  0.49383304 0.45633537 0.48752183 0.4806498\n",
            " 0.45636192 0.5407945  0.5242144  0.5029454  0.5106008  0.5529737\n",
            " 0.53741485 0.540264   0.5056204  0.49473822 0.55777186 0.5114192\n",
            " 0.51828253 0.45896298 0.4794695  0.4814746  0.5488977  0.49523872\n",
            " 0.51118004 0.48926112 0.5243071  0.46734756 0.55161613 0.5052823\n",
            " 0.48231077 0.56314963 0.4769204  0.51214856 0.53917116 0.50143\n",
            " 0.5152486  0.46593654 0.5067653  0.50732195 0.47349864 0.47406876\n",
            " 0.5406723  0.5408722  0.51820356 0.5350205  0.5338438  0.5054954\n",
            " 0.53327686 0.5249001  0.49358302 0.5236651  0.5088116  0.5256146\n",
            " 0.51458734 0.5342964  0.5013021  0.5256393  0.5578812  0.5411677\n",
            " 0.48275113 0.47656432 0.48875737 0.50262946 0.5113299  0.54029983\n",
            " 0.5370662  0.52303404 0.53657204 0.5426954  0.5021565  0.51305336\n",
            " 0.50761974 0.54411376 0.5351224  0.4932359  0.53020334 0.47934347\n",
            " 0.4632785  0.5166808  0.5156712  0.5281822  0.5427924  0.5070422\n",
            " 0.49174738 0.5027625  0.4764134  0.4959809  0.4869105  0.5125184\n",
            " 0.48201472 0.45462206 0.46659207 0.5190097  0.54127496 0.54093236\n",
            " 0.55639213 0.52960026 0.4875948  0.48730838 0.5466023  0.48268902\n",
            " 0.49052265 0.5119015  0.4984666  0.4680734  0.52850986 0.523429\n",
            " 0.5399287  0.5213351  0.4896852  0.4505704  0.55052215 0.49325362\n",
            " 0.5088061  0.45761874 0.5047766  0.5007494  0.54235166 0.5308358\n",
            " 0.51297    0.49340326 0.5449973  0.4758288  0.56124854 0.53548896\n",
            " 0.49357942 0.54919815 0.4607918  0.5486899  0.5138015  0.46682113\n",
            " 0.525921   0.4809137  0.5151658  0.46688566 0.49033037 0.47850534\n",
            " 0.5114713  0.51028895 0.5597596  0.5398987  0.49773404 0.51361763\n",
            " 0.49736118 0.54331905 0.4866956  0.5524895  0.5484982  0.5372698\n",
            " 0.47247002 0.5395788  0.46187243 0.51057917 0.5222374  0.46895108\n",
            " 0.4611359  0.53187126 0.4834159  0.49025658 0.49139744 0.46567193\n",
            " 0.4554014  0.51889634 0.50400406 0.483118   0.5275412  0.55309796\n",
            " 0.55429226 0.5363011  0.5357177  0.5165501  0.50836456 0.5274677\n",
            " 0.51302534 0.520158   0.49079826 0.5580251  0.50471056 0.52805805\n",
            " 0.48801616 0.50831383 0.5089203  0.44902793 0.5299266  0.5047952\n",
            " 0.5512424  0.47052115 0.49926424 0.5564056  0.4886972  0.49324977\n",
            " 0.53545254 0.47409993 0.47463855 0.51071507 0.53043205 0.5511625\n",
            " 0.4679569  0.5151184  0.4764884  0.49482813 0.49463287 0.555452\n",
            " 0.52024776 0.5128714  0.46596476 0.5495549  0.47379875 0.48706257\n",
            " 0.4695196  0.54459816 0.4998148  0.5099135  0.48248136 0.46175745\n",
            " 0.4561801  0.49459732 0.47975776 0.50390846 0.5388101  0.52530336\n",
            " 0.52514154 0.5108351  0.48761535 0.52030885 0.5168307  0.4891705\n",
            " 0.49312276 0.49531218 0.4980175  0.5404733  0.5272166  0.5329577\n",
            " 0.5235012  0.46906903 0.48104092 0.51151806 0.45299459 0.496554\n",
            " 0.52414036 0.54122543 0.48674685 0.489753   0.48101476 0.5299548\n",
            " 0.44280085 0.5146583  0.47454944 0.5257776  0.49115992 0.49433067\n",
            " 0.54362094 0.5316665  0.56932545 0.50150067 0.53710604 0.53793573\n",
            " 0.5220575  0.5347327  0.49437517 0.53765786 0.48180264 0.56714535\n",
            " 0.5161973  0.539973   0.4920925  0.5471321  0.53320426 0.4459155\n",
            " 0.47898805 0.48664716 0.5361093  0.49063912 0.51251554 0.51925564\n",
            " 0.53208905 0.53321934 0.5512627  0.4658417  0.5100968  0.55055124\n",
            " 0.5360339  0.47402766 0.45536593 0.47113115 0.53299826 0.4625949\n",
            " 0.48690796 0.4822922  0.5468036  0.52543765 0.5332149  0.4920755\n",
            " 0.4910791  0.50061613 0.4778867  0.47512132 0.5123606  0.49976468\n",
            " 0.56092316 0.47812328 0.50557274 0.52923656 0.54549146 0.48518762\n",
            " 0.5348475  0.56320775 0.49199778 0.5512661  0.4885201  0.4900349\n",
            " 0.5129405  0.52017087 0.47604135 0.5135745  0.52366155 0.4910155\n",
            " 0.5512703  0.4938958  0.54493076 0.53997415 0.50457627 0.523989\n",
            " 0.53401583 0.5181851  0.5020617  0.516569   0.47543862 0.4703346\n",
            " 0.47792917 0.52366763 0.5447627  0.5383502  0.47271279 0.4638369\n",
            " 0.54550123]\n",
            "[0.49009475 0.48062016 0.52799311 0.44702842 0.53402239 0.52282515\n",
            " 0.54694229 0.4952627  0.50215332 0.53574505 0.52196382 0.54349699\n",
            " 0.51937984 0.51593454 0.57622739 0.49009475 0.58397933 0.51421189\n",
            " 0.53229974 0.34969854 0.50043066 0.44444444 0.52799311 0.54263566\n",
            " 0.6416882  0.63738157 0.49095607 0.50215332 0.53229974 0.50559862\n",
            " 0.55986219 0.40310078 0.42204996 0.46770026 0.46683893 0.42204996\n",
            " 0.44272179 0.42635659 0.64427218 0.54952627 0.48148148 0.53229974\n",
            " 0.52799311 0.54349699 0.54091301 0.56761413 0.49612403 0.60551249\n",
            " 0.5667528  0.5796727  0.44186047 0.50215332 0.56072351 0.5374677\n",
            " 0.33074935 0.48234281 0.61412575 0.55124892 0.53402239 0.54263566\n",
            " 0.42291128 0.48751077 0.64685616 0.46770026 0.68819983 0.74677003\n",
            " 0.74677003 0.6089578  0.44186047 0.61326443 0.43583118 0.56847545\n",
            " 0.416882   0.44444444 0.54005168 0.50559862 0.58570198 0.37898363\n",
            " 0.42377261 0.53832903 0.40999139 0.49009475 0.51765719 0.38070629\n",
            " 0.51248923 0.44099914 0.58828596 0.43238587 0.46425495 0.4918174\n",
            " 0.45391904 0.54005168 0.45822567 0.53488372 0.38070629 0.50559862\n",
            " 0.67011197 0.45564169 0.41860465 0.625323   0.52282515 0.58828596\n",
            " 0.4952627  0.5960379  0.56158484 0.52368648 0.60809647 0.41257537\n",
            " 0.53832903 0.51937984 0.52368648 0.60981912 0.48923342 0.4754522\n",
            " 0.46080965 0.47372954 0.40568475 0.51593454 0.48664944 0.42204996\n",
            " 0.63910422 0.48492679 0.68217054 0.42635659 0.62015504 0.45822567\n",
            " 0.44272179 0.52971576 0.45219638 0.55297158 0.57105943 0.59431525\n",
            " 0.52282515 0.59173127 0.4203273  0.31007752 0.56416882 0.55641688\n",
            " 0.5047373  0.59689922 0.38329027 0.52713178 0.5245478  0.38931955\n",
            " 0.51421189 0.57278208 0.40568475 0.60551249 0.51851852 0.55555556\n",
            " 0.65719208 0.49784668 0.68130922 0.41429802 0.61929371 0.65977606\n",
            " 0.58656331 0.53057709 0.4039621  0.5374677  0.46683893 0.48492679\n",
            " 0.48148148 0.60378984 0.54608096 0.39793282 0.36692506 0.54866494\n",
            " 0.45219638 0.47286822 0.37898363 0.49870801 0.35658915 0.63652024\n",
            " 0.50129199 0.55211025 0.50645995 0.39793282 0.40654608 0.43755383\n",
            " 0.35486649 0.49698536 0.48923342 0.60378984 0.57364341 0.60120586\n",
            " 0.48062016 0.5374677  0.48664944 0.38931955 0.46425495 1.\n",
            " 0.67011197 0.38673557 0.48492679 0.45650301 0.53229974 0.48751077\n",
            " 0.44186047 0.44099914 0.57450474 0.43669251 0.56416882 0.63910422\n",
            " 0.40826873 0.45822567 0.55297158 0.49698536 0.54177433 0.64082687\n",
            " 0.68647717 0.60723514 0.64944014 0.64341085 0.60637382 0.44875108\n",
            " 0.56589147 0.55727821 0.60723514 0.58225668 0.58570198 0.60292851\n",
            " 0.45908699 0.54694229 0.38587425 0.42291128 0.62015504 0.53574505\n",
            " 0.6287683  0.50990525 0.72265289 0.583118   0.6124031  0.49698536\n",
            " 0.40482343 0.55900086 0.41860465 0.54091301 0.54521964 0.52971576\n",
            " 0.49956934 0.49095607 0.54694229 0.56847545 0.63479759 0.58656331\n",
            " 0.53143842 0.5667528  0.59086994 0.4788975  0.4952627  0.39018088\n",
            " 0.416882   0.54349699 0.33936262 0.44875108 0.63135228 0.625323\n",
            " 0.42894057 0.63996555 0.50904393 0.68303187 0.44702842 0.46511628\n",
            " 0.49095607 0.46856158 0.45305771 0.54091301 0.47975883 0.53057709\n",
            " 0.50904393 0.2213609  0.45822567 0.55641688 0.63049096 0.43238587\n",
            " 0.4461671  0.63135228 0.59259259 0.49698536 0.6744186  0.61670973\n",
            " 0.56933678 0.67355728 0.36175711 0.35745047 0.51507321 0.47975883\n",
            " 0.55297158 0.36089578 0.43583118 0.50990525 0.5796727  0.36175711\n",
            " 0.53229974 0.72782084 0.63221361 0.39190353 0.54694229 0.50129199\n",
            " 0.59000861 0.60465116 0.34022394 0.50990525 0.55555556 0.53574505\n",
            " 0.39534884 0.47372954 0.55813953 0.38673557 0.34022394 0.44358312\n",
            " 0.59086994 0.51679587 0.60034453 0.52971576 0.56244617 0.52971576\n",
            " 0.55124892 0.60378984 0.53660637 0.62790698 0.63049096 0.52971576\n",
            " 0.49095607 0.63652024 0.42980189 0.55297158 0.62704565 0.47372954\n",
            " 0.37726098 0.51679587 0.66063738 0.41602067 0.69853575 0.39362618\n",
            " 0.36003445 0.55813953 0.64513351 0.54866494 0.58139535 0.55641688\n",
            " 0.56933678 0.49956934 0.42894057 0.55727821 0.66925065 0.59862188\n",
            " 0.44702842 0.55555556 0.66666667 0.52713178 0.49440138 0.55641688\n",
            " 0.37984496 0.38587425 0.53143842 0.26614987 0.5538329  0.66322136\n",
            " 0.55727821 0.43927649 0.4461671  0.42807924 0.50129199 0.52885444\n",
            " 0.61929371 0.47200689 0.48492679 0.46339363 0.59173127 0.60637382\n",
            " 0.4918174  0.58914729 0.53316107 0.4788975  0.40913006 0.6089578\n",
            " 0.54263566 0.58570198 0.49784668 0.57622739 0.51593454 0.6089578\n",
            " 0.48492679 0.55900086 0.59173127 0.52196382 0.5047373  0.52024117\n",
            " 0.40137812 0.63049096 0.43755383 0.62790698 0.49440138 0.42980189\n",
            " 0.54263566 0.48664944 0.53574505 0.65374677 0.5667528  0.56847545\n",
            " 0.40568475 0.4788975  0.45219638 0.48406546 0.4918174  0.5667528\n",
            " 0.60809647 0.5081826  0.45994832 0.49870801 0.48923342 0.49612403\n",
            " 0.61068045 0.56503015 0.50129199 0.50129199 0.65116279 0.53057709\n",
            " 0.32213609 0.65891473 0.45478036 0.63393626 0.46597761 0.48406546\n",
            " 0.66666667 0.57019811 0.53832903 0.40999139 0.55469423 0.58225668\n",
            " 0.48923342 0.32213609 0.46167097 0.55555556 0.44099914 0.65030146\n",
            " 0.49956934 0.61498708 0.7037037  0.45650301 0.56589147 0.27562446\n",
            " 0.40999139 0.43669251 0.43583118 0.55986219 0.49095607 0.6580534\n",
            " 0.63910422 0.41429802 0.3910422  0.42463394 0.52885444 0.54866494\n",
            " 0.59259259 0.50732127 0.43927649 0.45478036 0.51076658 0.41946598\n",
            " 0.48148148 0.54866494 0.62962963 0.58225668 0.48751077 0.49009475\n",
            " 0.43496985 0.30577089 0.43496985 0.41860465 0.51593454 0.47975883\n",
            " 0.49354005 0.56072351 0.4952627  0.49009475 0.53402239 0.60292851\n",
            " 0.4625323  0.60809647 0.45994832 0.4203273  0.55469423 0.49698536\n",
            " 0.49956934 0.50387597 0.62187769 0.56158484 0.54091301 0.53832903\n",
            " 0.61154177 0.54177433 0.62101637 0.51421189 0.47286822 0.53919035\n",
            " 0.4918174  0.56416882 0.46080965 0.45736434 0.5374677  0.40913006\n",
            " 0.46339363 0.40913006 0.52885444 0.66666667 0.60551249 0.33505599\n",
            " 0.58570198]\n",
            "The trained model has an aproximate error rate of 9.389451062026088 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "zwAKPA36B8U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d18095-1c40-48a1-b816-42a503d0e105",
        "id": "Wphmfh3WB4mu"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "3     5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "11    3  2020   1  15     2020-01-15             508  43.9  38.3  1019.4   \n",
            "12    5  2021   1   1     2021-01-01             257  39.6  29.3  1029.3   \n",
            "14    2  2022   1  25     2022-01-25             235  41.6  31.8  1013.2   \n",
            "18    7  2021   1   3     2021-01-03             186  41.1  32.3  1018.0   \n",
            "19    4  2020   1   2     2020-01-02             413  39.6  28.9  1011.8   \n",
            "\n",
            "    visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "3    10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "11    8.2  ...  51.1  35.1  0.02  999.9    1             0                 0   \n",
            "12   10.0  ...  54.0  33.1  0.13  999.9    0             0                 0   \n",
            "14   10.0  ...  48.9  30.0  0.00  999.9    0             0                 0   \n",
            "18   10.0  ...  53.1  39.0  0.00  999.9    0             0                 0   \n",
            "19   10.0  ...  46.0  33.1  0.01  999.9    0             0                 0   \n",
            "\n",
            "    hail  thunder  tornado_funnel_cloud  \n",
            "3      0        0                     0  \n",
            "11     0        0                 10000  \n",
            "12     0        0                     0  \n",
            "14     0        0                     0  \n",
            "18     0        0                     0  \n",
            "19     0        0                     0  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] != 2012]\n",
        "df_gust = df_gust.loc[df_gust[\"year\"] < 2020]\n",
        "cols = df_gust['NUM_COLLISIONS']\n",
        "df_gust = df_gust.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust[:6])\n",
        "df_gust.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "6c515585-671d-4cfd-c9cc-600b7a5b31e5",
        "id": "pzygHg-iB4mv"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  gust  NUM_COLLISIONS\n",
            "74    7  2016   1  17  18.1             451\n",
            "76    4  2014   1   9  20.0             561\n",
            "79    6  2019   1  19  21.0             479\n",
            "80    7  2015   1  11  17.1             341\n",
            "83    4  2015   1  29  20.0             519\n",
            "85    7  2019   1  13  15.9             374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day        year           mo           da         gust  \\\n",
              "count  1629.000000  1629.00000  1629.000000  1629.000000  1629.000000   \n",
              "mean      4.024555  2015.91283     6.278699    15.702885    27.511602   \n",
              "std       1.989070     2.01341     3.747683     8.667634     7.366770   \n",
              "min       1.000000  2013.00000     1.000000     1.000000    14.000000   \n",
              "25%       2.000000  2014.00000     3.000000     8.000000    22.000000   \n",
              "50%       4.000000  2016.00000     6.000000    16.000000    26.000000   \n",
              "75%       6.000000  2018.00000    10.000000    23.000000    31.100000   \n",
              "max       7.000000  2019.00000    12.000000    31.000000    71.100000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     1629.000000  \n",
              "mean       596.513198  \n",
              "std        104.479660  \n",
              "min        188.000000  \n",
              "25%        526.000000  \n",
              "50%        597.000000  \n",
              "75%        663.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0a4bb4d-2eea-4d90-a0c9-09afaaca9676\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.024555</td>\n",
              "      <td>2015.91283</td>\n",
              "      <td>6.278699</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.989070</td>\n",
              "      <td>2.01341</td>\n",
              "      <td>3.747683</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.00000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0a4bb4d-2eea-4d90-a0c9-09afaaca9676')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0a4bb4d-2eea-4d90-a0c9-09afaaca9676 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0a4bb4d-2eea-4d90-a0c9-09afaaca9676');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust.iloc[np.random.permutation(len(df_gust))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad74ee1c-030d-4011-8cb2-3f6bea0ee280",
        "id": "GA5wbn0KB4mw"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  gust\n",
            "1764    7  2019   6  16  24.1\n",
            "3338    7  2016  11  20  36.9\n",
            "2689    6  2016   9  24  24.1\n",
            "2120    4  2016   7  14  22.0\n",
            "3581    4  2013  12  19  27.0\n",
            "3308    6  2019  11  30  29.9\n",
            "      day  year  mo  da  gust  NUM_COLLISIONS\n",
            "1764    7  2019   6  16  24.1             509\n",
            "3338    7  2016  11  20  36.9             529\n",
            "2689    6  2016   9  24  24.1             612\n",
            "2120    4  2016   7  14  22.0             741\n",
            "3581    4  2013  12  19  27.0             607\n",
            "3308    6  2019  11  30  29.9             465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066797cd-c110-4303-da35-59beb11be7c6",
        "id": "iJRw3cGvB4mw"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1764    509\n",
            "3338    529\n",
            "2689    612\n",
            "2120    741\n",
            "3581    607\n",
            "3308    465\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a7a593-b580-462b-9f6a-9c8c883bf224",
        "id": "aZCjpnyzB4mw"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1303\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682b58f1-f790-44f8-bf20-a93efb587b1d",
        "id": "wrbkpT05B4mx"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff05e67ca90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.28196698, step = 1\n",
            "INFO:tensorflow:global_step/sec: 1123.22\n",
            "INFO:tensorflow:loss = 0.0086251795, step = 101 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1290.03\n",
            "INFO:tensorflow:loss = 0.008044067, step = 201 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1294.89\n",
            "INFO:tensorflow:loss = 0.00948257, step = 301 (0.082 sec)\n",
            "INFO:tensorflow:global_step/sec: 1342.43\n",
            "INFO:tensorflow:loss = 0.009223659, step = 401 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1414.6\n",
            "INFO:tensorflow:loss = 0.009794172, step = 501 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1357.52\n",
            "INFO:tensorflow:loss = 0.008056955, step = 601 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1411.74\n",
            "INFO:tensorflow:loss = 0.006747408, step = 701 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1493.12\n",
            "INFO:tensorflow:loss = 0.0073609743, step = 801 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1472.34\n",
            "INFO:tensorflow:loss = 0.008598522, step = 901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1479.39\n",
            "INFO:tensorflow:loss = 0.010741387, step = 1001 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1461.9\n",
            "INFO:tensorflow:loss = 0.008153164, step = 1101 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1257.88\n",
            "INFO:tensorflow:loss = 0.0072836317, step = 1201 (0.082 sec)\n",
            "INFO:tensorflow:global_step/sec: 1393.49\n",
            "INFO:tensorflow:loss = 0.0067488924, step = 1301 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1460.05\n",
            "INFO:tensorflow:loss = 0.007738815, step = 1401 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1423.26\n",
            "INFO:tensorflow:loss = 0.006344348, step = 1501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1421.61\n",
            "INFO:tensorflow:loss = 0.007808203, step = 1601 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1512.47\n",
            "INFO:tensorflow:loss = 0.008607324, step = 1701 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1495.48\n",
            "INFO:tensorflow:loss = 0.006690415, step = 1801 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1361.65\n",
            "INFO:tensorflow:loss = 0.0061682104, step = 1901 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1468.18\n",
            "INFO:tensorflow:loss = 0.0062603215, step = 2001 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1295.8\n",
            "INFO:tensorflow:loss = 0.0074445736, step = 2101 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1404.52\n",
            "INFO:tensorflow:loss = 0.008404028, step = 2201 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1467.18\n",
            "INFO:tensorflow:loss = 0.0069852388, step = 2301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1387.57\n",
            "INFO:tensorflow:loss = 0.008653704, step = 2401 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1428.08\n",
            "INFO:tensorflow:loss = 0.0074075838, step = 2501 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1404.15\n",
            "INFO:tensorflow:loss = 0.007792174, step = 2601 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1493.27\n",
            "INFO:tensorflow:loss = 0.0068587977, step = 2701 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1401.11\n",
            "INFO:tensorflow:loss = 0.008622099, step = 2801 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1351.66\n",
            "INFO:tensorflow:loss = 0.006937058, step = 2901 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1426.37\n",
            "INFO:tensorflow:loss = 0.0062934, step = 3001 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1469.1\n",
            "INFO:tensorflow:loss = 0.0059976755, step = 3101 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1303.19\n",
            "INFO:tensorflow:loss = 0.009901276, step = 3201 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1195.69\n",
            "INFO:tensorflow:loss = 0.006179485, step = 3301 (0.087 sec)\n",
            "INFO:tensorflow:global_step/sec: 1328.46\n",
            "INFO:tensorflow:loss = 0.009859631, step = 3401 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1434.21\n",
            "INFO:tensorflow:loss = 0.008677708, step = 3501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1442.39\n",
            "INFO:tensorflow:loss = 0.0050965613, step = 3601 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1424.24\n",
            "INFO:tensorflow:loss = 0.00922814, step = 3701 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1378.54\n",
            "INFO:tensorflow:loss = 0.006922371, step = 3801 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1556.84\n",
            "INFO:tensorflow:loss = 0.0063629025, step = 3901 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1482.79\n",
            "INFO:tensorflow:loss = 0.007822849, step = 4001 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1493.39\n",
            "INFO:tensorflow:loss = 0.0063413032, step = 4101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1480.23\n",
            "INFO:tensorflow:loss = 0.0067574857, step = 4201 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1482.04\n",
            "INFO:tensorflow:loss = 0.012234589, step = 4301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1468.14\n",
            "INFO:tensorflow:loss = 0.0053977966, step = 4401 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1518.2\n",
            "INFO:tensorflow:loss = 0.005536995, step = 4501 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1433.65\n",
            "INFO:tensorflow:loss = 0.007996816, step = 4601 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1487.19\n",
            "INFO:tensorflow:loss = 0.0076950393, step = 4701 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1315.53\n",
            "INFO:tensorflow:loss = 0.0065830834, step = 4801 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.75\n",
            "INFO:tensorflow:loss = 0.0081125675, step = 4901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1472.88\n",
            "INFO:tensorflow:loss = 0.0070504034, step = 5001 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1487.7\n",
            "INFO:tensorflow:loss = 0.005726281, step = 5101 (0.064 sec)\n",
            "INFO:tensorflow:global_step/sec: 1484.7\n",
            "INFO:tensorflow:loss = 0.008607339, step = 5201 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1609.96\n",
            "INFO:tensorflow:loss = 0.00632565, step = 5301 (0.062 sec)\n",
            "INFO:tensorflow:global_step/sec: 1551.69\n",
            "INFO:tensorflow:loss = 0.004810123, step = 5401 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.26\n",
            "INFO:tensorflow:loss = 0.007404395, step = 5501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1488.98\n",
            "INFO:tensorflow:loss = 0.007016445, step = 5601 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1536.2\n",
            "INFO:tensorflow:loss = 0.005106744, step = 5701 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1483.01\n",
            "INFO:tensorflow:loss = 0.007433802, step = 5801 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1478.93\n",
            "INFO:tensorflow:loss = 0.007201995, step = 5901 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1371.85\n",
            "INFO:tensorflow:loss = 0.0085068755, step = 6001 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1480.75\n",
            "INFO:tensorflow:loss = 0.00627428, step = 6101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1440.79\n",
            "INFO:tensorflow:loss = 0.009061938, step = 6201 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1266.47\n",
            "INFO:tensorflow:loss = 0.0076367566, step = 6301 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1411.32\n",
            "INFO:tensorflow:loss = 0.0074422243, step = 6401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1323.27\n",
            "INFO:tensorflow:loss = 0.0044321804, step = 6501 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1410.86\n",
            "INFO:tensorflow:loss = 0.0066266526, step = 6601 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1464.1\n",
            "INFO:tensorflow:loss = 0.007791572, step = 6701 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1459.43\n",
            "INFO:tensorflow:loss = 0.0053635547, step = 6801 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1423.71\n",
            "INFO:tensorflow:loss = 0.0054698545, step = 6901 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1523.78\n",
            "INFO:tensorflow:loss = 0.0073523815, step = 7001 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1457.37\n",
            "INFO:tensorflow:loss = 0.008460638, step = 7101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1366.56\n",
            "INFO:tensorflow:loss = 0.0054619038, step = 7201 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.55\n",
            "INFO:tensorflow:loss = 0.006643691, step = 7301 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1392.1\n",
            "INFO:tensorflow:loss = 0.007488771, step = 7401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1447.97\n",
            "INFO:tensorflow:loss = 0.00876477, step = 7501 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1445.73\n",
            "INFO:tensorflow:loss = 0.0075812405, step = 7601 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1281.77\n",
            "INFO:tensorflow:loss = 0.004689736, step = 7701 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1368.08\n",
            "INFO:tensorflow:loss = 0.00478401, step = 7801 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1351.55\n",
            "INFO:tensorflow:loss = 0.00616761, step = 7901 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1355.46\n",
            "INFO:tensorflow:loss = 0.007595187, step = 8001 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1397.73\n",
            "INFO:tensorflow:loss = 0.0062466776, step = 8101 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1386.05\n",
            "INFO:tensorflow:loss = 0.0064962124, step = 8201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1503.98\n",
            "INFO:tensorflow:loss = 0.0057587326, step = 8301 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1501.92\n",
            "INFO:tensorflow:loss = 0.0058056144, step = 8401 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1427.51\n",
            "INFO:tensorflow:loss = 0.007876768, step = 8501 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1466.81\n",
            "INFO:tensorflow:loss = 0.006234741, step = 8601 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1463.57\n",
            "INFO:tensorflow:loss = 0.0072707767, step = 8701 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1476.92\n",
            "INFO:tensorflow:loss = 0.008073916, step = 8801 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1492.74\n",
            "INFO:tensorflow:loss = 0.010409593, step = 8901 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1378\n",
            "INFO:tensorflow:loss = 0.006179935, step = 9001 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1399.63\n",
            "INFO:tensorflow:loss = 0.008377288, step = 9101 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1374.51\n",
            "INFO:tensorflow:loss = 0.007357262, step = 9201 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1420\n",
            "INFO:tensorflow:loss = 0.0072982963, step = 9301 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1410.33\n",
            "INFO:tensorflow:loss = 0.005123373, step = 9401 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1507.07\n",
            "INFO:tensorflow:loss = 0.0064798063, step = 9501 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.01\n",
            "INFO:tensorflow:loss = 0.0064724493, step = 9601 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1391.4\n",
            "INFO:tensorflow:loss = 0.007865655, step = 9701 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1324.37\n",
            "INFO:tensorflow:loss = 0.006678679, step = 9801 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1445.36\n",
            "INFO:tensorflow:loss = 0.007905092, step = 9901 (0.072 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00948199.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 94.98848753278486\n",
            "Just using average = 599.4090560245587 has RMSE of 102.91164198363217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_gust', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447ee5ef-8816-4415-c891-9238232c176d",
        "id": "ByVWr9PzB4mx"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff063120090>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n",
            "[0.45284078 0.547738   0.47461343 0.51207757 0.46765155 0.49457788\n",
            " 0.503318   0.48984322 0.5414449  0.47987384 0.5143559  0.57154804\n",
            " 0.46861786 0.4493055  0.49599162 0.4902836  0.4754573  0.50574386\n",
            " 0.44684383 0.53113925 0.52525324 0.48205996 0.553042   0.4711395\n",
            " 0.4901308  0.51685643 0.4807198  0.49802354 0.45696    0.46566376\n",
            " 0.5210303  0.512185   0.49370024 0.569932   0.4945628  0.48314148\n",
            " 0.56605995 0.52818453 0.4448419  0.48563427 0.49849766 0.48250577\n",
            " 0.46955365 0.5347459  0.51083946 0.46002743 0.514167   0.48035315\n",
            " 0.5484611  0.5314928  0.4413822  0.5181895  0.5191861  0.4483724\n",
            " 0.4379755  0.4734558  0.44937906 0.48535302 0.50331396 0.48243588\n",
            " 0.46735764 0.4664756  0.47279543 0.47518295 0.51549006 0.5512336\n",
            " 0.53743726 0.56948483 0.44908753 0.55321026 0.5306957  0.49990833\n",
            " 0.4903923  0.43070406 0.48095098 0.47428897 0.5625574  0.50087976\n",
            " 0.52210635 0.5327489  0.5211852  0.5579033  0.43109837 0.52885264\n",
            " 0.5118199  0.51344097 0.51326454 0.48555812 0.52339035 0.52544886\n",
            " 0.49508068 0.5742419  0.5139443  0.45820403 0.4986046  0.5262058\n",
            " 0.5377536  0.46158972 0.47498295 0.51267827 0.44768688 0.5187499\n",
            " 0.5464999  0.5775412  0.5041587  0.50748545 0.5291594  0.4807061\n",
            " 0.5327984  0.5030741  0.49556455 0.47563785 0.47469762 0.4919371\n",
            " 0.49545455 0.5310778  0.4538655  0.552791   0.4539909  0.55993456\n",
            " 0.4528965  0.47126767 0.5608431  0.5018599  0.47966984 0.54061085\n",
            " 0.5594461  0.46684197 0.5153031  0.4916738  0.47726366 0.5248315\n",
            " 0.5184791  0.44432676 0.48053926 0.5340441  0.5247999  0.4640351\n",
            " 0.515405   0.45929548 0.52047616 0.48141673 0.5053597  0.5375754\n",
            " 0.57141846 0.5034568  0.50465244 0.49675882 0.47622955 0.5359199\n",
            " 0.50708616 0.47663504 0.4608795  0.506885   0.50273204 0.50535053\n",
            " 0.5448316  0.504365   0.4489641  0.5295612  0.56772804 0.46928978\n",
            " 0.513578   0.5414895  0.47938317 0.5354623  0.5487183  0.46981683\n",
            " 0.5180661  0.46645266 0.57411754 0.5185754  0.48652187 0.55226976\n",
            " 0.5501766  0.48980954 0.49317205 0.50397646 0.50737375 0.46631312\n",
            " 0.49994025 0.5125655  0.5209812  0.49936864 0.5352334  0.48618886\n",
            " 0.4996684  0.52130663 0.5553567  0.5032627  0.5336314  0.56915385\n",
            " 0.44949648 0.47204575 0.52904356 0.45193735 0.43531504 0.5176577\n",
            " 0.5685695  0.5560844  0.53422666 0.52057487 0.51123124 0.5055254\n",
            " 0.49147588 0.5435874  0.45609048 0.55295044 0.5271087  0.49731252\n",
            " 0.5444171  0.5535332  0.5057676  0.51616555 0.48763752 0.513597\n",
            " 0.4387101  0.46911862 0.5098442  0.47860664 0.47832346 0.46793044\n",
            " 0.4782656  0.45316008 0.4794252  0.502905   0.48507196 0.53729665\n",
            " 0.4490027  0.50253046 0.48925287 0.468976   0.44580773 0.4868539\n",
            " 0.49929366 0.4710411  0.52304274 0.56816095 0.51939625 0.44665197\n",
            " 0.5519852  0.49070364 0.46423075 0.5565715  0.4440171  0.5712766\n",
            " 0.52208734 0.5669017  0.5088878  0.5079348  0.45737672 0.45932457\n",
            " 0.53073394 0.4616402  0.48403773 0.4881517  0.56847537 0.535295\n",
            " 0.51029074 0.5134373  0.485835   0.4770468  0.48452446 0.54480666\n",
            " 0.5340538  0.5312992  0.4724558  0.48593125 0.485074   0.4890681\n",
            " 0.5016597  0.51569414 0.48576248 0.53499943 0.52877474 0.53341156\n",
            " 0.43863806 0.53654957 0.4574254  0.54879993 0.5024532  0.54183424\n",
            " 0.49214152 0.51016915 0.4646971  0.53536433 0.52721655 0.442436\n",
            " 0.48248506 0.48859966 0.5061414  0.4572253  0.48935446 0.5219841\n",
            " 0.48092917 0.5081015  0.5464591  0.49968958 0.4598172  0.45976138\n",
            " 0.555038   0.49818346 0.49341285 0.46108392 0.52720493 0.48403186\n",
            " 0.43203363 0.50407493 0.51436925 0.51855826 0.48267186 0.43734816\n",
            " 0.56192595 0.52688444 0.5253382  0.53254783 0.537018   0.4558231\n",
            " 0.524357   0.47487706 0.44813314 0.5403639  0.47126383 0.50894487\n",
            " 0.55393803 0.45453218]\n",
            "[0.49009475 0.5796727  0.40999139 0.42291128 0.52024117 0.39190353\n",
            " 0.46770026 0.62704565 0.51765719 0.68819983 0.61929371 0.52713178\n",
            " 0.50215332 0.41257537 0.57881137 0.62273902 0.59000861 0.57019811\n",
            " 0.36950904 0.48664944 0.4625323  0.51421189 0.56416882 0.49095607\n",
            " 0.44702842 0.62360034 0.4788975  0.5538329  0.51593454 0.45736434\n",
            " 0.59259259 0.71490095 0.44186047 0.51421189 0.48923342 0.60292851\n",
            " 0.55211025 0.56503015 0.38329027 0.41343669 0.48751077 0.52368648\n",
            " 0.47028424 0.6287683  0.40999139 0.50990525 0.47459087 0.3910422\n",
            " 0.44875108 0.56158484 0.27562446 0.53919035 0.66925065 0.35745047\n",
            " 0.32213609 0.55469423 0.40568475 0.68303187 0.46425495 0.63824289\n",
            " 0.41515935 0.64857881 0.58139535 0.43066322 0.53057709 0.5245478\n",
            " 0.55297158 0.6089578  0.31093885 0.51679587 0.60465116 0.48492679\n",
            " 0.51593454 0.36950904 0.43238587 0.50904393 0.57450474 0.48751077\n",
            " 0.54349699 0.49612403 0.36950904 0.49095607 0.4005168  0.47286822\n",
            " 0.52540913 0.45822567 0.54091301 0.60551249 0.54091301 0.42980189\n",
            " 0.5211025  0.60206718 0.59173127 0.48148148 0.41343669 0.61154177\n",
            " 0.58656331 0.45994832 0.56158484 0.54780362 0.40913006 0.5047373\n",
            " 0.55641688 0.5503876  0.59431525 0.53660637 0.59086994 0.5245478\n",
            " 0.55813953 0.52196382 0.37984496 0.49956934 0.52282515 0.47372954\n",
            " 0.52971576 0.43152455 0.51937984 0.65891473 0.39534884 0.52282515\n",
            " 0.47803618 0.51507321 0.58484065 0.51076658 0.45391904 0.49698536\n",
            " 0.52196382 0.5374677  0.57105943 0.70456503 0.42204996 0.49009475\n",
            " 0.49612403 0.38673557 0.47372954 0.59086994 0.51937984 0.44272179\n",
            " 0.416882   0.34366925 0.57795004 0.44444444 0.49956934 0.52971576\n",
            " 0.39276486 0.48923342 0.56933678 0.51593454 0.51851852 0.49354005\n",
            " 0.49698536 0.42291128 0.48406546 0.55813953 0.67183463 0.54091301\n",
            " 0.60292851 0.48664944 0.4005168  0.4952627  0.53229974 0.44099914\n",
            " 0.59259259 0.44702842 0.66838932 0.58570198 0.61498708 0.50129199\n",
            " 0.53229974 0.40137812 0.58742463 0.61498708 0.45305771 0.49009475\n",
            " 0.63393626 0.45219638 0.49870801 0.43238587 0.51248923 0.47028424\n",
            " 0.57536606 0.42894057 0.47459087 0.48148148 0.63479759 0.63049096\n",
            " 0.45219638 0.59862188 0.44358312 0.16192937 0.61412575 0.46425495\n",
            " 0.45650301 0.54091301 0.44702842 0.50559862 0.38845823 0.54435831\n",
            " 0.4461671  0.54521964 0.63738157 0.57105943 0.44444444 0.46080965\n",
            " 0.57536606 0.51851852 0.46080965 0.4754522  0.52024117 0.48148148\n",
            " 0.56933678 0.49440138 0.7329888  0.6873385  0.60465116 0.56761413\n",
            " 0.33419466 0.58570198 0.51248923 0.5047373  0.46425495 0.45219638\n",
            " 0.41602067 0.4918174  0.49956934 0.5374677  0.45305771 0.5667528\n",
            " 0.38587425 0.46856158 0.44702842 0.42463394 0.34022394 0.39965547\n",
            " 0.56072351 0.53057709 0.5796727  0.52627046 0.55555556 0.39534884\n",
            " 0.56761413 0.46511628 0.39276486 0.45047373 0.48062016 0.42807924\n",
            " 0.46683893 0.40999139 0.36606374 0.48837209 0.45564169 0.44875108\n",
            " 0.48923342 0.60465116 0.49870801 0.46942291 0.43152455 0.44272179\n",
            " 0.46597761 0.64685616 0.31438415 0.49095607 0.36950904 0.54349699\n",
            " 0.56158484 0.51248923 0.61843239 0.38242894 0.38501292 0.52971576\n",
            " 0.51937984 0.52799311 0.54005168 0.60378984 0.46770026 0.55555556\n",
            " 0.3453919  0.56847545 0.36347976 0.64771748 0.54177433 0.55555556\n",
            " 0.48234281 0.4461671  0.49784668 0.51421189 0.57881137 0.32816537\n",
            " 0.83893196 0.49612403 0.52799311 0.43927649 0.40654608 0.52971576\n",
            " 0.41774332 0.50732127 0.52885444 0.45736434 0.50215332 0.51076658\n",
            " 0.45047373 0.4496124  0.71490095 0.43669251 0.40913006 0.47286822\n",
            " 0.34969854 0.43066322 0.56330749 0.86046512 0.49956934 0.36692506\n",
            " 0.50904393 0.50129199 0.35486649 0.40568475 0.59259259 0.40310078\n",
            " 0.6416882  0.50387597 0.46339363 0.49698536 0.50732127 0.41085271\n",
            " 0.66149871 0.46597761]\n",
            "The trained model has an aproximate error rate of 1.5466031676238288 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (Mxpsd)"
      ],
      "metadata": {
        "id": "OxfDYOaGD1Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693578e1-a154-4479-f067-c9dc2f94f073",
        "id": "9TtzqELvE-aN"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  NUM_COLLISIONS  temp  dewp     slp  \\\n",
            "1    5  2020   1  24     2020-01-24             524  37.3  33.7  1028.5   \n",
            "2    2  2021   1  12     2021-01-12             278  37.0  29.1  1019.0   \n",
            "3    5  2021   1  22     2021-01-22             254  36.5  28.4  1003.1   \n",
            "4    3  2021   1  27     2021-01-27             262  34.6  33.8  1012.8   \n",
            "5    2  2021   1  26     2021-01-26             263  31.9  23.4  1016.9   \n",
            "6    1  2022   1  24     2022-01-24             237  34.5  23.8  1010.6   \n",
            "\n",
            "   visib  ...   max   min  prcp   sndp  fog  rain_drizzle  snow_ice_pellets  \\\n",
            "1    6.5  ...  46.0  19.9  0.00  999.9    1             0                 0   \n",
            "2   10.0  ...  44.1  21.0  0.00  999.9    0             0                 0   \n",
            "3   10.0  ...  44.1  19.9  0.00  999.9    0             0                 0   \n",
            "4    8.0  ...  41.0  28.9  0.25  999.9    1             1                 0   \n",
            "5    9.0  ...  37.9  21.0  0.00  999.9    1             0                 0   \n",
            "6    9.7  ...  39.9  25.0  0.00  999.9    1             0                 0   \n",
            "\n",
            "   hail  thunder  tornado_funnel_cloud  \n",
            "1     0        0                     0  \n",
            "2     0        0                     0  \n",
            "3     0        0                     0  \n",
            "4     0        0                 11000  \n",
            "5     0     1000                  1000  \n",
            "6     0     1000                  1000  \n",
            "\n",
            "[6 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd = df.drop(columns=['collision_date', 'temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','gust','dewp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] != 2012]\n",
        "df_mxpsd = df_mxpsd.loc[df_mxpsd[\"year\"] < 2020]\n",
        "cols = df_mxpsd['NUM_COLLISIONS']\n",
        "df_mxpsd = df_mxpsd.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd.insert(loc=5, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd[:6])\n",
        "df_mxpsd.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "8e476db9-155c-4f96-8ab2-1cae9751b854",
        "id": "O_x5VoUvE-aO"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "49    4  2016   1  28    8.9             681\n",
            "51    5  2014   1  17    8.9             589\n",
            "54    1  2016   1  25    8.9             658\n",
            "55    5  2016   1  29    9.9             645\n",
            "58    5  2017   1  20    9.9             605\n",
            "59    7  2013   1  13    9.9             373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               day         year           mo           da        mxpsd  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      3.999608  2016.001567     6.520564    15.737172    17.240110   \n",
              "std       2.001469     2.000587     3.449204     8.797367     5.858333   \n",
              "min       1.000000  2013.000000     1.000000     1.000000     5.100000   \n",
              "25%       2.000000  2014.000000     4.000000     8.000000    13.000000   \n",
              "50%       4.000000  2016.000000     7.000000    16.000000    15.900000   \n",
              "75%       6.000000  2018.000000    10.000000    23.000000    20.000000   \n",
              "max       7.000000  2019.000000    12.000000    31.000000    49.000000   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count     2553.000000  \n",
              "mean       599.033686  \n",
              "std        100.284761  \n",
              "min        188.000000  \n",
              "25%        531.000000  \n",
              "50%        602.000000  \n",
              "75%        665.000000  \n",
              "max       1161.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d68425c-e793-46a3-9457-fa767cd5b786\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.999608</td>\n",
              "      <td>2016.001567</td>\n",
              "      <td>6.520564</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.001469</td>\n",
              "      <td>2.000587</td>\n",
              "      <td>3.449204</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>2019.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d68425c-e793-46a3-9457-fa767cd5b786')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d68425c-e793-46a3-9457-fa767cd5b786 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d68425c-e793-46a3-9457-fa767cd5b786');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd.iloc[np.random.permutation(len(df_mxpsd))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])\n",
        "print(shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359c3ca2-9d0e-4e6c-f0e8-e38adf71bdc7",
        "id": "oEaMTRvFE-aO"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da  mxpsd\n",
            "1024    1  2017   4  17   15.9\n",
            "2919    7  2016  10  30   15.9\n",
            "2501    1  2017   9  25    8.0\n",
            "2581    1  2013   9  16   13.0\n",
            "3503    3  2017  12  27   15.9\n",
            "2795    2  2013  10  22    8.9\n",
            "      day  year  mo  da  mxpsd  NUM_COLLISIONS\n",
            "1024    1  2017   4  17   15.9             488\n",
            "2919    7  2016  10  30   15.9             557\n",
            "2501    1  2017   9  25    8.0             726\n",
            "2581    1  2013   9  16   13.0             570\n",
            "3503    3  2017  12  27   15.9             577\n",
            "2795    2  2013  10  22    8.9             540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434edbb1-f4aa-435a-94d9-c168fa1a3c3c",
        "id": "2IjTestaE-aO"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024    488\n",
            "2919    557\n",
            "2501    726\n",
            "2581    570\n",
            "3503    577\n",
            "2795    540\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "print(trainsize)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "print(testsize)\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = 5\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8d8e53-4429-485f-fea1-7cd2c246608a",
        "id": "ihCguRRBE-aO"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2042\n",
            "511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/linear_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', optimizer=tf.train.AdamOptimizer(learning_rate=0.00001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "print(\"starting to train\");\n",
        "\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "#print(preds)\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "pred = format(str(predslistscale))\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c52c7d-5cb8-444f-9e13-b6f6e16fa9ea",
        "id": "-hI4RaYNE-aP"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff06316c990>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.27086252, step = 1\n",
            "INFO:tensorflow:global_step/sec: 1239.45\n",
            "INFO:tensorflow:loss = 0.007692973, step = 101 (0.086 sec)\n",
            "INFO:tensorflow:global_step/sec: 1386.65\n",
            "INFO:tensorflow:loss = 0.0068669785, step = 201 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1531.29\n",
            "INFO:tensorflow:loss = 0.0071460498, step = 301 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1500.44\n",
            "INFO:tensorflow:loss = 0.0064879535, step = 401 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1469.24\n",
            "INFO:tensorflow:loss = 0.007320543, step = 501 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1361.6\n",
            "INFO:tensorflow:loss = 0.008667931, step = 601 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1337.09\n",
            "INFO:tensorflow:loss = 0.006943797, step = 701 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1523.45\n",
            "INFO:tensorflow:loss = 0.008225562, step = 801 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1379.45\n",
            "INFO:tensorflow:loss = 0.006578462, step = 901 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1329.88\n",
            "INFO:tensorflow:loss = 0.008416539, step = 1001 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1449.18\n",
            "INFO:tensorflow:loss = 0.0073490934, step = 1101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1480.95\n",
            "INFO:tensorflow:loss = 0.0061159655, step = 1201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1462.39\n",
            "INFO:tensorflow:loss = 0.009611443, step = 1301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1367\n",
            "INFO:tensorflow:loss = 0.0076559633, step = 1401 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1447.33\n",
            "INFO:tensorflow:loss = 0.009013323, step = 1501 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1342.01\n",
            "INFO:tensorflow:loss = 0.005798159, step = 1601 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1463.26\n",
            "INFO:tensorflow:loss = 0.0070012943, step = 1701 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1315.86\n",
            "INFO:tensorflow:loss = 0.0049145706, step = 1801 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1459.34\n",
            "INFO:tensorflow:loss = 0.007943849, step = 1901 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1487.33\n",
            "INFO:tensorflow:loss = 0.0077362875, step = 2001 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1268.69\n",
            "INFO:tensorflow:loss = 0.0072000693, step = 2101 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1482.25\n",
            "INFO:tensorflow:loss = 0.007351027, step = 2201 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1455.33\n",
            "INFO:tensorflow:loss = 0.008497379, step = 2301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1352.12\n",
            "INFO:tensorflow:loss = 0.007950294, step = 2401 (0.076 sec)\n",
            "INFO:tensorflow:global_step/sec: 1412.98\n",
            "INFO:tensorflow:loss = 0.007002712, step = 2501 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1447.76\n",
            "INFO:tensorflow:loss = 0.006953197, step = 2601 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1272.71\n",
            "INFO:tensorflow:loss = 0.0057351626, step = 2701 (0.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.55\n",
            "INFO:tensorflow:loss = 0.0063442113, step = 2801 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1498.1\n",
            "INFO:tensorflow:loss = 0.008354744, step = 2901 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1194.44\n",
            "INFO:tensorflow:loss = 0.0069125984, step = 3001 (0.082 sec)\n",
            "INFO:tensorflow:global_step/sec: 1474.23\n",
            "INFO:tensorflow:loss = 0.0059756376, step = 3101 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1436.96\n",
            "INFO:tensorflow:loss = 0.006711388, step = 3201 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1495.99\n",
            "INFO:tensorflow:loss = 0.009108671, step = 3301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1454.37\n",
            "INFO:tensorflow:loss = 0.00557138, step = 3401 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1160.69\n",
            "INFO:tensorflow:loss = 0.004859241, step = 3501 (0.087 sec)\n",
            "INFO:tensorflow:global_step/sec: 1451.84\n",
            "INFO:tensorflow:loss = 0.00539013, step = 3601 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1461.07\n",
            "INFO:tensorflow:loss = 0.006974246, step = 3701 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1443.32\n",
            "INFO:tensorflow:loss = 0.0074421433, step = 3801 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1406.44\n",
            "INFO:tensorflow:loss = 0.007290232, step = 3901 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1498.88\n",
            "INFO:tensorflow:loss = 0.0050687543, step = 4001 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1477.93\n",
            "INFO:tensorflow:loss = 0.009487057, step = 4101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1484.26\n",
            "INFO:tensorflow:loss = 0.0070961607, step = 4201 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1484.6\n",
            "INFO:tensorflow:loss = 0.0068024946, step = 4301 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1337.66\n",
            "INFO:tensorflow:loss = 0.007200177, step = 4401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1509.62\n",
            "INFO:tensorflow:loss = 0.0049398364, step = 4501 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1491.04\n",
            "INFO:tensorflow:loss = 0.008777529, step = 4601 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1409.71\n",
            "INFO:tensorflow:loss = 0.008365045, step = 4701 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1427.49\n",
            "INFO:tensorflow:loss = 0.005546905, step = 4801 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1336.8\n",
            "INFO:tensorflow:loss = 0.0065494413, step = 4901 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1296.7\n",
            "INFO:tensorflow:loss = 0.005873479, step = 5001 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1375.89\n",
            "INFO:tensorflow:loss = 0.008918789, step = 5101 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1394.99\n",
            "INFO:tensorflow:loss = 0.0068512005, step = 5201 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1420.38\n",
            "INFO:tensorflow:loss = 0.0049090264, step = 5301 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1490\n",
            "INFO:tensorflow:loss = 0.007903097, step = 5401 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1515.69\n",
            "INFO:tensorflow:loss = 0.006170501, step = 5501 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1407.22\n",
            "INFO:tensorflow:loss = 0.0069822865, step = 5601 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1508.58\n",
            "INFO:tensorflow:loss = 0.006688077, step = 5701 (0.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 1496.72\n",
            "INFO:tensorflow:loss = 0.00674991, step = 5801 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1505.37\n",
            "INFO:tensorflow:loss = 0.0066573643, step = 5901 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1472.49\n",
            "INFO:tensorflow:loss = 0.005995863, step = 6001 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1362.94\n",
            "INFO:tensorflow:loss = 0.008654944, step = 6101 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1441.78\n",
            "INFO:tensorflow:loss = 0.0054575577, step = 6201 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1454.15\n",
            "INFO:tensorflow:loss = 0.0059249382, step = 6301 (0.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 1055.55\n",
            "INFO:tensorflow:loss = 0.007251917, step = 6401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1406.14\n",
            "INFO:tensorflow:loss = 0.0062875813, step = 6501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1427.37\n",
            "INFO:tensorflow:loss = 0.006246404, step = 6601 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1477.27\n",
            "INFO:tensorflow:loss = 0.0078334585, step = 6701 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1455.85\n",
            "INFO:tensorflow:loss = 0.006724094, step = 6801 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1405.62\n",
            "INFO:tensorflow:loss = 0.005977219, step = 6901 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.32\n",
            "INFO:tensorflow:loss = 0.008026611, step = 7001 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1478.85\n",
            "INFO:tensorflow:loss = 0.0060175615, step = 7101 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1456.97\n",
            "INFO:tensorflow:loss = 0.0047424873, step = 7201 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1446.16\n",
            "INFO:tensorflow:loss = 0.0076010353, step = 7301 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1385.21\n",
            "INFO:tensorflow:loss = 0.008378799, step = 7401 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1378.36\n",
            "INFO:tensorflow:loss = 0.007887931, step = 7501 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1423.48\n",
            "INFO:tensorflow:loss = 0.0058936016, step = 7601 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1300.34\n",
            "INFO:tensorflow:loss = 0.0069060763, step = 7701 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1288.59\n",
            "INFO:tensorflow:loss = 0.0066917795, step = 7801 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1375.89\n",
            "INFO:tensorflow:loss = 0.0060667004, step = 7901 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1456.99\n",
            "INFO:tensorflow:loss = 0.005485777, step = 8001 (0.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 1403.19\n",
            "INFO:tensorflow:loss = 0.0067324876, step = 8101 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1433.06\n",
            "INFO:tensorflow:loss = 0.006594685, step = 8201 (0.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 1323.76\n",
            "INFO:tensorflow:loss = 0.0068704737, step = 8301 (0.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 1261.79\n",
            "INFO:tensorflow:loss = 0.006164723, step = 8401 (0.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 1268.36\n",
            "INFO:tensorflow:loss = 0.005480827, step = 8501 (0.079 sec)\n",
            "INFO:tensorflow:global_step/sec: 1337.08\n",
            "INFO:tensorflow:loss = 0.0071104653, step = 8601 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1388.61\n",
            "INFO:tensorflow:loss = 0.0061843833, step = 8701 (0.072 sec)\n",
            "INFO:tensorflow:global_step/sec: 1444.11\n",
            "INFO:tensorflow:loss = 0.0073753363, step = 8801 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1346.56\n",
            "INFO:tensorflow:loss = 0.0067562684, step = 8901 (0.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1417.74\n",
            "INFO:tensorflow:loss = 0.0053254114, step = 9001 (0.070 sec)\n",
            "INFO:tensorflow:global_step/sec: 1363.03\n",
            "INFO:tensorflow:loss = 0.0073003396, step = 9101 (0.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 1177.09\n",
            "INFO:tensorflow:loss = 0.004773034, step = 9201 (0.085 sec)\n",
            "INFO:tensorflow:global_step/sec: 1408.07\n",
            "INFO:tensorflow:loss = 0.0062534264, step = 9301 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1291.69\n",
            "INFO:tensorflow:loss = 0.0060551288, step = 9401 (0.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 1330.85\n",
            "INFO:tensorflow:loss = 0.008025428, step = 9501 (0.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 1258.73\n",
            "INFO:tensorflow:loss = 0.0077333413, step = 9601 (0.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 1391.15\n",
            "INFO:tensorflow:loss = 0.0059670424, step = 9701 (0.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 1404.78\n",
            "INFO:tensorflow:loss = 0.0054084184, step = 9801 (0.071 sec)\n",
            "INFO:tensorflow:global_step/sec: 1315.29\n",
            "INFO:tensorflow:loss = 0.007295782, step = 9901 (0.078 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/linear_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.006411911.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 90.92124313307619\n",
            "Just using average = 598.3687561214496 has RMSE of 95.58877737762168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.LinearRegressor(model_dir='/tmp/linear_regression_trained_model_mxpsd', enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b7d101-2162-4b76-b529-fbd664e21df6",
        "id": "5vjLYo0bE-aP"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff063076a10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/linear_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/linear_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n",
            "[0.55664074 0.515768   0.5580088  0.46999112 0.49476013 0.5406565\n",
            " 0.506726   0.5268847  0.5087789  0.4979605  0.5219636  0.4720591\n",
            " 0.55834895 0.5423378  0.52125597 0.5383248  0.5123896  0.5168648\n",
            " 0.5671846  0.5617724  0.53588825 0.5535562  0.53042525 0.5536596\n",
            " 0.4828721  0.49335313 0.5178631  0.49463364 0.47945145 0.48343104\n",
            " 0.48866022 0.5546254  0.5399067  0.5019694  0.48999566 0.539752\n",
            " 0.5393192  0.4653684  0.5555799  0.4643746  0.47413298 0.4862343\n",
            " 0.5030523  0.5148423  0.5228981  0.5655355  0.5220009  0.5613953\n",
            " 0.52137446 0.5192911  0.5320688  0.5040822  0.565997   0.47489133\n",
            " 0.46501958 0.5415519  0.57073015 0.5554183  0.4726756  0.5236754\n",
            " 0.51009387 0.5524381  0.519096   0.55840987 0.5715418  0.48918727\n",
            " 0.5508579  0.5149744  0.47929227 0.4947867  0.5101786  0.50774735\n",
            " 0.52697563 0.5017576  0.55243886 0.5287167  0.49873173 0.5275012\n",
            " 0.4936037  0.4891511  0.5496686  0.51515156 0.5254449  0.4812897\n",
            " 0.5456325  0.53978205 0.53203857 0.5566334  0.5521704  0.51711667\n",
            " 0.52501756 0.47090426 0.49021882 0.56615144 0.5197309  0.5080559\n",
            " 0.51232433 0.5184234  0.55667025 0.50430363 0.5066732  0.55029696\n",
            " 0.49991304 0.52383137 0.48063424 0.48050332 0.5154655  0.5123615\n",
            " 0.53334266 0.51163447 0.50380325 0.49015236 0.51154953 0.49894992\n",
            " 0.5034076  0.5148788  0.53857577 0.47332603 0.49941713 0.5007806\n",
            " 0.5091012  0.51823527 0.5441373  0.5102139  0.5609561  0.5662212\n",
            " 0.5727125  0.5287167  0.53789115 0.4845005  0.53853804 0.4926133\n",
            " 0.50625634 0.5007261  0.5386349  0.48828444 0.4837462  0.53739506\n",
            " 0.4666699  0.47067183 0.51515156 0.48415208 0.5051672  0.47497633\n",
            " 0.48465258 0.525689   0.4977437  0.53617775 0.46530476 0.5068492\n",
            " 0.4971442  0.53350013 0.493199   0.46020514 0.5080953  0.5327589\n",
            " 0.5229611  0.48815823 0.52610916 0.52672005 0.5632017  0.57134676\n",
            " 0.4539304  0.48918656 0.49805287 0.56650585 0.56329286 0.5341245\n",
            " 0.48184377 0.5129483  0.48262972 0.5321027  0.44945103 0.5586045\n",
            " 0.45752916 0.5221172  0.5182642  0.47608885 0.52364147 0.5400937\n",
            " 0.48097643 0.5078143  0.49413496 0.5452911  0.4977437  0.49851537\n",
            " 0.5327288  0.5274391  0.51801896 0.48974726 0.49966374 0.5314457\n",
            " 0.541744   0.55716455 0.5082213  0.51695704 0.57380915 0.53643113\n",
            " 0.53101724 0.49014488 0.48710734 0.55396837 0.52268136 0.5314805\n",
            " 0.5019415  0.5366411  0.5233032  0.55866075 0.49992004 0.50536156\n",
            " 0.51039714 0.4939491  0.53568065 0.50868934 0.535278   0.5139746\n",
            " 0.53238696 0.48943657 0.50685567 0.5303883  0.51820564 0.50175726\n",
            " 0.555863   0.509898   0.48632073 0.4526519  0.48905966 0.56715745\n",
            " 0.48452887 0.48386595 0.5093725  0.5511952  0.53573847 0.53922844\n",
            " 0.52774596 0.4650808  0.52277863 0.5201345  0.52262414 0.5267538\n",
            " 0.49167407 0.54217803 0.5200041  0.49447423 0.53826344 0.45864788\n",
            " 0.5578787  0.47783124 0.50813013 0.5727513  0.5420867  0.4535553\n",
            " 0.48210138 0.53711563 0.52419966 0.49541494 0.54052925 0.5168074\n",
            " 0.5551834  0.5080615  0.5008787  0.5242286  0.48213008 0.5622126\n",
            " 0.5595997  0.52494264 0.51944476 0.52784336 0.5232151  0.5356425\n",
            " 0.5109623  0.5090537  0.50637764 0.46949434 0.5036191  0.53315985\n",
            " 0.5605567  0.48776603 0.5103609  0.5175788  0.4840841  0.53978884\n",
            " 0.54839665 0.53045356 0.5588801  0.5010679  0.5466333  0.5149985\n",
            " 0.5430119  0.52877307 0.5110477  0.5649428  0.48492694 0.5141067\n",
            " 0.48337492 0.48148456 0.46086323 0.5116475  0.5072505  0.4742975\n",
            " 0.49549565 0.5156537  0.55075413 0.49285844 0.5239224  0.5330995\n",
            " 0.52373916 0.5187274  0.56537503 0.48290494 0.5260674  0.537824\n",
            " 0.5236479  0.5722501  0.5519428  0.5019682  0.53180474 0.49326462\n",
            " 0.5195429  0.5113316  0.51592964 0.5656669  0.53742224 0.48092386\n",
            " 0.47074005 0.5144375  0.5260439  0.49668357 0.56500995 0.5209333\n",
            " 0.56994706 0.52243024 0.5274099  0.53524816 0.53878903 0.5512977\n",
            " 0.53023505 0.5349061  0.51036793 0.49028254 0.5029678  0.48564154\n",
            " 0.53496253 0.53144515 0.53645116 0.52696913 0.54236126 0.4679774\n",
            " 0.48968977 0.5403338  0.5233106  0.5619617  0.4816666  0.5447364\n",
            " 0.48785138 0.53602487 0.5652285  0.54096276 0.45859087 0.52775294\n",
            " 0.5773818  0.49801716 0.52171624 0.47049502 0.46797857 0.5598495\n",
            " 0.5516566  0.49301097 0.47261354 0.49454156 0.48278528 0.5433603\n",
            " 0.5503823  0.49294233 0.50219154 0.48527583 0.52871066 0.4989485\n",
            " 0.49951273 0.48420691 0.5170745  0.5278437  0.5128857  0.57461333\n",
            " 0.5032478  0.4925733  0.5198223  0.4839096  0.5122282  0.5226477\n",
            " 0.46938577 0.4669139  0.52953446 0.5393198  0.47727734 0.48841363\n",
            " 0.5668031  0.47647265 0.52329785 0.50397927 0.5037101  0.57604665\n",
            " 0.49897808 0.5292473  0.52115893 0.51447415 0.5075638  0.48574492\n",
            " 0.46624133 0.52174634 0.5003742  0.5134726  0.54799193 0.55203867\n",
            " 0.55259854 0.52147126 0.5159905  0.5255712  0.54317904 0.5166476\n",
            " 0.52528995 0.56372595 0.5199431  0.5360638  0.50430405 0.55919313\n",
            " 0.5592837  0.5655344  0.5275919  0.49160752 0.5150612  0.49491987\n",
            " 0.4829625  0.5327653  0.47574633 0.4842725  0.48953432 0.4720179\n",
            " 0.47342452 0.5493949  0.5032985  0.5382005  0.49628145 0.5168361\n",
            " 0.5074453  0.547739   0.5200112  0.5441993  0.50601727 0.47519204\n",
            " 0.49040258 0.5421163  0.49288666 0.53589195 0.50215864 0.4920134\n",
            " 0.5253884  0.5433906  0.5237112  0.51599616 0.5159629  0.55856353\n",
            " 0.49226487 0.51217306 0.5279982  0.54441607 0.52625686 0.5247734\n",
            " 0.4469886  0.52252805 0.46701694 0.48452184 0.5112126  0.49678472\n",
            " 0.5554239  0.49559292 0.48538437 0.4875714  0.48101252 0.5193523\n",
            " 0.52752405 0.5267869  0.54435295 0.5132572  0.47982815 0.5658146\n",
            " 0.4727106  0.5727124  0.49833155 0.4752272  0.5330682  0.47618583\n",
            " 0.48496225 0.52143645 0.49637502 0.5344369  0.5637203  0.5097454\n",
            " 0.48535997 0.5433376  0.49285066 0.50233966 0.47504395 0.5244626\n",
            " 0.4711852  0.475844   0.5729403  0.5053252  0.5080713  0.53969103\n",
            " 0.53776515]\n",
            "[0.66149871 0.60378984 0.55727821 0.2962963  0.5047373  0.47631352\n",
            " 0.49698536 0.41257537 0.52713178 0.47028424 0.53488372 0.41343669\n",
            " 0.56761413 0.82687339 0.6287683  0.5667528  0.61584841 0.62962963\n",
            " 0.47631352 0.42463394 0.47631352 0.47975883 0.54952627 0.51593454\n",
            " 0.54005168 0.43066322 0.66838932 0.54608096 0.45219638 0.45822567\n",
            " 0.4918174  0.45908699 0.64254953 0.64427218 0.57536606 0.57364341\n",
            " 0.5538329  0.42204996 0.55211025 0.46597761 0.41946598 0.41429802\n",
            " 0.60034453 0.45994832 0.53919035 0.53229974 0.41085271 0.65202412\n",
            " 0.61929371 0.6580534  0.56072351 0.63135228 0.50732127 0.44013781\n",
            " 0.44444444 0.62101637 0.3712317  0.5503876  0.53402239 0.54091301\n",
            " 0.41085271 0.55986219 0.49440138 0.53919035 0.33936262 0.65116279\n",
            " 0.54349699 0.49267873 0.416882   0.46339363 0.43066322 0.47975883\n",
            " 0.57019811 0.51593454 0.54349699 0.4039621  0.62704565 0.66838932\n",
            " 0.43496985 0.45994832 0.55641688 0.52971576 0.56244617 0.43669251\n",
            " 0.51937984 0.56503015 0.55986219 0.45478036 0.64771748 0.52971576\n",
            " 0.71576227 0.40913006 0.52713178 0.60723514 0.58828596 0.63221361\n",
            " 0.54349699 0.56761413 0.47286822 0.47631352 0.47631352 0.61498708\n",
            " 0.55124892 0.52540913 0.42894057 0.55297158 0.59086994 0.64341085\n",
            " 0.5503876  0.56330749 0.5047373  0.51421189 0.53832903 0.51248923\n",
            " 0.46167097 0.71490095 0.47114556 0.50129199 0.64857881 0.46339363\n",
            " 0.51765719 0.63824289 0.61068045 0.62790698 0.51507321 0.57622739\n",
            " 0.36778639 0.51851852 0.50301464 0.53057709 0.47631352 0.56072351\n",
            " 0.5374677  0.45219638 0.69422911 0.46511628 0.43927649 0.53488372\n",
            " 0.42118863 0.4332472  0.53660637 0.40223945 0.4461671  0.48148148\n",
            " 0.56847545 0.53402239 0.51937984 0.57622739 0.36434109 0.44530577\n",
            " 0.60292851 0.48062016 0.30060293 0.33850129 0.54521964 0.5667528\n",
            " 0.66925065 0.34022394 0.59173127 0.55900086 0.52799311 0.51421189\n",
            " 0.38501292 0.5047373  0.69853575 0.58914729 0.5047373  0.583118\n",
            " 0.49612403 0.54177433 0.60551249 0.56761413 0.45822567 0.63824289\n",
            " 0.43496985 0.47717485 0.57795004 0.43669251 0.5503876  0.51507321\n",
            " 0.42377261 0.45564169 0.40826873 0.58914729 0.35745047 0.41860465\n",
            " 0.34625323 0.59345392 0.57622739 0.52024117 0.5374677  0.66494401\n",
            " 0.36175711 0.60034453 0.61326443 0.51765719 0.49956934 0.65202412\n",
            " 0.59086994 0.38845823 0.56158484 0.53919035 0.55211025 0.49095607\n",
            " 0.50645995 0.58914729 0.53574505 0.53143842 0.44702842 0.45391904\n",
            " 0.43238587 0.34797588 0.46425495 0.53660637 0.55555556 0.58570198\n",
            " 0.36864772 0.58139535 0.5538329  0.55555556 0.46080965 0.55900086\n",
            " 0.52971576 0.49870801 0.4625323  0.33850129 0.3910422  0.50387597\n",
            " 0.47028424 0.41946598 0.52540913 0.54263566 0.5047373  0.51248923\n",
            " 0.51937984 0.35745047 0.59862188 0.44444444 0.44186047 0.60120586\n",
            " 0.55900086 0.49354005 0.54091301 0.55900086 0.61929371 0.37209302\n",
            " 0.59517657 0.54005168 0.52627046 0.48751077 0.47200689 0.43841516\n",
            " 0.52024117 0.58656331 0.60809647 0.47114556 0.60206718 0.49440138\n",
            " 0.51679587 0.58656331 0.47286822 0.63738157 0.44099914 0.55297158\n",
            " 0.45650301 0.51937984 0.50301464 0.63652024 0.53057709 0.59345392\n",
            " 0.52971576 0.55297158 0.4788975  0.38156761 0.57105943 0.54005168\n",
            " 0.50990525 0.50990525 0.56761413 0.66666667 0.44358312 0.57622739\n",
            " 0.62790698 0.56330749 0.45047373 0.62790698 0.61412575 0.43238587\n",
            " 0.52196382 0.49612403 0.54177433 0.37898363 0.52627046 0.43410853\n",
            " 0.60637382 0.53488372 0.37812231 0.65374677 0.66322136 0.50043066\n",
            " 0.45305771 0.49870801 0.36520241 0.65030146 0.48234281 0.46683893\n",
            " 0.4918174  0.48406546 0.54435831 0.50732127 0.44702842 0.47459087\n",
            " 0.56761413 0.43152455 0.5503876  0.39276486 0.42204996 0.44272179\n",
            " 0.67355728 0.57622739 0.5211025  0.55727821 0.55813953 0.45736434\n",
            " 0.43066322 0.46425495 0.54780362 0.72782084 0.50904393 0.49956934\n",
            " 0.37898363 0.53832903 0.62704565 0.60809647 0.51593454 0.59000861\n",
            " 0.59086994 0.5667528  0.56933678 0.45564169 0.50387597 0.48492679\n",
            " 0.38931955 0.5667528  0.5081826  0.64082687 0.56847545 0.40826873\n",
            " 0.45908699 0.54952627 0.41602067 0.61929371 0.43583118 0.2213609\n",
            " 0.44702842 0.6416882  0.55986219 0.48664944 0.41429802 0.58742463\n",
            " 0.54866494 0.43755383 0.51593454 0.53229974 0.43927649 0.5047373\n",
            " 0.59345392 0.40740741 0.4918174  0.47200689 0.44099914 0.4952627\n",
            " 0.53057709 0.80878553 0.33936262 0.54091301 0.58570198 0.54694229\n",
            " 0.49009475 0.49956934 0.57450474 0.59776055 0.53919035 0.63652024\n",
            " 0.40568475 0.5047373  0.60292851 0.50990525 0.53919035 0.54091301\n",
            " 0.44875108 0.34797588 0.60292851 0.56416882 0.47459087 0.47717485\n",
            " 0.54435831 0.47286822 0.32213609 0.56072351 0.42549526 0.65374677\n",
            " 0.52799311 0.54608096 0.54005168 0.59173127 0.42894057 0.50129199\n",
            " 0.51937984 0.55900086 0.59086994 0.6546081  0.53919035 0.55641688\n",
            " 0.4952627  0.42291128 0.59086994 0.51765719 0.63049096 0.50990525\n",
            " 0.47631352 0.53143842 0.59689922 0.63910422 0.51421189 0.62962963\n",
            " 0.45047373 0.43927649 0.59431525 0.64685616 0.52196382 0.51421189\n",
            " 0.43669251 0.50215332 0.47200689 0.55900086 0.44702842 0.42463394\n",
            " 0.5081826  0.47803618 0.60120586 0.56503015 0.4788975  0.61584841\n",
            " 0.49870801 0.57536606 0.57881137 0.49956934 0.47286822 0.41085271\n",
            " 0.4332472  0.56158484 0.68819983 0.54694229 0.53660637 0.43066322\n",
            " 0.5211025  0.4496124  0.74677003 0.49095607 0.57019811 0.5538329\n",
            " 0.39793282 0.54091301 0.52971576 0.52368648 0.64254953 0.32213609\n",
            " 0.44530577 0.53057709 0.42635659 0.51248923 0.51937984 0.38501292\n",
            " 0.62446167 0.50387597 0.46942291 0.59431525 0.45478036 0.52799311\n",
            " 0.48664944 0.57795004 0.52799311 0.52196382 0.39534884 0.55813953\n",
            " 0.46080965 0.4952627  0.48320413 0.47200689 0.51593454 0.40913006\n",
            " 0.50559862 0.45822567 0.65202412 0.47114556 0.59689922 0.55986219\n",
            " 0.52024117 0.52282515 0.58484065 0.41515935 0.48234281 0.42894057\n",
            " 0.41429802 0.46511628 0.62187769 0.36175711 0.4754522  0.61068045\n",
            " 0.60981912]\n",
            "The trained model has an aproximate error rate of 2.3489167665198822 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others\n"
      ],
      "metadata": {
        "id": "9_3Jr46KHnik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Neural Network (DNN)"
      ],
      "metadata": {
        "id": "xIvme6cXJF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precipition (prcp)"
      ],
      "metadata": {
        "id": "cixnwflQxNZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/prcp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacea052-7451-498e-9c4b-87b6fde89572",
        "id": "F9YrBdhTYhyE"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prcp_dnn = df.drop(columns=['temp', 'dewp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud'])\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] != 2012]\n",
        "df_prcp_dnn = df_prcp_dnn.loc[df_prcp_dnn[\"year\"] < 2020]\n",
        "cols = df_prcp_dnn['NUM_COLLISIONS']\n",
        "df_prcp_dnn = df_prcp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_prcp_dnn.insert(loc=25, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_prcp_dnn[:6])\n",
        "df_prcp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "jw5elr-LbRBF",
        "outputId": "ab160779-b46f-4827-dbdd-ccd36b49c4c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  Dec  \\\n",
            "49  2016  28  0.09    0             0                 0     0    0    0    0   \n",
            "51  2014  17  0.00    1             0                 0     0    0    0    0   \n",
            "54  2016  25  0.02    0             0                 0     0    0    0    0   \n",
            "55  2016  29  0.00    0             0                 0     0    0    0    0   \n",
            "58  2017  20  0.00    0             0                 0     0    0    0    0   \n",
            "59  2013  13  0.01    1             0                 0     0    0    0    0   \n",
            "\n",
            "    ...  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49  ...    0    0    0    0    0    0    0    0    1             681  \n",
            "51  ...    0    0    0    0    0    0    1    0    0             589  \n",
            "54  ...    0    0    0    0    0    1    0    0    0             658  \n",
            "55  ...    0    0    0    0    0    0    1    0    0             645  \n",
            "58  ...    0    0    0    0    0    0    1    0    0             605  \n",
            "59  ...    0    0    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         prcp          fog  rain_drizzle  \\\n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000   2539.000000   \n",
              "mean   2015.989366    15.745569     0.122588     0.253249      0.375345   \n",
              "std       1.996126     8.803199     0.329143     0.434958      0.484307   \n",
              "min    2013.000000     1.000000     0.000000     0.000000      0.000000   \n",
              "25%    2014.000000     8.000000     0.000000     0.000000      0.000000   \n",
              "50%    2016.000000    16.000000     0.000000     0.000000      0.000000   \n",
              "75%    2018.000000    23.000000     0.060000     1.000000      1.000000   \n",
              "max    2019.000000    31.000000     3.760000     1.000000      1.000000   \n",
              "\n",
              "       snow_ice_pellets         hail          Apr          Aug          Dec  \\\n",
              "count       2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean           0.085467     0.000394     0.082316     0.083497     0.085467   \n",
              "std            0.279630     0.019846     0.274899     0.276687     0.279630   \n",
              "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "       ...          Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  ...  2539.000000  2539.000000  2539.000000  2539.000000  2539.000000   \n",
              "mean   ...     0.082710     0.085467     0.079953     0.142970     0.143364   \n",
              "std    ...     0.275497     0.279630     0.271273     0.350111     0.350512   \n",
              "min    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%    ...     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max    ...     1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2539.000000  2539.000000  2539.000000  2539.000000     2539.000000  \n",
              "mean      0.143757     0.142182     0.142576     0.142182      599.135093  \n",
              "std       0.350913     0.349305     0.349709     0.349305      100.299164  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47303b78-aa89-4aa5-94ef-144c116e4a66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>prcp</th>\n",
              "      <th>fog</th>\n",
              "      <th>rain_drizzle</th>\n",
              "      <th>snow_ice_pellets</th>\n",
              "      <th>hail</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "      <td>2539.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.989366</td>\n",
              "      <td>15.745569</td>\n",
              "      <td>0.122588</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>0.375345</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.082316</td>\n",
              "      <td>0.083497</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.085467</td>\n",
              "      <td>0.079953</td>\n",
              "      <td>0.142970</td>\n",
              "      <td>0.143364</td>\n",
              "      <td>0.143757</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>0.142576</td>\n",
              "      <td>0.142182</td>\n",
              "      <td>599.135093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.996126</td>\n",
              "      <td>8.803199</td>\n",
              "      <td>0.329143</td>\n",
              "      <td>0.434958</td>\n",
              "      <td>0.484307</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.019846</td>\n",
              "      <td>0.274899</td>\n",
              "      <td>0.276687</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275497</td>\n",
              "      <td>0.279630</td>\n",
              "      <td>0.271273</td>\n",
              "      <td>0.350111</td>\n",
              "      <td>0.350512</td>\n",
              "      <td>0.350913</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>0.349709</td>\n",
              "      <td>0.349305</td>\n",
              "      <td>100.299164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47303b78-aa89-4aa5-94ef-144c116e4a66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47303b78-aa89-4aa5-94ef-144c116e4a66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47303b78-aa89-4aa5-94ef-144c116e4a66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_prcp_dnn.iloc[np.random.permutation(len(df_prcp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpJi3P_8YcIq",
        "outputId": "420dd2aa-98c0-41b8-fa98-a7a710c98c0e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  prcp  fog  rain_drizzle  snow_ice_pellets  hail  Apr  Aug  \\\n",
            "2213  2016  30  0.00    0             0                 0     0    0    1   \n",
            "275   2017  11  0.33    0             1                 0     0    0    0   \n",
            "650   2014   1  0.00    0             0                 0     0    0    0   \n",
            "3525  2017   7  0.02    0             0                 0     0    0    0   \n",
            "642   2018  23  0.07    0             0                 0     0    0    0   \n",
            "1057  2014   9  0.45    0             0                 0     0    1    0   \n",
            "\n",
            "      Dec  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2213    0  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "275     0  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "650     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
            "3525    1  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "642     0  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "1057    0  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_A0EFvxZpeA",
        "outputId": "ea561961-bd14-4b37-bf71-dbe1ebb4a1c4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2213    622\n",
            "275     629\n",
            "650     497\n",
            "3525    693\n",
            "642     732\n",
            "1057    595\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdO_kvOZZuii",
        "outputId": "9ccab4d3-08a6-4134-eb3e-c6111840dc26"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_prcp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaTuIaPRfMxx",
        "outputId": "91d05cf4-45a6-49d3-8405-051d069e1d71"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff06326e250>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:loss = 125586.39, step = 1\n",
            "INFO:tensorflow:global_step/sec: 732.861\n",
            "INFO:tensorflow:loss = 0.21339825, step = 101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.989\n",
            "INFO:tensorflow:loss = 0.1458404, step = 201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.26\n",
            "INFO:tensorflow:loss = 0.1365065, step = 301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.901\n",
            "INFO:tensorflow:loss = 0.14307836, step = 401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.158\n",
            "INFO:tensorflow:loss = 0.11546454, step = 501 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1037.87\n",
            "INFO:tensorflow:loss = 0.08977402, step = 601 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.4\n",
            "INFO:tensorflow:loss = 0.09675571, step = 701 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.542\n",
            "INFO:tensorflow:loss = 0.08122686, step = 801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.188\n",
            "INFO:tensorflow:loss = 0.06656361, step = 901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.16\n",
            "INFO:tensorflow:loss = 0.090711854, step = 1001 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.496\n",
            "INFO:tensorflow:loss = 0.06675537, step = 1101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.256\n",
            "INFO:tensorflow:loss = 0.047448933, step = 1201 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1085.03\n",
            "INFO:tensorflow:loss = 0.038091507, step = 1301 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.781\n",
            "INFO:tensorflow:loss = 0.036309145, step = 1401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.91\n",
            "INFO:tensorflow:loss = 0.04200376, step = 1501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1062.15\n",
            "INFO:tensorflow:loss = 0.030242119, step = 1601 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1047.19\n",
            "INFO:tensorflow:loss = 0.028841477, step = 1701 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.21\n",
            "INFO:tensorflow:loss = 0.021841176, step = 1801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.354\n",
            "INFO:tensorflow:loss = 0.016726518, step = 1901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.569\n",
            "INFO:tensorflow:loss = 0.01089229, step = 2001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.298\n",
            "INFO:tensorflow:loss = 0.012857328, step = 2101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1002.09\n",
            "INFO:tensorflow:loss = 0.019250423, step = 2201 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1054.6\n",
            "INFO:tensorflow:loss = 0.011883627, step = 2301 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.84\n",
            "INFO:tensorflow:loss = 0.009729302, step = 2401 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.195\n",
            "INFO:tensorflow:loss = 0.008871341, step = 2501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.165\n",
            "INFO:tensorflow:loss = 0.010357233, step = 2601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.384\n",
            "INFO:tensorflow:loss = 0.016724512, step = 2701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 757.086\n",
            "INFO:tensorflow:loss = 0.0073548406, step = 2801 (0.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.342\n",
            "INFO:tensorflow:loss = 0.00706937, step = 2901 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.74\n",
            "INFO:tensorflow:loss = 0.013407819, step = 3001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.07\n",
            "INFO:tensorflow:loss = 0.022329178, step = 3101 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.9\n",
            "INFO:tensorflow:loss = 0.9957446, step = 3201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.33\n",
            "INFO:tensorflow:loss = 0.15458113, step = 3301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1036.19\n",
            "INFO:tensorflow:loss = 0.033558726, step = 3401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.448\n",
            "INFO:tensorflow:loss = 0.01723721, step = 3501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.106\n",
            "INFO:tensorflow:loss = 0.029214442, step = 3601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1029.21\n",
            "INFO:tensorflow:loss = 3.1256285, step = 3701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.123\n",
            "INFO:tensorflow:loss = 0.006871424, step = 3801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.777\n",
            "INFO:tensorflow:loss = 2.8425512, step = 3901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.68\n",
            "INFO:tensorflow:loss = 0.076087855, step = 4001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.746\n",
            "INFO:tensorflow:loss = 0.71110153, step = 4101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1057.07\n",
            "INFO:tensorflow:loss = 0.026424961, step = 4201 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.84\n",
            "INFO:tensorflow:loss = 0.00605728, step = 4301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.26\n",
            "INFO:tensorflow:loss = 0.005584327, step = 4401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 954.555\n",
            "INFO:tensorflow:loss = 0.0062930826, step = 4501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.7\n",
            "INFO:tensorflow:loss = 0.009287559, step = 4601 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.29\n",
            "INFO:tensorflow:loss = 0.27152154, step = 4701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014.63\n",
            "INFO:tensorflow:loss = 0.017689805, step = 4801 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033.49\n",
            "INFO:tensorflow:loss = 0.01973609, step = 4901 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.182\n",
            "INFO:tensorflow:loss = 0.4306364, step = 5001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 922.559\n",
            "INFO:tensorflow:loss = 0.08272317, step = 5101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1039.7\n",
            "INFO:tensorflow:loss = 0.02090478, step = 5201 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.39\n",
            "INFO:tensorflow:loss = 0.020164877, step = 5301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.37\n",
            "INFO:tensorflow:loss = 0.33042276, step = 5401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.2\n",
            "INFO:tensorflow:loss = 0.19272056, step = 5501 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1065.84\n",
            "INFO:tensorflow:loss = 0.02595859, step = 5601 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.699\n",
            "INFO:tensorflow:loss = 0.10210486, step = 5701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1013.35\n",
            "INFO:tensorflow:loss = 0.0041082497, step = 5801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.621\n",
            "INFO:tensorflow:loss = 0.59493554, step = 5901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.768\n",
            "INFO:tensorflow:loss = 0.017000664, step = 6001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.899\n",
            "INFO:tensorflow:loss = 0.012963736, step = 6101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.79\n",
            "INFO:tensorflow:loss = 0.08433356, step = 6201 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014.68\n",
            "INFO:tensorflow:loss = 1.4685544, step = 6301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.139\n",
            "INFO:tensorflow:loss = 0.0074939537, step = 6401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.033\n",
            "INFO:tensorflow:loss = 0.0062750992, step = 6501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1057.69\n",
            "INFO:tensorflow:loss = 0.17844796, step = 6601 (0.091 sec)\n",
            "INFO:tensorflow:global_step/sec: 1036.64\n",
            "INFO:tensorflow:loss = 0.020781232, step = 6701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 762.243\n",
            "INFO:tensorflow:loss = 0.23777977, step = 6801 (0.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.298\n",
            "INFO:tensorflow:loss = 0.007953955, step = 6901 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1065.87\n",
            "INFO:tensorflow:loss = 3.5548136, step = 7001 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.554\n",
            "INFO:tensorflow:loss = 0.01766563, step = 7101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.37\n",
            "INFO:tensorflow:loss = 0.014947416, step = 7201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.88\n",
            "INFO:tensorflow:loss = 0.014533697, step = 7301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.845\n",
            "INFO:tensorflow:loss = 0.010038555, step = 7401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.69\n",
            "INFO:tensorflow:loss = 0.010855375, step = 7501 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1085.29\n",
            "INFO:tensorflow:loss = 0.0093494635, step = 7601 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.78\n",
            "INFO:tensorflow:loss = 0.0071899127, step = 7701 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 868.863\n",
            "INFO:tensorflow:loss = 0.007842862, step = 7801 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.49\n",
            "INFO:tensorflow:loss = 0.00590141, step = 7901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1060.78\n",
            "INFO:tensorflow:loss = 0.0056918133, step = 8001 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.534\n",
            "INFO:tensorflow:loss = 0.0059695486, step = 8101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1059.72\n",
            "INFO:tensorflow:loss = 0.0054574627, step = 8201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1042.99\n",
            "INFO:tensorflow:loss = 0.008255351, step = 8301 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.22\n",
            "INFO:tensorflow:loss = 0.006048654, step = 8401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.234\n",
            "INFO:tensorflow:loss = 0.0041087926, step = 8501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1057.14\n",
            "INFO:tensorflow:loss = 0.0064277174, step = 8601 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014.18\n",
            "INFO:tensorflow:loss = 0.004527954, step = 8701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.11\n",
            "INFO:tensorflow:loss = 0.007338593, step = 8801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.757\n",
            "INFO:tensorflow:loss = 0.006626851, step = 8901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.83\n",
            "INFO:tensorflow:loss = 0.006114698, step = 9001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.978\n",
            "INFO:tensorflow:loss = 0.0062059844, step = 9101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1063.44\n",
            "INFO:tensorflow:loss = 0.004260773, step = 9201 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.06\n",
            "INFO:tensorflow:loss = 0.0062398594, step = 9301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.314\n",
            "INFO:tensorflow:loss = 0.005314892, step = 9401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 968.894\n",
            "INFO:tensorflow:loss = 0.005600079, step = 9501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1047.06\n",
            "INFO:tensorflow:loss = 0.0070595155, step = 9601 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.29\n",
            "INFO:tensorflow:loss = 0.0060749818, step = 9701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.254\n",
            "INFO:tensorflow:loss = 0.0070048673, step = 9801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.668\n",
            "INFO:tensorflow:loss = 0.007583984, step = 9901 (0.108 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_prcp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0064945417.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 84.84903754233527\n",
            "Just using average = 598.8050221565732 has RMSE of 97.02374587140838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_prcp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db04428-f64a-4087-d89a-461a71997e78",
        "id": "Ld6baV60hPOs"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff062e31b50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_prcp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_prcp/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.53398955 0.54161865 0.5170258  0.41488212 0.51392347 0.52405334\n",
            " 0.47674736 0.50921977 0.48811716 0.5183796  0.5045387  0.5307376\n",
            " 0.5018388  0.47520146 0.5100182  0.54444134 0.50718445 0.4675887\n",
            " 0.563306   0.47968167 0.50087845 0.409858   0.55063665 0.5247508\n",
            " 0.48154104 0.5181476  0.55130076 0.4897993  0.5553692  0.47166947\n",
            " 0.52934533 0.53669703 0.5125815  0.4756523  0.53374577 0.54030013\n",
            " 0.5424858  0.5092124  0.51411724 0.508694   0.49558157 0.5191568\n",
            " 0.54637694 0.49556643 0.52535594 0.5701045  0.5116277  0.45969743\n",
            " 0.511182   0.50846064 0.56613135 0.5011097  0.43986717 0.4627719\n",
            " 0.5160634  0.5286639  0.52235925 0.52916133 0.40699974 0.47095713\n",
            " 0.4797127  0.5099393  0.555771   0.53634465 0.5245134  0.4553373\n",
            " 0.53089696 0.48551467 0.4686514  0.55081064 0.5404421  0.48438203\n",
            " 0.48307312 0.5444004  0.5264542  0.51868147 0.5063669  0.48636115\n",
            " 0.54961365 0.47714847 0.5047677  0.5082794  0.5321127  0.49253583\n",
            " 0.46418363 0.48944935 0.5011878  0.46203455 0.4991766  0.43332943\n",
            " 0.5164447  0.48999405 0.48932457 0.55250716 0.47401956 0.5593049\n",
            " 0.43051925 0.52461714 0.547442   0.5558877  0.5510684  0.5281238\n",
            " 0.50366926 0.51027197 0.5287928  0.52194387 0.53461534 0.53755\n",
            " 0.5515604  0.53379464 0.5134723  0.47544864 0.5203084  0.5478773\n",
            " 0.46970212 0.5255986  0.49236146 0.5334412  0.5210455  0.53088003\n",
            " 0.5166732  0.52986383 0.5115328  0.53193074 0.46665496 0.5411666\n",
            " 0.49868983 0.5707493  0.53567195 0.55088884 0.5301995  0.5116644\n",
            " 0.45736492 0.4453805  0.47185844 0.52187586 0.5431551  0.47447202\n",
            " 0.5445575  0.53006965 0.5363516  0.5232649  0.55884415 0.51128006\n",
            " 0.53964967 0.5448258  0.5340052  0.48683527 0.51973623 0.4959142\n",
            " 0.51630926 0.47625786 0.514782   0.53334266 0.4613734  0.5083878\n",
            " 0.5312664  0.5247449  0.48025715 0.45008224 0.52521586 0.49636665\n",
            " 0.4543758  0.49915066 0.53767234 0.5453653  0.55224943 0.51921344\n",
            " 0.51041174 0.5452612  0.5065768  0.486477   0.49454248 0.4833301\n",
            " 0.56477964 0.5496933  0.53093386 0.49757737 0.5685944  0.47309262\n",
            " 0.4027613  0.423177   0.5410144  0.50058085 0.49591854 0.4854879\n",
            " 0.4998506  0.5155924  0.5226139  0.48870376 0.53460824 0.5451531\n",
            " 0.516756   0.46698812 0.4783585  0.49305317 0.416902   0.48865435\n",
            " 0.5121347  0.52098864 0.42675474 0.4444342  0.5289176  0.5232819\n",
            " 0.51811165 0.5000766  0.5242765  0.5207814  0.48645785 0.51598114\n",
            " 0.47252747 0.4486067  0.5153934  0.50866383 0.523287   0.53267515\n",
            " 0.52159655 0.54176944 0.4745857  0.51868397 0.5063633  0.4982507\n",
            " 0.50757146 0.5105175  0.54511994 0.5224519  0.48451924 0.4882357\n",
            " 0.46938226 0.5405575  0.43559062 0.49217087 0.4543232  0.48912495\n",
            " 0.52091575 0.5168494  0.5590511  0.5261899  0.54181427 0.48696318\n",
            " 0.47885257 0.5234816  0.49272323 0.563436   0.5403457  0.46754673\n",
            " 0.5095228  0.4719386  0.43540144 0.46485648 0.55022895 0.44155636\n",
            " 0.49690032 0.4915898  0.52586627 0.5129425  0.5203724  0.5433158\n",
            " 0.5108322  0.52180254 0.47917995 0.5175583  0.5130898  0.4902868\n",
            " 0.43447003 0.508813   0.5420312  0.49959567 0.5064056  0.49383098\n",
            " 0.5232339  0.56183875 0.5282569  0.5268772  0.5521715  0.5320748\n",
            " 0.51412463 0.504469   0.49909034 0.52186984 0.5085376  0.47529557\n",
            " 0.5236833  0.49348465 0.47405285 0.5263187  0.51085293 0.51608974\n",
            " 0.50905293 0.52939516 0.56993246 0.49522352 0.5032115  0.53775823\n",
            " 0.51797533 0.46959382 0.44095716 0.5177193  0.53669745 0.5381965\n",
            " 0.52782696 0.496451   0.4617457  0.4669046  0.40249008 0.44012296\n",
            " 0.47574213 0.4761034  0.49361405 0.5356619  0.52632725 0.49212497\n",
            " 0.41994163 0.4990701  0.50802755 0.59326214 0.44985047 0.47711107\n",
            " 0.45452452 0.43146268 0.42986113 0.46919268 0.52277553 0.54696554\n",
            " 0.47104222 0.5117031  0.4754884  0.519285   0.52401567 0.5146458\n",
            " 0.48725194 0.4707859  0.521948   0.51052505 0.5089086  0.51384956\n",
            " 0.5142931  0.45003143 0.502443   0.47821894 0.50959945 0.44989204\n",
            " 0.47320497 0.5055428  0.5137689  0.43213326 0.5376674  0.52492136\n",
            " 0.45330665 0.4651385  0.51261103 0.5106492  0.5519963  0.52846384\n",
            " 0.48678604 0.4675212  0.48614323 0.47104138 0.5210626  0.51550925\n",
            " 0.54988587 0.5088447  0.55191624 0.570437   0.51169026 0.48308378\n",
            " 0.48121214 0.50499624 0.5344885  0.56368476 0.52463514 0.48786822\n",
            " 0.52650386 0.5202204  0.5162004  0.4526253  0.5608309  0.51639616\n",
            " 0.48679566 0.4651976  0.53440905 0.52905023 0.40525684 0.52362084\n",
            " 0.5101059  0.51004726 0.5312845  0.5068454  0.5263886  0.5348779\n",
            " 0.5020404  0.5773168  0.47158688 0.52193207 0.4992941  0.4781132\n",
            " 0.51586854 0.48223433 0.52940005 0.53731924 0.42903492 0.53182024\n",
            " 0.52576005 0.52218884 0.5015173  0.5539161  0.39615446 0.5346336\n",
            " 0.40436208 0.43273434 0.5245319  0.49670845 0.5400864  0.5336003\n",
            " 0.4687021  0.53380233 0.5018497  0.48878327 0.5422363  0.52847594\n",
            " 0.45963073 0.48722365 0.4080653  0.45816365 0.46585354 0.5318629\n",
            " 0.52112174 0.4890396  0.516315   0.5413059  0.53405225 0.56051123\n",
            " 0.5315184  0.49763831 0.4994449  0.5279282  0.52276605 0.5326727\n",
            " 0.58244914 0.5017209  0.43956026 0.53431624 0.5352878  0.4686069\n",
            " 0.52936584 0.52867585 0.5225338  0.54268825 0.5628261  0.52209604\n",
            " 0.52182233 0.53367054 0.51257604 0.49364266 0.5153198  0.5418344\n",
            " 0.52842593 0.53635216 0.51971257 0.4999504  0.499718   0.54842633\n",
            " 0.46988606 0.57108563 0.49229422 0.54203105 0.52345467 0.555585\n",
            " 0.5201038  0.50329626 0.5177655  0.5005786  0.54681695 0.53294957\n",
            " 0.5273342  0.5247221  0.44926694 0.5015623  0.55648    0.5264648\n",
            " 0.5208923  0.50689363 0.4636499  0.5150283  0.4811766  0.5295307\n",
            " 0.5479649  0.5361186  0.49890864 0.5038419  0.48475036 0.47851366\n",
            " 0.5295911  0.4509878  0.5446669  0.5442853  0.49475628 0.50563484\n",
            " 0.5753164  0.5212292  0.5593135  0.55621964 0.5071581  0.4762634\n",
            " 0.5357284  0.491485   0.5108646  0.52685964]\n",
            "[0.38156761 0.59862188 0.58225668 0.38587425 0.56933678 0.5211025\n",
            " 0.49009475 0.51162791 0.4918174  0.51851852 0.54177433 0.58139535\n",
            " 0.45478036 0.42635659 0.57536606 0.47459087 0.47286822 0.40913006\n",
            " 0.63135228 0.56330749 0.43927649 0.42204996 0.4918174  0.61498708\n",
            " 0.46683893 0.56072351 0.47631352 0.47631352 0.58656331 0.44530577\n",
            " 0.49956934 0.47459087 0.55727821 0.43755383 0.53229974 0.60120586\n",
            " 0.62704565 0.45650301 0.60034453 0.51765719 0.40826873 0.5211025\n",
            " 0.72437554 0.54521964 0.53402239 0.58053402 0.51335056 0.36347976\n",
            " 0.59259259 0.52196382 0.66494401 0.60723514 0.42118863 0.40913006\n",
            " 0.58225668 0.46511628 0.4952627  0.50732127 0.41429802 0.54608096\n",
            " 0.51421189 0.51851852 0.55211025 0.62790698 0.54005168 0.50301464\n",
            " 0.45908699 0.48664944 0.56589147 0.59173127 0.64513351 0.4918174\n",
            " 0.53919035 0.46856158 0.69853575 0.44358312 0.47459087 0.4203273\n",
            " 0.36606374 0.59259259 0.5667528  0.52713178 0.5960379  0.40137812\n",
            " 0.4788975  0.46080965 0.44530577 0.48492679 0.37984496 0.45478036\n",
            " 0.60206718 0.4625323  0.5081826  0.55813953 0.56761413 0.71748493\n",
            " 0.3453919  0.53316107 0.5667528  0.60292851 0.56330749 0.4918174\n",
            " 0.49784668 0.56072351 0.49784668 0.44444444 0.5081826  0.71403962\n",
            " 0.45822567 0.4952627  0.61412575 0.49784668 0.4039621  0.61843239\n",
            " 0.51335056 0.50904393 0.45391904 0.47286822 0.5047373  0.50215332\n",
            " 0.55641688 0.60034453 0.46080965 0.62446167 0.50129199 0.58484065\n",
            " 0.55555556 0.60120586 0.58397933 0.57364341 0.51593454 0.54435831\n",
            " 0.45908699 0.38587425 0.53057709 0.53919035 0.63738157 0.55124892\n",
            " 0.63996555 0.55900086 0.33936262 0.40568475 0.56847545 0.5081826\n",
            " 0.63307494 0.58139535 0.625323   0.50645995 0.35228252 0.55813953\n",
            " 0.55900086 0.55641688 0.56503015 0.45650301 0.48148148 0.52799311\n",
            " 0.53057709 0.65202412 0.48664944 0.43238587 0.57278208 0.54091301\n",
            " 0.41946598 0.51507321 0.4918174  0.45994832 0.83893196 0.37295435\n",
            " 0.55900086 0.63910422 0.54521964 0.57795004 0.60292851 0.77174849\n",
            " 0.49009475 0.58656331 0.58570198 0.45822567 0.66666667 0.46080965\n",
            " 0.44530577 0.34969854 0.59345392 0.52024117 0.51765719 0.44186047\n",
            " 0.55469423 0.374677   0.54263566 0.60034453 0.52627046 0.64857881\n",
            " 0.60637382 0.5081826  0.34022394 0.44702842 0.416882   0.47200689\n",
            " 0.61929371 0.53660637 0.36778639 0.44099914 0.48751077 0.59689922\n",
            " 0.55555556 0.48492679 0.49612403 0.74677003 0.45305771 0.48062016\n",
            " 0.43669251 0.53574505 0.62962963 0.52799311 0.44702842 0.52885444\n",
            " 0.5211025  0.51593454 0.44272179 0.62962963 0.52540913 0.5047373\n",
            " 0.48234281 0.53057709 0.45736434 0.46856158 0.48578811 0.52799311\n",
            " 0.56330749 0.59431525 0.59689922 0.42807924 0.46856158 0.51851852\n",
            " 0.65374677 0.65891473 0.66838932 0.4952627  0.56761413 0.4918174\n",
            " 0.47114556 0.54349699 0.53660637 0.46597761 0.67700258 0.36864772\n",
            " 0.47975883 0.46683893 0.416882   0.42204996 0.62015504 0.40999139\n",
            " 0.49095607 0.37553833 0.48664944 0.48406546 0.61584841 0.56933678\n",
            " 0.48837209 0.57536606 0.25064599 0.42721792 0.53229974 1.\n",
            " 0.44099914 0.62015504 0.52196382 0.57364341 0.55986219 0.60809647\n",
            " 0.52971576 0.51507321 0.4952627  0.43238587 0.4754522  0.43496985\n",
            " 0.60809647 0.5245478  0.42549526 0.57622739 0.48837209 0.39448751\n",
            " 0.59862188 0.49870801 0.41429802 0.48406546 0.55641688 0.55813953\n",
            " 0.53488372 0.57536606 0.68819983 0.49784668 0.43152455 0.59259259\n",
            " 0.50732127 0.40310078 0.49354005 0.62187769 0.46425495 0.55813953\n",
            " 0.45822567 0.47459087 0.45305771 0.43496985 0.38845823 0.42980189\n",
            " 0.54349699 0.47286822 0.44272179 0.54694229 0.60723514 0.44530577\n",
            " 0.43496985 0.50215332 0.49956934 0.60981912 0.53229974 0.54005168\n",
            " 0.35745047 0.41257537 0.39793282 0.47717485 0.49009475 0.58570198\n",
            " 0.47631352 0.51421189 0.49956934 0.58914729 0.48664944 0.48751077\n",
            " 0.42118863 0.53660637 0.54263566 0.50990525 0.40913006 0.57536606\n",
            " 0.43152455 0.49267873 0.51851852 0.40913006 0.51937984 0.39362618\n",
            " 0.374677   0.49009475 0.50559862 0.35917313 0.51937984 0.50559862\n",
            " 0.45478036 0.45994832 0.47975883 0.48406546 0.60292851 0.63652024\n",
            " 0.53574505 0.48148148 0.51937984 0.45822567 0.6089578  0.49698536\n",
            " 0.43238587 0.51937984 0.67355728 0.56330749 0.53832903 0.51937984\n",
            " 0.34625323 0.50990525 0.59086994 0.5667528  0.60378984 0.47631352\n",
            " 0.4918174  0.60378984 0.22739018 0.3712317  0.57536606 0.54866494\n",
            " 0.55124892 0.51765719 0.57364341 0.5503876  0.41774332 0.48492679\n",
            " 0.51507321 0.52799311 0.52540913 0.64685616 0.50387597 0.57881137\n",
            " 0.48148148 0.61584841 0.44444444 0.60809647 0.54005168 0.43410853\n",
            " 0.5374677  0.5047373  0.56244617 0.71576227 0.43238587 0.56158484\n",
            " 0.54177433 0.5374677  0.4005168  0.45133506 0.36950904 0.58656331\n",
            " 0.42204996 0.37639966 0.44358312 0.4788975  0.46511628 0.64857881\n",
            " 0.48148148 0.63307494 0.38845823 0.49784668 0.58484065 0.53402239\n",
            " 0.4496124  0.41946598 0.3875969  0.40999139 0.4625323  0.59173127\n",
            " 0.47459087 0.48837209 0.58139535 0.58656331 0.52282515 0.58828596\n",
            " 0.45047373 0.45305771 0.55986219 0.43152455 0.54005168 0.58397933\n",
            " 0.64944014 0.57278208 0.41429802 0.60206718 0.65030146 0.47286822\n",
            " 0.46942291 0.57278208 0.46425495 0.56072351 0.70887166 0.52971576\n",
            " 0.60034453 0.57019811 0.50387597 0.59086994 0.41429802 0.4754522\n",
            " 0.53660637 0.69681309 0.60206718 0.52971576 0.54694229 0.37639966\n",
            " 0.47803618 0.50301464 0.5047373  0.41860465 0.47975883 0.64685616\n",
            " 0.42549526 0.42291128 0.55555556 0.47114556 0.53143842 0.6416882\n",
            " 0.5503876  0.55297158 0.43669251 0.52368648 0.53488372 0.38845823\n",
            " 0.57105943 0.33850129 0.48923342 0.49267873 0.58139535 0.54952627\n",
            " 0.59345392 0.60034453 0.55124892 0.46770026 0.49354005 0.42291128\n",
            " 0.56933678 0.45564169 0.50043066 0.66149871 0.52799311 0.56589147\n",
            " 0.62704565 0.58742463 0.52799311 0.65288544 0.5667528  0.45736434\n",
            " 0.58656331 0.41343669 0.54263566 0.61412575]\n",
            "The trained model has an aproximate error rate of 11.038719020783901 which equates to 2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dew Point (dewp)\n"
      ],
      "metadata": {
        "id": "yT91LA7Xx77W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/dewp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe0a0d1-6c6a-4459-b2aa-ac9e69ee325f",
        "id": "zQAH_kVzyAOD"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dewp_dnn = df.drop(columns=['temp', 'prcp', 'slp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] != 2012]\n",
        "df_dewp_dnn = df_dewp_dnn.loc[df_dewp_dnn[\"year\"] < 2020]\n",
        "cols = df_dewp_dnn['NUM_COLLISIONS']\n",
        "df_dewp_dnn = df_dewp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_dewp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_dewp_dnn[:6])\n",
        "df_dewp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "7a96d765-ac94-4e6c-ccf6-b9073c578d0b",
        "id": "_qzGppAwyAOE"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  24.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  35.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  21.2    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29  36.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  32.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  44.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da         dewp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2015.999217    15.723679    44.163170     0.082192     0.084932   \n",
              "std       2.000000     8.801271    16.995303     0.274710     0.278834   \n",
              "min    2013.000000     1.000000    -6.700000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    32.150000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    45.300000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    58.500000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    74.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084932     0.077104     0.084932     0.084540     0.082192  ...   \n",
              "std       0.278834     0.266808     0.278834     0.278251     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142857      599.109980  \n",
              "std       0.349996     0.349996     0.349996     0.349996      100.277185  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-059a7281-947e-435f-bc4e-0d50c2505d6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>dewp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.999217</td>\n",
              "      <td>15.723679</td>\n",
              "      <td>44.163170</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>599.109980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.801271</td>\n",
              "      <td>16.995303</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>100.277185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>32.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-059a7281-947e-435f-bc4e-0d50c2505d6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-059a7281-947e-435f-bc4e-0d50c2505d6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-059a7281-947e-435f-bc4e-0d50c2505d6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_dewp_dnn.iloc[np.random.permutation(len(df_dewp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f2d871-046f-4bc9-d9ef-c9b37fe87c81",
        "id": "z5-eCTxByAOE"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  dewp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "2445  2019  21  68.5    0    1    0    0    0    0    0  ...    0    0    0   \n",
            "1121  2013  23  40.0    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "505   2016   4  49.4    0    0    0    1    0    0    0  ...    0    0    0   \n",
            "801   2014  22  27.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "2647  2014   8  55.3    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "843   2018  27  28.7    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "2445    0    0    0    0    0    1    0  \n",
            "1121    0    1    0    0    0    0    0  \n",
            "505     0    0    0    0    0    0    1  \n",
            "801     0    0    0    0    0    0    0  \n",
            "2647    1    0    0    1    0    0    0  \n",
            "843     0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f313f0d-09b4-4953-dbc8-051aad3aad6c",
        "id": "BbrOrPfQyAOE"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2445    522\n",
            "1121    513\n",
            "505     609\n",
            "801     493\n",
            "2647    598\n",
            "843     627\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8402251-f49b-40cf-a6b9-dfef4766c7c1",
        "id": "T2v7BylMyAOE"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_dewp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ba69c9-31ce-4d62-b729-24ff580681f4",
        "id": "HMGFsHtkyAOE"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff062d8f050>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:loss = 41988.633, step = 1\n",
            "INFO:tensorflow:global_step/sec: 794.948\n",
            "INFO:tensorflow:loss = 0.896653, step = 101 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.97\n",
            "INFO:tensorflow:loss = 0.77257544, step = 201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1045.91\n",
            "INFO:tensorflow:loss = 0.45631158, step = 301 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.182\n",
            "INFO:tensorflow:loss = 0.41470122, step = 401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.174\n",
            "INFO:tensorflow:loss = 0.2373845, step = 501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.41\n",
            "INFO:tensorflow:loss = 0.16055687, step = 601 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.35\n",
            "INFO:tensorflow:loss = 0.1007534, step = 701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026.77\n",
            "INFO:tensorflow:loss = 0.062080175, step = 801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.989\n",
            "INFO:tensorflow:loss = 0.034763895, step = 901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.276\n",
            "INFO:tensorflow:loss = 0.028833173, step = 1001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1053.25\n",
            "INFO:tensorflow:loss = 0.018453296, step = 1101 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1028.43\n",
            "INFO:tensorflow:loss = 0.012509272, step = 1201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.076\n",
            "INFO:tensorflow:loss = 0.010788899, step = 1301 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1044.3\n",
            "INFO:tensorflow:loss = 0.010198725, step = 1401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1047.05\n",
            "INFO:tensorflow:loss = 0.0066500623, step = 1501 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.793\n",
            "INFO:tensorflow:loss = 0.005926492, step = 1601 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.285\n",
            "INFO:tensorflow:loss = 0.0059783077, step = 1701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.62\n",
            "INFO:tensorflow:loss = 0.007756699, step = 1801 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.67\n",
            "INFO:tensorflow:loss = 0.0043593924, step = 1901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1025.69\n",
            "INFO:tensorflow:loss = 0.0055320896, step = 2001 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.38\n",
            "INFO:tensorflow:loss = 0.0038231849, step = 2101 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.34\n",
            "INFO:tensorflow:loss = 0.0065790187, step = 2201 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1072.06\n",
            "INFO:tensorflow:loss = 0.00827956, step = 2301 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1015.24\n",
            "INFO:tensorflow:loss = 0.005790625, step = 2401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.34\n",
            "INFO:tensorflow:loss = 0.0056652185, step = 2501 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.263\n",
            "INFO:tensorflow:loss = 0.007912843, step = 2601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1055.42\n",
            "INFO:tensorflow:loss = 0.0063687605, step = 2701 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1088.61\n",
            "INFO:tensorflow:loss = 0.005362126, step = 2801 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026.86\n",
            "INFO:tensorflow:loss = 0.0074536568, step = 2901 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.348\n",
            "INFO:tensorflow:loss = 0.005797364, step = 3001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.01\n",
            "INFO:tensorflow:loss = 0.004967402, step = 3101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.01\n",
            "INFO:tensorflow:loss = 0.0045066364, step = 3201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.18\n",
            "INFO:tensorflow:loss = 0.0053607593, step = 3301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.895\n",
            "INFO:tensorflow:loss = 0.0056869104, step = 3401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1058.3\n",
            "INFO:tensorflow:loss = 0.004646342, step = 3501 (0.091 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.04\n",
            "INFO:tensorflow:loss = 0.0047630253, step = 3601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1058.44\n",
            "INFO:tensorflow:loss = 0.005345407, step = 3701 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.263\n",
            "INFO:tensorflow:loss = 0.0047496036, step = 3801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 979.376\n",
            "INFO:tensorflow:loss = 0.0045372606, step = 3901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.734\n",
            "INFO:tensorflow:loss = 0.012242756, step = 4001 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 1051.67\n",
            "INFO:tensorflow:loss = 0.017301248, step = 4101 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1084.2\n",
            "INFO:tensorflow:loss = 0.012316212, step = 4201 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.29\n",
            "INFO:tensorflow:loss = 0.015245835, step = 4301 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1080.87\n",
            "INFO:tensorflow:loss = 0.009698935, step = 4401 (0.091 sec)\n",
            "INFO:tensorflow:global_step/sec: 1071.07\n",
            "INFO:tensorflow:loss = 0.011066419, step = 4501 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 859.755\n",
            "INFO:tensorflow:loss = 0.010409886, step = 4601 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 1055.3\n",
            "INFO:tensorflow:loss = 0.1324909, step = 4701 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1029.05\n",
            "INFO:tensorflow:loss = 0.36074132, step = 4801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.227\n",
            "INFO:tensorflow:loss = 0.007637828, step = 4901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.735\n",
            "INFO:tensorflow:loss = 0.022418573, step = 5001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.797\n",
            "INFO:tensorflow:loss = 0.060193487, step = 5101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.778\n",
            "INFO:tensorflow:loss = 0.31711173, step = 5201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1002.48\n",
            "INFO:tensorflow:loss = 0.16549715, step = 5301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.32\n",
            "INFO:tensorflow:loss = 0.026033562, step = 5401 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.442\n",
            "INFO:tensorflow:loss = 0.00462719, step = 5501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 933.53\n",
            "INFO:tensorflow:loss = 0.061867885, step = 5601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1081.66\n",
            "INFO:tensorflow:loss = 0.008296922, step = 5701 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.47\n",
            "INFO:tensorflow:loss = 0.15565255, step = 5801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.214\n",
            "INFO:tensorflow:loss = 0.021880735, step = 5901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1054.59\n",
            "INFO:tensorflow:loss = 2.0190501, step = 6001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1063.76\n",
            "INFO:tensorflow:loss = 0.0073468583, step = 6101 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1074.79\n",
            "INFO:tensorflow:loss = 0.0075635156, step = 6201 (0.090 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.864\n",
            "INFO:tensorflow:loss = 0.048889294, step = 6301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.55\n",
            "INFO:tensorflow:loss = 0.050269194, step = 6401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1024.48\n",
            "INFO:tensorflow:loss = 0.012587467, step = 6501 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1079.9\n",
            "INFO:tensorflow:loss = 0.010445738, step = 6601 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.83\n",
            "INFO:tensorflow:loss = 0.124987796, step = 6701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1039.79\n",
            "INFO:tensorflow:loss = 0.00709418, step = 6801 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.86\n",
            "INFO:tensorflow:loss = 0.0053026937, step = 6901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.477\n",
            "INFO:tensorflow:loss = 0.03812501, step = 7001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1048.27\n",
            "INFO:tensorflow:loss = 0.029748172, step = 7101 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033.95\n",
            "INFO:tensorflow:loss = 0.021822896, step = 7201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.5\n",
            "INFO:tensorflow:loss = 0.08568883, step = 7301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1049.5\n",
            "INFO:tensorflow:loss = 0.0059615993, step = 7401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1048.14\n",
            "INFO:tensorflow:loss = 0.005161383, step = 7501 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.764\n",
            "INFO:tensorflow:loss = 0.007425937, step = 7601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.446\n",
            "INFO:tensorflow:loss = 0.03147954, step = 7701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1011.01\n",
            "INFO:tensorflow:loss = 0.01772001, step = 7801 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.755\n",
            "INFO:tensorflow:loss = 0.006957017, step = 7901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1039.68\n",
            "INFO:tensorflow:loss = 0.005941183, step = 8001 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.62\n",
            "INFO:tensorflow:loss = 0.0056743072, step = 8101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1052.31\n",
            "INFO:tensorflow:loss = 0.034230996, step = 8201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 901.024\n",
            "INFO:tensorflow:loss = 0.007840652, step = 8301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1042.57\n",
            "INFO:tensorflow:loss = 0.0044488832, step = 8401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1051.07\n",
            "INFO:tensorflow:loss = 0.010241285, step = 8501 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1080.88\n",
            "INFO:tensorflow:loss = 0.0060049677, step = 8601 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.223\n",
            "INFO:tensorflow:loss = 0.009164796, step = 8701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 968.192\n",
            "INFO:tensorflow:loss = 0.006100845, step = 8801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.95\n",
            "INFO:tensorflow:loss = 0.019479968, step = 8901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.83\n",
            "INFO:tensorflow:loss = 0.004304954, step = 9001 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.135\n",
            "INFO:tensorflow:loss = 0.0056588585, step = 9101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1015.94\n",
            "INFO:tensorflow:loss = 0.004026835, step = 9201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1008.2\n",
            "INFO:tensorflow:loss = 0.0070222393, step = 9301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.952\n",
            "INFO:tensorflow:loss = 0.014135145, step = 9401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.919\n",
            "INFO:tensorflow:loss = 0.028848719, step = 9501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.974\n",
            "INFO:tensorflow:loss = 0.02151859, step = 9601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 963.709\n",
            "INFO:tensorflow:loss = 0.003969183, step = 9701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033\n",
            "INFO:tensorflow:loss = 0.0076698326, step = 9801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.578\n",
            "INFO:tensorflow:loss = 0.003571733, step = 9901 (0.109 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_dewp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.008756684.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 111.0434696557508\n",
            "Just using average = 598.8840508806262 has RMSE of 98.9618419937131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_dewp', hidden_units=[20,18,13], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a9659e-c630-4b39-f521-d560e4fc65b2",
        "id": "NCz6izVhyAOF"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff05f648d90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_dewp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_dewp/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47293308 0.4661208  0.50386965 0.47377914 0.5409767  0.4247111\n",
            " 0.35220084 0.49011683 0.48777962 0.5428697  0.42196664 0.31290406\n",
            " 0.43790182 0.44153386 0.48399454 0.43879056 0.44979125 0.41787437\n",
            " 0.42273122 0.37880167 0.45410633 0.4766802  0.3464664  0.44413587\n",
            " 0.3446467  0.44061399 0.42643547 0.4920067  0.4744917  0.43529135\n",
            " 0.47771668 0.4854771  0.5136565  0.41682816 0.40898955 0.3742489\n",
            " 0.4680891  0.3766319  0.43941024 0.4730195  0.33534604 0.43892264\n",
            " 0.46058774 0.46027243 0.42426816 0.47786957 0.4327618  0.3837127\n",
            " 0.47326058 0.48072028 0.43163693 0.42435777 0.34137136 0.50533414\n",
            " 0.4866422  0.4863485  0.5170871  0.4987314  0.44671232 0.3687474\n",
            " 0.43214214 0.35221758 0.46011615 0.46182477 0.4895174  0.41493767\n",
            " 0.41424352 0.38315883 0.48386434 0.4490556  0.5347531  0.4336978\n",
            " 0.47816873 0.46794328 0.41635808 0.45023856 0.4972243  0.4410442\n",
            " 0.46587542 0.39563453 0.4753736  0.34190145 0.39143208 0.3983619\n",
            " 0.43861973 0.45278952 0.50985265 0.42010576 0.44087103 0.45072493\n",
            " 0.48995516 0.468423   0.45376715 0.3980234  0.3673118  0.4056085\n",
            " 0.47955066 0.44441715 0.48298898 0.42421556 0.46409836 0.4541023\n",
            " 0.51578987 0.4884491  0.5337442  0.41446716 0.45798644 0.49408948\n",
            " 0.52473867 0.50395256 0.42112902 0.45527944 0.40711477 0.46695054\n",
            " 0.5412271  0.46497023 0.4310858  0.5101762  0.34505868 0.44358853\n",
            " 0.48031703 0.4552289  0.35784984 0.4184649  0.4414054  0.33204964\n",
            " 0.45352396 0.46890414 0.5056792  0.47994468 0.49637198 0.5105062\n",
            " 0.51140386 0.50383407 0.47271803 0.4492556  0.42460114 0.37421596\n",
            " 0.45423698 0.463288   0.3425118  0.37258953 0.38491789 0.3929429\n",
            " 0.46345225 0.48996657 0.45216006 0.45758548 0.4471545  0.45897248\n",
            " 0.5150371  0.3773932  0.5463137  0.4457019  0.49182937 0.44264096\n",
            " 0.42208302 0.48730856 0.49883845 0.4239864  0.49811447 0.491566\n",
            " 0.42400956 0.49290177 0.5042988  0.4748318  0.37353477 0.47107258\n",
            " 0.45644084 0.5108524  0.48024696 0.5056974  0.437422   0.42625895\n",
            " 0.46428558 0.44354975 0.54958236 0.46981078 0.44721785 0.4475815\n",
            " 0.43256024 0.49595356 0.42694828 0.49363333 0.4961333  0.4083507\n",
            " 0.47477573 0.5010208  0.5158518  0.4585428  0.44501603 0.3938329\n",
            " 0.5000805  0.45635325 0.4174184  0.40812233 0.3993548  0.4898671\n",
            " 0.45487958 0.48292315 0.48335    0.4471432  0.46864676 0.50963646\n",
            " 0.51261127 0.41421536 0.4820585  0.4846425  0.46168602 0.39859873\n",
            " 0.5209621  0.38361916 0.47707596 0.48608506 0.4144921  0.37940496\n",
            " 0.49179596 0.44528657 0.4812935  0.450195   0.45364198 0.5019621\n",
            " 0.44455126 0.4431992  0.40837047 0.47894803 0.460096   0.47881165\n",
            " 0.49080554 0.47763819 0.49097347 0.46221876 0.40077952 0.47976726\n",
            " 0.46121988 0.36615458 0.4781716  0.47929168 0.47933772 0.4753774\n",
            " 0.4045883  0.46389252 0.48885307 0.43461642 0.37593055 0.34142676\n",
            " 0.44876412 0.45555922 0.43958327 0.4598546  0.45242912 0.42096624\n",
            " 0.49168408 0.4789338  0.46455005 0.4322034  0.4898638  0.47874665\n",
            " 0.3578441  0.44449052 0.4917802  0.46531817 0.48254058 0.43568632\n",
            " 0.4754734  0.54650366 0.44759473 0.43630418 0.46099812 0.47279775\n",
            " 0.43373495 0.41508758 0.49306178 0.5074431  0.47125772 0.38824114\n",
            " 0.32984096 0.5062035  0.4729843  0.49303302 0.4389119  0.45786723\n",
            " 0.49248654 0.4734699  0.47664005 0.44367576 0.47939542 0.50445294\n",
            " 0.45717978 0.49538106 0.5465921  0.42646432 0.43298465 0.48135728\n",
            " 0.38580048 0.47073135 0.50778717 0.46525067 0.39144647 0.48508117\n",
            " 0.43192512 0.39482763 0.4190722  0.48649585 0.45752558 0.42131558\n",
            " 0.4344598  0.42214867 0.44048938 0.37874615 0.44740054 0.45968214\n",
            " 0.4200487  0.41229197 0.47575587 0.5181802  0.4840147  0.5329649\n",
            " 0.47778082 0.35075805 0.40218306 0.40793607 0.47701886 0.40730703\n",
            " 0.4438232  0.48804834 0.48601526 0.48924363 0.41736704 0.51532483\n",
            " 0.480951   0.48194417 0.44249174 0.505164   0.46726605 0.41737494\n",
            " 0.42343643 0.5342358  0.53151876 0.4623536  0.4270798  0.48965698\n",
            " 0.46504363 0.43594533 0.40827316 0.51758444 0.47941774 0.33894086\n",
            " 0.35807288 0.48772714 0.47747084 0.39994597 0.4634198  0.4513134\n",
            " 0.37894464 0.4817847  0.4942924  0.4663947  0.40307245 0.46216857\n",
            " 0.49129665 0.33023608 0.42460456 0.4329786  0.48680463 0.4387388\n",
            " 0.4464402  0.48017755 0.47582164 0.4137674  0.5102507  0.42344528\n",
            " 0.45926228 0.45032236 0.42254847 0.4208801  0.47594818 0.47458103\n",
            " 0.43354866 0.5097271  0.4787955  0.5134441  0.4959114  0.46198216\n",
            " 0.43923208 0.4271232  0.36752594 0.49158135 0.32604703 0.4521686\n",
            " 0.4418728  0.54989505 0.4412425  0.45515338 0.50685656 0.47731194\n",
            " 0.456036   0.5084765  0.3776003  0.34807467 0.4347448  0.4780771\n",
            " 0.45179078 0.40414867 0.41585132 0.5115178  0.46198267 0.45022833\n",
            " 0.51544267 0.48304012 0.45012975 0.45525852 0.4364507  0.4581989\n",
            " 0.46322712 0.5051284  0.4779596  0.47407162 0.42280576 0.45457396\n",
            " 0.48606512 0.4667244  0.4539599  0.4798736  0.44192    0.37873927\n",
            " 0.43921474 0.490946   0.37909287 0.45100173 0.47157398 0.5009861\n",
            " 0.37402925 0.48166114 0.41918135 0.48015746 0.32240823 0.5400736\n",
            " 0.3510285  0.44171163 0.40353936 0.36481094 0.45637363 0.4567236\n",
            " 0.489237   0.41896898 0.3352488  0.48943877 0.36623314 0.33607945\n",
            " 0.54885185 0.38109317 0.34615013 0.45224404 0.3632841  0.4472181\n",
            " 0.42521113 0.46133733 0.44168285 0.3882753  0.4595533  0.4547268\n",
            " 0.45041943 0.42902893 0.4574956  0.32842624 0.48401707 0.46621007\n",
            " 0.49648508 0.3694345  0.509581   0.41009188 0.5002326  0.47331986\n",
            " 0.44484738 0.4610839  0.45216876 0.47111723 0.46179858 0.43050417\n",
            " 0.43981603 0.46829662 0.45171794 0.32143626 0.35456958 0.5249176\n",
            " 0.37472966 0.5106003  0.5139872  0.46830803 0.43043113 0.4010934\n",
            " 0.46690398 0.4456595  0.3816101  0.4983552  0.48396286 0.4836893\n",
            " 0.47973388 0.51450264 0.47813943 0.40662313 0.46781555 0.35947645\n",
            " 0.49040812 0.35014376 0.45980117 0.33107772 0.47504675 0.45460048\n",
            " 0.41931435]\n",
            "[0.63652024 0.59000861 0.58742463 0.36175711 0.56847545 0.50043066\n",
            " 0.39793282 0.57019811 0.6089578  0.59689922 0.49612403 0.4005168\n",
            " 0.48492679 0.49698536 0.52799311 0.56761413 0.56416882 0.50904393\n",
            " 0.59431525 0.43066322 0.32213609 0.51162791 0.34797588 0.4788975\n",
            " 0.38845823 0.58484065 0.58656331 0.61843239 0.57019811 0.45908699\n",
            " 0.50559862 0.68217054 0.58053402 0.42894057 0.43841516 0.4788975\n",
            " 0.45908699 0.44788975 0.49095607 0.46425495 0.3453919  0.56072351\n",
            " 0.51507321 0.48492679 0.47114556 0.58656331 0.51937984 0.26873385\n",
            " 0.50904393 0.51679587 0.4203273  0.50129199 0.35486649 0.69164513\n",
            " 0.57622739 0.51937984 0.58139535 0.6124031  0.49095607 0.35228252\n",
            " 0.57795004 0.38070629 0.53229974 0.47286822 0.5245478  0.42721792\n",
            " 0.51765719 0.43152455 0.60809647 0.56503015 0.60120586 0.30749354\n",
            " 0.5374677  0.51248923 0.49784668 0.49956934 0.60292851 0.39965547\n",
            " 0.52627046 0.49095607 0.65030146 0.43496985 0.43669251 0.48492679\n",
            " 0.55986219 0.56933678 0.47286822 0.59173127 0.55986219 0.54694229\n",
            " 0.59259259 0.43152455 0.62015504 0.46770026 0.48751077 0.49784668\n",
            " 0.57536606 0.56072351 0.56589147 0.4461671  0.4039621  0.374677\n",
            " 0.69939707 0.68475452 0.65891473 0.51765719 0.52196382 0.61670973\n",
            " 0.55986219 0.55986219 0.47372954 0.47975883 0.54005168 0.52971576\n",
            " 0.70456503 0.60378984 0.59173127 0.52799311 0.40310078 0.50301464\n",
            " 0.56158484 0.49956934 0.4005168  0.49009475 0.46339363 0.41774332\n",
            " 0.62446167 0.45305771 0.58656331 0.57278208 0.69595177 0.4754522\n",
            " 0.57364341 0.60120586 0.583118   0.54780362 0.49870801 0.44358312\n",
            " 0.44444444 0.55813953 0.36089578 0.44099914 0.51335056 0.46856158\n",
            " 0.50732127 0.53574505 0.39534884 0.55555556 0.33936262 0.35486649\n",
            " 0.54608096 0.44358312 0.66322136 0.5047373  0.37639966 0.44702842\n",
            " 0.39276486 0.48148148 0.62187769 0.49267873 0.64857881 0.45822567\n",
            " 0.53402239 0.49095607 0.63996555 0.45564169 0.38587425 0.53402239\n",
            " 0.46511628 0.59431525 0.63652024 0.48492679 0.49870801 0.52799311\n",
            " 0.47459087 0.47717485 0.38845823 0.58828596 0.43755383 0.5538329\n",
            " 0.44186047 0.5081826  0.49784668 0.55986219 0.58570198 0.39448751\n",
            " 0.53488372 0.60551249 0.27648579 0.44272179 0.58053402 0.41946598\n",
            " 0.65288544 0.55211025 0.54263566 0.47975883 0.43669251 0.49612403\n",
            " 0.51421189 0.56933678 0.58225668 0.54177433 0.44358312 0.56847545\n",
            " 0.65374677 0.56589147 0.38070629 0.65202412 0.54349699 0.34625323\n",
            " 0.64082687 0.43496985 0.55986219 0.46339363 0.4918174  0.40137812\n",
            " 0.48492679 0.45822567 0.50301464 0.54694229 0.51593454 0.50387597\n",
            " 0.4952627  0.60723514 0.53057709 0.53229974 0.59689922 0.48837209\n",
            " 0.67700258 0.54952627 0.56072351 0.28251507 0.47803618 0.57622739\n",
            " 0.52971576 0.45564169 0.56589147 0.52627046 0.45650301 0.44702842\n",
            " 0.50301464 0.47286822 0.52713178 0.56847545 0.40913006 0.42980189\n",
            " 0.54091301 0.59776055 0.55211025 0.5211025  0.55555556 0.58656331\n",
            " 0.5667528  0.52282515 0.47975883 0.45305771 0.5081826  0.62790698\n",
            " 0.43066322 0.54177433 0.49095607 0.52713178 0.46511628 0.45736434\n",
            " 0.60465116 0.68130922 0.49009475 0.45305771 0.65202412 0.55211025\n",
            " 0.54091301 0.48837209 0.68217054 0.6546081  0.52885444 0.43927649\n",
            " 0.35486649 0.57105943 0.53919035 0.62962963 0.4918174  0.5081826\n",
            " 0.52368648 0.52024117 0.5211025  0.46597761 0.61498708 0.55727821\n",
            " 0.5211025  0.52799311 0.64341085 0.50215332 0.48320413 0.80878553\n",
            " 0.34797588 0.47803618 0.66494401 0.60378984 0.44358312 0.64857881\n",
            " 0.48751077 0.50904393 0.45822567 0.62015504 0.55900086 0.4005168\n",
            " 0.47717485 0.57708872 0.44358312 0.4788975  0.50990525 0.63996555\n",
            " 0.49870801 0.42635659 0.47975883 0.55727821 0.58828596 0.66838932\n",
            " 0.60981912 0.39965547 0.39190353 0.48406546 0.47631352 0.45564169\n",
            " 0.57622739 0.53660637 0.53574505 0.47459087 0.5047373  0.58656331\n",
            " 0.46167097 0.63135228 0.53057709 0.64082687 0.59259259 0.5211025\n",
            " 0.45908699 0.66063738 0.55900086 0.55986219 0.5374677  0.58484065\n",
            " 0.5796727  0.50301464 0.60465116 0.57622739 0.59862188 0.46597761\n",
            " 0.34797588 0.4952627  0.49095607 0.46942291 0.53919035 0.57450474\n",
            " 0.4332472  0.5667528  0.53488372 0.5503876  0.48148148 0.43496985\n",
            " 0.43583118 0.27562446 0.44099914 0.44444444 0.46856158 0.44875108\n",
            " 0.46080965 0.5994832  0.53229974 0.34366925 0.55297158 0.47286822\n",
            " 0.50215332 0.60723514 0.46339363 0.47114556 0.51248923 0.49698536\n",
            " 0.5081826  0.55297158 0.57881137 0.74677003 0.64254953 0.49698536\n",
            " 0.51248923 0.4754522  0.33936262 0.59431525 0.82773471 0.48578811\n",
            " 0.51765719 0.61154177 0.55641688 0.48751077 0.51851852 0.60637382\n",
            " 0.50990525 0.70111972 0.47975883 0.39534884 0.42894057 0.50215332\n",
            " 0.58828596 0.46942291 0.56330749 0.58656331 0.5047373  0.5047373\n",
            " 0.72782084 0.65374677 0.46511628 0.38931955 0.51679587 0.56416882\n",
            " 0.51335056 0.38587425 0.54263566 0.63652024 0.54866494 0.59173127\n",
            " 0.50732127 0.5211025  0.51593454 0.53574505 0.42291128 0.48062016\n",
            " 0.48062016 0.59862188 0.39793282 0.55986219 0.54349699 0.58914729\n",
            " 0.51076658 0.64944014 0.44444444 0.54177433 0.38329027 0.65891473\n",
            " 0.43496985 0.5047373  0.4625323  0.42980189 0.43583118 0.57019811\n",
            " 0.61843239 0.57708872 0.36434109 0.62618432 0.46942291 0.41429802\n",
            " 0.63221361 0.41171404 0.46597761 0.40999139 0.31438415 0.46856158\n",
            " 0.42635659 0.71490095 0.49612403 0.40654608 0.48837209 0.59000861\n",
            " 0.42549526 0.63221361 0.60809647 0.33850129 0.55555556 0.5245478\n",
            " 0.51851852 0.45994832 0.58570198 0.44013781 0.63824289 0.61068045\n",
            " 0.53574505 0.54005168 0.47459087 0.60206718 0.51765719 0.49956934\n",
            " 0.63824289 0.55986219 0.49870801 0.44530577 0.416882   0.55813953\n",
            " 0.3712317  0.59259259 0.54349699 0.48751077 0.56330749 0.38673557\n",
            " 0.61498708 0.56416882 0.374677   0.63393626 0.4952627  0.6089578\n",
            " 0.42291128 0.5211025  0.53057709 0.5081826  0.52627046 0.4754522\n",
            " 0.63221361 0.41515935 0.6546081  0.33419466 0.61584841 0.50301464\n",
            " 0.50559862]\n",
            "The trained model has an aproximate error rate of 77.33457654067215 which equates to 13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sea Level Pressure(slp)\n"
      ],
      "metadata": {
        "id": "eJ4eYJryNjGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/slp_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b963ae-e3d2-41b4-8cd1-ed58c8f45b19",
        "id": "mhUamxCZOTrA"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_slp_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','gust','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] != 2012]\n",
        "df_slp_dnn = df_slp_dnn.loc[df_slp_dnn[\"year\"] < 2020]\n",
        "cols = df_slp_dnn['NUM_COLLISIONS']\n",
        "df_slp_dnn = df_slp_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_slp_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_slp_dnn[:6])\n",
        "df_slp_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "99eed74c-620b-42e4-9827-d4d6345d9cc3",
        "id": "P-VEj2lxOTrB"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28  1016.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17  1014.8    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25  1021.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29   999.4    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20  1015.5    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13  1020.7    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da          slp          Apr          Aug  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean   2016.000391    15.719765  1016.777221     0.082192     0.084932   \n",
              "std       2.000294     8.796698     7.628429     0.274710     0.278834   \n",
              "min    2013.000000     1.000000   989.500000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000  1012.200000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000  1016.700000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000  1021.700000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000  1044.200000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000  ...   \n",
              "mean      0.084540     0.077104     0.084932     0.084932     0.082192  ...   \n",
              "std       0.278251     0.266808     0.278834     0.278834     0.274710  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000  2555.000000   \n",
              "mean      0.082192     0.084932     0.082192     0.143249     0.142857   \n",
              "std       0.274710     0.278834     0.274710     0.350395     0.349996   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2555.000000  2555.000000  2555.000000  2555.000000     2555.000000  \n",
              "mean      0.142857     0.142857     0.142857     0.142466      599.147162  \n",
              "std       0.349996     0.349996     0.349996     0.349596      100.268048  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aeaa1a7a-13e2-4869-9e09-4b1aa308ca58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>slp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "      <td>2555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.000391</td>\n",
              "      <td>15.719765</td>\n",
              "      <td>1016.777221</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084540</td>\n",
              "      <td>0.077104</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.084932</td>\n",
              "      <td>0.082192</td>\n",
              "      <td>0.143249</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142466</td>\n",
              "      <td>599.147162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000294</td>\n",
              "      <td>8.796698</td>\n",
              "      <td>7.628429</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278251</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.278834</td>\n",
              "      <td>0.274710</td>\n",
              "      <td>0.350395</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349996</td>\n",
              "      <td>0.349596</td>\n",
              "      <td>100.268048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1012.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1016.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1021.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1044.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aeaa1a7a-13e2-4869-9e09-4b1aa308ca58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aeaa1a7a-13e2-4869-9e09-4b1aa308ca58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aeaa1a7a-13e2-4869-9e09-4b1aa308ca58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_slp_dnn.iloc[np.random.permutation(len(df_slp_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91991322-c7e9-4d05-ac2e-7fa04476de98",
        "id": "EG2EcMoLOTrC"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da     slp  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "635   2019   1  1026.9    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "383   2017  28  1029.7    0    0    0    1    0    0    0  ...    0    0    0   \n",
            "3662  2019   2   996.1    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "2130  2017  13  1015.5    0    0    0    0    0    1    0  ...    0    0    0   \n",
            "1681  2017  22  1014.3    0    0    0    0    0    0    1  ...    0    0    0   \n",
            "2579  2017  13  1011.8    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "635     0    0    0    0    1    0    0  \n",
            "383     0    1    0    0    0    0    0  \n",
            "3662    0    0    0    1    0    0    0  \n",
            "2130    0    0    0    0    0    0    1  \n",
            "1681    0    0    0    0    0    0    1  \n",
            "2579    1    0    0    0    0    1    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48eefa48-c668-457b-bb60-51f921e07f52",
        "id": "pd6Uk6a9OTrC"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "635     711\n",
            "383     591\n",
            "3662    580\n",
            "2130    759\n",
            "1681    845\n",
            "2579    738\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dabf025-69b4-4bef-83a2-c94d1d84ecc5",
        "id": "QX1fWn3jOTrC"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_slp', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2885118f-7f66-4100-a067-b771d8d49d6f",
        "id": "C9uL6bF7OTrD"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff05f55ffd0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:loss = 509618.3, step = 1\n",
            "INFO:tensorflow:global_step/sec: 517.174\n",
            "INFO:tensorflow:loss = 0.57567465, step = 101 (0.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 699.395\n",
            "INFO:tensorflow:loss = 0.106411755, step = 201 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.54\n",
            "INFO:tensorflow:loss = 0.11206909, step = 301 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 670.819\n",
            "INFO:tensorflow:loss = 0.10552477, step = 401 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.368\n",
            "INFO:tensorflow:loss = 0.08431721, step = 501 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 669.738\n",
            "INFO:tensorflow:loss = 0.10270215, step = 601 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 645.414\n",
            "INFO:tensorflow:loss = 0.09978469, step = 701 (0.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.782\n",
            "INFO:tensorflow:loss = 0.1075381, step = 801 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.883\n",
            "INFO:tensorflow:loss = 0.085142426, step = 901 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 712.233\n",
            "INFO:tensorflow:loss = 0.090775564, step = 1001 (0.140 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.54\n",
            "INFO:tensorflow:loss = 0.08263959, step = 1101 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.025\n",
            "INFO:tensorflow:loss = 0.087478526, step = 1201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.1\n",
            "INFO:tensorflow:loss = 0.08542131, step = 1301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.163\n",
            "INFO:tensorflow:loss = 0.07723136, step = 1401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.376\n",
            "INFO:tensorflow:loss = 0.08696579, step = 1501 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.857\n",
            "INFO:tensorflow:loss = 0.07106139, step = 1601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1008.71\n",
            "INFO:tensorflow:loss = 0.06542064, step = 1701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1052.91\n",
            "INFO:tensorflow:loss = 0.07856412, step = 1801 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.421\n",
            "INFO:tensorflow:loss = 0.06016783, step = 1901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.25\n",
            "INFO:tensorflow:loss = 0.05359651, step = 2001 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.734\n",
            "INFO:tensorflow:loss = 0.05523827, step = 2101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.626\n",
            "INFO:tensorflow:loss = 0.044715643, step = 2201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.97\n",
            "INFO:tensorflow:loss = 0.056182817, step = 2301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.261\n",
            "INFO:tensorflow:loss = 0.050018795, step = 2401 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.196\n",
            "INFO:tensorflow:loss = 0.05276172, step = 2501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.6\n",
            "INFO:tensorflow:loss = 0.04736416, step = 2601 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.966\n",
            "INFO:tensorflow:loss = 0.04132996, step = 2701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.999\n",
            "INFO:tensorflow:loss = 0.034264922, step = 2801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 819.76\n",
            "INFO:tensorflow:loss = 0.032584887, step = 2901 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 964.248\n",
            "INFO:tensorflow:loss = 0.033081576, step = 3001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.683\n",
            "INFO:tensorflow:loss = 0.023178525, step = 3101 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1088.55\n",
            "INFO:tensorflow:loss = 0.03175165, step = 3201 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.939\n",
            "INFO:tensorflow:loss = 0.028231531, step = 3301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.61\n",
            "INFO:tensorflow:loss = 0.021511648, step = 3401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1060.94\n",
            "INFO:tensorflow:loss = 0.02690981, step = 3501 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1034.8\n",
            "INFO:tensorflow:loss = 0.016472902, step = 3601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1047.59\n",
            "INFO:tensorflow:loss = 0.025437517, step = 3701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.18\n",
            "INFO:tensorflow:loss = 0.015898779, step = 3801 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.546\n",
            "INFO:tensorflow:loss = 0.015904646, step = 3901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.911\n",
            "INFO:tensorflow:loss = 0.017369768, step = 4001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.598\n",
            "INFO:tensorflow:loss = 0.015672235, step = 4101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1035.23\n",
            "INFO:tensorflow:loss = 0.012932162, step = 4201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.46\n",
            "INFO:tensorflow:loss = 0.013560175, step = 4301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.052\n",
            "INFO:tensorflow:loss = 0.00963029, step = 4401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.51\n",
            "INFO:tensorflow:loss = 0.01336832, step = 4501 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.537\n",
            "INFO:tensorflow:loss = 0.011439146, step = 4601 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 1070.89\n",
            "INFO:tensorflow:loss = 0.008110512, step = 4701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.571\n",
            "INFO:tensorflow:loss = 0.008690749, step = 4801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.783\n",
            "INFO:tensorflow:loss = 0.011198977, step = 4901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1030.85\n",
            "INFO:tensorflow:loss = 0.0075662294, step = 5001 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.36\n",
            "INFO:tensorflow:loss = 0.008706138, step = 5101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.151\n",
            "INFO:tensorflow:loss = 0.0072484748, step = 5201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1030.72\n",
            "INFO:tensorflow:loss = 0.00795917, step = 5301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 912.313\n",
            "INFO:tensorflow:loss = 0.008333197, step = 5401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.48\n",
            "INFO:tensorflow:loss = 0.008994054, step = 5501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1008.11\n",
            "INFO:tensorflow:loss = 0.0067040334, step = 5601 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1053.54\n",
            "INFO:tensorflow:loss = 0.004473592, step = 5701 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.853\n",
            "INFO:tensorflow:loss = 0.008044407, step = 5801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 897.93\n",
            "INFO:tensorflow:loss = 0.008549117, step = 5901 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 925.092\n",
            "INFO:tensorflow:loss = 0.0074361153, step = 6001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.32\n",
            "INFO:tensorflow:loss = 0.005900192, step = 6101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.116\n",
            "INFO:tensorflow:loss = 0.0063911797, step = 6201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.698\n",
            "INFO:tensorflow:loss = 0.0059439787, step = 6301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1036.97\n",
            "INFO:tensorflow:loss = 0.006855971, step = 6401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.669\n",
            "INFO:tensorflow:loss = 0.0045047114, step = 6501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.65\n",
            "INFO:tensorflow:loss = 0.007963041, step = 6601 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1029.58\n",
            "INFO:tensorflow:loss = 0.0069921594, step = 6701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.01\n",
            "INFO:tensorflow:loss = 0.007859911, step = 6801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.401\n",
            "INFO:tensorflow:loss = 0.005331994, step = 6901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.41\n",
            "INFO:tensorflow:loss = 0.006674486, step = 7001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.47\n",
            "INFO:tensorflow:loss = 0.006082858, step = 7101 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 965.506\n",
            "INFO:tensorflow:loss = 0.0048839794, step = 7201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.28\n",
            "INFO:tensorflow:loss = 0.005165741, step = 7301 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.85\n",
            "INFO:tensorflow:loss = 0.004188004, step = 7401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1101.19\n",
            "INFO:tensorflow:loss = 0.0053059123, step = 7501 (0.091 sec)\n",
            "INFO:tensorflow:global_step/sec: 1019.87\n",
            "INFO:tensorflow:loss = 0.007145423, step = 7601 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1079.32\n",
            "INFO:tensorflow:loss = 0.0053542233, step = 7701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1051.51\n",
            "INFO:tensorflow:loss = 0.004353668, step = 7801 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.757\n",
            "INFO:tensorflow:loss = 0.00502841, step = 7901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.059\n",
            "INFO:tensorflow:loss = 0.0049247574, step = 8001 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.419\n",
            "INFO:tensorflow:loss = 0.006474154, step = 8101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 888.232\n",
            "INFO:tensorflow:loss = 0.004598568, step = 8201 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 993.348\n",
            "INFO:tensorflow:loss = 0.0053378376, step = 8301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.801\n",
            "INFO:tensorflow:loss = 0.006347282, step = 8401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1001.35\n",
            "INFO:tensorflow:loss = 0.004414576, step = 8501 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1054.6\n",
            "INFO:tensorflow:loss = 0.005191108, step = 8601 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026.59\n",
            "INFO:tensorflow:loss = 0.0043919906, step = 8701 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1016.19\n",
            "INFO:tensorflow:loss = 0.006590511, step = 8801 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.483\n",
            "INFO:tensorflow:loss = 0.0056743864, step = 8901 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.001\n",
            "INFO:tensorflow:loss = 0.0052028727, step = 9001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026.52\n",
            "INFO:tensorflow:loss = 0.007950122, step = 9101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.21\n",
            "INFO:tensorflow:loss = 0.007034312, step = 9201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.161\n",
            "INFO:tensorflow:loss = 0.0036980938, step = 9301 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.1\n",
            "INFO:tensorflow:loss = 0.013073612, step = 9401 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1063.93\n",
            "INFO:tensorflow:loss = 0.011195545, step = 9501 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1056.88\n",
            "INFO:tensorflow:loss = 0.027047541, step = 9601 (0.091 sec)\n",
            "INFO:tensorflow:global_step/sec: 1013.64\n",
            "INFO:tensorflow:loss = 0.021823157, step = 9701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1062.84\n",
            "INFO:tensorflow:loss = 0.02160817, step = 9801 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.893\n",
            "INFO:tensorflow:loss = 0.060016282, step = 9901 (0.103 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_slp/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.11156096.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 325.9053488231647\n",
            "Just using average = 599.6526418786693 has RMSE of 97.54128152017972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_slp', hidden_units=[19,15,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06c672b-8309-4b84-b44c-bc6dbbb3c87e",
        "id": "mTPv83i1OTrD"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0633adc90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_slp', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_slp/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7650063  0.85170496 0.71542037 0.8176734  0.8068422  0.8713988\n",
            " 0.8307384  0.81752706 0.8352272  0.85883594 0.7978668  0.77376044\n",
            " 0.6616859  0.80041957 0.8630664  0.83913916 0.8236946  0.70176065\n",
            " 0.74473643 0.77849674 0.7872642  0.80802774 0.82792807 0.7610702\n",
            " 0.8308375  0.74603474 0.8560032  0.6451653  0.7078787  0.7178435\n",
            " 0.81761664 0.7678099  0.8876463  0.68051    0.7846068  0.82772434\n",
            " 0.8339045  0.7605138  0.8323385  0.81982964 0.8397672  0.7170262\n",
            " 0.81011254 0.81248116 0.75252414 0.767634   0.70598274 0.86346376\n",
            " 0.84157044 0.7962547  0.8030907  0.8396168  0.84075147 0.73308915\n",
            " 0.79712987 0.8009829  0.76609534 0.845349   0.7811843  0.90376514\n",
            " 0.8302456  0.7032007  0.7854685  0.78940177 0.7246854  0.8118154\n",
            " 0.8267008  0.87193406 0.7947912  0.7421315  0.78726    0.80716985\n",
            " 0.79577297 0.7920041  0.7779674  0.75607795 0.7794394  0.8437662\n",
            " 0.77941066 0.81955916 0.8080253  0.86207616 0.7811331  0.77929676\n",
            " 0.82407117 0.6843045  0.6945485  0.7932344  0.84839094 0.75510675\n",
            " 0.77446455 0.70975775 0.85862404 0.8471307  0.7463473  0.82155085\n",
            " 0.8608894  0.8080071  0.7571392  0.81537735 0.72069865 0.7737778\n",
            " 0.82787126 0.79671466 0.7705099  0.6831897  0.7570271  0.76828074\n",
            " 0.7114241  0.8426743  0.69444835 0.7176666  0.67238307 0.82360804\n",
            " 0.8439343  0.8210274  0.8516632  0.74229974 0.7976807  0.7002564\n",
            " 0.86933994 0.8197965  0.7906238  0.72458965 0.79041106 0.85636413\n",
            " 0.7268982  0.68523103 0.8425274  0.8655912  0.79350805 0.7396729\n",
            " 0.8477592  0.8430205  0.81411344 0.66087806 0.8274188  0.71534806\n",
            " 0.7734049  0.7963918  0.8122437  0.8214243  0.81674    0.74515516\n",
            " 0.7982833  0.7260567  0.81652355 0.7840605  0.7859016  0.71039414\n",
            " 0.79985875 0.7209733  0.7838053  0.7649325  0.80960053 0.78648233\n",
            " 0.72848445 0.82776874 0.78420985 0.72804534 0.833988   0.7906746\n",
            " 0.81360227 0.8815012  0.7159792  0.7000285  0.80087185 0.8293097\n",
            " 0.8081474  0.7869663  0.8275157  0.8656534  0.66657764 0.7657095\n",
            " 0.63875264 0.75227845 0.7905717  0.81508607 0.84467393 0.8506549\n",
            " 0.72829217 0.75472873 0.7961478  0.79556096 0.8077021  0.8179351\n",
            " 0.8341568  0.7989138  0.76869345 0.6694885  0.8167653  0.7817529\n",
            " 0.8541887  0.7650757  0.7951497  0.7497973  0.8048767  0.7949061\n",
            " 0.80768687 0.8632179  0.8164267  0.7698463  0.7551338  0.83532065\n",
            " 0.791984   0.8152779  0.830715   0.79149157 0.8528975  0.7703217\n",
            " 0.80244994 0.7614923  0.74055773 0.8042594  0.83060235 0.82528305\n",
            " 0.69929045 0.82454664 0.67891866 0.6975086  0.8978077  0.76312375\n",
            " 0.8145826  0.8741412  0.8187291  0.7551912  0.7832252  0.80137944\n",
            " 0.795353   0.7573544  0.8048634  0.67684054 0.8140735  0.77213025\n",
            " 0.7562158  0.76711196 0.7391888  0.6939746  0.8281719  0.862413\n",
            " 0.6784345  0.8194642  0.9060008  0.82879853 0.80284065 0.7313026\n",
            " 0.6909735  0.8063151  0.82357186 0.7893594  0.791298   0.7779227\n",
            " 0.80215824 0.8061424  0.7725946  0.66972953 0.7990559  0.83510226\n",
            " 0.8193211  0.78284854 0.80902445 0.81644756 0.7917886  0.7579194\n",
            " 0.8032634  0.7778007  0.7937722  0.7742569  0.8033817  0.8321263\n",
            " 0.7728886  0.8052847  0.7630951  0.83652115 0.8056029  0.82044303\n",
            " 0.8098015  0.75287586 0.7860269  0.7794616  0.6474524  0.75844234\n",
            " 0.7850301  0.8016217  0.830043   0.7751687  0.7814603  0.78699505\n",
            " 0.8207191  0.8770281  0.75800437 0.79669195 0.78002745 0.7922864\n",
            " 0.80596596 0.84395045 0.85360366 0.791784   0.72013116 0.7574903\n",
            " 0.8398221  0.76171935 0.81470627 0.65415144 0.7615624  0.79734856\n",
            " 0.8014389  0.7756904  0.76062274 0.8046019  0.7011448  0.83866405\n",
            " 0.8987249  0.7591034  0.82674885 0.70721203 0.8290804  0.8002595\n",
            " 0.80809665 0.8399916  0.78983355 0.75415987 0.7510925  0.6888111\n",
            " 0.86015445 0.78024113 0.7579793  0.8565859  0.748632   0.7989164\n",
            " 0.75160456 0.79508114 0.8127909  0.81898916 0.70709133 0.719199\n",
            " 0.7680607  0.7720695  0.78850245 0.7590104  0.82598776 0.8598447\n",
            " 0.79587996 0.80289984 0.782569   0.8190693  0.78772295 0.8535805\n",
            " 0.822857   0.7648924  0.87145805 0.86880296 0.74023026 0.81837785\n",
            " 0.8013873  0.80729496 0.7051757  0.78166336 0.77618486 0.7956322\n",
            " 0.77587783 0.8336189  0.79188037 0.7239784  0.76225114 0.7604738\n",
            " 0.7939155  0.7906599  0.8100808  0.8289963  0.82362133 0.85496897\n",
            " 0.81106555 0.79391426 0.7867837  0.8006057  0.7642676  0.8056772\n",
            " 0.7838605  0.79859066 0.7570318  0.8345653  0.8069111  0.80213547\n",
            " 0.65988016 0.72015667 0.6847694  0.82711434 0.80842465 0.80532855\n",
            " 0.78256446 0.7616527  0.76762635 0.7806063  0.84309644 0.6670109\n",
            " 0.85143316 0.8237095  0.8559746  0.81623244 0.7355791  0.75639886\n",
            " 0.8299384  0.90546334 0.75312    0.7035765  0.77626276 0.81096506\n",
            " 0.80121213 0.73532224 0.7867537  0.823163   0.7946473  0.81051695\n",
            " 1.0067981  0.81495816 0.7993521  0.79498637 0.79770285 0.68740404\n",
            " 0.8271639  0.8015454  0.7160173  0.6926804  0.7702645  0.822611\n",
            " 0.7959064  0.7773966  0.6743216  0.74557596 0.72964305 0.76266795\n",
            " 0.7913528  0.8458231  0.6528396  0.7781772  0.67886305 0.7705828\n",
            " 0.80338573 0.8129314  0.82214445 0.67737997 0.81954896 0.7653864\n",
            " 0.7544889  0.6617257  0.8244924  0.774719   0.80429006 0.79057205\n",
            " 0.7991022  0.8971424  0.79263365 0.8420901  0.7529473  0.89487696\n",
            " 0.8063689  0.8334722  0.8081465  0.76047987 0.779098   0.8247974\n",
            " 0.77939475 0.7577029  0.75013095 0.77899665 0.80475307 0.8512713\n",
            " 0.8229516  0.7488887  0.8186845  0.69563955 0.838084   0.7728939\n",
            " 0.8037042  0.724449   0.8068049  0.8177255  0.7194109  0.8265414\n",
            " 0.7251058  0.7878142  0.8233197  0.8153608  0.7198993  0.8057119\n",
            " 0.8206143  0.7223389  0.84382    0.7973078  0.79063255 0.7811013\n",
            " 0.838819   0.7061793  0.77406675 0.7758825  0.7674031  0.8683943\n",
            " 0.7943346  0.8046996  0.65997714 0.76173586 0.7967366  0.7820854\n",
            " 0.79293394 0.7661371  0.6613737  0.80416316 0.8572614  0.6855001\n",
            " 0.8137361 ]\n",
            "[0.61670973 0.6873385  0.4332472  0.53660637 0.42463394 0.66838932\n",
            " 0.52368648 0.46942291 0.58914729 0.56416882 0.52196382 0.49267873\n",
            " 0.44530577 0.43927649 0.60551249 0.54608096 0.59431525 0.40913006\n",
            " 0.49095607 0.53143842 0.54866494 0.59259259 0.52368648 0.47717485\n",
            " 0.61843239 0.59173127 0.50904393 0.32213609 0.47459087 0.50559862\n",
            " 0.55813953 0.49354005 0.56847545 0.38845823 0.59000861 0.56416882\n",
            " 0.5374677  0.41085271 0.62187769 0.67011197 0.53919035 0.44358312\n",
            " 0.6124031  0.5503876  0.57708872 0.50301464 0.48062016 0.51507321\n",
            " 0.54091301 0.52799311 0.66666667 0.63049096 0.64685616 0.49956934\n",
            " 0.55986219 0.50215332 0.44358312 0.63996555 0.56933678 0.64513351\n",
            " 0.58656331 0.48578811 0.49956934 0.46597761 0.51076658 0.54780362\n",
            " 0.48923342 0.65977606 0.4625323  0.50301464 0.49095607 0.54177433\n",
            " 0.54694229 0.4039621  0.52196382 0.41343669 0.37984496 0.53143842\n",
            " 0.54177433 0.61326443 0.42204996 0.61929371 0.62273902 0.51421189\n",
            " 0.37639966 0.43066322 0.38242894 0.54521964 0.66494401 0.41946598\n",
            " 0.48406546 0.44099914 0.63393626 0.55297158 0.41257537 0.59689922\n",
            " 0.57708872 0.50732127 0.56330749 0.59000861 0.44788975 0.44444444\n",
            " 0.59000861 0.64427218 0.37898363 0.4754522  0.34625323 0.51421189\n",
            " 0.37984496 0.53832903 0.40482343 0.41860465 0.40568475 0.59345392\n",
            " 0.56158484 0.49095607 0.55641688 0.58484065 0.53574505 0.50387597\n",
            " 0.57364341 0.63738157 0.56589147 0.43927649 0.5047373  0.57450474\n",
            " 0.50301464 0.41429802 0.5994832  0.54608096 0.5245478  0.416882\n",
            " 0.53574505 0.46511628 0.49698536 0.3453919  0.34022394 0.48148148\n",
            " 0.48664944 0.4625323  0.59259259 0.50990525 0.58656331 0.44444444\n",
            " 0.45478036 0.43066322 0.57622739 0.56503015 0.65202412 0.41085271\n",
            " 0.36175711 0.40826873 0.60206718 0.48837209 0.37898363 0.53574505\n",
            " 0.45047373 0.41860465 0.55727821 0.51335056 0.51335056 0.54694229\n",
            " 0.32988803 0.59431525 0.5047373  0.40137812 0.60809647 0.52540913\n",
            " 0.4918174  0.52196382 0.59345392 0.5538329  0.37812231 0.44875108\n",
            " 0.36950904 0.34022394 0.47459087 0.60981912 0.64857881 0.60120586\n",
            " 0.39793282 0.47200689 0.69509044 0.47028424 0.45650301 0.53229974\n",
            " 0.55727821 0.45736434 0.4496124  0.48923342 0.63824289 0.46339363\n",
            " 0.64685616 0.56761413 0.49009475 0.4754522  0.55555556 0.63910422\n",
            " 0.6089578  0.66408269 0.44530577 0.54005168 0.47286822 0.49009475\n",
            " 0.44358312 0.52627046 0.68217054 0.54780362 0.57364341 0.53574505\n",
            " 0.72265289 0.4918174  0.44875108 0.4039621  0.4918174  0.53402239\n",
            " 0.40654608 0.51937984 0.41515935 0.39879414 0.38845823 0.51851852\n",
            " 0.51248923 0.56330749 0.57364341 0.34711456 0.5245478  0.54694229\n",
            " 0.5047373  0.56072351 0.47459087 0.44444444 0.33763997 0.49612403\n",
            " 0.52024117 0.45305771 0.43496985 0.30060293 0.56847545 0.60292851\n",
            " 0.38587425 0.48837209 0.59689922 0.5374677  0.51593454 0.36520241\n",
            " 0.33074935 0.63049096 0.5211025  0.49440138 0.48923342 0.47459087\n",
            " 0.51248923 0.53316107 0.50990525 0.40482343 0.63135228 0.53574505\n",
            " 0.53402239 0.50043066 0.55641688 0.51162791 0.52713178 0.40137812\n",
            " 0.61412575 0.43410853 0.46770026 0.44702842 0.4625323  0.60551249\n",
            " 0.55986219 0.50387597 0.46856158 0.64082687 0.65202412 0.56847545\n",
            " 0.59173127 0.46683893 0.47631352 0.55986219 0.42204996 0.68217054\n",
            " 0.51162791 0.50559862 0.56589147 0.53919035 0.40913006 0.54177433\n",
            " 0.53402239 0.625323   0.4496124  0.58742463 0.59259259 0.54694229\n",
            " 0.45736434 0.56244617 0.56761413 0.54177433 0.42635659 0.50990525\n",
            " 0.53574505 0.41602067 0.58225668 0.34969854 0.44013781 0.55555556\n",
            " 0.5667528  0.53660637 0.33936262 0.57105943 0.44875108 0.42463394\n",
            " 0.64427218 0.58570198 0.49956934 0.4332472  0.55211025 0.57278208\n",
            " 0.4918174  0.60465116 0.57278208 0.82687339 0.42463394 0.39190353\n",
            " 0.5211025  0.50301464 0.44272179 0.65374677 0.42807924 0.63393626\n",
            " 0.56589147 0.51076658 0.54952627 0.40999139 0.3875969  0.40740741\n",
            " 0.42291128 0.4918174  0.55124892 0.50387597 0.60809647 0.62273902\n",
            " 0.44444444 0.62790698 0.42807924 0.58484065 0.50387597 0.47286822\n",
            " 0.54349699 0.4203273  0.7037037  0.68819983 0.43583118 0.57622739\n",
            " 0.60723514 0.5994832  0.4496124  0.54349699 0.52713178 0.53316107\n",
            " 0.4918174  0.62618432 0.55555556 0.50387597 0.47631352 0.44099914\n",
            " 0.56761413 0.53660637 0.53402239 0.50215332 0.5081826  0.64341085\n",
            " 0.46597761 0.60465116 0.46856158 0.64341085 0.41343669 0.57881137\n",
            " 0.47114556 0.34969854 0.44702842 0.56761413 0.5796727  0.51507321\n",
            " 0.35658915 0.45564169 0.4332472  0.49956934 0.47631352 0.55986219\n",
            " 0.6089578  0.46597761 0.48837209 0.52971576 0.69250646 0.41429802\n",
            " 0.64082687 0.5245478  0.5538329  0.56244617 0.5667528  0.50043066\n",
            " 0.55986219 0.63996555 0.55555556 0.45822567 0.53919035 0.58570198\n",
            " 0.60378984 0.38673557 0.53229974 0.6124031  0.41860465 0.53919035\n",
            " 0.76141258 0.56330749 0.60378984 0.52540913 0.52627046 0.44272179\n",
            " 0.63049096 0.52971576 0.45908699 0.47114556 0.50904393 0.45047373\n",
            " 0.4005168  0.57881137 0.38156761 0.43410853 0.47286822 0.46683893\n",
            " 0.42291128 0.65374677 0.36003445 0.5960379  0.37639966 0.64599483\n",
            " 0.56158484 0.60981912 0.47975883 0.38673557 0.59259259 0.48923342\n",
            " 0.63221361 0.39276486 0.53316107 0.49440138 0.54866494 0.53229974\n",
            " 0.47286822 0.60981912 0.54521964 0.625323   0.31007752 0.62704565\n",
            " 0.47028424 0.51076658 0.56589147 0.46339363 0.35400517 0.51937984\n",
            " 0.53488372 0.44875108 0.49009475 0.50215332 0.46425495 0.62360034\n",
            " 0.54177433 0.45391904 0.56933678 0.47372954 0.6089578  0.44186047\n",
            " 0.42894057 0.43669251 0.58914729 0.4918174  0.4788975  0.58742463\n",
            " 0.49784668 0.45822567 0.53316107 0.62618432 0.44530577 0.55813953\n",
            " 0.64513351 0.41085271 0.55641688 0.64513351 0.51421189 0.51248923\n",
            " 0.5503876  0.40223945 0.54177433 0.53919035 0.53919035 0.56847545\n",
            " 0.55124892 0.54263566 0.4005168  0.47114556 0.42549526 0.2213609\n",
            " 0.57536606 0.44444444 0.36434109 0.5538329  0.50129199 0.31438415\n",
            " 0.5211025 ]\n",
            "The trained model has an aproximate error rate of -316.74745141480065 which equates to -53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gust"
      ],
      "metadata": {
        "id": "lHX1HoDlQwJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/gust_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7fb6da5-befd-45a8-8f78-a44c1989374a",
        "id": "Tf_wMIWXQ7zg"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd  gust  \\\n",
            "3   2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0  20.0   \n",
            "11  2020  15             508  43.9  38.3  1019.4    8.2   5.4   14.0  15.0   \n",
            "12  2021   1             257  39.6  29.3  1029.3   10.0   7.6   14.0  20.0   \n",
            "14  2022  25             235  41.6  31.8  1013.2   10.0   9.6   15.0  19.0   \n",
            "18  2021   3             186  41.1  32.3  1018.0   10.0  10.3   19.0  27.0   \n",
            "19  2020   2             413  39.6  28.9  1011.8   10.0  13.0   19.0  26.0   \n",
            "\n",
            "    ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "3   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "11  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "12  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "14  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "18  ...    0    0    0    0    0    1    0    0    0    0  \n",
            "19  ...    0    0    0    0    0    0    0    0    0    1  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gust_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','mxpsd','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] != 2012]\n",
        "df_gust_dnn = df_gust_dnn.loc[df_gust_dnn[\"year\"] < 2020]\n",
        "cols = df_gust_dnn['NUM_COLLISIONS']\n",
        "df_gust_dnn = df_gust_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_gust_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_gust_dnn[:6])\n",
        "df_gust_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "65ad22a3-706e-41eb-9782-4f5f0d613877",
        "id": "oTbpzolhQ7zh"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "74  2016  17  18.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "76  2014   9  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "79  2019  19  21.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "80  2015  11  17.1    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "83  2015  29  20.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "85  2019  13  15.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "74    0    1    0    0    0    0             451  \n",
            "76    0    0    0    0    0    1             561  \n",
            "79    0    0    0    0    0    0             479  \n",
            "80    0    1    0    0    0    0             341  \n",
            "83    0    0    0    0    0    1             519  \n",
            "85    0    1    0    0    0    0             374  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             year           da         gust          Apr          Aug  \\\n",
              "count  1629.00000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean   2015.91283    15.702885    27.511602     0.095764     0.042357   \n",
              "std       2.01341     8.667634     7.366770     0.294358     0.201465   \n",
              "min    2013.00000     1.000000    14.000000     0.000000     0.000000   \n",
              "25%    2014.00000     8.000000    22.000000     0.000000     0.000000   \n",
              "50%    2016.00000    16.000000    26.000000     0.000000     0.000000   \n",
              "75%    2018.00000    23.000000    31.100000     0.000000     0.000000   \n",
              "max    2019.00000    31.000000    71.100000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000  ...   \n",
              "mean      0.104359     0.095150     0.108656     0.046041     0.061387  ...   \n",
              "std       0.305819     0.293513     0.311302     0.209637     0.240113  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000  1629.000000   \n",
              "mean      0.096378     0.087784     0.071209     0.139963     0.141191   \n",
              "std       0.295200     0.283067     0.257253     0.347055     0.348325   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  1629.000000  1629.000000  1629.000000  1629.000000     1629.000000  \n",
              "mean      0.139963     0.151627     0.138122     0.145488      596.513198  \n",
              "std       0.347055     0.358769     0.345133     0.352700      104.479660  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      526.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      597.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      663.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc8091c5-6f10-4996-9d32-445da589c84b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>gust</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1629.00000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "      <td>1629.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2015.91283</td>\n",
              "      <td>15.702885</td>\n",
              "      <td>27.511602</td>\n",
              "      <td>0.095764</td>\n",
              "      <td>0.042357</td>\n",
              "      <td>0.104359</td>\n",
              "      <td>0.095150</td>\n",
              "      <td>0.108656</td>\n",
              "      <td>0.046041</td>\n",
              "      <td>0.061387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096378</td>\n",
              "      <td>0.087784</td>\n",
              "      <td>0.071209</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.141191</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.151627</td>\n",
              "      <td>0.138122</td>\n",
              "      <td>0.145488</td>\n",
              "      <td>596.513198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.01341</td>\n",
              "      <td>8.667634</td>\n",
              "      <td>7.366770</td>\n",
              "      <td>0.294358</td>\n",
              "      <td>0.201465</td>\n",
              "      <td>0.305819</td>\n",
              "      <td>0.293513</td>\n",
              "      <td>0.311302</td>\n",
              "      <td>0.209637</td>\n",
              "      <td>0.240113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.295200</td>\n",
              "      <td>0.283067</td>\n",
              "      <td>0.257253</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.348325</td>\n",
              "      <td>0.347055</td>\n",
              "      <td>0.358769</td>\n",
              "      <td>0.345133</td>\n",
              "      <td>0.352700</td>\n",
              "      <td>104.479660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.00000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>526.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.00000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>597.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.00000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>663.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.00000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>71.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc8091c5-6f10-4996-9d32-445da589c84b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc8091c5-6f10-4996-9d32-445da589c84b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc8091c5-6f10-4996-9d32-445da589c84b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_gust_dnn.iloc[np.random.permutation(len(df_gust_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90dc7502-fe97-425b-ca15-c2aecc700d58",
        "id": "a_4bidEbQ7zi"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  gust  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "1792  2013  19  28.0    0    0    0    0    0    0    1  ...    0    0    0   \n",
            "302   2016  10  41.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "1132  2016  10  28.9    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "295   2019  24  42.0    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "3029  2017  31  29.9    0    0    0    0    0    0    0  ...    0    0    1   \n",
            "3459  2017  31  19.0    0    0    1    0    0    0    0  ...    0    0    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1792    0    0    0    0    0    1    0  \n",
            "302     0    0    1    0    0    0    0  \n",
            "1132    0    0    1    0    0    0    0  \n",
            "295     0    0    0    0    0    0    1  \n",
            "3029    0    1    0    0    0    0    0  \n",
            "3459    0    0    1    0    0    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93326540-8009-408e-90fc-c1be97ab6da8",
        "id": "l4TDr10XQ7zi"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1792    622\n",
            "302     462\n",
            "1132    499\n",
            "295     592\n",
            "3029    696\n",
            "3459    462\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb92215-b19d-411e-fc2a-37cf3806cde2",
        "id": "75AKCrz7Q7zi"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_gust', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b609ef0e-962f-437e-bf3a-c77a84f660e1",
        "id": "v4av8KXMQ7zj"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0633ad250>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:loss = 64.971466, step = 1\n",
            "INFO:tensorflow:global_step/sec: 719.654\n",
            "INFO:tensorflow:loss = 0.053713143, step = 101 (0.143 sec)\n",
            "INFO:tensorflow:global_step/sec: 808.333\n",
            "INFO:tensorflow:loss = 0.07465811, step = 201 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.883\n",
            "INFO:tensorflow:loss = 0.04166411, step = 301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.47\n",
            "INFO:tensorflow:loss = 0.03414039, step = 401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.067\n",
            "INFO:tensorflow:loss = 0.0359189, step = 501 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 969.441\n",
            "INFO:tensorflow:loss = 0.023819624, step = 601 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.353\n",
            "INFO:tensorflow:loss = 0.01677313, step = 701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.93\n",
            "INFO:tensorflow:loss = 0.017698165, step = 801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.77\n",
            "INFO:tensorflow:loss = 0.009321813, step = 901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.313\n",
            "INFO:tensorflow:loss = 0.01238913, step = 1001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.92\n",
            "INFO:tensorflow:loss = 0.007664933, step = 1101 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1029.83\n",
            "INFO:tensorflow:loss = 0.005672721, step = 1201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.349\n",
            "INFO:tensorflow:loss = 0.0067013544, step = 1301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.076\n",
            "INFO:tensorflow:loss = 0.004729247, step = 1401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.163\n",
            "INFO:tensorflow:loss = 0.006311389, step = 1501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.546\n",
            "INFO:tensorflow:loss = 0.0066308216, step = 1601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.796\n",
            "INFO:tensorflow:loss = 0.005032746, step = 1701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.333\n",
            "INFO:tensorflow:loss = 0.0052515985, step = 1801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1037.76\n",
            "INFO:tensorflow:loss = 0.00871585, step = 1901 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.44\n",
            "INFO:tensorflow:loss = 0.005563353, step = 2001 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.71\n",
            "INFO:tensorflow:loss = 0.004870066, step = 2101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 835.411\n",
            "INFO:tensorflow:loss = 0.004172497, step = 2201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.946\n",
            "INFO:tensorflow:loss = 0.005883548, step = 2301 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.038\n",
            "INFO:tensorflow:loss = 0.0058137625, step = 2401 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.496\n",
            "INFO:tensorflow:loss = 0.004596011, step = 2501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.99\n",
            "INFO:tensorflow:loss = 0.00452095, step = 2601 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.082\n",
            "INFO:tensorflow:loss = 0.0038446654, step = 2701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.881\n",
            "INFO:tensorflow:loss = 0.0075047915, step = 2801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.74\n",
            "INFO:tensorflow:loss = 0.0033160672, step = 2901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.099\n",
            "INFO:tensorflow:loss = 0.005047013, step = 3001 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.25\n",
            "INFO:tensorflow:loss = 0.005494103, step = 3101 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.637\n",
            "INFO:tensorflow:loss = 0.013043503, step = 3201 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.443\n",
            "INFO:tensorflow:loss = 0.023214724, step = 3301 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1072.37\n",
            "INFO:tensorflow:loss = 0.0040015467, step = 3401 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 994.271\n",
            "INFO:tensorflow:loss = 0.005521411, step = 3501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.86\n",
            "INFO:tensorflow:loss = 0.014621692, step = 3601 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.36\n",
            "INFO:tensorflow:loss = 0.013924994, step = 3701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.46\n",
            "INFO:tensorflow:loss = 0.011204476, step = 3801 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.83\n",
            "INFO:tensorflow:loss = 0.0052619544, step = 3901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.392\n",
            "INFO:tensorflow:loss = 0.0068173343, step = 4001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.222\n",
            "INFO:tensorflow:loss = 0.003656858, step = 4101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 879.776\n",
            "INFO:tensorflow:loss = 0.005942378, step = 4201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 1069.57\n",
            "INFO:tensorflow:loss = 0.011707096, step = 4301 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014.47\n",
            "INFO:tensorflow:loss = 0.014855348, step = 4401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1002.02\n",
            "INFO:tensorflow:loss = 0.010100004, step = 4501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.742\n",
            "INFO:tensorflow:loss = 0.0041081365, step = 4601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.629\n",
            "INFO:tensorflow:loss = 0.01292984, step = 4701 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1037.51\n",
            "INFO:tensorflow:loss = 0.0068248264, step = 4801 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1057.22\n",
            "INFO:tensorflow:loss = 0.006384911, step = 4901 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.64\n",
            "INFO:tensorflow:loss = 0.024351768, step = 5001 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1014.73\n",
            "INFO:tensorflow:loss = 0.0071235094, step = 5101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.974\n",
            "INFO:tensorflow:loss = 0.009793543, step = 5201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.68\n",
            "INFO:tensorflow:loss = 0.026038108, step = 5301 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1046.02\n",
            "INFO:tensorflow:loss = 0.0058578406, step = 5401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1023.2\n",
            "INFO:tensorflow:loss = 0.0077137426, step = 5501 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.405\n",
            "INFO:tensorflow:loss = 0.055719875, step = 5601 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.113\n",
            "INFO:tensorflow:loss = 0.0031698095, step = 5701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1008.2\n",
            "INFO:tensorflow:loss = 0.005070308, step = 5801 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.35\n",
            "INFO:tensorflow:loss = 0.021730835, step = 5901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1025.41\n",
            "INFO:tensorflow:loss = 0.00999596, step = 6001 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033.98\n",
            "INFO:tensorflow:loss = 0.0035038032, step = 6101 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1028.58\n",
            "INFO:tensorflow:loss = 0.015455603, step = 6201 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.426\n",
            "INFO:tensorflow:loss = 0.005447978, step = 6301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.515\n",
            "INFO:tensorflow:loss = 0.0060628136, step = 6401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 990.529\n",
            "INFO:tensorflow:loss = 0.0055804136, step = 6501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.406\n",
            "INFO:tensorflow:loss = 0.01367306, step = 6601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.849\n",
            "INFO:tensorflow:loss = 0.0069997166, step = 6701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 988.083\n",
            "INFO:tensorflow:loss = 0.004811827, step = 6801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.25\n",
            "INFO:tensorflow:loss = 0.0036578444, step = 6901 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.956\n",
            "INFO:tensorflow:loss = 0.0047475463, step = 7001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.66\n",
            "INFO:tensorflow:loss = 0.009959377, step = 7101 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1038.88\n",
            "INFO:tensorflow:loss = 0.0051907795, step = 7201 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.795\n",
            "INFO:tensorflow:loss = 0.0044065546, step = 7301 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.292\n",
            "INFO:tensorflow:loss = 0.007875821, step = 7401 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.74\n",
            "INFO:tensorflow:loss = 0.025737327, step = 7501 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.92\n",
            "INFO:tensorflow:loss = 0.003808838, step = 7601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.786\n",
            "INFO:tensorflow:loss = 0.033585653, step = 7701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 959.817\n",
            "INFO:tensorflow:loss = 0.0040777787, step = 7801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1054.93\n",
            "INFO:tensorflow:loss = 0.010659825, step = 7901 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.806\n",
            "INFO:tensorflow:loss = 0.0061068125, step = 8001 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.515\n",
            "INFO:tensorflow:loss = 0.0074764984, step = 8101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.069\n",
            "INFO:tensorflow:loss = 0.0055612167, step = 8201 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.159\n",
            "INFO:tensorflow:loss = 0.0064410754, step = 8301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.033\n",
            "INFO:tensorflow:loss = 0.014581408, step = 8401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.241\n",
            "INFO:tensorflow:loss = 0.0076537295, step = 8501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.176\n",
            "INFO:tensorflow:loss = 0.006048058, step = 8601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.425\n",
            "INFO:tensorflow:loss = 0.0060958434, step = 8701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.661\n",
            "INFO:tensorflow:loss = 0.007401638, step = 8801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.507\n",
            "INFO:tensorflow:loss = 0.005316871, step = 8901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.973\n",
            "INFO:tensorflow:loss = 0.0049855215, step = 9001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.447\n",
            "INFO:tensorflow:loss = 0.015479216, step = 9101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 881.575\n",
            "INFO:tensorflow:loss = 0.01633116, step = 9201 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.11\n",
            "INFO:tensorflow:loss = 0.004230818, step = 9301 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 903.67\n",
            "INFO:tensorflow:loss = 0.004290454, step = 9401 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.159\n",
            "INFO:tensorflow:loss = 0.0063860035, step = 9501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1013.56\n",
            "INFO:tensorflow:loss = 0.005033448, step = 9601 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1034.21\n",
            "INFO:tensorflow:loss = 0.004652842, step = 9701 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1041.44\n",
            "INFO:tensorflow:loss = 0.004145972, step = 9801 (0.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.43\n",
            "INFO:tensorflow:loss = 0.010400343, step = 9901 (0.100 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_gust/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0037995428.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 109.47309609016882\n",
            "Just using average = 594.3284727551803 has RMSE of 112.76313210041813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8e041e-b5e2-4100-f949-c78f3b45e987",
        "id": "NdhxllohQ7zj"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff06551a890>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5453121  0.60026455 0.6028755  0.4785436  0.5091958  0.56914186\n",
            " 0.45735314 0.62659085 0.61591715 0.61809313 0.6044074  0.6023531\n",
            " 0.5059775  0.5864702  0.54461575 0.59787965 0.46467242 0.5054646\n",
            " 0.5726779  0.5931157  0.54931164 0.6487645  0.62044567 0.48291382\n",
            " 0.57112676 0.49956048 0.59300834 0.6160402  0.60995895 0.5823898\n",
            " 0.5824373  0.6284571  0.60100967 0.5356183  0.687197   0.42860606\n",
            " 0.61914915 0.5985282  0.66340464 0.6110858  0.6481132  0.5683931\n",
            " 0.6199402  0.500204   0.49694598 0.5678619  0.59557885 0.48710504\n",
            " 0.5841349  0.5850111  0.5249624  0.60065144 0.53344035 0.52301383\n",
            " 0.5369082  0.5610239  0.55145556 0.66302645 0.6240687  0.6524092\n",
            " 0.60970265 0.5323507  0.5642173  0.56589836 0.5918983  0.57658297\n",
            " 0.58732563 0.6114651  0.6243283  0.5540429  0.6111728  0.62210685\n",
            " 0.5409451  0.5999031  0.5043643  0.60960186 0.5706019  0.4779831\n",
            " 0.54000986 0.6122121  0.5391625  0.55473936 0.6238618  0.60633\n",
            " 0.62376064 0.57474834 0.56689227 0.5489578  0.5791922  0.5475176\n",
            " 0.64448017 0.6115682  0.5629765  0.5734373  0.6233088  0.6097036\n",
            " 0.62255794 0.6287036  0.5739746  0.5462353  0.44927013 0.60228944\n",
            " 0.589084   0.57821816 0.49852616 0.5929775  0.57006323 0.49633563\n",
            " 0.46960345 0.59645087 0.59742767 0.59850675 0.53409195 0.50753975\n",
            " 0.58041215 0.53170156 0.65296495 0.5172507  0.6601414  0.59581465\n",
            " 0.6255372  0.5514151  0.50572354 0.56749904 0.5919043  0.5715705\n",
            " 0.6035876  0.6600585  0.65927565 0.54948133 0.596943   0.5622767\n",
            " 0.5266276  0.47834152 0.65240324 0.5246839  0.5098461  0.6169624\n",
            " 0.6247309  0.47787336 0.6363677  0.5716615  0.61539376 0.59452075\n",
            " 0.59758365 0.52407926 0.5846868  0.5968811  0.6258187  0.5422357\n",
            " 0.5767876  0.5994721  0.575605   0.59209967 0.5655906  0.58853245\n",
            " 0.6552177  0.46322548 0.45931265 0.5866816  0.6057042  0.4759302\n",
            " 0.584395   0.46763226 0.6153758  0.6563164  0.49593145 0.4978838\n",
            " 0.5728913  0.49065784 0.5856514  0.548484   0.54588026 0.62592983\n",
            " 0.5790113  0.58908576 0.6523152  0.5868652  0.66041267 0.65537906\n",
            " 0.62944555 0.6428985  0.59432954 0.59198594 0.6874888  0.6256747\n",
            " 0.605488   0.5542765  0.43810213 0.64606595 0.5956277  0.55088997\n",
            " 0.45384437 0.4860912  0.63572514 0.5666463  0.50988096 0.56981915\n",
            " 0.57195663 0.62416387 0.58917177 0.58425367 0.63327384 0.55016583\n",
            " 0.6054819  0.53757447 0.6009782  0.55051684 0.65979844 0.61523724\n",
            " 0.4805161  0.57117003 0.56195754 0.6287954  0.5314848  0.6562417\n",
            " 0.61192197 0.6571879  0.6514507  0.54849    0.57982725 0.6409152\n",
            " 0.5910451  0.60300344 0.50296485 0.6260866  0.57564384 0.60558933\n",
            " 0.66240853 0.63673455 0.5450142  0.61323065 0.57263976 0.63282293\n",
            " 0.5700263  0.64048886 0.5754158  0.6521177  0.5275672  0.5307453\n",
            " 0.56753993 0.42795208 0.61753297 0.5898424  0.5889536  0.6293863\n",
            " 0.5791159  0.59255064 0.5359091  0.48311028 0.58536077 0.54193556\n",
            " 0.61938983 0.56623924 0.5017601  0.6004533  0.6223015  0.5688568\n",
            " 0.5440781  0.6281437  0.66367584 0.50089467 0.683916   0.5509182\n",
            " 0.5593516  0.59219444 0.43186912 0.51885885 0.5044262  0.59290516\n",
            " 0.5785433  0.5337341  0.55352366 0.54455465 0.6140794  0.6088534\n",
            " 0.57518214 0.5905811  0.566256   0.5511007  0.64477444 0.43709454\n",
            " 0.6260358  0.547455   0.5524515  0.5066284  0.6116412  0.58403814\n",
            " 0.53235304 0.47703525 0.55406153 0.54978615 0.57911307 0.4312259\n",
            " 0.6154066  0.5862146  0.5677461  0.63105124 0.6137837  0.5733295\n",
            " 0.6232104  0.5886435  0.47361878 0.5262592  0.53935575 0.5752097\n",
            " 0.5900341  0.6162165  0.6317176  0.5876221  0.5566782  0.5708154\n",
            " 0.6575364  0.5850457  0.6006235  0.5892919  0.4645269  0.5939145\n",
            " 0.5946002  0.5760588  0.5983706  0.6267394  0.49824995 0.57812536\n",
            " 0.5697941  0.53605384]\n",
            "[0.47459087 0.36175711 0.52024117 0.47114556 0.43927649 0.37898363\n",
            " 0.44530577 0.63652024 0.5245478  0.49440138 0.58570198 0.52627046\n",
            " 0.43238587 0.47631352 0.46511628 0.49009475 0.38845823 0.43238587\n",
            " 0.58053402 0.57019811 0.49870801 0.58656331 0.52282515 0.44099914\n",
            " 0.43066322 0.40913006 0.63824289 0.53574505 0.54694229 0.47114556\n",
            " 0.60723514 0.5960379  0.48148148 0.43410853 0.70456503 0.36950904\n",
            " 0.49095607 0.56416882 0.60981912 0.63910422 0.60034453 0.5047373\n",
            " 0.52971576 0.45650301 0.43152455 0.49956934 0.64427218 0.49267873\n",
            " 0.47459087 0.48664944 0.46942291 0.57450474 0.62015504 0.47200689\n",
            " 0.47372954 0.54177433 0.57019811 0.7037037  0.64685616 0.53143842\n",
            " 0.49956934 0.45650301 0.45564169 0.51421189 0.76055125 0.55555556\n",
            " 0.44530577 0.58484065 0.59086994 0.61670973 0.6089578  0.5796727\n",
            " 0.40913006 0.54608096 0.51076658 0.49870801 0.48406546 0.38845823\n",
            " 0.56589147 0.68475452 0.49784668 0.44186047 0.59086994 0.68044789\n",
            " 0.58484065 0.53919035 0.58484065 0.43583118 0.52885444 0.42291128\n",
            " 0.65891473 0.50990525 0.53143842 0.59173127 0.48406546 0.36864772\n",
            " 0.53660637 0.69595177 0.52799311 0.5047373  0.38587425 0.4918174\n",
            " 0.4496124  0.47975883 0.34625323 0.54694229 0.59776055 0.40137812\n",
            " 0.374677   0.56416882 0.64857881 0.63738157 0.50301464 0.43669251\n",
            " 0.60034453 0.41085271 0.71403962 0.46511628 0.71490095 0.51421189\n",
            " 0.56761413 0.43152455 0.39276486 0.49009475 0.4039621  0.6089578\n",
            " 0.51507321 0.63824289 0.67355728 0.40913006 0.54866494 0.4788975\n",
            " 0.39276486 0.46511628 0.56416882 0.55900086 0.54521964 0.45650301\n",
            " 0.59862188 0.416882   0.31007752 0.48234281 0.50129199 0.63049096\n",
            " 0.56072351 0.45994832 0.5374677  0.63996555 0.60378984 0.34022394\n",
            " 0.5503876  0.56416882 0.43238587 0.54349699 0.55124892 0.32213609\n",
            " 0.63910422 0.40310078 0.48923342 0.5245478  0.53057709 0.44875108\n",
            " 0.48320413 0.47372954 0.60723514 0.53057709 0.51335056 0.36864772\n",
            " 0.49440138 0.45478036 0.51679587 0.47803618 0.47200689 0.53402239\n",
            " 0.50559862 0.52196382 0.61757106 0.45478036 0.83893196 0.62360034\n",
            " 0.53488372 0.67183463 0.59345392 0.59086994 0.68130922 0.51937984\n",
            " 0.49698536 0.54263566 0.32385874 0.71748493 0.4918174  0.50387597\n",
            " 0.37812231 0.42463394 0.49612403 0.45219638 0.46167097 0.4461671\n",
            " 0.54005168 0.54780362 0.49784668 0.54521964 0.56503015 1.\n",
            " 0.5211025  0.52024117 0.34022394 0.45736434 0.46597761 0.44788975\n",
            " 0.4496124  0.54263566 0.5047373  0.67011197 0.60551249 0.7329888\n",
            " 0.46856158 0.55813953 0.54521964 0.49354005 0.48234281 0.52885444\n",
            " 0.55555556 0.64254953 0.47803618 0.63135228 0.62446167 0.43152455\n",
            " 0.61929371 0.56330749 0.55986219 0.53919035 0.48664944 0.6873385\n",
            " 0.44702842 0.57708872 0.47286822 0.66838932 0.57450474 0.48148148\n",
            " 0.55555556 0.45822567 0.59345392 0.5047373  0.51851852 0.49870801\n",
            " 0.68130922 0.45908699 0.44702842 0.41515935 0.52540913 0.54952627\n",
            " 0.60034453 0.68217054 0.4788975  0.5667528  0.56589147 0.56072351\n",
            " 0.54263566 0.59776055 0.69336779 0.38242894 0.66408269 0.53057709\n",
            " 0.4788975  0.57622739 0.32730405 0.38673557 0.33505599 0.38156761\n",
            " 0.38931955 0.42894057 0.34366925 0.48923342 0.41860465 0.58225668\n",
            " 0.59345392 0.60206718 0.45478036 0.34022394 0.59345392 0.27562446\n",
            " 0.6416882  0.46511628 0.48923342 0.40740741 0.55727821 0.57019811\n",
            " 0.40913006 0.48751077 0.47975883 0.56158484 0.48492679 0.33850129\n",
            " 0.57364341 0.56244617 0.50215332 0.68217054 0.65030146 0.47803618\n",
            " 0.59086994 0.56244617 0.30060293 0.48664944 0.50990525 0.52713178\n",
            " 0.53574505 0.54349699 0.86046512 0.49440138 0.55124892 0.51507321\n",
            " 0.67786391 0.5503876  0.31955211 0.63910422 0.50387597 0.60637382\n",
            " 0.48837209 0.46339363 0.43496985 0.62962963 0.45908699 0.47286822\n",
            " 0.55986219 0.39018088]\n",
            "The trained model has an aproximate error rate of -62.53483869830157 which equates to -10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maximum Sustained Wind Speed (mxpsd)"
      ],
      "metadata": {
        "id": "tKaVpVT8T55I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/mxpsd_clean_dnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb09b22-f1f3-40d0-ac87-6734ea9ea35d",
        "id": "_LbHDT1WUbJs"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mxpsd_dnn = df.drop(columns=['temp', 'prcp', 'dewp','visib','max','min','sndp','wdsp','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] != 2012]\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.loc[df_mxpsd_dnn[\"year\"] < 2020]\n",
        "cols = df_mxpsd_dnn['NUM_COLLISIONS']\n",
        "df_mxpsd_dnn = df_mxpsd_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_mxpsd_dnn.insert(loc=21, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_mxpsd_dnn[:6])\n",
        "df_mxpsd_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "887e01d3-ac1e-437d-fab3-9a1e3d84552e",
        "id": "OoJSkkKHUbJ3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  Nov  Oct  Sep  \\\n",
            "49  2016  28    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "51  2014  17    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "54  2016  25    8.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "55  2016  29    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "58  2017  20    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "59  2013  13    9.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "\n",
            "    Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    1             681  \n",
            "51    0    0    0    1    0    0             589  \n",
            "54    0    0    1    0    0    0             658  \n",
            "55    0    0    0    1    0    0             645  \n",
            "58    0    0    0    1    0    0             605  \n",
            "59    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              year           da        mxpsd          Apr          Aug  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean   2016.001567    15.737172    17.240110     0.082256     0.084998   \n",
              "std       2.000587     8.797367     5.858333     0.274808     0.278933   \n",
              "min    2013.000000     1.000000     5.100000     0.000000     0.000000   \n",
              "25%    2014.000000     8.000000    13.000000     0.000000     0.000000   \n",
              "50%    2016.000000    16.000000    15.900000     0.000000     0.000000   \n",
              "75%    2018.000000    23.000000    20.000000     0.000000     0.000000   \n",
              "max    2019.000000    31.000000    49.000000     1.000000     1.000000   \n",
              "\n",
              "               Dec          Feb          Jan          Jul          Jun  ...  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000  ...   \n",
              "mean      0.084998     0.077164     0.084998     0.084998     0.082256  ...   \n",
              "std       0.278933     0.266904     0.278933     0.278933     0.274808  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000  2553.000000   \n",
              "mean      0.081864     0.084998     0.081473     0.143361     0.142969   \n",
              "std       0.274212     0.278933     0.273613     0.350509     0.350110   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2553.000000  2553.000000  2553.000000  2553.000000     2553.000000  \n",
              "mean      0.142969     0.142969     0.142577     0.142186      599.033686  \n",
              "std       0.350110     0.350110     0.349710     0.349309      100.284761  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12dd779f-7f96-4e60-b6a7-c0191a84cff5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "      <td>2553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.001567</td>\n",
              "      <td>15.737172</td>\n",
              "      <td>17.240110</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.077164</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.082256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081864</td>\n",
              "      <td>0.084998</td>\n",
              "      <td>0.081473</td>\n",
              "      <td>0.143361</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142969</td>\n",
              "      <td>0.142577</td>\n",
              "      <td>0.142186</td>\n",
              "      <td>599.033686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.000587</td>\n",
              "      <td>8.797367</td>\n",
              "      <td>5.858333</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.266904</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.274808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274212</td>\n",
              "      <td>0.278933</td>\n",
              "      <td>0.273613</td>\n",
              "      <td>0.350509</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.350110</td>\n",
              "      <td>0.349710</td>\n",
              "      <td>0.349309</td>\n",
              "      <td>100.284761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12dd779f-7f96-4e60-b6a7-c0191a84cff5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12dd779f-7f96-4e60-b6a7-c0191a84cff5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12dd779f-7f96-4e60-b6a7-c0191a84cff5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_mxpsd_dnn.iloc[np.random.permutation(len(df_mxpsd_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96bdb157-8aa1-4dd1-cf80-c9e493639435",
        "id": "z1Ut5LCiUbJ4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  mxpsd  Apr  Aug  Dec  Feb  Jan  Jul  Jun  ...  May  Nov  Oct  \\\n",
            "1692  2013   4   15.0    0    0    0    0    0    0    1  ...    0    0    0   \n",
            "3196  2017  14   17.1    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "493   2018   5   20.0    0    0    0    1    0    0    0  ...    0    0    0   \n",
            "1373  2017  18   15.9    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "237   2014  11   22.9    0    0    0    0    1    0    0  ...    0    0    0   \n",
            "3113  2017  24    9.9    0    0    0    0    0    0    0  ...    0    1    0   \n",
            "\n",
            "      Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1692    0    1    0    0    0    0    0  \n",
            "3196    0    1    0    0    0    0    0  \n",
            "493     0    0    0    1    0    0    0  \n",
            "1373    0    0    0    0    0    0    1  \n",
            "237     0    0    0    0    0    0    0  \n",
            "3113    0    0    0    0    1    0    0  \n",
            "\n",
            "[6 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970b59f0-eec8-4b92-daa9-a2587cea074c",
        "id": "QibQoUleUbJ4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1692    613\n",
            "3196    647\n",
            "493     706\n",
            "1373    911\n",
            "237     465\n",
            "3113    502\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bd4e23-e0cf-474b-817c-fca9204798ed",
        "id": "-C_tqkmUUbJ5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_mxpsd', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_mxpsd', hidden_units=[19,15,11], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d0a2ed-b88f-4a26-85bb-37ddc94d3421",
        "id": "C3O7xWeMUbJ5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0631a0110>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_mxpsd', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:loss = 85647.24, step = 1\n",
            "INFO:tensorflow:global_step/sec: 534.607\n",
            "INFO:tensorflow:loss = 1.0603168, step = 101 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 690.448\n",
            "INFO:tensorflow:loss = 0.5262594, step = 201 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.89\n",
            "INFO:tensorflow:loss = 0.5222115, step = 301 (0.147 sec)\n",
            "INFO:tensorflow:global_step/sec: 668.151\n",
            "INFO:tensorflow:loss = 0.58860123, step = 401 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 673.646\n",
            "INFO:tensorflow:loss = 0.52257407, step = 501 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 655.22\n",
            "INFO:tensorflow:loss = 0.428842, step = 601 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 694.461\n",
            "INFO:tensorflow:loss = 0.44657278, step = 701 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 689.327\n",
            "INFO:tensorflow:loss = 0.44755977, step = 801 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.926\n",
            "INFO:tensorflow:loss = 0.42192852, step = 901 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 691.957\n",
            "INFO:tensorflow:loss = 0.3922971, step = 1001 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 709.142\n",
            "INFO:tensorflow:loss = 0.38804543, step = 1101 (0.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 650.692\n",
            "INFO:tensorflow:loss = 0.43147224, step = 1201 (0.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 675.623\n",
            "INFO:tensorflow:loss = 0.27255988, step = 1301 (0.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 716.83\n",
            "INFO:tensorflow:loss = 0.31045914, step = 1401 (0.139 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.72\n",
            "INFO:tensorflow:loss = 0.19999951, step = 1501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.941\n",
            "INFO:tensorflow:loss = 0.24866483, step = 1601 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.115\n",
            "INFO:tensorflow:loss = 0.20643106, step = 1701 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1039.4\n",
            "INFO:tensorflow:loss = 0.22721615, step = 1801 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.37\n",
            "INFO:tensorflow:loss = 0.23568487, step = 1901 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1048.16\n",
            "INFO:tensorflow:loss = 0.15553324, step = 2001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.28\n",
            "INFO:tensorflow:loss = 0.14597362, step = 2101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.405\n",
            "INFO:tensorflow:loss = 0.13264391, step = 2201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 981.046\n",
            "INFO:tensorflow:loss = 0.11758447, step = 2301 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.51\n",
            "INFO:tensorflow:loss = 0.12137933, step = 2401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1004.11\n",
            "INFO:tensorflow:loss = 0.1049122, step = 2501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.828\n",
            "INFO:tensorflow:loss = 0.093126155, step = 2601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.98\n",
            "INFO:tensorflow:loss = 0.07897936, step = 2701 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.027\n",
            "INFO:tensorflow:loss = 0.062419955, step = 2801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1021.54\n",
            "INFO:tensorflow:loss = 0.06457691, step = 2901 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1048.73\n",
            "INFO:tensorflow:loss = 0.060098212, step = 3001 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.326\n",
            "INFO:tensorflow:loss = 0.04309454, step = 3101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.714\n",
            "INFO:tensorflow:loss = 0.024945892, step = 3201 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.898\n",
            "INFO:tensorflow:loss = 0.032289155, step = 3301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1036.11\n",
            "INFO:tensorflow:loss = 0.029175913, step = 3401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 1016.05\n",
            "INFO:tensorflow:loss = 0.03193499, step = 3501 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.768\n",
            "INFO:tensorflow:loss = 0.029986508, step = 3601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.7\n",
            "INFO:tensorflow:loss = 0.028025758, step = 3701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1024.43\n",
            "INFO:tensorflow:loss = 0.019241564, step = 3801 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1012.06\n",
            "INFO:tensorflow:loss = 0.014843706, step = 3901 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1034.54\n",
            "INFO:tensorflow:loss = 0.0113553945, step = 4001 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.23\n",
            "INFO:tensorflow:loss = 0.013580639, step = 4101 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1017.68\n",
            "INFO:tensorflow:loss = 0.013513946, step = 4201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.134\n",
            "INFO:tensorflow:loss = 0.010665568, step = 4301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1066.09\n",
            "INFO:tensorflow:loss = 0.0096648, step = 4401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 886.263\n",
            "INFO:tensorflow:loss = 0.008835621, step = 4501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.897\n",
            "INFO:tensorflow:loss = 0.0076671643, step = 4601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1030.6\n",
            "INFO:tensorflow:loss = 0.0062703555, step = 4701 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1027.01\n",
            "INFO:tensorflow:loss = 0.006357518, step = 4801 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.558\n",
            "INFO:tensorflow:loss = 0.005905528, step = 4901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.115\n",
            "INFO:tensorflow:loss = 0.007065449, step = 5001 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.204\n",
            "INFO:tensorflow:loss = 0.008963382, step = 5101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1045.04\n",
            "INFO:tensorflow:loss = 0.0066372394, step = 5201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 1025.2\n",
            "INFO:tensorflow:loss = 0.0069780815, step = 5301 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1031.89\n",
            "INFO:tensorflow:loss = 0.0049525825, step = 5401 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 828.475\n",
            "INFO:tensorflow:loss = 0.00726964, step = 5501 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 913.428\n",
            "INFO:tensorflow:loss = 0.007252666, step = 5601 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1042.19\n",
            "INFO:tensorflow:loss = 0.006233975, step = 5701 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 955.851\n",
            "INFO:tensorflow:loss = 0.008085558, step = 5801 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.687\n",
            "INFO:tensorflow:loss = 0.0037966697, step = 5901 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1028.65\n",
            "INFO:tensorflow:loss = 0.0036321338, step = 6001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.285\n",
            "INFO:tensorflow:loss = 0.013702126, step = 6101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 968.845\n",
            "INFO:tensorflow:loss = 0.0067000245, step = 6201 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.978\n",
            "INFO:tensorflow:loss = 0.004208537, step = 6301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 992.845\n",
            "INFO:tensorflow:loss = 0.0045540277, step = 6401 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.778\n",
            "INFO:tensorflow:loss = 0.0065816324, step = 6501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 888.948\n",
            "INFO:tensorflow:loss = 0.0070807533, step = 6601 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 932.745\n",
            "INFO:tensorflow:loss = 0.004677452, step = 6701 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.186\n",
            "INFO:tensorflow:loss = 0.009377343, step = 6801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1006.68\n",
            "INFO:tensorflow:loss = 0.0077874656, step = 6901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1013.01\n",
            "INFO:tensorflow:loss = 0.015999088, step = 7001 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.238\n",
            "INFO:tensorflow:loss = 0.004321813, step = 7101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.564\n",
            "INFO:tensorflow:loss = 0.026206832, step = 7201 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 884.594\n",
            "INFO:tensorflow:loss = 0.08553984, step = 7301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.39\n",
            "INFO:tensorflow:loss = 0.0153731555, step = 7401 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1009.26\n",
            "INFO:tensorflow:loss = 0.2216114, step = 7501 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.155\n",
            "INFO:tensorflow:loss = 0.15195179, step = 7601 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1020.32\n",
            "INFO:tensorflow:loss = 0.008908097, step = 7701 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 989.853\n",
            "INFO:tensorflow:loss = 0.04386139, step = 7801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1040.86\n",
            "INFO:tensorflow:loss = 0.122042105, step = 7901 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.57\n",
            "INFO:tensorflow:loss = 0.39776337, step = 8001 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.432\n",
            "INFO:tensorflow:loss = 0.03907896, step = 8101 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1000.38\n",
            "INFO:tensorflow:loss = 0.25288033, step = 8201 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1048.11\n",
            "INFO:tensorflow:loss = 0.09320173, step = 8301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 991.303\n",
            "INFO:tensorflow:loss = 0.51855147, step = 8401 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1045.29\n",
            "INFO:tensorflow:loss = 0.5657393, step = 8501 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.371\n",
            "INFO:tensorflow:loss = 0.02119001, step = 8601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 1026\n",
            "INFO:tensorflow:loss = 0.5891017, step = 8701 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 987.85\n",
            "INFO:tensorflow:loss = 0.3117571, step = 8801 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 1010.16\n",
            "INFO:tensorflow:loss = 0.014939981, step = 8901 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 1037.04\n",
            "INFO:tensorflow:loss = 0.02505644, step = 9001 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.224\n",
            "INFO:tensorflow:loss = 0.4035734, step = 9101 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1049.95\n",
            "INFO:tensorflow:loss = 0.60248303, step = 9201 (0.092 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.04\n",
            "INFO:tensorflow:loss = 1.0214088, step = 9301 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 1043.34\n",
            "INFO:tensorflow:loss = 0.5570653, step = 9401 (0.095 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.245\n",
            "INFO:tensorflow:loss = 1.0635748, step = 9501 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 948.195\n",
            "INFO:tensorflow:loss = 0.019592965, step = 9601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 1036.02\n",
            "INFO:tensorflow:loss = 0.0068869526, step = 9701 (0.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.549\n",
            "INFO:tensorflow:loss = 0.99812734, step = 9801 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.516\n",
            "INFO:tensorflow:loss = 0.042794917, step = 9901 (0.102 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_mxpsd/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.004836966.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_mxpsd/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 81.16662150752263\n",
            "Just using average = 600.0881488736533 has RMSE of 98.35068083241072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictors[trainsize:].values))\n",
        "print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_gust', hidden_units=[23,19,11], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45472fb-e14c-400c-c8e0-c8fe03ec5803",
        "id": "NoIPNaT3UbJ5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff062d55d90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_gust', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511\n",
            "511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_gust/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5527696  0.52508855 0.6127819  0.581434   0.61828417 0.61175823\n",
            " 0.63292027 0.59236956 0.5249682  0.5626297  0.5715749  0.60353154\n",
            " 0.47396922 0.581      0.6268806  0.5770799  0.62899625 0.47361183\n",
            " 0.6106329  0.56895494 0.5959188  0.6305055  0.62277585 0.5162532\n",
            " 0.61939365 0.52758753 0.59395343 0.5659039  0.684877   0.681315\n",
            " 0.5774379  0.6243287  0.57529247 0.5911565  0.47798577 0.4547477\n",
            " 0.5292468  0.5924553  0.6320611  0.5995403  0.4729641  0.55078274\n",
            " 0.5829641  0.49724042 0.60598487 0.5737326  0.62647694 0.61818373\n",
            " 0.6239849  0.55638236 0.5590047  0.4733155  0.5410132  0.5791286\n",
            " 0.6186458  0.46091318 0.6114701  0.6151172  0.54484683 0.60004145\n",
            " 0.63365906 0.55278414 0.59576213 0.6487696  0.58636767 0.47622463\n",
            " 0.6195855  0.4709046  0.58732903 0.4305575  0.52301335 0.50923723\n",
            " 0.57919985 0.63262874 0.55146307 0.5498693  0.60074824 0.6320026\n",
            " 0.6370218  0.6030973  0.5943333  0.62034535 0.56754833 0.5484992\n",
            " 0.5458823  0.6076782  0.631516   0.6207136  0.59210306 0.59914637\n",
            " 0.52468896 0.5974265  0.53309166 0.6624088  0.6127751  0.5969448\n",
            " 0.597587   0.61482733 0.57244384 0.5081821  0.5487809  0.57221365\n",
            " 0.53402644 0.4966309  0.5692656  0.48255807 0.48341867 0.47573334\n",
            " 0.524084   0.58311063 0.59255564 0.50502735 0.6617171  0.56567097\n",
            " 0.6166739  0.58243966 0.6261434  0.54130065 0.5994762  0.5771454\n",
            " 0.6154074  0.5556096  0.6176325  0.45646092 0.5440005  0.6249215\n",
            " 0.55928874 0.5682852  0.6011059  0.5643213  0.5929805  0.6051936\n",
            " 0.4760228  0.64868724 0.57885414 0.6058769  0.6097519  0.5841361\n",
            " 0.5850118  0.59094614 0.6104248  0.5926625  0.5068923  0.6100696\n",
            " 0.6223067  0.6493328  0.6094136  0.5685053  0.5890801  0.542593\n",
            " 0.5812915  0.56636137 0.5856548  0.62267065 0.5823018  0.5065932\n",
            " 0.6437595  0.5980351  0.6170847  0.5756456  0.57261205 0.6539269\n",
            " 0.598041   0.56875443 0.5802089  0.58571607 0.5057659  0.6082096\n",
            " 0.50598156 0.59104353 0.5678716  0.47528985 0.5990974  0.62742865\n",
            " 0.65493584 0.60846007 0.6168634  0.61127967 0.5885451  0.5537735\n",
            " 0.55538017 0.6277019  0.49699175 0.47704205 0.6240587  0.6223629\n",
            " 0.5769472  0.6434663  0.5886679  0.5866855  0.58974266 0.504639\n",
            " 0.531855   0.5488189  0.6227712  0.5307527  0.6116464  0.47623572\n",
            " 0.6178332  0.43027157 0.62175    0.5934779  0.62975746 0.623774\n",
            " 0.62371707 0.5324415  0.57564896 0.53758645 0.6225715  0.5282273\n",
            " 0.5498096  0.607457   0.6029745  0.58064103 0.5802206  0.49760664\n",
            " 0.55573416 0.5323573  0.6481959  0.62779003 0.6034625  0.5618346\n",
            " 0.50789404 0.6334785  0.6039498  0.56850255 0.58178926 0.66135997\n",
            " 0.5893983  0.57583207 0.61408114 0.5493147  0.60634434 0.57354957\n",
            " 0.59562594 0.575423   0.51921654 0.5182008  0.62256277 0.62760514\n",
            " 0.50376606 0.49894696 0.5906854  0.56096065 0.5844001  0.5676436\n",
            " 0.59160775 0.5908678  0.50350827 0.58701646 0.52777475 0.52272606\n",
            " 0.58005697 0.61418104 0.5808692  0.6164541  0.56689775 0.5611879\n",
            " 0.5878922  0.5603164  0.5921938  0.5917047  0.5703915  0.6257409\n",
            " 0.57443815 0.60615665 0.5996871  0.43384334 0.50217354 0.6065608\n",
            " 0.6294011  0.4508756  0.5873694  0.47723773 0.6630531  0.5797666\n",
            " 0.578779   0.49493954 0.54680896 0.6055869  0.5892968  0.509886\n",
            " 0.5513846  0.6481153  0.5971292  0.5265601  0.55835056 0.62985396\n",
            " 0.47426862 0.6291101  0.61623335 0.5085939  0.6147642  0.6258446\n",
            " 0.58500874 0.69046205 0.5941675  0.47154608 0.5765702  0.59721565\n",
            " 0.614369   0.5434446  0.6349805  0.5311299  0.62146413 0.50510496\n",
            " 0.58928573 0.6578356  0.45544454 0.62155414 0.6222753  0.56161016\n",
            " 0.5629806  0.6052693  0.6232168  0.50557274 0.5860583  0.60077494\n",
            " 0.5850541  0.59387356 0.62422353 0.5775757  0.46467465 0.49167663\n",
            " 0.6282431  0.48059773 0.60714227 0.58276606 0.62367964 0.5982078\n",
            " 0.5756102  0.59487945 0.57838655 0.62212205 0.5462103  0.4364538\n",
            " 0.5520943  0.6379619  0.45124018 0.54828066 0.59106183 0.5848232\n",
            " 0.55531937 0.47884366 0.64245975 0.6321625  0.61407155 0.5708337\n",
            " 0.61082065 0.5723757  0.57568765 0.5905907  0.64223284 0.47119567\n",
            " 0.5465164  0.5994477  0.6669366  0.4995944  0.5560327  0.5936663\n",
            " 0.46793315 0.55313766 0.54886    0.5687265  0.6488436  0.6470932\n",
            " 0.5063356  0.56524915 0.5993589  0.6201676  0.5101486  0.6015\n",
            " 0.5050796  0.6001256  0.43158454 0.6438301  0.553506   0.5962814\n",
            " 0.46632475 0.43941838 0.5889561  0.66367954 0.6480393  0.50436956\n",
            " 0.58279496 0.56996906 0.6171942  0.5734427  0.586007   0.48740286\n",
            " 0.55921936 0.57457596 0.52145106 0.5988539  0.6486858  0.6267831\n",
            " 0.59134525 0.61492026 0.6153783  0.46323252 0.64745295 0.58341956\n",
            " 0.536929   0.5337364  0.4547972  0.65240645 0.59980834 0.60781026\n",
            " 0.612152   0.6175396  0.65473247 0.6542641  0.5958156  0.5688602\n",
            " 0.59814084 0.64849627 0.5069862  0.613435   0.58697337 0.47984096\n",
            " 0.57979596 0.6568763  0.5745449  0.5411375  0.54830855 0.530849\n",
            " 0.53852165 0.4551469  0.48711047 0.5488546  0.49745715 0.590053\n",
            " 0.53597444 0.52859664 0.65502584 0.5075377  0.47526532 0.57854784\n",
            " 0.6513996  0.5647005  0.593662   0.5919066  0.63282627 0.5025046\n",
            " 0.58404076 0.54788524 0.4794839  0.57911575 0.63004243 0.6083073\n",
            " 0.5627425  0.66498685 0.44927636 0.6160337  0.61994207 0.59906435\n",
            " 0.5856935  0.6182021  0.68653035 0.6271504  0.44761142 0.62371004\n",
            " 0.5745331  0.49632275 0.63156354 0.5698219  0.595982   0.45604602\n",
            " 0.617327   0.6185411  0.6021151  0.62853694 0.586873   0.5780893\n",
            " 0.58181465 0.6012101  0.61568016 0.6024043  0.58217126 0.64739364\n",
            " 0.5955219  0.59824365 0.61109185 0.6229587  0.4811623  0.6455297\n",
            " 0.62920207 0.54540104 0.59688467 0.5951689  0.61658305 0.570944\n",
            " 0.5936284  0.58732885 0.51275563 0.65652055 0.5256095  0.5853622\n",
            " 0.4805195  0.5713631  0.5935762  0.61107105 0.6242741  0.606792\n",
            " 0.6269404  0.5798521  0.6210662  0.5024072  0.48870164 0.5879863\n",
            " 0.64767915]\n",
            "[0.47114556 0.41171404 0.55900086 0.48751077 0.51076658 0.61757106\n",
            " 0.6089578  0.60292851 0.46942291 0.47372954 0.6089578  0.49267873\n",
            " 0.46339363 0.72265289 0.49440138 0.55469423 0.43583118 0.38070629\n",
            " 0.54694229 0.40999139 0.4952627  0.60034453 0.57881137 0.44875108\n",
            " 0.60034453 0.47803618 0.53316107 0.51421189 0.59689922 0.64857881\n",
            " 0.49870801 0.59086994 0.44186047 0.54866494 0.38845823 0.40482343\n",
            " 0.51593454 0.5503876  0.55727821 0.56589147 0.33936262 0.47114556\n",
            " 0.54177433 0.40913006 0.42463394 0.4918174  0.68044789 0.64254953\n",
            " 0.5081826  0.48406546 0.37037037 0.44099914 0.51765719 0.48492679\n",
            " 0.66322136 0.38156761 0.58484065 0.65719208 0.46339363 0.56330749\n",
            " 0.52540913 0.48148148 0.51248923 0.58656331 0.57881137 0.41429802\n",
            " 0.57881137 0.4005168  0.50129199 0.44530577 0.47200689 0.43496985\n",
            " 0.46683893 0.65288544 0.57019811 0.48578811 0.57450474 0.53919035\n",
            " 0.47286822 0.47028424 0.59345392 0.63824289 0.55555556 0.49354005\n",
            " 0.47200689 0.50215332 0.60292851 0.49095607 0.54349699 0.52971576\n",
            " 0.55900086 0.64857881 0.4461671  0.61929371 0.69509044 0.54866494\n",
            " 0.56072351 0.53919035 0.47459087 0.46770026 0.48837209 0.44702842\n",
            " 0.50559862 0.42549526 0.4625323  0.40913006 0.36175711 0.39879414\n",
            " 0.45994832 0.53660637 0.45908699 0.43066322 0.625323   0.52885444\n",
            " 0.59259259 0.60723514 0.40999139 0.48664944 0.56416882 0.49009475\n",
            " 0.57364341 0.51937984 0.63307494 0.36950904 0.38845823 0.60034453\n",
            " 0.47717485 0.4952627  0.46080965 0.39879414 0.54694229 0.63910422\n",
            " 0.47459087 0.59689922 0.57881137 0.51421189 0.54263566 0.47459087\n",
            " 0.45391904 0.45564169 0.50732127 0.47803618 0.46856158 0.66149871\n",
            " 0.56589147 0.64082687 0.52368648 0.50645995 0.4496124  0.49870801\n",
            " 0.46942291 0.52971576 0.51679587 0.58656331 0.52627046 0.4918174\n",
            " 0.52971576 0.47631352 0.65977606 0.62446167 0.40913006 0.55727821\n",
            " 0.60120586 0.62790698 0.50990525 0.58570198 0.26873385 0.53143842\n",
            " 0.43238587 0.55555556 0.45564169 0.31438415 0.46425495 0.4496124\n",
            " 0.61068045 0.54952627 0.57622739 0.58570198 0.32213609 0.51937984\n",
            " 0.47028424 0.52885444 0.41774332 0.48751077 0.32988803 0.59689922\n",
            " 0.50990525 0.70198105 0.55469423 0.5245478  0.5211025  0.40826873\n",
            " 0.50559862 0.25064599 0.54091301 0.48148148 0.55727821 0.35228252\n",
            " 0.60981912 0.38329027 0.5245478  0.60120586 0.56416882 0.57019811\n",
            " 0.45908699 0.44875108 0.62273902 0.53832903 0.64082687 0.48148148\n",
            " 0.47286822 0.6744186  0.5960379  0.47975883 0.54952627 0.43927649\n",
            " 0.54349699 0.40913006 0.63996555 0.48664944 0.51765719 0.48406546\n",
            " 0.49784668 0.6089578  0.49956934 0.51421189 0.58053402 0.62273902\n",
            " 0.2213609  0.54435831 0.41860465 0.49870801 0.50990525 0.47028424\n",
            " 0.51937984 0.47286822 0.49095607 0.47803618 0.53660637 0.66666667\n",
            " 0.47717485 0.45564169 0.49267873 0.57105943 0.51162791 0.47286822\n",
            " 0.54866494 0.59431525 0.39362618 0.56847545 0.46080965 0.56933678\n",
            " 0.55469423 0.60292851 0.4203273  0.50301464 0.58484065 0.55641688\n",
            " 0.52368648 0.46425495 0.5538329  0.63652024 0.45219638 0.55469423\n",
            " 0.57019811 0.6709733  0.51335056 0.32127476 0.42635659 0.47975883\n",
            " 0.56158484 0.38501292 0.52024117 0.43927649 0.6873385  0.58914729\n",
            " 0.48234281 0.40137812 0.42377261 0.43152455 0.63910422 0.46167097\n",
            " 0.56503015 0.60034453 0.46770026 0.5667528  0.53660637 0.59173127\n",
            " 0.4461671  0.62015504 0.51248923 0.34797588 0.5503876  0.50215332\n",
            " 0.48837209 0.63479759 0.49354005 0.39793282 0.42807924 0.51937984\n",
            " 0.54091301 0.51335056 0.60723514 0.49956934 0.64857881 0.45822567\n",
            " 0.57881137 0.58225668 0.36434109 0.51593454 0.6546081  0.48923342\n",
            " 0.53143842 0.54866494 0.54263566 0.37209302 0.51335056 0.40999139\n",
            " 0.50732127 0.44358312 0.56330749 0.46856158 0.38845823 0.52024117\n",
            " 0.56244617 0.37812231 0.52799311 0.50301464 0.64857881 0.60206718\n",
            " 0.43238587 0.28251507 0.60551249 0.42894057 0.52971576 0.33850129\n",
            " 0.47631352 0.47631352 0.37726098 0.55900086 0.22739018 0.41515935\n",
            " 0.59173127 0.37812231 0.58656331 0.57364341 0.63049096 0.52196382\n",
            " 0.42721792 0.55900086 0.63738157 0.60206718 0.59259259 0.39965547\n",
            " 0.49784668 0.53316107 0.56072351 0.49354005 0.51162791 0.60378984\n",
            " 0.35486649 0.53143842 0.63221361 0.53919035 0.36606374 0.5538329\n",
            " 0.42721792 0.55555556 0.53057709 0.55469423 0.50301464 0.51162791\n",
            " 0.36003445 0.60551249 0.35400517 0.6287683  0.51248923 0.49784668\n",
            " 0.40137812 0.41774332 0.51851852 0.69336779 0.56847545 0.51076658\n",
            " 0.583118   0.53574505 0.48062016 0.56503015 0.49095607 0.50387597\n",
            " 0.53832903 0.48923342 0.34711456 0.42807924 0.60551249 0.58397933\n",
            " 0.50559862 0.60809647 0.60723514 0.40310078 0.50904393 0.54177433\n",
            " 0.41946598 0.42894057 0.33936262 0.56416882 0.55555556 0.55641688\n",
            " 0.53919035 0.51679587 0.57278208 0.58742463 0.51765719 0.53229974\n",
            " 0.60292851 0.64944014 0.44444444 0.5245478  0.26098191 0.49354005\n",
            " 0.4918174  0.52971576 0.60809647 0.42807924 0.50215332 0.49956934\n",
            " 0.44358312 0.38329027 0.49267873 0.49956934 0.43496985 0.44702842\n",
            " 0.45305771 0.5374677  0.58742463 0.43669251 0.41429802 0.53143842\n",
            " 0.50129199 0.43927649 0.65374677 0.76055125 0.6873385  0.36089578\n",
            " 0.57019811 0.374677   0.40999139 0.68130922 0.51765719 0.54005168\n",
            " 0.44013781 0.62704565 0.38587425 0.61412575 0.52971576 0.49354005\n",
            " 0.48492679 0.56330749 0.64513351 0.58656331 0.43841516 0.57536606\n",
            " 0.54435831 0.33074935 0.52971576 0.4461671  0.51248923 0.34022394\n",
            " 0.51593454 0.59862188 0.53660637 0.45047373 0.45478036 0.57536606\n",
            " 0.38931955 0.54952627 0.51076658 0.54952627 0.4625323  0.55813953\n",
            " 0.60378984 0.56158484 0.63910422 0.61757106 0.44186047 0.53574505\n",
            " 0.45047373 0.49612403 0.63996555 0.58656331 0.60465116 0.53574505\n",
            " 0.63996555 0.5503876  0.38845823 0.55297158 0.45736434 0.52540913\n",
            " 0.4496124  0.51248923 0.49009475 0.5503876  0.52971576 0.55555556\n",
            " 0.6124031  0.51507321 0.55211025 0.40223945 0.4788975  0.35658915\n",
            " 0.5211025 ]\n",
            "The trained model has an aproximate error rate of -73.63368005202007 which equates to -12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Others"
      ],
      "metadata": {
        "id": "Dzn0MDJwVdg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/datadnn.csv', index_col=0, )\n",
        "print(df[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6522939-6288-4ce9-ade8-066d9590030a",
        "id": "r3AhZB1bVybn"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS  temp  dewp     slp  visib  wdsp  mxpsd   gust  \\\n",
            "1  2020  24             524  37.3  33.7  1028.5    6.5   3.3    8.0  999.9   \n",
            "2  2021  12             278  37.0  29.1  1019.0   10.0   6.5   12.0  999.9   \n",
            "3  2021  22             254  36.5  28.4  1003.1   10.0   7.8   12.0   20.0   \n",
            "4  2021  27             262  34.6  33.8  1012.8    8.0   7.8   12.0  999.9   \n",
            "5  2021  26             263  31.9  23.4  1016.9    9.0   7.4   12.0  999.9   \n",
            "6  2022  24             237  34.5  23.8  1010.6    9.7   7.4   12.0  999.9   \n",
            "\n",
            "   ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "2  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  ...    0    0    0    0    0    0    0    1    0    0  \n",
            "4  ...    0    0    0    0    0    0    0    0    1    0  \n",
            "5  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  ...    0    0    0    0    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn = df.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "dnn = dnn.loc[dnn[\"year\"] != 2012]\n",
        "dnn = dnn.loc[dnn[\"year\"] < 2020]\n",
        "cols = dnn['NUM_COLLISIONS']\n",
        "dnn = dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "dnn.insert(loc=26, column='NUM_COLLISIONS', value=cols)\n",
        "print(dnn[:6])\n",
        "dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "8933f4e4-23c6-4949-bbaf-42c1e2814650",
        "id": "izyExLEIVybo"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  Nov  Oct  \\\n",
            "49  2016  28  35.0   10.0   4.3  46.0  23.0  999.9    0    0  ...    0    0   \n",
            "51  2014  17  38.6    6.7   3.7  44.1  32.0  999.9    0    0  ...    0    0   \n",
            "54  2016  25  33.5   10.0   6.5  37.9  30.0  999.9    0    0  ...    0    0   \n",
            "55  2016  29  41.3   10.0   5.9  45.0  23.0  999.9    0    0  ...    0    0   \n",
            "58  2017  20  39.9   10.0   4.3  45.0  37.0  999.9    0    0  ...    0    0   \n",
            "59  2013  13  45.4    4.3   5.8  46.9  44.1  999.9    0    0  ...    0    0   \n",
            "\n",
            "    Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "49    0    0    0    0    0    0    1             681  \n",
            "51    0    0    0    0    1    0    0             589  \n",
            "54    0    0    0    1    0    0    0             658  \n",
            "55    0    0    0    0    1    0    0             645  \n",
            "58    0    0    0    0    1    0    0             605  \n",
            "59    0    0    1    0    0    0    0             373  \n",
            "\n",
            "[6 rows x 27 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         year           da         temp        visib         wdsp  \\\n",
              "count  2556.0  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean   2016.0    15.725743    51.487715     8.295618    10.682864   \n",
              "std       2.0     8.800168    14.162738     2.207870     4.242687   \n",
              "min    2013.0     1.000000     5.800000     0.200000     2.700000   \n",
              "25%    2014.0     8.000000    40.400000     7.100000     7.700000   \n",
              "50%    2016.0    16.000000    52.000000     9.400000    10.100000   \n",
              "75%    2018.0    23.000000    63.900000    10.000000    13.000000   \n",
              "max    2019.0    31.000000    77.500000    10.000000    39.300000   \n",
              "\n",
              "               max          min          sndp          Apr          Aug  ...  \\\n",
              "count  2556.000000  2556.000000  2.556000e+03  2556.000000  2556.000000  ...   \n",
              "mean     59.564280    43.869757  9.999000e+02     0.082160     0.084898  ...   \n",
              "std      14.279867    14.722751  2.274182e-13     0.274661     0.278785  ...   \n",
              "min      18.000000    -2.000000  9.999000e+02     0.000000     0.000000  ...   \n",
              "25%      48.000000    33.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "50%      60.100000    44.100000  9.999000e+02     0.000000     0.000000  ...   \n",
              "75%      72.000000    55.900000  9.999000e+02     0.000000     0.000000  ...   \n",
              "max      90.000000    71.600000  9.999000e+02     1.000000     1.000000  ...   \n",
              "\n",
              "               Nov          Oct          Sep          Mon          Sat  \\\n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000  2556.000000   \n",
              "mean      0.082160     0.084898     0.082160     0.143192     0.142801   \n",
              "std       0.274661     0.278785     0.274661     0.350338     0.349939   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "               Sun          Thu          Tue          Wed  NUM_COLLISIONS  \n",
              "count  2556.000000  2556.000000  2556.000000  2556.000000     2556.000000  \n",
              "mean      0.142801     0.142801     0.142801     0.142801      599.118936  \n",
              "std       0.349939     0.349939     0.349939     0.349939      100.258581  \n",
              "min       0.000000     0.000000     0.000000     0.000000      188.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000      531.000000  \n",
              "50%       0.000000     0.000000     0.000000     0.000000      602.000000  \n",
              "75%       0.000000     0.000000     0.000000     0.000000      665.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000     1161.000000  \n",
              "\n",
              "[8 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b8ab28f-290b-4c04-9f5c-60648d0a4994\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2556.0</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2.556000e+03</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "      <td>2556.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>15.725743</td>\n",
              "      <td>51.487715</td>\n",
              "      <td>8.295618</td>\n",
              "      <td>10.682864</td>\n",
              "      <td>59.564280</td>\n",
              "      <td>43.869757</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.084898</td>\n",
              "      <td>0.082160</td>\n",
              "      <td>0.143192</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>0.142801</td>\n",
              "      <td>599.118936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.0</td>\n",
              "      <td>8.800168</td>\n",
              "      <td>14.162738</td>\n",
              "      <td>2.207870</td>\n",
              "      <td>4.242687</td>\n",
              "      <td>14.279867</td>\n",
              "      <td>14.722751</td>\n",
              "      <td>2.274182e-13</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.278785</td>\n",
              "      <td>0.274661</td>\n",
              "      <td>0.350338</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>0.349939</td>\n",
              "      <td>100.258581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2013.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2014.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>40.400000</td>\n",
              "      <td>7.100000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>33.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>531.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>9.400000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>60.100000</td>\n",
              "      <td>44.100000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>602.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2018.0</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>63.900000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>55.900000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>665.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2019.0</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>39.300000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>71.600000</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1161.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b8ab28f-290b-4c04-9f5c-60648d0a4994')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b8ab28f-290b-4c04-9f5c-60648d0a4994 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b8ab28f-290b-4c04-9f5c-60648d0a4994');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = dnn.iloc[np.random.permutation(len(dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f3b852-2186-4752-b16b-09d79f5231a5",
        "id": "pOJhsz3dVybo"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      year  da  temp  visib  wdsp   max   min   sndp  Apr  Aug  ...  May  Nov  \\\n",
            "684   2019  18  34.5   10.0  10.8  41.0  27.0  999.9    0    0  ...    0    0   \n",
            "2200  2019  31  70.3    9.1   5.3  79.0  57.9  999.9    0    1  ...    0    0   \n",
            "2075  2019   1  66.9   10.0  10.4  77.0  62.1  999.9    0    0  ...    0    0   \n",
            "3588  2015  18  54.2    2.4  11.3  60.1  42.1  999.9    0    0  ...    0    0   \n",
            "2101  2018  11  68.3    9.3   8.9  73.9  63.0  999.9    0    0  ...    0    0   \n",
            "2730  2015  21  63.9   10.0  17.3  80.1  61.0  999.9    0    0  ...    0    0   \n",
            "\n",
            "      Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "684     0    0    0    0    1    0    0    0  \n",
            "2200    0    0    0    0    0    0    0    0  \n",
            "2075    0    0    0    0    1    0    0    0  \n",
            "3588    0    0    0    0    0    1    0    0  \n",
            "2101    0    0    0    0    0    0    1    0  \n",
            "2730    0    1    0    0    1    0    0    0  \n",
            "\n",
            "[6 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48976b66-568f-4ad8-ae74-23bf40d0178c",
        "id": "iOF3gP_XVybp"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "684     575\n",
            "2200    502\n",
            "2075    693\n",
            "3588    767\n",
            "2101    627\n",
            "2730    624\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d5e04f-aa00-413b-ce27-16976d83fa79",
        "id": "7XAEry6uVybp"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], optimizer=tf.train.AdamOptimizer(learning_rate=0.001), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8f8f6d-ae85-4022-99a7-ae296cb70e16",
        "id": "sb-ehJ2tVybq"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff062d1d290>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 62422.805, step = 1\n",
            "INFO:tensorflow:global_step/sec: 460.124\n",
            "INFO:tensorflow:loss = 0.9070146, step = 101 (0.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 657.54\n",
            "INFO:tensorflow:loss = 0.21452484, step = 201 (0.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.969\n",
            "INFO:tensorflow:loss = 0.20359117, step = 301 (0.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.558\n",
            "INFO:tensorflow:loss = 0.17017865, step = 401 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.407\n",
            "INFO:tensorflow:loss = 0.17433774, step = 501 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.177\n",
            "INFO:tensorflow:loss = 0.17765565, step = 601 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.605\n",
            "INFO:tensorflow:loss = 0.18322639, step = 701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.013\n",
            "INFO:tensorflow:loss = 0.1777083, step = 801 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 635.729\n",
            "INFO:tensorflow:loss = 0.15583268, step = 901 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.771\n",
            "INFO:tensorflow:loss = 0.19044523, step = 1001 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.719\n",
            "INFO:tensorflow:loss = 0.1739669, step = 1101 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 649.321\n",
            "INFO:tensorflow:loss = 0.13364306, step = 1201 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.651\n",
            "INFO:tensorflow:loss = 0.15590574, step = 1301 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.63\n",
            "INFO:tensorflow:loss = 0.12485102, step = 1401 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.562\n",
            "INFO:tensorflow:loss = 0.16946483, step = 1501 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.226\n",
            "INFO:tensorflow:loss = 0.15591812, step = 1601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.186\n",
            "INFO:tensorflow:loss = 0.14106092, step = 1701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.143\n",
            "INFO:tensorflow:loss = 0.13662797, step = 1801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.251\n",
            "INFO:tensorflow:loss = 0.12125142, step = 1901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.08\n",
            "INFO:tensorflow:loss = 0.117713675, step = 2001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 798.638\n",
            "INFO:tensorflow:loss = 0.13552192, step = 2101 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.968\n",
            "INFO:tensorflow:loss = 0.11189815, step = 2201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 889.345\n",
            "INFO:tensorflow:loss = 0.124878265, step = 2301 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 478.782\n",
            "INFO:tensorflow:loss = 0.13054298, step = 2401 (0.210 sec)\n",
            "INFO:tensorflow:global_step/sec: 586.849\n",
            "INFO:tensorflow:loss = 0.10811512, step = 2501 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.14\n",
            "INFO:tensorflow:loss = 0.11655311, step = 2601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 939.389\n",
            "INFO:tensorflow:loss = 0.07718517, step = 2701 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 878.265\n",
            "INFO:tensorflow:loss = 0.10818604, step = 2801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.006\n",
            "INFO:tensorflow:loss = 0.11108016, step = 2901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 787.456\n",
            "INFO:tensorflow:loss = 0.09856123, step = 3001 (0.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 982.51\n",
            "INFO:tensorflow:loss = 0.07905428, step = 3101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 851.525\n",
            "INFO:tensorflow:loss = 0.090085834, step = 3201 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 929.504\n",
            "INFO:tensorflow:loss = 0.0727183, step = 3301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 874.484\n",
            "INFO:tensorflow:loss = 0.09324552, step = 3401 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.855\n",
            "INFO:tensorflow:loss = 0.077462286, step = 3501 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.334\n",
            "INFO:tensorflow:loss = 0.07980143, step = 3601 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 887.854\n",
            "INFO:tensorflow:loss = 0.07672717, step = 3701 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.695\n",
            "INFO:tensorflow:loss = 0.080437176, step = 3801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.182\n",
            "INFO:tensorflow:loss = 0.06758212, step = 3901 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.255\n",
            "INFO:tensorflow:loss = 0.059770457, step = 4001 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 914.541\n",
            "INFO:tensorflow:loss = 0.05013956, step = 4101 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 939.74\n",
            "INFO:tensorflow:loss = 0.057822622, step = 4201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.214\n",
            "INFO:tensorflow:loss = 0.058741815, step = 4301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.617\n",
            "INFO:tensorflow:loss = 0.05370917, step = 4401 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 895.983\n",
            "INFO:tensorflow:loss = 0.045613483, step = 4501 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.589\n",
            "INFO:tensorflow:loss = 0.045816306, step = 4601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.92\n",
            "INFO:tensorflow:loss = 0.040178463, step = 4701 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.935\n",
            "INFO:tensorflow:loss = 0.034329742, step = 4801 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.765\n",
            "INFO:tensorflow:loss = 0.0370811, step = 4901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 936.938\n",
            "INFO:tensorflow:loss = 0.031906676, step = 5001 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 432.415\n",
            "INFO:tensorflow:loss = 0.036005296, step = 5101 (0.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 766.129\n",
            "INFO:tensorflow:loss = 0.032915436, step = 5201 (0.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.085\n",
            "INFO:tensorflow:loss = 0.045366272, step = 5301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.495\n",
            "INFO:tensorflow:loss = 0.02376853, step = 5401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.611\n",
            "INFO:tensorflow:loss = 0.03237583, step = 5501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 938.292\n",
            "INFO:tensorflow:loss = 0.023532607, step = 5601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.515\n",
            "INFO:tensorflow:loss = 0.020718604, step = 5701 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.625\n",
            "INFO:tensorflow:loss = 0.021880105, step = 5801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 921.975\n",
            "INFO:tensorflow:loss = 0.029887697, step = 5901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.915\n",
            "INFO:tensorflow:loss = 0.021562632, step = 6001 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.293\n",
            "INFO:tensorflow:loss = 0.026248313, step = 6101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.141\n",
            "INFO:tensorflow:loss = 0.026190046, step = 6201 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.639\n",
            "INFO:tensorflow:loss = 0.026341781, step = 6301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.775\n",
            "INFO:tensorflow:loss = 0.015971126, step = 6401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.106\n",
            "INFO:tensorflow:loss = 0.026028711, step = 6501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 833.17\n",
            "INFO:tensorflow:loss = 0.017308194, step = 6601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 896.947\n",
            "INFO:tensorflow:loss = 0.01797363, step = 6701 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 917.325\n",
            "INFO:tensorflow:loss = 0.017033195, step = 6801 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.933\n",
            "INFO:tensorflow:loss = 0.013509039, step = 6901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 844.4\n",
            "INFO:tensorflow:loss = 0.014766447, step = 7001 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.928\n",
            "INFO:tensorflow:loss = 0.012166822, step = 7101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 942.508\n",
            "INFO:tensorflow:loss = 0.013635676, step = 7201 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.415\n",
            "INFO:tensorflow:loss = 0.02432583, step = 7301 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 980.816\n",
            "INFO:tensorflow:loss = 0.01694504, step = 7401 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 904.504\n",
            "INFO:tensorflow:loss = 0.009761989, step = 7501 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 840.424\n",
            "INFO:tensorflow:loss = 0.008272013, step = 7601 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 966.12\n",
            "INFO:tensorflow:loss = 0.012198061, step = 7701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.048\n",
            "INFO:tensorflow:loss = 0.018379726, step = 7801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 951.208\n",
            "INFO:tensorflow:loss = 0.010937552, step = 7901 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.931\n",
            "INFO:tensorflow:loss = 0.012199189, step = 8001 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 953.409\n",
            "INFO:tensorflow:loss = 0.012590563, step = 8101 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 877.479\n",
            "INFO:tensorflow:loss = 0.011146827, step = 8201 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 978.692\n",
            "INFO:tensorflow:loss = 0.009321662, step = 8301 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 855.937\n",
            "INFO:tensorflow:loss = 0.010346426, step = 8401 (0.120 sec)\n",
            "INFO:tensorflow:global_step/sec: 825.951\n",
            "INFO:tensorflow:loss = 0.010800231, step = 8501 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.787\n",
            "INFO:tensorflow:loss = 0.00915608, step = 8601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 864.944\n",
            "INFO:tensorflow:loss = 0.015528568, step = 8701 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 924.823\n",
            "INFO:tensorflow:loss = 0.008460669, step = 8801 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.27\n",
            "INFO:tensorflow:loss = 0.009310005, step = 8901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.63\n",
            "INFO:tensorflow:loss = 0.0090829, step = 9001 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 830.376\n",
            "INFO:tensorflow:loss = 0.014772706, step = 9101 (0.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.409\n",
            "INFO:tensorflow:loss = 0.009537733, step = 9201 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 812.332\n",
            "INFO:tensorflow:loss = 0.009278538, step = 9301 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 902.564\n",
            "INFO:tensorflow:loss = 0.008277442, step = 9401 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 865.01\n",
            "INFO:tensorflow:loss = 0.0109102465, step = 9501 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 930.052\n",
            "INFO:tensorflow:loss = 0.008601977, step = 9601 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.549\n",
            "INFO:tensorflow:loss = 0.012570329, step = 9701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 907.082\n",
            "INFO:tensorflow:loss = 0.00760164, step = 9801 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 893.784\n",
            "INFO:tensorflow:loss = 0.029678036, step = 9901 (0.113 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.052899987.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 200.0920343269968\n",
            "Just using average = 598.5009784735812 has RMSE of 102.40434664573948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model', hidden_units=[17,13,9,7], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edf6a2b-abe0-4a29-d223-1ef8993cd35b",
        "id": "ssTSefyuVybq"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0633db250>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.018e+03 2.800e+01 4.180e+01 ... 1.000e+00 0.000e+00 0.000e+00]\n",
            " [2.019e+03 1.500e+01 4.770e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
            " [2.019e+03 2.400e+01 4.650e+01 ... 0.000e+00 0.000e+00 1.000e+00]\n",
            " ...\n",
            " [2.017e+03 1.400e+01 6.710e+01 ... 0.000e+00 1.000e+00 0.000e+00]\n",
            " [2.019e+03 1.400e+01 3.900e+01 ... 0.000e+00 0.000e+00 1.000e+00]\n",
            " [2.018e+03 3.000e+01 3.190e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.36530006 0.33201686 0.33876437 0.39548087 0.41206425 0.26414177\n",
            " 0.38562542 0.25478423 0.4111431  0.30683085 0.43572986 0.55800706\n",
            " 0.38391826 0.34203178 0.31752744 0.34235957 0.4200231  0.3278911\n",
            " 0.43051374 0.29856765 0.3727539  0.42756575 0.27777952 0.45883965\n",
            " 0.4030261  0.291053   0.35166708 0.35408887 0.40930805 0.29552352\n",
            " 0.44592187 0.29199043 0.21379322 0.37832585 0.39681888 0.41883114\n",
            " 0.14582789 0.452552   0.4109158  0.37340945 0.30726424 0.29460692\n",
            " 0.23756367 0.35156873 0.3147342  0.31703484 0.256946   0.306409\n",
            " 0.34769306 0.3773957  0.385972   0.41513005 0.48961103 0.4867388\n",
            " 0.41107792 0.27690652 0.35358885 0.27408764 0.5044761  0.43070415\n",
            " 0.44484153 0.45298895 0.40233502 0.24800393 0.37577367 0.3919853\n",
            " 0.38498646 0.1967065  0.36263064 0.33329514 0.32758033 0.3548625\n",
            " 0.13716313 0.33788556 0.4628078  0.32252157 0.2572303  0.38998067\n",
            " 0.38326132 0.23711333 0.29624403 0.38244683 0.46537665 0.37485287\n",
            " 0.30217564 0.43594217 0.33240566 0.31857488 0.28626344 0.32211396\n",
            " 0.46227542 0.42242068 0.3662821  0.36039272 0.42342284 0.45054212\n",
            " 0.49804002 0.33056387 0.46937302 0.31444064 0.38656846 0.34289825\n",
            " 0.37172702 0.33285013 0.3796429  0.41699484 0.2915251  0.4121729\n",
            " 0.41495457 0.32935676 0.3408535  0.41531536 0.35829037 0.30400166\n",
            " 0.35143223 0.36103305 0.33013946 0.2556851  0.34936786 0.3724529\n",
            " 0.3853082  0.3827508  0.43462992 0.4018325  0.24655199 0.27418733\n",
            " 0.381351   0.38879377 0.3938192  0.4623728  0.38485909 0.38657382\n",
            " 0.30084696 0.3836987  0.2864945  0.47885355 0.42941004 0.4417987\n",
            " 0.3939134  0.48894343 0.3859021  0.33374926 0.41098115 0.5078253\n",
            " 0.2939396  0.35933912 0.5092689  0.3850337  0.4441468  0.38347375\n",
            " 0.34498295 0.3614597  0.4299342  0.30031937 0.34652483 0.27428266\n",
            " 0.39579636 0.3326098  0.30513537 0.35890633 0.39729846 0.37475944\n",
            " 0.41470987 0.3329736  0.41592336 0.395029   0.46385637 0.3814104\n",
            " 0.44653675 0.41125202 0.26948556 0.39252013 0.41017553 0.27254373\n",
            " 0.35451844 0.4081108  0.28963688 0.36777294 0.33698222 0.39297843\n",
            " 0.3754041  0.34948283 0.3460724  0.25788453 0.39369246 0.2072106\n",
            " 0.35906833 0.3746691  0.3755907  0.40168056 0.41504806 0.3255807\n",
            " 0.4122238  0.51233387 0.43433824 0.43148264 0.3541874  0.39556125\n",
            " 0.36788988 0.21345973 0.37457138 0.39680213 0.35003746 0.50279313\n",
            " 0.3883547  0.270005   0.40755284 0.37451604 0.37419644 0.3766562\n",
            " 0.41454136 0.47322285 0.3277995  0.29902232 0.37879917 0.16522112\n",
            " 0.29326853 0.33409327 0.29971135 0.3515293  0.33000493 0.28469297\n",
            " 0.2543491  0.40997642 0.34142834 0.3289896  0.35509065 0.33004516\n",
            " 0.27188975 0.51793337 0.2422657  0.34380797 0.34479338 0.39050338\n",
            " 0.36099282 0.4219724  0.5458194  0.23587525 0.42020142 0.46289104\n",
            " 0.38415036 0.459749   0.37466034 0.34543774 0.55516165 0.3474574\n",
            " 0.4036947  0.3366218  0.36186674 0.25647175 0.332568   0.25699925\n",
            " 0.41731277 0.30849388 0.23173773 0.3378356  0.28787777 0.4257271\n",
            " 0.46712413 0.41994357 0.3655613  0.3533443  0.3256021  0.4472651\n",
            " 0.4496234  0.4454172  0.4494413  0.37048173 0.36837542 0.42164093\n",
            " 0.31480265 0.31080228 0.41396675 0.31688663 0.2917862  0.2066775\n",
            " 0.330805   0.37167957 0.43182036 0.35009527 0.39436606 0.3509243\n",
            " 0.3151469  0.37620902 0.4063076  0.2826687  0.37069318 0.53937733\n",
            " 0.37983954 0.2677285  0.21534127 0.4056377  0.30124453 0.2618078\n",
            " 0.2886051  0.36520877 0.38364798 0.43064058 0.41868132 0.3639453\n",
            " 0.36838004 0.39530003 0.4107641  0.3885326  0.48012456 0.3763274\n",
            " 0.3853607  0.34175083 0.40114102 0.38020664 0.4323481  0.42062536\n",
            " 0.39035103 0.39434633 0.4416273  0.48511136 0.4871202  0.2967373\n",
            " 0.3364215  0.4132196  0.36175802 0.39426115 0.43980598 0.3655371\n",
            " 0.42629513 0.43139806 0.27895617 0.3676945  0.397896   0.38328707\n",
            " 0.17252606 0.21195316 0.31311914 0.41408196 0.38798654 0.37345347\n",
            " 0.26072627 0.47084197 0.35285324 0.36416322 0.31371024 0.33024558\n",
            " 0.28239262 0.39465037 0.4240424  0.36414698 0.32152575 0.42726567\n",
            " 0.3717981  0.43639135 0.31176695 0.38608685 0.3641047  0.3768525\n",
            " 0.34083614 0.43777934 0.4894211  0.45677623 0.37140253 0.37889212\n",
            " 0.43317994 0.32099605 0.38247967 0.20694321 0.40732545 0.40860888\n",
            " 0.2898933  0.5107608  0.25921208 0.5636906  0.46901837 0.40687203\n",
            " 0.34493405 0.2980718  0.39686656 0.21719056 0.25318125 0.39607397\n",
            " 0.3790722  0.4092332  0.35947716 0.4096828  0.45563227 0.5142793\n",
            " 0.3608034  0.38976726 0.3676496  0.43417436 0.3686918  0.32716158\n",
            " 0.3725846  0.37414458 0.46335733 0.36277673 0.35543495 0.5470562\n",
            " 0.410702   0.3185293  0.43732825 0.40790352 0.35092497 0.44060996\n",
            " 0.36165586 0.31188688 0.3974353  0.4154368  0.38817242 0.2990427\n",
            " 0.47025546 0.32154885 0.2320793  0.3594214  0.3090894  0.4215221\n",
            " 0.3715218  0.35715985 0.36103317 0.36935458 0.42717084 0.2527916\n",
            " 0.4279035  0.4325301  0.32232293 0.39108843 0.30384442 0.439798\n",
            " 0.36341265 0.3779791  0.33721897 0.4065355  0.49972388 0.33858576\n",
            " 0.37350008 0.31890815 0.5171776  0.3091313  0.37414968 0.40323594\n",
            " 0.35109025 0.37755275 0.3744707  0.5592793  0.23542556 0.36729896\n",
            " 0.49386704 0.41197369 0.30257246 0.3937346  0.37876257 0.4001507\n",
            " 0.3603785  0.41915092 0.38719717 0.46641576 0.39073992 0.3350573\n",
            " 0.36440176 0.40371156 0.46251523 0.422267   0.34001476 0.4493719\n",
            " 0.2822116  0.40834248 0.35244262 0.32253215 0.24304283 0.43490717\n",
            " 0.33770922 0.34059933 0.3524002  0.37909704 0.4537218  0.32539126\n",
            " 0.38039124 0.40714726 0.39611572 0.48494422 0.38347584 0.43275174\n",
            " 0.31552073 0.3629248  0.24163178 0.31985036 0.29911527 0.3602253\n",
            " 0.26944867 0.401148   0.34595227 0.39178532 0.3754432  0.3379957\n",
            " 0.36216962 0.42695528 0.37966204 0.42330816 0.41402823 0.47025642\n",
            " 0.4029423  0.27257085 0.3205851  0.39714321 0.37597233 0.42028922\n",
            " 0.37449616 0.40365627 0.26214808 0.32422566 0.42380503 0.45357457\n",
            " 0.33971402 0.37786195]\n",
            "[0.58656331 0.4332472  0.50990525 0.52196382 0.55727821 0.46167097\n",
            " 0.57450474 0.33419466 0.37639966 0.38070629 0.50387597 0.583118\n",
            " 0.64857881 0.49095607 0.47372954 0.48406546 0.65374677 0.59173127\n",
            " 0.49612403 0.42291128 0.5211025  0.47803618 0.46683893 0.48234281\n",
            " 0.53574505 0.39879414 0.61929371 0.53832903 0.45305771 0.53229974\n",
            " 0.54349699 0.4203273  0.44530577 0.62790698 0.53660637 0.57019811\n",
            " 0.32816537 0.50904393 0.51679587 0.62101637 0.45908699 0.54091301\n",
            " 0.44702842 0.62446167 0.45478036 0.46080965 0.29371232 0.39965547\n",
            " 0.53488372 0.51076658 0.39018088 0.50215332 0.60120586 0.65202412\n",
            " 0.57881137 0.62273902 0.42463394 0.45650301 0.57622739 0.51679587\n",
            " 0.4754522  0.5667528  0.45650301 0.48923342 0.50215332 0.53143842\n",
            " 0.61412575 0.35400517 0.54435831 0.49870801 0.41343669 0.5667528\n",
            " 0.82773471 0.61498708 0.50301464 0.53402239 0.38587425 0.59689922\n",
            " 0.583118   0.48578811 0.77174849 0.48923342 0.59086994 0.33763997\n",
            " 0.50732127 0.66666667 0.47631352 0.48923342 0.37984496 0.48923342\n",
            " 0.56589147 0.72782084 0.56847545 0.45736434 0.54694229 0.66322136\n",
            " 0.63221361 0.42118863 0.64771748 0.50645995 0.58225668 0.52540913\n",
            " 0.55986219 0.46339363 0.6287683  0.46942291 0.57278208 0.51421189\n",
            " 0.4918174  0.6546081  0.50387597 0.583118   0.49354005 0.52540913\n",
            " 0.49095607 0.45564169 0.54694229 0.41429802 0.53229974 0.6709733\n",
            " 0.48406546 0.45650301 0.51851852 0.49009475 0.41343669 0.51937984\n",
            " 0.64513351 0.58225668 0.52971576 0.36864772 0.52971576 0.60981912\n",
            " 0.59259259 0.4788975  0.4332472  0.7002584  0.47286822 0.60551249\n",
            " 0.52627046 0.60120586 0.45908699 0.51507321 0.44702842 0.66149871\n",
            " 0.50387597 0.4788975  0.80878553 0.55555556 0.40137812 0.5994832\n",
            " 0.54866494 0.62790698 0.52799311 0.44099914 0.63049096 0.44099914\n",
            " 0.4918174  0.46942291 0.57536606 0.6124031  0.47028424 0.41602067\n",
            " 0.48406546 0.60809647 0.55211025 0.53660637 0.52282515 0.47114556\n",
            " 0.46511628 0.52282515 0.45391904 0.65202412 0.54349699 0.41774332\n",
            " 0.53660637 0.43669251 0.60206718 0.60378984 0.47286822 0.56330749\n",
            " 0.56589147 0.47200689 0.57622739 0.38845823 0.6089578  0.32472007\n",
            " 0.34711456 0.82687339 0.45822567 0.54435831 0.43496985 0.52024117\n",
            " 0.54694229 0.54091301 0.48923342 0.65719208 0.54608096 0.51421189\n",
            " 0.55727821 0.37209302 0.42377261 0.63479759 0.52799311 0.83893196\n",
            " 0.5374677  0.39793282 0.6873385  0.47286822 0.58570198 0.49956934\n",
            " 0.41343669 0.66925065 0.48148148 0.46683893 0.60723514 0.52799311\n",
            " 0.58914729 0.37812231 0.50559862 0.44702842 0.46511628 0.56589147\n",
            " 0.45564169 0.58053402 0.6709733  0.59345392 0.4754522  0.51765719\n",
            " 0.44272179 0.56847545 0.38587425 0.53229974 0.5047373  0.61843239\n",
            " 0.48406546 0.61412575 0.47200689 0.4461671  0.53574505 0.63996555\n",
            " 0.60034453 0.61757106 0.63910422 0.53488372 0.63307494 0.43583118\n",
            " 0.53229974 0.34022394 0.58484065 0.416882   0.49354005 0.56330749\n",
            " 0.5503876  0.53057709 0.31093885 0.54005168 0.45219638 0.61584841\n",
            " 0.62273902 0.54694229 0.50215332 0.56847545 0.6124031  0.62790698\n",
            " 0.62618432 0.4039621  0.61929371 0.52024117 0.47803618 0.42894057\n",
            " 0.48923342 0.50129199 0.34797588 0.45736434 0.41085271 0.52024117\n",
            " 0.61154177 0.51421189 0.52885444 0.57105943 0.50990525 0.60034453\n",
            " 0.43152455 0.60465116 0.54349699 0.55555556 0.52885444 0.69681309\n",
            " 0.5796727  0.48923342 0.4461671  0.52971576 0.38587425 0.56933678\n",
            " 0.41343669 0.56589147 0.56589147 0.71231697 0.625323   0.4005168\n",
            " 0.583118   0.43066322 0.64685616 0.53832903 0.46942291 0.47459087\n",
            " 0.49440138 0.34280792 0.48406546 0.48406546 0.57278208 0.53488372\n",
            " 0.59431525 0.46683893 0.44099914 0.67355728 0.53488372 0.34797588\n",
            " 0.40913006 0.22739018 0.56244617 0.6416882  0.56158484 0.42463394\n",
            " 0.48664944 0.47631352 0.46080965 0.48492679 0.71490095 0.63652024\n",
            " 0.37812231 0.40654608 0.46597761 0.37898363 0.43927649 0.43066322\n",
            " 0.40223945 0.69939707 0.43496985 0.55986219 0.43410853 0.5374677\n",
            " 0.47286822 0.56503015 0.57536606 0.42807924 0.50301464 0.25064599\n",
            " 0.6287683  0.55727821 0.4625323  0.43238587 0.52282515 0.5994832\n",
            " 0.51248923 0.56847545 0.49956934 0.54780362 0.54263566 0.46511628\n",
            " 0.52799311 0.54177433 0.63479759 0.4203273  0.52971576 0.44358312\n",
            " 0.49956934 0.60981912 0.43496985 0.5667528  0.56072351 0.51851852\n",
            " 0.49698536 0.4918174  0.49267873 0.34797588 0.45391904 0.57105943\n",
            " 0.48148148 0.51593454 0.40913006 0.48837209 0.55297158 0.56847545\n",
            " 0.49870801 0.58656331 0.59173127 0.60809647 0.48062016 0.46339363\n",
            " 0.52024117 0.50559862 0.63049096 0.50732127 0.53402239 0.35486649\n",
            " 0.51248923 0.5081826  0.60809647 0.625323   0.51248923 0.5245478\n",
            " 0.64944014 0.56072351 0.51765719 0.60292851 0.54263566 0.32816537\n",
            " 0.58914729 0.4461671  0.38673557 0.55813953 0.47717485 0.5503876\n",
            " 0.46597761 0.35228252 0.52368648 0.56933678 0.62360034 0.35745047\n",
            " 0.53143842 0.55900086 0.68217054 0.44272179 0.53574505 0.56503015\n",
            " 0.49095607 0.45650301 0.60378984 0.60292851 0.43583118 0.47717485\n",
            " 0.49956934 0.42635659 0.48062016 0.40482343 0.53316107 0.55986219\n",
            " 0.44444444 0.53832903 0.59000861 0.46770026 0.36950904 0.53143842\n",
            " 0.50904393 0.35658915 0.60723514 0.44358312 0.36864772 0.56933678\n",
            " 0.40913006 0.4952627  0.6089578  0.61068045 0.54952627 0.43496985\n",
            " 0.54263566 0.49095607 0.60809647 0.6580534  0.3910422  0.47200689\n",
            " 0.53229974 0.44530577 0.53660637 0.49784668 0.35228252 0.55813953\n",
            " 0.49784668 0.50215332 0.53229974 0.69164513 0.67786391 0.51765719\n",
            " 0.50215332 0.57536606 0.5538329  0.69250646 0.45564169 0.68044789\n",
            " 0.52799311 0.38673557 0.46597761 0.47717485 0.40223945 0.48751077\n",
            " 0.40137812 0.48751077 0.53660637 0.57622739 0.61498708 0.42118863\n",
            " 0.59086994 0.58053402 0.51335056 0.50904393 0.56761413 0.51593454\n",
            " 0.51679587 0.4625323  0.51421189 0.47286822 0.52196382 0.60551249\n",
            " 0.55986219 0.40568475 0.4005168  0.40826873 0.46597761 0.64944014\n",
            " 0.52627046 0.49870801]\n",
            "The trained model has an aproximate error rate of 172.0337666275445 which equates to 29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location\n"
      ],
      "metadata": {
        "id": "kHVsefPtZOYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REF https://www.geeksforgeeks.org/read-a-zipped-file-as-a-pandas-dataframe/ 12/11"
      ],
      "metadata": {
        "id": "rxJZ5Y6xoUBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc = pd.read_csv('https://raw.githubusercontent.com/matthew110395/12004210_DataAnalytics/main/locdnn.zip', index_col=0,compression='zip' )\n",
        "print(df_loc[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3604add8-8fcc-4eac-fa40-674dfdd8d998",
        "id": "d4VkDncof2uz"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da  NUM_COLLISIONS   latitude  longitude  temp  dewp     slp  visib  \\\n",
            "1  2018   2               1  40.681750 -73.967480  14.7   2.0  1024.9   10.0   \n",
            "2  2018   2               1  40.645370 -73.945110  14.7   2.0  1024.9   10.0   \n",
            "3  2018   2               1  40.614830 -73.998380  14.7   2.0  1024.9   10.0   \n",
            "4  2018   2               1  40.592190 -74.087395  14.7   2.0  1024.9   10.0   \n",
            "5  2018   2               1  40.769817 -73.782370  14.7   2.0  1024.9   10.0   \n",
            "6  2018   2               1  40.660175 -73.928200  14.7   2.0  1024.9   10.0   \n",
            "\n",
            "   wdsp  ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "2  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "3  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "4  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "5  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "6  12.9  ...    0    0    0    0    1    0    0    0    0    0  \n",
            "\n",
            "[6 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_loc_dnn = df_loc.drop(columns=[ 'prcp', 'dewp','mxpsd','gust','slp','thunder','tornado_funnel_cloud','fog','rain_drizzle','snow_ice_pellets','hail'])\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] != 2012]\n",
        "df_loc_dnn = df_loc_dnn.loc[df_loc_dnn[\"year\"] < 2020]\n",
        "cols = df_loc_dnn['NUM_COLLISIONS']\n",
        "df_loc_dnn = df_loc_dnn.drop(columns=['NUM_COLLISIONS'])\n",
        "df_loc_dnn.insert(loc=28, column='NUM_COLLISIONS', value=cols)\n",
        "print(df_loc_dnn[:6])\n",
        "df_loc_dnn.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "4205ba7e-3bc6-411f-d320-a68e89e0d278",
        "id": "hl2pF-yuf2u0"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  da   latitude  longitude  temp  visib  wdsp   max  min   sndp  ...  \\\n",
            "1  2018   2  40.681750 -73.967480  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "2  2018   2  40.645370 -73.945110  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "3  2018   2  40.614830 -73.998380  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "4  2018   2  40.592190 -74.087395  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "5  2018   2  40.769817 -73.782370  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "6  2018   2  40.660175 -73.928200  14.7   10.0  12.9  18.0  5.0  999.9  ...   \n",
            "\n",
            "   Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  NUM_COLLISIONS  \n",
            "1    0    0    0    1    0    0    0    0    0               1  \n",
            "2    0    0    0    1    0    0    0    0    0               1  \n",
            "3    0    0    0    1    0    0    0    0    0               1  \n",
            "4    0    0    0    1    0    0    0    0    0               1  \n",
            "5    0    0    0    1    0    0    0    0    0               1  \n",
            "6    0    0    0    1    0    0    0    0    0               1  \n",
            "\n",
            "[6 rows x 29 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               year            da      latitude     longitude          temp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   2.016149e+03  1.566747e+01  4.072402e+01 -7.392063e+01  5.203610e+01   \n",
              "std    1.970562e+00  8.752606e+00  7.845179e-02  8.651997e-02  1.410404e+01   \n",
              "min    2.013000e+03  1.000000e+00  4.049895e+01 -7.425453e+01  5.800000e+00   \n",
              "25%    2.014000e+03  8.000000e+00  4.066895e+01 -7.397644e+01  4.100000e+01   \n",
              "50%    2.016000e+03  1.600000e+01  4.072234e+01 -7.392891e+01  5.300000e+01   \n",
              "75%    2.018000e+03  2.300000e+01  4.076838e+01 -7.386641e+01  6.430000e+01   \n",
              "max    2.019000e+03  3.100000e+01  4.091288e+01 -7.366301e+01  7.750000e+01   \n",
              "\n",
              "              visib          wdsp           max           min          sndp  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   8.262622e+00  1.066076e+01  6.012695e+01  4.442143e+01  9.999000e+02   \n",
              "std    2.220453e+00  4.197681e+00  1.424273e+01  1.468128e+01  2.842172e-12   \n",
              "min    2.000000e-01  2.700000e+00  1.800000e+01 -2.000000e+00  9.999000e+02   \n",
              "25%    7.000000e+00  7.700000e+00  4.890000e+01  3.310000e+01  9.999000e+02   \n",
              "50%    9.300000e+00  1.010000e+01  6.100000e+01  4.500000e+01  9.999000e+02   \n",
              "75%    1.000000e+01  1.290000e+01  7.300000e+01  5.700000e+01  9.999000e+02   \n",
              "max    1.000000e+01  3.930000e+01  9.000000e+01  7.160000e+01  9.999000e+02   \n",
              "\n",
              "       ...           Nov           Oct           Sep           Mon  \\\n",
              "count  ...  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   ...  8.510758e-02  8.877971e-02  8.587448e-02  1.487626e-01   \n",
              "std    ...  2.790418e-01  2.844256e-01  2.801787e-01  3.558544e-01   \n",
              "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "                Sat           Sun           Thu           Tue           Wed  \\\n",
              "count  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06  1.311775e+06   \n",
              "mean   1.173738e-01  1.438814e-01  1.597195e-01  1.467698e-01  1.505395e-01   \n",
              "std    3.218653e-01  3.509695e-01  3.663458e-01  3.538765e-01  3.575996e-01   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "\n",
              "       NUM_COLLISIONS  \n",
              "count    1.311775e+06  \n",
              "mean     1.026582e+00  \n",
              "std      1.784039e-01  \n",
              "min      1.000000e+00  \n",
              "25%      1.000000e+00  \n",
              "50%      1.000000e+00  \n",
              "75%      1.000000e+00  \n",
              "max      1.100000e+01  \n",
              "\n",
              "[8 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31b7475e-c68f-41e7-8f18-0de9eaaeadcd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>da</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>temp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>sndp</th>\n",
              "      <th>...</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "      <td>1.311775e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.016149e+03</td>\n",
              "      <td>1.566747e+01</td>\n",
              "      <td>4.072402e+01</td>\n",
              "      <td>-7.392063e+01</td>\n",
              "      <td>5.203610e+01</td>\n",
              "      <td>8.262622e+00</td>\n",
              "      <td>1.066076e+01</td>\n",
              "      <td>6.012695e+01</td>\n",
              "      <td>4.442143e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>8.510758e-02</td>\n",
              "      <td>8.877971e-02</td>\n",
              "      <td>8.587448e-02</td>\n",
              "      <td>1.487626e-01</td>\n",
              "      <td>1.173738e-01</td>\n",
              "      <td>1.438814e-01</td>\n",
              "      <td>1.597195e-01</td>\n",
              "      <td>1.467698e-01</td>\n",
              "      <td>1.505395e-01</td>\n",
              "      <td>1.026582e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.970562e+00</td>\n",
              "      <td>8.752606e+00</td>\n",
              "      <td>7.845179e-02</td>\n",
              "      <td>8.651997e-02</td>\n",
              "      <td>1.410404e+01</td>\n",
              "      <td>2.220453e+00</td>\n",
              "      <td>4.197681e+00</td>\n",
              "      <td>1.424273e+01</td>\n",
              "      <td>1.468128e+01</td>\n",
              "      <td>2.842172e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>2.790418e-01</td>\n",
              "      <td>2.844256e-01</td>\n",
              "      <td>2.801787e-01</td>\n",
              "      <td>3.558544e-01</td>\n",
              "      <td>3.218653e-01</td>\n",
              "      <td>3.509695e-01</td>\n",
              "      <td>3.663458e-01</td>\n",
              "      <td>3.538765e-01</td>\n",
              "      <td>3.575996e-01</td>\n",
              "      <td>1.784039e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.013000e+03</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>4.049895e+01</td>\n",
              "      <td>-7.425453e+01</td>\n",
              "      <td>5.800000e+00</td>\n",
              "      <td>2.000000e-01</td>\n",
              "      <td>2.700000e+00</td>\n",
              "      <td>1.800000e+01</td>\n",
              "      <td>-2.000000e+00</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.014000e+03</td>\n",
              "      <td>8.000000e+00</td>\n",
              "      <td>4.066895e+01</td>\n",
              "      <td>-7.397644e+01</td>\n",
              "      <td>4.100000e+01</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>7.700000e+00</td>\n",
              "      <td>4.890000e+01</td>\n",
              "      <td>3.310000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.016000e+03</td>\n",
              "      <td>1.600000e+01</td>\n",
              "      <td>4.072234e+01</td>\n",
              "      <td>-7.392891e+01</td>\n",
              "      <td>5.300000e+01</td>\n",
              "      <td>9.300000e+00</td>\n",
              "      <td>1.010000e+01</td>\n",
              "      <td>6.100000e+01</td>\n",
              "      <td>4.500000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.018000e+03</td>\n",
              "      <td>2.300000e+01</td>\n",
              "      <td>4.076838e+01</td>\n",
              "      <td>-7.386641e+01</td>\n",
              "      <td>6.430000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.290000e+01</td>\n",
              "      <td>7.300000e+01</td>\n",
              "      <td>5.700000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.019000e+03</td>\n",
              "      <td>3.100000e+01</td>\n",
              "      <td>4.091288e+01</td>\n",
              "      <td>-7.366301e+01</td>\n",
              "      <td>7.750000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>3.930000e+01</td>\n",
              "      <td>9.000000e+01</td>\n",
              "      <td>7.160000e+01</td>\n",
              "      <td>9.999000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.100000e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31b7475e-c68f-41e7-8f18-0de9eaaeadcd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31b7475e-c68f-41e7-8f18-0de9eaaeadcd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31b7475e-c68f-41e7-8f18-0de9eaaeadcd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df_loc_dnn.iloc[np.random.permutation(len(df_loc_dnn))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:-1]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a381ec02-c298-4d69-f429-0123b890be62",
        "id": "cPhXM4DJf2u1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         year  da   latitude  longitude  temp  visib  wdsp   max   min   sndp  \\\n",
            "1144965  2019  16  40.667835  -73.89484  41.1    7.9  10.6  48.9  30.0  999.9   \n",
            "780661   2014  27  40.836869  -73.94894  63.9    9.8  13.6  75.0  59.0  999.9   \n",
            "123122   2017   9  40.666150  -73.80575  45.9    9.1  17.7  52.0  41.0  999.9   \n",
            "1530982  2018   6  40.650490  -74.01179  55.5   10.0   6.0  66.9  46.0  999.9   \n",
            "1441299  2019   6  40.754770  -73.82997  68.9    1.5   7.5  73.9  59.0  999.9   \n",
            "699748   2016  27  40.660860  -73.89883  40.0    9.8   5.4  46.9  37.0  999.9   \n",
            "\n",
            "         ...  May  Nov  Oct  Sep  Mon  Sat  Sun  Thu  Tue  Wed  \n",
            "1144965  ...    0    0    0    0    0    0    0    0    0    0  \n",
            "780661   ...    0    0    0    0    0    0    0    1    0    0  \n",
            "123122   ...    0    0    0    0    0    0    0    0    0    1  \n",
            "1530982  ...    0    0    1    0    0    0    0    0    0    0  \n",
            "1441299  ...    0    0    0    0    0    0    0    0    0    0  \n",
            "699748   ...    0    0    0    0    0    1    0    0    0    0  \n",
            "\n",
            "[6 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all rows for the 2nd column (i.e. 1)\n",
        "targets = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of the targets data.\n",
        "print(targets[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b211a87-7211-4333-a7b6-2e9b9d6aebda",
        "id": "sol3wzXcf2u2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1144965    2\n",
            "780661     1\n",
            "123122     1\n",
            "1530982    1\n",
            "1441299    1\n",
            "699748     1\n",
            "Name: NUM_COLLISIONS, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a training set i.e. 80% of the length of the shuffle array\n",
        "trainsize = int(len(shuffle['NUM_COLLISIONS'])*0.8)\n",
        "# The test set size is 100% - 80% = 20% of the length of the shuffle array.\n",
        "testsize = len(shuffle['NUM_COLLISIONS']) - trainsize\n",
        "\n",
        "# Define the number of output values (targets)\n",
        "noutputs = 1\n",
        "# Define the number of input values (predictors)\n",
        "nppredictors = len(shuffle.columns) - noutputs\n",
        "print(nppredictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d659297-ec77-48b9-c207-85ec38171b02",
        "id": "_fZ3mPt-f2u2"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_regression_trained_model_loc', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_COLLISIONS, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE_COLLISIONS\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets[trainsize:].values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(shuffle['NUM_COLLISIONS'][:trainsize])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((shuffle['NUM_COLLISIONS'][trainsize:] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd280be5-ec3a-4f32-a7d3-195d3b407c6e",
        "id": "FEMInwLGf2u3"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff05e586910>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting to train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:loss = 88380.15, step = 1\n",
            "INFO:tensorflow:global_step/sec: 734.647\n",
            "INFO:tensorflow:loss = 0.010075731, step = 101 (0.141 sec)\n",
            "INFO:tensorflow:global_step/sec: 911.274\n",
            "INFO:tensorflow:loss = 0.009417284, step = 201 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.185\n",
            "INFO:tensorflow:loss = 0.0086057875, step = 301 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033.76\n",
            "INFO:tensorflow:loss = 0.007711338, step = 401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 941.208\n",
            "INFO:tensorflow:loss = 0.0067865015, step = 501 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.806\n",
            "INFO:tensorflow:loss = 0.005872124, step = 601 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 946.2\n",
            "INFO:tensorflow:loss = 0.004995689, step = 701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 967.785\n",
            "INFO:tensorflow:loss = 0.004171527, step = 801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 905.408\n",
            "INFO:tensorflow:loss = 0.0034301612, step = 901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 939.42\n",
            "INFO:tensorflow:loss = 0.0027694916, step = 1001 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 1022.69\n",
            "INFO:tensorflow:loss = 0.0021953175, step = 1101 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 1033.51\n",
            "INFO:tensorflow:loss = 0.0017079213, step = 1201 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 836.293\n",
            "INFO:tensorflow:loss = 0.0013065172, step = 1301 (0.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 937.581\n",
            "INFO:tensorflow:loss = 0.0009801348, step = 1401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 824.77\n",
            "INFO:tensorflow:loss = 0.0007183666, step = 1501 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 973.378\n",
            "INFO:tensorflow:loss = 0.00051613356, step = 1601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1007.59\n",
            "INFO:tensorflow:loss = 0.00036471445, step = 1701 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 822.901\n",
            "INFO:tensorflow:loss = 0.00025131425, step = 1801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.757\n",
            "INFO:tensorflow:loss = 0.00016874727, step = 1901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 998.513\n",
            "INFO:tensorflow:loss = 0.000110836176, step = 2001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 957.427\n",
            "INFO:tensorflow:loss = 7.0796195e-05, step = 2101 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 920.433\n",
            "INFO:tensorflow:loss = 4.4015716e-05, step = 2201 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.348\n",
            "INFO:tensorflow:loss = 2.6826663e-05, step = 2301 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 996.823\n",
            "INFO:tensorflow:loss = 1.561078e-05, step = 2401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 962.692\n",
            "INFO:tensorflow:loss = 8.960538e-06, step = 2501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 950.398\n",
            "INFO:tensorflow:loss = 4.9028226e-06, step = 2601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 976.239\n",
            "INFO:tensorflow:loss = 2.698802e-06, step = 2701 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.392\n",
            "INFO:tensorflow:loss = 1.373917e-06, step = 2801 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.636\n",
            "INFO:tensorflow:loss = 6.9629994e-07, step = 2901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 956.217\n",
            "INFO:tensorflow:loss = 3.4433924e-07, step = 3001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 871.446\n",
            "INFO:tensorflow:loss = 2.3638745e-07, step = 3101 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.534\n",
            "INFO:tensorflow:loss = 9.415663e-08, step = 3201 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 974.998\n",
            "INFO:tensorflow:loss = 4.1373205e-08, step = 3301 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 999.196\n",
            "INFO:tensorflow:loss = 4.7802267e-08, step = 3401 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 892.303\n",
            "INFO:tensorflow:loss = 5.6032903e-08, step = 3501 (0.115 sec)\n",
            "INFO:tensorflow:global_step/sec: 943.066\n",
            "INFO:tensorflow:loss = 3.8538275e-08, step = 3601 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 862.445\n",
            "INFO:tensorflow:loss = 1.7627297e-08, step = 3701 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.593\n",
            "INFO:tensorflow:loss = 1.7219698e-08, step = 3801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 983.959\n",
            "INFO:tensorflow:loss = 1.1696247e-08, step = 3901 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 1003.55\n",
            "INFO:tensorflow:loss = 1.7033791e-08, step = 4001 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 961.553\n",
            "INFO:tensorflow:loss = 1.7004815e-08, step = 4101 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 908.487\n",
            "INFO:tensorflow:loss = 2.2463645e-08, step = 4201 (0.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 944.178\n",
            "INFO:tensorflow:loss = 3.9530413e-08, step = 4301 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.934\n",
            "INFO:tensorflow:loss = 2.7956752e-08, step = 4401 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.597\n",
            "INFO:tensorflow:loss = 2.2490333e-08, step = 4501 (0.102 sec)\n",
            "INFO:tensorflow:global_step/sec: 863.503\n",
            "INFO:tensorflow:loss = 1.1481654e-08, step = 4601 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 927.726\n",
            "INFO:tensorflow:loss = 1.6990482e-08, step = 4701 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.416\n",
            "INFO:tensorflow:loss = 1.6983883e-08, step = 4801 (0.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.801\n",
            "INFO:tensorflow:loss = 1.1506454e-08, step = 4901 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 972.173\n",
            "INFO:tensorflow:loss = 1.1485389e-08, step = 5001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.134\n",
            "INFO:tensorflow:loss = 3.3459198e-08, step = 5101 (0.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 883.024\n",
            "INFO:tensorflow:loss = 3.4079516e-08, step = 5201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 1021\n",
            "INFO:tensorflow:loss = 1.6984105e-08, step = 5301 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 867.088\n",
            "INFO:tensorflow:loss = 4.2728757e-10, step = 5401 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.32\n",
            "INFO:tensorflow:loss = 2.2492527e-08, step = 5501 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 894.575\n",
            "INFO:tensorflow:loss = 2.7944736e-08, step = 5601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 870.979\n",
            "INFO:tensorflow:loss = 1.6988395e-08, step = 5701 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 1005.76\n",
            "INFO:tensorflow:loss = 5.5996473e-08, step = 5801 (0.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.264\n",
            "INFO:tensorflow:loss = 6.024799e-09, step = 5901 (0.104 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.573\n",
            "INFO:tensorflow:loss = 2.2482645e-08, step = 6001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 919.34\n",
            "INFO:tensorflow:loss = 2.7963896e-08, step = 6101 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 913.933\n",
            "INFO:tensorflow:loss = 3.960073e-08, step = 6201 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 869.551\n",
            "INFO:tensorflow:loss = 2.2479338e-08, step = 6301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.062\n",
            "INFO:tensorflow:loss = 5.992231e-09, step = 6401 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.731\n",
            "INFO:tensorflow:loss = 1.6988611e-08, step = 6501 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 977.812\n",
            "INFO:tensorflow:loss = 1.1450276e-08, step = 6601 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 1016.9\n",
            "INFO:tensorflow:loss = 3.352615e-08, step = 6701 (0.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 931.794\n",
            "INFO:tensorflow:loss = 3.345165e-08, step = 6801 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 906.814\n",
            "INFO:tensorflow:loss = 6.1162714e-10, step = 6901 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 985.062\n",
            "INFO:tensorflow:loss = 1.1517046e-08, step = 7001 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 995.047\n",
            "INFO:tensorflow:loss = 6.07194e-09, step = 7101 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 952.573\n",
            "INFO:tensorflow:loss = 1.1469818e-08, step = 7201 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 910.91\n",
            "INFO:tensorflow:loss = 1.1547223e-08, step = 7301 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.802\n",
            "INFO:tensorflow:loss = 1.699813e-08, step = 7401 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 909.892\n",
            "INFO:tensorflow:loss = 1.6990068e-08, step = 7501 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 960.943\n",
            "INFO:tensorflow:loss = 6.0356635e-09, step = 7601 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 984.763\n",
            "INFO:tensorflow:loss = 7.369458e-08, step = 7701 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 975.54\n",
            "INFO:tensorflow:loss = 2.8591824e-08, step = 7801 (0.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 430.439\n",
            "INFO:tensorflow:loss = 6.0275687e-09, step = 7901 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 830.978\n",
            "INFO:tensorflow:loss = 2.8576512e-08, step = 8001 (0.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 928.878\n",
            "INFO:tensorflow:loss = 3.960792e-08, step = 8101 (0.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 810.653\n",
            "INFO:tensorflow:loss = 3.3465227e-08, step = 8201 (0.126 sec)\n",
            "INFO:tensorflow:global_step/sec: 923.327\n",
            "INFO:tensorflow:loss = 1.1458622e-08, step = 8301 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 1032.72\n",
            "INFO:tensorflow:loss = 2.7955956e-08, step = 8401 (0.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 997.283\n",
            "INFO:tensorflow:loss = 1.7037276e-08, step = 8501 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 872.214\n",
            "INFO:tensorflow:loss = 6.225533e-08, step = 8601 (0.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 926.155\n",
            "INFO:tensorflow:loss = 6.0323124e-09, step = 8701 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 986.092\n",
            "INFO:tensorflow:loss = 1.14900836e-08, step = 8801 (0.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 940.49\n",
            "INFO:tensorflow:loss = 3.347675e-08, step = 8901 (0.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 873.157\n",
            "INFO:tensorflow:loss = 2.2468228e-08, step = 9001 (0.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 827.568\n",
            "INFO:tensorflow:loss = 1.6991695e-08, step = 9101 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 945.378\n",
            "INFO:tensorflow:loss = 5.9446332e-09, step = 9201 (0.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 958.867\n",
            "INFO:tensorflow:loss = 1.698255e-08, step = 9301 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 949.511\n",
            "INFO:tensorflow:loss = 1.15537535e-08, step = 9401 (0.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 993.758\n",
            "INFO:tensorflow:loss = 3.4051574e-08, step = 9501 (0.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 970.717\n",
            "INFO:tensorflow:loss = 2.804497e-08, step = 9601 (0.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 882.718\n",
            "INFO:tensorflow:loss = 1.2506942e-07, step = 9701 (0.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 900.133\n",
            "INFO:tensorflow:loss = 3.9524444e-08, step = 9801 (0.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 971.826\n",
            "INFO:tensorflow:loss = 2.250913e-08, step = 9901 (0.100 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_regression_trained_model_loc/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.14521015e-08.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNNRegression has RMSE of 0.18163767653611632\n",
            "Just using average = 1.0265051171123096 has RMSE of 0.1815919467771684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[trainsize:].values)\n",
        "#print(len(targets[trainsize:].values.reshape(testsize, noutputs)/SCALE_COLLISIONS))\n",
        "\n",
        "#testd = pd.DataFrame.from_records(predictors[trainsize:].values,columns=['day','year','month','da','prcp','fog','rain','snow','hail'])\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_regression_trained_model_loc', hidden_units=[16,7,3], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors[trainsize:].values)))\n",
        "preds = estimator.predict(x=predictors[trainsize:].values)\n",
        "\n",
        "print(preds['scores'])\n",
        "print(targets[trainsize:].values/SCALE_COLLISIONS)\n",
        "\n",
        "testdf = pd.DataFrame.from_dict(data={\n",
        "    'pred': preds['scores'],\n",
        "    'actual': targets[trainsize:].values/SCALE_COLLISIONS\n",
        "})\n",
        "\n",
        "testdf['diff'] = testdf['actual'] - testdf['pred']\n",
        "\n",
        "error=testdf['diff'].mean()*SCALE_COLLISIONS\n",
        "avgcol=targets[trainsize:].mean()\n",
        "print('The trained model has an aproximate error rate of {0} which equates to {1}%'.format(error, round((error/avgcol)*100),1));\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816741d1-e934-4906-a2f7-a4c83c688349",
        "id": "UoS_IOAwf2u5"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0631b53d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_regression_trained_model_loc', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.01800000e+03 2.90000000e+01 4.07349930e+01 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [2.01300000e+03 1.50000000e+01 4.07645465e+01 ... 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01500000e+03 2.20000000e+01 4.08282960e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.01400000e+03 1.00000000e+00 4.05958903e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01500000e+03 1.00000000e+00 4.09046015e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.01600000e+03 1.10000000e+01 4.08534620e+01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_regression_trained_model_loc/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00088096 0.00088096 0.00088096 ... 0.00088096 0.00088096 0.00088096]\n",
            "[0.00086133 0.00086133 0.00086133 ... 0.00086133 0.00086133 0.00086133]\n",
            "The trained model has an aproximate error rate of 0.004093819009957885 which equates to 0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcyH_em_Vg-s"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}